{
    "fullpath": "/Users/takyeonlee/Documents/work/_\u1109\u116e\u110b\u1165\u11b8_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173792.pdf",
    "paper_id": "3173574.3173792",
    "venue": "CHI 18",
    "keywords": [
        "Augmented reality",
        "Virtual reality",
        "3D window management"
    ],
    "paragraph_containing_keyword": "ABSTRACT \nIn augmented and virtual reality (AR and VR), there may be \nmany 3D planar windows with 2D texts, images, and videos \non them. However, managing the position, orientation, and \nscale of such a window in an immersive 3D workspace can \nbe  difficult.  Projective  Windows  strategically  uses  the \nabsolute and apparent sizes of the window at various stages \nof  the  interaction  to  enable  the  grabbing,  moving,  scaling, \nand releasing of the window in one continuous hand gesture. \nWith  it,  the  user  can  quickly  and  intuitively  manage  and \ninteract  with  windows  in  space  without  any  controller \nhardware  or  dedicated  widget.  Through  an  evaluation,  we \ndemonstrate that our technique is performant and preferable, \nand that projective geometry plays an important role in the \ndesign of spatial user interfaces. \nAuthor Keywords \nAugmented reality; virtual reality; 3D window management  \nACM Classification Keywords \nH.5.2.  Information  interfaces  and  presentation  (e.g.,  HCI): \nUser Interfaces-Interaction styles, Windowing systems \nINTRODUCTION \nWe  imagine  an  immersive  future  of  computing  where \nminimal  gear  worn  over  the  eyes  brings  virtual  interactive \nelements to the space around the user. These virtual elements \nmay  merely  enhance  the  user\u2019s  experience  with  the  reality \n(AR) or may completely overwrite it (VR).",
    "paragraph_after_keyword": "Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies \nbear  this  notice  and  the  full  citation  on  the  first  page.  Copyrights  for \ncomponents  of  this  work  owned  by  others  than  ACM  must  be  honored. \nAbstracting  with  credit  is  permitted.  To  copy  otherwise,  or  republish,  to \npost on servers or to redistribute to lists, requires prior specific permission \nand/or a fee. Request permissions from Permissions@acm.org. \n \nCHI 2018, April 21\u201326, 2018, Montreal, QC, Canada \n\u00a9 2018 Association for Computing Machinery. \nACM ISBN 978-1-4503-5620-6/18/04\u2026$15.00 \nhttps://doi.org/10.1145/3173574.3173792",
    "doi": "10.1145/3173574.3173792",
    "sections": [
        {
            "word_count": 403,
            "figure_citations": {
                "1": [
                    "Figure 1)."
                ]
            },
            "section_index": 0,
            "title": "INTRODUCTION"
        },
        {
            "word_count": 61,
            "figure_citations": {},
            "section_index": 1,
            "title": "RELATED WORK"
        },
        {
            "word_count": 1205,
            "figure_citations": {
                "2": [
                    "Figure 2a).",
                    "Figure 2b) the fingers, and completes the selection by making the tips of the fingers touch (Figure 2c)."
                ],
                "3": [
                    "Figure 3a), i.",
                    "Figure 3b) while maintaining the same apparent size, thus appearing the same to the user (Figure 3a, b insets) as in the image plane interaction proposed by Pierce et al."
                ],
                "4": [
                    "Figure 4a), or smaller by taking it away from the face (Figure 4b).",
                    "Figure 4a, b), whereas the window is erected against a horizontal surface, perpendicular to the user\u2019s gaze, to enforce the best viewing angle (Figure 4c)."
                ],
                "5": [
                    "Figure 5a)."
                ]
            },
            "section_index": 2,
            "title": "PROJECTIVE WINDOWS"
        },
        {
            "word_count": 115,
            "figure_citations": {
                "6": [
                    "Figure 6a) for tracking hand movement and posture within the user\u2019s view frustum, the Leap Motion sensor\u2019s front-facing camera for a video feed of the real world, the Unity 3D engine for integrating virtual and physical elements (Figure 6b), and an Intel quad-core i7 3."
                ]
            },
            "section_index": 3,
            "title": "IMPLEMENTATION"
        },
        {
            "word_count": 428,
            "figure_citations": {
                "7": [
                    "Figure 7) to a widget-based ray-casting technique (RC) using the same controller."
                ],
                "5": [
                    "Figure 5b), when the zoom is large, the slope is steep, and when the zoom is small, the slope is gentle, so the user has finer control when the zoom is smaller."
                ]
            },
            "section_index": 4,
            "title": "EVALUATION"
        },
        {
            "word_count": 1487,
            "figure_citations": {
                "8": [
                    "Figure 8h), and others were move widgets (Figure 8f, g).",
                    "Figure 8, 9)."
                ],
                "9": [
                    "Figure 9) to simultaneously compare the discrete levels of various factors in the later statistical analyses: For example, moving a window from A to a target at C requires a linear movement of 4 m and an angular movement of -60\u00b0, whereas A to D requires 2 m and 30\u00b0 respectively.",
                    "Figure 9)."
                ],
                "10": [
                    "Figure 10a).",
                    "Figure 10b)."
                ],
                "11": [
                    "Figure 11a\u25a0, F4,1675 = 45, p < 0.",
                    "Figure 11b\u25a0, F8,1671 = 11, p < 0.",
                    "Figure 11c\u25a0, F8,1671 = 17, p < 0.",
                    "Figure 11d\u25a0, F6,1673 = 54, p < 0.",
                    "Figure 11a\u25a0, F4,1675 = 19, p < 0.",
                    "Figure 11b\u25a0, F8,1671 = 13, p < 0.",
                    "Figure 11d\u25a0, F6,1673 = 23, p < 0."
                ],
                "12": [
                    "Figure 12c\u25a0, F8,1671 = 59, p < 0.",
                    "Figure 12d\u25a0, F6,1673 = 30, p < 0.",
                    "Figure 12a\u25a0, F4,1675 = 8.",
                    "Figure 12b\u25a0, F8,1671 = 5.",
                    "Figure 12c\u25a0, F8,1761 = 15, p < 0.",
                    "Figure 12d\u25a0, F6,1673 = 51, p < 0.",
                    "Figure 12a\u25a0, F4,1675 = 15, p < 0.",
                    "Figure 12b\u25a0, F8,1671 = 17, p < 0."
                ],
                "13": [
                    "Figure 13)."
                ]
            },
            "section_index": 5,
            "title": "EXPERIMENT SESSION"
        },
        {
            "word_count": 247,
            "figure_citations": {},
            "section_index": 6,
            "title": "DISCUSSION"
        },
        {
            "word_count": 743,
            "figure_citations": {
                "11": [
                    "Figure 11d) and with an absolute size ratio of 1 for RC (Figure 11c), irrespective of the technique.",
                    "Figure 11d, 12d)."
                ]
            },
            "section_index": 7,
            "title": "EXPLORATION SESSION"
        },
        {
            "word_count": 415,
            "figure_citations": {
                "14": [
                    "Figure 14) using our implementation (Figure 6).",
                    "Figure 14a), the user can pull picture windows out of a laptop screen and easily scale and place them anywhere on nearby walls for visual reference, just as he or she would sticky notes, but with the ability to freely change the size.",
                    "Figure 14a), the user can pick up a window from a laptop screen and place it on a tablet device to quickly change the input from typing to drawing, without having to swap applications.",
                    "Figure 14b), the user can perform the grab gesture to instantly scan a notebook page and then generate a projective window from it, to scale and place it anywhere for reference.",
                    "Figure 14c), the user can pick up a small movie window from a nearby table, play the preview of the movie by bringing it closer to the face [1], and then start playing the movie by projecting it onto a vertical wall.",
                    "Figure 14d), the user can use the entire unbounded scene as a workspace, even projecting windows across large distances."
                ]
            },
            "section_index": 8,
            "title": "USER SCENARIOS"
        },
        {
            "word_count": 607,
            "figure_citations": {},
            "section_index": 9,
            "title": "REFERENCES"
        }
    ],
    "title": "Projective Windows: Bringing Windows in Space to the Fingertip",
    "authors": "Joon Hyub Lee, Sang-Gyun An, Yongkwan Kim, Seok-Hyung Bae",
    "abstract": "In augmented and virtual reality (AR and VR), there may be many 3D planar windows with 2D texts, images, and videos on them. However, managing the position, orientation, and scale of such a window in an immersive 3D workspace can be difficult. Projective Windows strategically uses the absolute and apparent sizes of the window at various stages of the interaction to enable the grabbing, moving, scaling, and releasing of the window in one continuous hand gesture. With it, the user can quickly and intuitively manage and interact with windows in space without any controller hardware or dedicated widget. Through an evaluation, we demonstrate that our technique is performant and preferable, and that projective geometry plays an important role in the design of spatial user interfaces.",
    "publication": {
        "venue": "CHI '18",
        "venue_full": "Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
        "year": "2018",
        "date": "2018/4/21"
    },
    "version": 3
}