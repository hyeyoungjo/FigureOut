[{"filename":"2662155.2662246","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/2662155.2662246.pdf","paper_id":"2662155.2662246","venue":"chi2020","keywords":["Posture change","Unobtrusive interaction","Virtual Reality"],"paragraph_containing_keyword":"Author Keywords \nPosture change; Unobtrusive interaction; Virtual Reality;","paragraph_after_keyword":"CCS Concepts \n•Human-centered  computing  →  Human  computer  inter-\naction (HCI); Interaction techniques; Virtual reality;","doi":"10.1145/3313831.3376794","sections":[{"word_count":75,"figure_citations":{},"section_index":0,"title":"KAIST"},{"word_count":893,"figure_citations":{},"section_index":1,"title":"INTRODUCTION"},{"word_count":921,"figure_citations":{},"section_index":2,"title":"RELATED WORK"},{"word_count":3666,"figure_citations":{"2":["Figure 2)."],"3":["Figure 3, we placed the target object 2 m from the eyes and six degrees below the eye level based on an ergonomic recommendation [49]."],"5":["Figure 5 shows the positive responses means and standard errors, and the ﬁtted psychometric func- Paper 665 tion."],"7":["Figure 7), we compared the three conditions (unnoticeable slow motion, single swift motion, and no motion) on three distinct manipulations left (1."]},"section_index":3,"title":"PERCEPTION THRESHOLD STUDY"},{"word_count":913,"figure_citations":{"8":["Figure 8)."]},"section_index":4,"title":"RESULTS"},{"word_count":458,"figure_citations":{"9":["Figure 9) to explore the design space of interfaces that induce posture changes."]},"section_index":5,"title":"APPLICATIONS"},{"word_count":636,"figure_citations":{"8":["Figure 8, there is a range that the participants engage continuous neck rotation and beyond that range with abrupt body movements (e."]},"section_index":6,"title":"DISCUSSION"},{"word_count":330,"figure_citations":{},"section_index":7,"title":"CONCLUSION AND FUTURE WORK"},{"word_count":34,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGMENTS"},{"word_count":273,"figure_citations":{},"section_index":9,"title":"REFERENCES"},{"word_count":2098,"figure_citations":{},"section_index":10,"title":"CANADIAN INFORMATION PROCESSING"}],"title":"Body Follows Eye: Unobtrusive Posture Manipulation Through a Dynamic Content Position in Virtual Reality","authors":"Joon Gi Shin, Doheon Kim, Chaehan So, Daniel Saakes","abstract":"While virtual objects are likely to be a part of future interfaces, we lack knowledge of how the dynamic position of virtual objects influences users' posture. In this study, we investigated users' posture change following the unobtrusive and swift motions of a content window in virtual reality (VR). In two perception studies, we estimated the perception threshold on undetectable slow motions and displacement during an eye blink. In a formative study, we compared users' performance, posture change as well as subjective responses on unobtrusive, swift, and no motions. Based on the result, we designed concept applications and explored potential design space of moving virtual content for unobtrusive posture change. With our study, we discuss the interfaces that control users and the initial design guidelines of unobtrusive posture manipulation.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1.  A virtual window moved at slow speed and during an eye blink to unobtrusively manipulate users’ posture.  With two perception threshold \nstudies, We estimated unnoticeable motion speed and displacement per blink.  The formative study showed that unobtrusive motions triggered body \nmovements in a sequence (neck, torso, and chair) without affecting users’ performance. \n","bbox":[53.929,461.80277,566.4318799999998,487.70527],"page":"1"},"2":{"caption":"Figure  2.  We  tested  six  blocks  of  motion:  three  translations  {BTX \n(left, right), BTY (up, down), BTZ (forward, backward)} and three ro-\ntations  {BRX  (up,  down),  BRY  (left,  right),  BRZ  (clockwise,  counter-\nclockwise)}. \n","bbox":[320.83099,566.5145200000001,566.4424799999998,601.3832699999999],"page":"3"},"3":{"caption":"Figure 3.  Based on ergonomics guidelines,  we positioned the virtual window within a comfortable viewing distance and angle.  The 0.73 by 0.73 m \nwindow was placed at 6 degrees below and 2 meters away from the eyes. \n","bbox":[53.64208,583.75202,566.15293,600.68827],"page":"4"},"4":{"caption":"Figure 4.  The plotted result of users’ detection threshold on constant slow translations (up) and rotations (bottom):  The x-axis shows motion speeds, \nand the y-axis shows the probability of the participants’ responses for observed motion direction. The psychometric function shows upper threshold (p \n= 75%) for positive direction and lower threshold (p = 25%) for negative direction. Speeds within the blue area are considered unobtrusive. \n","bbox":[53.929,475.00977,567.1491799999995,500.91227],"page":"5"},"5":{"caption":"Figure 5.  The plotted result of users’ detection threshold on the translations (up) and rotations (down) during a blink:  Displacements within the blue \narea are considered unobtrusive. \n","bbox":[53.929,483.97602,566.1529299999993,500.91227],"page":"6"},"6":{"caption":"Figure 6. We measured the participants’ IPD with a wearable ruler and \nadjusted the HMD (top). We logged the participants’ motion with HMD \nand each tracker on their chest as well as the chair (bottom). \n","bbox":[53.929,498.20077,298.99056,524.10327],"page":"7"},"7":{"caption":"Figure  7.  We  manipulated  participants’  posture  from  upright  sitting \nto  leaning  left  (A),  forward  (B),  and  backward  (C).  For  each  posture \nmanipulation, we compared three conditions: not-moving as the baseline \nbehavior, swift motion, and unobtrusive motion. \n","bbox":[321.094,528.5945200000001,566.15556,563.46327],"page":"7"},"8":{"caption":"Figure 8. We plotted the motion data of a representative participant for each motion: Left translation (top), receding (middle), and rotating up (bottom). \nDuring the left translation and up rotation, unobtrusive motions triggered continuous neck and back movements.  The swift motion triggered abrupt \nand mixed body movements. During the receding motions, the swift motion triggered a couple of chair movements in big steps, whereas the unobtrusive \nmotion triggered several chair movements in smaller steps. \n","bbox":[53.929,490.07651999999996,567.5476799999993,524.9452699999999],"page":"8"},"9":{"caption":"Figure 9.  We designed three concept applications.  The ﬁrst application \nmakes a user rollover from prone to supine and to lying position (top). \nThe second application coerces a user to stand up (left). The third appli-\ncation triggers active sitting (right). \n","bbox":[53.66599,453.47552,300.38530999999995,488.34427],"page":"9"}},"crops":{"1":{"crop_coord":[144.80278027777777,525.7899474999998,1572.1374005555554,837.3361291666666],"bbox":[53.9290009,492.3589935,564.1694642,600.9156189],"page":"1"},"2":{"crop_coord":[886.9277613888887,168.22777638888888,1560.927742777778,521.5611266666666],"bbox":[321.0939941,606.0379944,560.1339874,729.6380005],"page":"3"},"3":{"crop_coord":[144.80278027777777,168.22616583333337,1572.2794766666666,523.4916602777778],"bbox":[53.9290009,605.3430023,564.2206116,729.6385803],"page":"4"},"4":{"crop_coord":[144.80278027777777,168.22803055555548,1572.2794766666666,800.6500158333332],"bbox":[53.9290009,505.5659943,564.2206116,729.637909],"page":"5"},"5":{"crop_coord":[144.80278027777777,168.22803055555548,1572.2794766666666,800.6500158333332],"bbox":[53.9290009,505.5659943,564.2206116,729.637909],"page":"6"},"6":{"crop_coord":[144.80278027777777,168.22777638888888,819.4694436111112,736.22779],"bbox":[53.9290009,528.7579956,293.2089997,729.6380005],"page":"7"},"7":{"crop_coord":[886.9277613888887,168.22777638888888,1560.927742777778,626.894455],"bbox":[321.0939941,568.1179962,560.1339874,729.6380005],"page":"7"},"8":{"crop_coord":[144.80278027777777,168.2272255555557,1572.2794766666666,733.8888719444444],"bbox":[53.9290009,529.6000061,564.2206116,729.6381987999999],"page":"8"},"9":{"crop_coord":[144.80278027777777,168.22777638888888,819.4694436111112,835.5611080555556],"bbox":[53.9290009,492.9980011,293.2089997,729.6380005],"page":"9"}}}},{"filename":"3173574.3173587","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173587.pdf","paper_id":"3173574.3173587","venue":"CHI 18","keywords":["Preschoolers","Rule compliance","Engagement","Parental mediation","Augmented reality (AR)","Smart media usage"],"paragraph_containing_keyword":"ABSTRACT \nThere has been a growing concern over the huge increase in \nuse of smart media by young children. This study explores \nthe  possibility  of  using  augmented-reality(AR)  for  regulat-\ning  preschoolers’  media  usage  behavior.  With  MABLE \n(mobile application for behavioral learning and education), \nparents  can  provide  AR-assisted  feedback  by  changing  fa-\ncial  expressions  and  sound  effects.  When  overlaying  a \nsmart media, which has MABLE running, in front of a QR \nmarker on a puppet, a facial expression is displayed on top \nof the puppet’s face. A two-week long experiment with 36 \nparent-child  pairs  showed  that  compared  to  using  just  the \npuppet,  using  MABLE  showed  higher  amount  of  engage-\nment among preschoolers. For the effectiveness of parental \nmediation  in  terms  of  self-control,  our  data  showed  mixed \nresults. MABLE  had positive effects  in  that the amount of \nrule-compliance  increased  and  problematic  behaviors  de-\ncreased,  whereas  the  level  of  behavioral  dependency  on \nsmart media was not influenced. \nAuthor Keywords \nPreschoolers; rule compliance; engagement; parental medi-\nation; augmented reality (AR); smart media usage. \nACM Classification Keywords \nH.5.1.  Information  interfaces  and  presentation  (e.g.,  HCI): \nMultimedia Information Systems. \nINTRODUCTION \nIn  many  technology-driven  societies,  smart  media  have \ndeeply  integrated  into  young  people’s  everyday  lives  [10, \n14]. Recent studies indicated that on average, Korean chil-","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advantage and that copies bear this notice and the full cita-\ntion on the first page. Copyrights for components of this work owned by others than \nACM must be honored. Abstracting with credit is permitted. To copy otherwise, or \nrepublish, to post on servers or to redistribute to lists, requires prior specific permis-\nsion and/or a fee. Request permissions from permissions@acm.org. \nCHI 2018, April 21–26, 2018, Montréal, QC, Canada. \n© 2018 ACM ISBN 978-1-4503-5620-6/18/04...$15.00. \nhttps://doi.org/10.1145/3173574.3173587","doi":"10.1145/3173574.3173587","sections":[{"word_count":526,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1486,"figure_citations":{"1":["Figure 1)."]},"section_index":1,"title":"RELATED WORK"},{"word_count":712,"figure_citations":{},"section_index":2,"title":"METHODS"},{"word_count":1356,"figure_citations":{},"section_index":3,"title":"RESULTS"},{"word_count":1411,"figure_citations":{},"section_index":4,"title":"DISCUSSIONS"},{"word_count":492,"figure_citations":{},"section_index":5,"title":"LIMITATIONS OF THE STUDY"},{"word_count":18,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":273,"figure_citations":{},"section_index":7,"title":"REFERENCES"},{"word_count":841,"figure_citations":{},"section_index":8,"title":"CONCLUSION"}],"title":"MABLE: Mediating Young Children's Smart Media Usage with Augmented Reality","authors":"Gahgene Gweon, Bugeun Kim, Jinyoung Kim, Kung Jin Lee, Jungwook Rhim, Jueun Choi","abstract":"There has been a growing concern over the huge increase in use of smart media by young children. This study explores the possibility of using augmented-reality(AR) for regulat-ing preschoolers' media usage behavior. With MABLE (mobile application for behavioral learning and education), parents can provide AR-assisted feedback by changing facial expressions and sound effects. When overlaying a smart media, which has MABLE running, in front of a QR marker on a puppet, a facial expression is displayed on top of the puppet's face. A two-week long experiment with 36 parent-child pairs showed that compared to using just the puppet, using MABLE showed higher amount of engage-ment among preschoolers. For the effectiveness of parental mediation in terms of self-control, our data showed mixed results. MABLE had positive effects in that the amount of rule-compliance increased and problematic behaviors de-creased, whereas the level of behavioral dependency on smart media was not influenced.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. Experimental Study Procedure and Measurement \n","bbox":[60.48,409.848,290.96999999999997,418.848],"page":"4"},"2":{"caption":"Figure 2. Facial expressions that provided by MABLE \n","bbox":[333.96,527.568,544.173,536.568],"page":"3"}},"crops":{"1":{"crop_coord":[145,164.9996947222222,825.0000338888889,531.3360763888887],"bbox":[54,602.5190125,295.2000122,730.8001099],"page":"3"},"2":{"crop_coord":[145,536.0000272222223,824.9999491666666,605.6666819444446],"bbox":[54,575.7599945,295.19998169999997,597.2399902],"page":"3"},"3":{"crop_coord":[878.3333333333334,164.9996947222222,1554.999872777778,699.3004183333334],"bbox":[318,542.0518494,557.9999542,730.8001099],"page":"3"},"4":{"crop_coord":[878.3333333333334,689.3333944444445,1554.9999575000002,755.6667244444444],"bbox":[318,521.7599792,557.9999847,542.039978],"page":"3"},"5":{"crop_coord":[146.38888888888889,158.19444444444443,826.4360977777778,1024.5832147222222],"bbox":[54.5,424.9500427,295.71699520000004,733.25],"page":"4"},"6":{"crop_coord":[145,1016.6666327777777,824.9999491666666,1080.3332775000001],"bbox":[54,404.8800201,295.19998169999997,424.2000122],"page":"4"}}}},{"filename":"3173574.3173616","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173616.pdf","paper_id":"3173574.3173616","venue":"CHI 18","keywords":["Virtual reality","Target acquisition","Eyes-free","Proprioception"],"paragraph_containing_keyword":"Author Keywords\nVirtual reality; target acquisition; eyes-free; proprioception.","paragraph_after_keyword":"INTRODUCTION\nVirtual and augmented reality provides great potentials for\nvarious applications, such as gaming [10], education [23],\nmedical training [14] and so on. In VR/AR, people directly\nacquire and manipulate virtual objects as if in the real world.\n†donates the corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for components of this work owned by others than\nACM must be honored. Abstracting with credit is permitted. To copy otherwise,\nor republish, to post on servers or to redistribute to lists, requires prior speciﬁc\npermission and/or a fee. Request permissions from permissions@acm.org.","doi":"10.1145/3173574.3173616","sections":[{"word_count":748,"figure_citations":{"1":["Figure 1 illustrates such an interaction."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":5118,"figure_citations":{"1":["Figure 1, we sampled 60 positions on a circular sphere surface around the user’s body."],"2":["Figure 2 shows the setting of the experiment and the task interface."],"3":["Figure 3 summarizes the result on different horizontal and vertical angle levels.","Figure 3) and participants could see the targets when acquiring them."],"4":["Figure 4 shows, the minimum distance between targets and comfort levels distributed symmetrically on both sides.","Figure 4, we found a negative correlation between the distance and the rating, supported by a Point-Biserial test (correlation = -0."],"5":["Figure 5, the user rotated from the left to the right, and the relative horizontal angle of the target to the user changed from alpha to phi."],"6":["Figure 6 visualized the main results of this experiment."],"7":["Figure 7(c) visualizes the difference of the averaged offsets for twelve horizontal angles of the target positions.","Figure 7(d) shows, the spatial offset of the acquisition increased with the number of rotations users performed before the acquisition.","Figure 7(a), along the horizontal angle of the target position, the horizontal angular offset distributed symmetrically about the point of 0 degrees.","Figure 7(c) shows, similar to the spatial offset, the movement amplitude increased as the target positions changed from 0 degrees in the front to both sides."],"8":["Figure 8 shows the interpolation result of the standard deviations, which designers could refer to arrange target locations for eyes-free target acquisitions."]},"section_index":1,"title":"RELATED WORK"},{"word_count":2009,"figure_citations":{"9":["Figure 9 shows, the targets were rendered as two 3×3 grids of spheres out of view, with one on each side of the user’s body.","Figure 9 shows, the spheres were arranged as two three by three grids on both sides of users, which had equal distance (65cm) to users’ chest."],"11":["Figure 11 visualizes the subjective ratings in all dimensions."]},"section_index":2,"title":"TARGET ACQUISITION"},{"word_count":457,"figure_citations":{},"section_index":3,"title":"DISCUSSION"},{"word_count":197,"figure_citations":{},"section_index":4,"title":"LIMITATION AND FUTURE WORK"},{"word_count":191,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":59,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGEMENT"},{"word_count":1780,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality","authors":"Yukang Yan, Chun Yu, Xiaojuan Ma, Shuai Huang, Hasan Iqbal, Yuanchun Shi","abstract":"Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":2},"fig":{"captions":{"1":{"caption":"Figure 1. An illustrative scenario of eyes-free target acquisition in vir-\ntual reality. A user is doing design and he fetches tools in the interaction\nspace around the body in an eyes-free manner. The FOV (ﬁeld of view)\nsize of his HMD is visualized (110 - 120 degrees).\n","bbox":[321.094,392.50927,565.4860800000001,427.37827],"page":"1"},"2":{"caption":"Figure 2. The setting of the experiment and the task interface in Study1.\nA user moved the controller to acquire the virtual sphere in the 5 × 5\ngrid. Then he turned head to see which sphere he had acquired (the red\none).\n","bbox":[320.807,551.21227,565.55781,586.08127],"page":"3"},"3":{"caption":"Figure 3. The circles summarize the comfort level (color) and the mini-\nmum distance between targets (radius) for different target positions (cen-\nters) when users acquired them.\n","bbox":[53.929,602.94827,298.3210800000001,628.85127],"page":"4"},"4":{"caption":"Figure 4. The averaged distances between targets and comfort ratings of\nthe target positions with different horizontal angles.\n","bbox":[321.094,83.91627000000001,564.1630600000001,100.85227],"page":"4"},"5":{"caption":"Figure 5. The concept of the experiment settings. The furniture indicates\nthe virtual surroundings of the participants. The red spheres indicate\nthe twelve directions that the participants rotate to, and the green sphere\nindicates the positioned target.\n","bbox":[321.094,555.9362699999999,564.1630600000001,590.80527],"page":"5"},"6":{"caption":"Figure 6. Summary of the main results of this experiment. The centers of\nthe circles are the averaged positions of twelve acquisitions of 24 users\nand radiuses are the standard deviations. The blue lines visualize the\noffsets of the average positions from the target’s actual positions. All\ncoordinates and lengths are converted to angles in degrees.\n","bbox":[53.929,566.2732699999999,296.9980600000002,610.10827],"page":"6"},"7":{"caption":"Figure 7. (a) The horizontal angular offset of the acquisitions at differ-\nent horizontal angles; (b) The vertical angular offset of the acquisitions\nat different horizontal angles; (c) The spatial offset and head movement\namplitudes of the acquisitions at different horizontal angles; (d) The spa-\ntial offset of the acquisitions after different times of rotations.\n","bbox":[321.094,500.27826999999996,565.4860800000001,544.1142699999999],"page":"6"},"8":{"caption":"Figure 8. The interpolation of the standard deviations of the target ac-\nquisition points on the whole surface, visualized into a heat map.\n","bbox":[321.094,239.23827,565.48608,256.17427],"page":"7"},"9":{"caption":"Figure 9. The concept of the experiment setting. Targets were located on\nboth sides of users’ body. A shortcut of the target layout was visualized\nin the front, as well as the character for the second task.\n","bbox":[321.094,561.74627,564.1630600000002,587.64927],"page":"8"},"10":{"caption":"Figure 10. The speed (average acquisition time) and accuracy (rate) re-\nsults of the acquisitions in different conditions. The error bars represent\nthe standard deviations.\n","bbox":[53.929,565.7582699999999,298.3210800000001,591.66127],"page":"9"},"11":{"caption":"Figure 11. User’s subjective ratings for the experience using two ap-\nproaches to acquire targets. The error bars represent the standard devi-\nations.\n","bbox":[321.094,572.73727,565.4860800000001,598.63927],"page":"9"}},"crops":{"1":{"crop_coord":[954.447225,626.7861225747222,1504.5852053172223,1004.9083200000001],"bbox":[345.401001,432.0330048,539.8506739142,564.5569958731],"page":"1"},"2":{"crop_coord":[954.447225,168.22785444444446,1504.6215916666667,564.0666877777779],"bbox":[345.401001,590.7359924,539.863773,729.6379724],"page":"3"},"3":{"crop_coord":[144.80278027777777,168.22710316000033,829.9960486466667,445.261120277778],"bbox":[53.9290009,633.5059967,296.9985775128,729.6382428623999],"page":"4"},"4":{"crop_coord":[988.2083130555557,1549.8150891858334,1470.8393325141667,1911.9249894444445],"bbox":[357.5549927,105.5070038,527.7021597051,232.26656789310002],"page":"4"},"5":{"crop_coord":[937.5666808333334,168.2268231755556,1521.515110122222,550.9444258333334],"bbox":[339.3240051,595.4600067,545.945439644,729.6383436568],"page":"5"},"6":{"crop_coord":[144.80278027777777,168.22795080722247,829.987842018889,497.32499861111114],"bbox":[53.9290009,614.7630005,296.9956231268,729.6379377093999],"page":"6"},"7":{"crop_coord":[886.9277613888887,168.22698006305566,1572.129851485,680.6416744444447],"bbox":[321.0939941,548.7689972,564.1667465346,729.6382871773],"page":"6"},"8":{"crop_coord":[920.6916469444444,1089.5517629561111,1538.3882956925,1480.4750144444445],"bbox":[333.2489929,260.8289948,552.0197864493,397.9613653358],"page":"7"},"9":{"crop_coord":[988.2083130555557,168.2273885044446,1470.839297162222,559.7111086111112],"bbox":[357.5549927,592.3040009,527.7021469783999,729.6381401384],"page":"8"},"10":{"crop_coord":[144.80278027777777,168.2274746491665,829.9894871794444,548.5666824999998],"bbox":[53.9290009,596.3159943,296.99621538459996,729.6381091263],"page":"9"},"11":{"crop_coord":[900.4305605555555,168.22881840944427,1558.6219034033334,529.1833158333335],"bbox":[325.9550018,603.2940063,559.3038852252,729.6376253726],"page":"9"}}}},{"filename":"3173574.3173620","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173620.pdf","paper_id":"3173574.3173620","venue":"CHI 18","keywords":["Mixed reality","Remote collaboration","Augmented reality","Virtual reality","Remote embodiment","Avatar","Redirected","Gaze","Gesture","Awareness"],"doi":"10.1145/3173574.3173620","paragraph_containing_keyword":"Author Keywords \nMixed  reality;  remote  collaboration;  augmented  reality; \nvirtual  reality;  remote  embodiment;  avatar;  redirected,  gaze, \ngesture; awareness.  \nACM Classification Keywords \nH.5.1.  Information  interfaces  and  presentation  (e.g.,  HCI): \nMultimedia  Information  Systems—Artificial,  augmented, \nand virtual realities. \nINTRODUCTION \nThis paper explores how adaptive avatars can improve Mixed \nReality (MR) remote collaboration. MR is a technology that \nseamlessly bridges real and virtual worlds. In the near future, \nMR  collaboration  between  users  in  the  real  world  using \nAugmented Reality (AR) and remote users in Virtual Reality \n(VR)  may  be  commonplace.  A  MR  remote  collaboration \ninvolves a local AR user sharing their real-world information \nwith a remote VR user, such as the reconstructed task space \nnecessary for spatial awareness and understanding. Like any \nremote  collaborative  systems,  one  of  the  main  goals  of  MR \ncollaboration is to enable people to feel  co-present.  AR and","sections":[{"word_count":502,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":3685,"figure_citations":{"2":["Figure 2a and 2b).","Figure 2c and 2d).","Figure 2e and 2f)."],"3":["Figure 3 shows an example of this with an application named “Snow Dome,” a remote MR collaboration application that demonstrates how AR and VR collaboration can be enhanced with multi-scale interaction.","Figure 3a.","Figure 3b."]},"section_index":1,"title":"RELATED WORK"},{"word_count":3526,"figure_citations":{"3":["Figure 3d and 3e show the original AR space and the result of the reconstruction showed to the VR user."],"4":["Figure 4).","Figure 4).","Figure 4)."]},"section_index":2,"title":"USER STUDY"},{"word_count":610,"figure_citations":{},"section_index":3,"title":"DISCUSSION"},{"word_count":257,"figure_citations":{},"section_index":4,"title":"CONCLUSION AND FUTURE WORK"},{"word_count":23,"figure_citations":{},"section_index":5,"title":"ACKNOWLEDGMENTS"},{"word_count":1623,"figure_citations":{},"section_index":6,"title":"REFERENCES"}],"title":"Mini-Me: An Adaptive Avatar for Mixed Reality Remote Collaboration","authors":"Thammathip Piumsomboon, Gun A. Lee, Jonathon D. Hart, Barrett Ens, Robert W. Lindeman, Bruce H. Thomas, Mark Billinghurst","abstract":"We present Mini-Me, an adaptive avatar for enhancing Mixed Reality (MR) remote collaboration between a local Augmented Reality (AR) user and a remote Virtual Reality (VR) user. The Mini-Me avatar represents the VR user's gaze direction and body gestures while it transforms in size and orientation to stay within the AR user's field of view. A user study was conducted to evaluate Mini-Me in two collaborative scenarios: an asymmetric remote expert in VR assisting a local worker in AR, and a symmetric collaboration in urban planning. We found that the presence of the Mini-Me significantly improved Social Presence and the overall experience of MR collaboration.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. Mini-Me Concept: (a) MR view through the HoloLens of a remote VR user’s Mini-Me avatar standing on a table and a \nlife-size avatar standing on the right, both avatars gaze toward the same place and point at the same target with gaze and pointing \nof the Mini-Me avatar redirected, (b) A male (VR user) and a female (AR user) avatars in VR, (c) Design features of the Mini-Me \navatar, (d) The local AR user moving a tea box, (e) + (f) The remote VR user is pointing at a box and the Mini-Me in the AR user’s \nFOV has its gaze and pointing gesture redirected to the same box (g) A remote VR user is pointing. \n","bbox":[59.4,327.746,554.962,378.166],"page":"1"},"2":{"caption":"Figure 2: The AR user views the Mini-Me from two different perspectives showing Mini-Me consistently gazing and pointing at the \nsame location: a) in front of the whiteboard, b) side view of the whiteboard, c) As the AR user gazes at the remote VR user’s life-\nsize avatar, the Mini-Me moves toward this avatar and d) fuses with it and disappears, e) AR user can gaze at the Mini-Me and \nperform an air-tap to pin it in place or f) Tap again to unpin it from that location. \n","bbox":[54,601.4286400000001,560.941,641.716],"page":"5"},"3":{"caption":"Figure  3:  a)  Scaled  down  VR  user’s  perspective  seeing  the  AR  user  as  a  giant,  b)  VR  user  shrunk  down  interacting  inside  the \nminiature dome, c) VR user is a giant looking down at the AR reconstructed space, d) The real experimental space for the AR user, \nand e) its virtual reconstruction for the VR user \n","bbox":[54.24,614.62864,561.517,644.476],"page":"6"},"4":{"caption":"Figure 4. Baseline condition illustrating how the remote VR user’s avatar look through a HoloLens. (a-d) Tea Party task (a) VR \nuser asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar’s hand to a correct tea box with \na virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of \nthe box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user \nlooks at the VR user’s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h) \nAR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here. \n","bbox":[54,444.55863999999997,561.421,505.606],"page":"7"},"5":{"caption":"Figure 5. Task completion time. \n","bbox":[377.95000000000005,507.8860000000007,499.153,516.8860000000008],"page":"8"}},"crops":{"1":{"crop_coord":[161.66666666666666,555.3889041666669,1538.2777658333332,1134.0000066666669],"bbox":[60,385.5599976,551.9799957,590.2599945],"page":"1"},"2":{"crop_coord":[144.58335027777778,164.9999913888888,1556.388888888889,427.22222222222223],"bbox":[53.8500061,640,558.5,730.8000031],"page":"5"},"3":{"crop_coord":[145.69444444444446,165.13888888888883,1558.249995,390.0000169444444],"bbox":[54.25,653.3999939,559.1699982,730.75],"page":"6"},"4":{"crop_coord":[145,164.99999166666692,1562.0833419444446,794.305538611111],"bbox":[54,507.8500061,560.5500031,730.800003],"page":"7"},"5":{"crop_coord":[989.1666497222224,405.2500066666667,1440.4166666666667,746.9166563888891],"bbox":[357.8999939,524.9100037,516.75,644.3099976],"page":"8"},"6":{"crop_coord":[153.33333333333334,179.70555555555555,1561.681403145933,939.7222222222222],"bbox":[57,455.5,560.4053051325359,725.506],"page":"9"}}}},{"filename":"3173574.3173628","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173628.pdf","paper_id":"3173574.3173628","venue":"CHI 18","keywords":["Nomadic virtual reality","Asymmetric virtual reality","Multi-user virtual reality","Co-located multiplayer"],"doi":"10.1145/3173574.3173628","paragraph_containing_keyword":"Author Keywords\nNomadic virtual reality; asymmetric\nvirtual reality; multi-user virtual reality; co-located multiplayer","paragraph_after_keyword":"INTRODUCTION\nMobile VR (Virtual Reality) HMDs (Head-mounted Displays)\nare currently mostly based on smartphones and a case outfitted\nwith lenses (e.g. Samsung GearVR, Google Daydream). A recent\ndevelopment focuses on mobile VR HMDs which are not based\non smartphones but offer an untethered headset with embedded\nhardware, inside-out tracking and some form of input capabilities\n(e.g. Intel Alloy). Both these device types enable the interaction\nscenario of Nomadic VR [18, 29], where users can immerse them-\nselves inside a virtual world wherever and whenever they wish.\nThis nomadic interaction scenario comes with several challenges\nsuch as the unknown and uninstrumented environment [18].\nSince current mobile VR HMDs are designed exclusively for the\nwearing user (HMD User), every other person in this environment","sections":[{"word_count":1075,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":783,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1714,"figure_citations":{"4":["Figure 4 shows the interaction/engagement gradient starting from the most engaged (touch) to the least engaged (observing).","Figure 4 a,b).","Figure 4 c,d) enables additional applications without the need to battle Paper 54 exclusion and isolation (e."]},"section_index":2,"title":"FACEDISPLAY"},{"word_count":1312,"figure_citations":{"7":["Figure 7 a)."]},"section_index":3,"title":"APPLICATIONS"},{"word_count":829,"figure_citations":{"8":["Figure 8 summarizes the scores of the GEQ, SUS and SAM and Figure 9 shows responses for our own questions."]},"section_index":4,"title":"EVALUATION"},{"word_count":54,"figure_citations":{},"section_index":5,"title":"HMD"},{"word_count":1148,"figure_citations":{"10":["Figure 10)."]},"section_index":6,"title":"NHMD"},{"word_count":999,"figure_citations":{},"section_index":7,"title":"DISCUSSION"},{"word_count":334,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":44,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGMENTS"},{"word_count":1732,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"FaceDisplay: Towards Asymmetric Multi-User Interaction for Nomadic Virtual Reality","authors":"Jan Gugenheimer, Evgeny Stemasov, Harpreet Sareen, Enrico Rukzio","abstract":"Mobile VR HMDs enable scenarios where they are being used in public, excluding all the people in the surrounding (Non-HMD Users) and reducing them to be sole bystanders. We present FaceDisplay, a modified VR HMD consisting of three touch sensitive displays and a depth camera attached to its back. People in the surrounding can perceive the virtual world through the displays and interact with the HMD user via touch or gestures. To further explore the design space of FaceDisplay, we implemented three applications (FruitSlicer, SpaceFace and Conductor) each presenting different sets of aspects of the asymmetric co-located interaction (e.g. gestures vs touch). We conducted an exploratory user study (n=16), observing pairs of people experiencing two of the applications and showing a high level of enjoyment and social interaction with and without an HMD. Based on the findings we derive design considerations for asymmetric co-located VR applications and argue that VR HMDs are currently designed having only the HMD user in mind but should also include Non-HMD Users.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":2},"fig":{"captions":{"1":{"caption":"Figure 1. FaceDisplay is a modified VR HMD consisting of three touch sensitive displays and a depth camera attached to its back (a-c). This allows people in\nthe surrounding to perceive the virtual world through the displays and interact with the HMD user either through touch (e) or gestures (d).\n","bbox":[53.933,380.65327,564.1564600000006,397.58927],"page":"1"},"2":{"caption":"Figure 2. The hardware prototype of FaceDisplay, consisting of three\ntouchscreens and a Leap Motion depth camera attached to the back and\nthe sides of an Oculus Rift DK2\n","bbox":[321.098,378.48427,564.15909,404.38626999999997],"page":"3"},"3":{"caption":"Figure 3. The technical setup used to reduce the weight of the FaceDisplay\nprototype, using a key retractor (a) and a pair of springs (b).\n","bbox":[53.933,411.85527,297.27304,428.79227],"page":"4"},"4":{"caption":"Figure 4. Interaction Gradient for FaceDisplay. Starting from the most engaged a: touch to b:gesture, c: external device and d: observing.\n","bbox":[87.28,572.0562699999999,530.8094899999999,580.02627],"page":"5"},"5":{"caption":"Figure 5. The FruitSlicer application with its outside view (a), inside view\n(b), interaction concepts (c) and visualization metaphor (d).\n","bbox":[320.83500000000004,159.74727,564.4460099999999,176.68427],"page":"5"},"6":{"caption":"Figure 6. The SpaceFace application and its outside view (a), inside view (b)\ninteraction and visualization concept (c) and physical interaction scenario\n(d).\n","bbox":[53.67,535.2832699999999,297.52808,561.1862699999999],"page":"6"},"7":{"caption":"Figure 7. The Conductor application showing its outside view (a), inside\nview (b), hand tracking region (c) and interaction scenario (d).\n","bbox":[320.899,544.2492699999999,564.15909,561.1862699999999],"page":"6"},"8":{"caption":"Figure 8. The distribution of our data from (a) the GEQ In-Game Module, GEQ Social Module, the SUS and (b) the SAM questionnaire. All bar charts\nshowing the mean with standard deviation.\n","bbox":[53.933,580.43827,564.1564600000007,597.37427],"page":"8"},"9":{"caption":"Figure 9. Boxplots of our own questions on discomfort \"I felt uncomfortable\ntouching/being touched/gesturing/being gestured at\", understanding \"I was\nalways able to understand the current state of the game\" and agency \"I was\nalways able to influence the outcome of the game\"\n","bbox":[53.932999999999964,410.73027,296.99771,445.59927],"page":"8"},"10":{"caption":"Figure 10. A variety of physical interaction poses participants used during the study emphasizing the vast possibilities of physical interaction arising from\nSpaceFace: (a) The Kraken: The Non-HMD User abused his power and wraps around the HMD User to restrict his motions. (b) The Leg-press: the HMD User\nutilizes his legs to either find or push the Non-HMD User away. (d) The Hedgehog: the HMD User rolls in like a hedgehog to hide from the attacks.\n","bbox":[53.933,507.57527,564.1590100000001,533.47727],"page":"9"}},"crops":{"1":{"crop_coord":[144.80278027777777,593.5730063888888,1572.4850041666668,1087.655503888889],"bbox":[53.9290009,402.2440186,564.2946015,576.5137177],"page":"1"},"2":{"crop_coord":[886.9277613888887,509.4349416666669,1572.1077727777777,1068.7749566666666],"bbox":[321.0939941,409.0410156,564.1587982,606.803421],"page":"3"},"3":{"crop_coord":[144.80278027777777,535.9979502777777,829.9222227777776,1000.9805805555554],"bbox":[53.9290009,433.446991,296.97200019999997,597.2407379],"page":"4"},"4":{"crop_coord":[178.26666083333333,168.24918111111123,1538.561918333333,575.3499688888888],"bbox":[65.9759979,586.6740112,552.0822906,729.6302948],"page":"5"},"5":{"crop_coord":[886.9277613888887,1244.4436477777779,1572.104212222222,1701.2805430555557],"bbox":[321.0939941,181.3390045,564.1575164,342.20028679999996],"page":"5"},"6":{"crop_coord":[144.80278027777777,168.23680472222236,829.9222227777776,633.2194350000001],"bbox":[53.9290009,565.8410034,296.97200019999997,729.6347503],"page":"6"},"7":{"crop_coord":[886.9277613888887,168.23680472222236,1572.0472038888888,633.2194350000001],"bbox":[321.0939941,565.8410034,564.1369933999999,729.6347503],"page":"6"},"8":{"crop_coord":[140.21962317,168.359756204264,1570.4184134338886,535.7223714264861],"bbox":[52.2790643412,600.939946286465,563.5506288362,729.590487766465],"page":"8"},"9":{"crop_coord":[148.68518826388888,636.2213256493058,831.8272129166668,956.5877720729166],"bbox":[55.326667775,449.42840205375,297.65779665,561.1603227662499],"page":"8"},"10":{"crop_coord":[144.80278027777777,168.27045888888898,1571.7638566666665,710.188836111111],"bbox":[53.9290009,538.132019,564.0349884,729.6226348],"page":"9"}}}},{"filename":"3173574.3173638","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173638.pdf","paper_id":"3173574.3173638","venue":"CHI 18","keywords":["Virtual Reality","Stereoscopic","360","Subtitles","Video interface"],"paragraph_containing_keyword":"Author Keywords\nVirtual Reality; stereoscopic; 360; subtitles; video interface.","paragraph_after_keyword":"INTRODUCTION\nThe recent consumer availability of virtual reality (VR) head-\nmounted displays (HMDs) has created considerable interest\nin capturing and sharing 360° video, particularly stereoscopic\n360° video. These videos are creating new media for enter-\ntainment [29], news and documentaries, real estate, data visu-\nalization [8], virtual tours [14], and free-viewpoint video [20].\nStereoscopic 360° video, which provides a much greater sense\nof immersion than monoscopic video, is becoming increas-\ningly available due to many recent advances in camera tech-\nnology [2, 30], and new processing tools [22] and editing\ninterfaces [32, 33] are being developed for these videos.\nHowever, the extra sense of depth in stereoscopic VR video\ncan be problematic to users of VR video applications. Com-\nmon user interface (UI) widgets like video navigation, sub-\ntitles, annotations, and tool palettes are often rendered on\ntop of the video. Each widget must be rendered at a speciﬁc\nperceived depth, which is controlled by varying the disparity\n(difference in horizontal position) between the left and right\neye views. However, the perceived depth of the stereoscopic","doi":"10.1145/3173574.3173638","sections":[{"word_count":665,"figure_citations":{"1":["Figure 1), there is a conflict between the stereopsis depth cue and the occlusion depth cue."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":447,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":382,"figure_citations":{"1":["Figure 1 Left)."]},"section_index":2,"title":"DEPTH CONFLICTS IN VR VIDEO INTERFACES"},{"word_count":118,"figure_citations":{},"section_index":3,"title":"TECHNIQUES CONSIDERED"},{"word_count":10,"figure_citations":{},"section_index":4,"title":"A"},{"word_count":2,"figure_citations":{},"section_index":5,"title":"B"},{"word_count":1400,"figure_citations":{"1":["Figure 1, the current scene is still not geometrically consistent."],"2":["Figure 2: Overview of Dynamic Depth.","Figure 2A).","Figure 2C)."],"3":["Figure 3: Halo Blur blurs the video content around the UI.","Figure 3)."],"4":["Figure 4: Illustration of the Subtitles task (Left) and the Search task (Right)."]},"section_index":6,"title":"C"},{"word_count":2558,"figure_citations":{"4":["Figure 4 Left), participants were asked to watch videos with subtitles.","Figure 4 Right), participants were asked to search for a target scene as quickly and accurately as possible."],"5":["Figure 5 summarizes the ratings in both tasks.","Figure 5: (Left) Summary of participants’ ratings to the subjective questionnaire in both tasks.","Figure 5)."]},"section_index":7,"title":"USER STUDY"},{"word_count":173,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":39,"figure_citations":{"1":["Figure 1, 2, and 3 use images from YouTube users Kevin Kunze under a Creative Commons license."]},"section_index":9,"title":"ACKNOWLEDGEMENT"},{"word_count":1462,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"Depth Conflict Reduction for Stereo VR Video Interfaces","authors":"Cuong Nguyen, Stephen DiVerdi, Aaron Hertzmann, Feng Liu","abstract":"Applications for viewing and editing 360° video often render user interface (UI) elements on top of the video. For stereoscopic video, in which the perceived depth varies over the image, the perceived depth of the video can conflict with that of the UI elements, creating discomfort and making it hard to shift focus. To address this problem, we explore two new techniques that adjust the UI rendering based on the video content. The first technique dynamically adjusts the perceived depth of the UI to avoid depth conflict, and the second blurs the video in a halo around the UI. We conduct a user study to assess the effectiveness of these techniques in two stereoscopic VR video tasks: video watching with subtitles, and video search.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":2},"fig":{"captions":{"1":{"caption":"Figure 1: Depth conﬂict illustration. An interface (the video\nplayer) overlays a video object (the actress) but is actually\nbehind her in depth. Left: The resulting graphics are uncom-\nfortable to view in VR (e.g., the areas behind the text 00:01 in\nthe insets are different between the left and right views). View-\ners may also experience difﬁculty changing focus between the\ninterface and the video. Right: illustration of the conﬂicting\ndepth cues perceived by the same viewer in the same view.\n© Kevin Kunze\n","bbox":[320.337,426.483992,565.92316792,524.117992],"page":"1"},"2":{"caption":"Figure 2: Overview of Dynamic Depth. (A) We pre-process the input video to ﬁnd feature points and left/right disparities (e.g.,\ngreen lines on the actress) (B) Features points are mapped to the VR view and shown as the green dots (only for illustrative\npurposes). Dynamic Depth estimates the perceived depth of the video based on these points. It detects when depth conﬂict occurs\nby comparing the depths between the UI and the video. (C) Dynamic Depth moves the UI closer to the viewer to reduce depth\nconﬂicts. Notice in the insets that the areas around the interface’s corner are more geometrically consistent compared to the same\nscene in Figure 1. © Kevin Kunze\n","bbox":[53.929,533.4749919999999,565.4271484359998,598.232992],"page":"3"},"3":{"caption":"Figure 3: Halo Blur blurs the video content around the UI.\nInsets: compared to the same scene in Figure 1, the current\nscene is still not geometrically consistent. However, the blur\neffects mask high-frequency spatial information in the video\nand make the details from the UI clearer. © Kevin Kunze\n","bbox":[53.929,512.789992,298.7480056600001,566.588992],"page":"4"},"4":{"caption":"Figure 4: Illustration of the Subtitles task (Left) and the Search\ntask (Right). Please refer to the video demo for a better assess-\nment. © Kevin Kunze\n","bbox":[321.094,564.7349919999999,565.83190684,596.615992],"page":"4"},"5":{"caption":"Figure 5: (Left) Summary of participants’ ratings to the subjective questionnaire in both tasks. (Middle and Right) Task time and\ntask error summary of the Search task.\n","bbox":[53.929,438.330992,564.1837261839999,459.252992],"page":"6"}},"crops":{"1":{"crop_coord":[886.9277613888887,454.3305799999999,1576.9277699999998,721.8305713888888],"bbox":[321.0939941,533.9409943,565.8939972,626.6409912],"page":"1"},"2":{"crop_coord":[144.80278027777777,168.22663270416666,1606.3052809694443,515.955556111111],"bbox":[53.9290009,608.0559998,576.469901149,729.6384122265],"page":"3"},"3":{"crop_coord":[148.64998722222222,168.23053138888892,826.1499958333333,603.8555483333332],"bbox":[55.3139954,576.4120026,295.6139985,729.6370087],"page":"4"},"4":{"crop_coord":[886.9277613888887,168.22647944444452,1583.7025958333334,520.4472350000001],"bbox":[321.0939941,606.4389954,568.3329345,729.6384674],"page":"4"},"5":{"crop_coord":[144.80278027777777,509.17124521333335,1619.0076576811111,902.0111],"bbox":[53.9290009,469.076004,581.0427567652,606.8983517232],"page":"6"}}}},{"filename":"3173574.3173673","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173673.pdf","paper_id":"3173574.3173673","venue":"CHI 18","keywords":["Virtual Reality","Amplified movement","Object manipulation","Translational gain","Redirected walking"],"paragraph_containing_keyword":"ABSTRACT \nRoom-scale Virtual Reality (VR) has become an affordable \nconsumer reality, with applications ranging from entertain-\nment  to  productivity.  However,  the  limited  physical  space \navailable for room-scale VR in the typical home or office en-\nvironment poses a significant problem. To solve this, physi-\ncal  spaces  can  be  extended  by  amplifying  the  mapping  of \nphysical to virtual movement (translational gain). Although \namplified movement has been used since the earliest days of \nVR, little is known about how it influences reach-based in-\nteractions with virtual objects, now a standard feature of con-\nsumer VR. Consequently, this paper explores the picking and \nplacing of virtual objects in VR for the first time, with trans-\nlational  gains  of  between  1x  (a  one-to-one  mapping  of  a \n3.5m*3.5m  virtual  space to the same  sized physical space) \nand 3x (10.5m*10.5m virtual mapped to 3.5m*3.5m physi-\ncal). Results show that reaching accuracy is maintained for \nup to 2x gain, however going beyond this diminishes accu-\nracy and increases simulator sickness and  perceived work-\nload. We suggest gain levels of 1.5x to 1.75x can be utilized \nwithout  compromising  the  usability  of  a  VR  task,  signifi-\ncantly expanding the bounds of interactive room-scale VR. \nAuthor Keywords \nVirtual  Reality; amplified  movement;  object  manipulation; \ntranslational gain; redirected walking. \nACM Classification Keywords \nH.5.m. Information interfaces and presentation (e.g., HCI): \nMiscellaneous; \nINTRODUCTION \nVirtual environments (VEs)  provide limitless  opportunities \nconstrained  only  by  the  creator’s  imagination.  In  practice, \nVEs that are intended to be explored by real-world walking \nare  severely  limited  by  the  physical  space  available  in  the \noffices  and  homes  [41]  where  commercial  Virtual  Reality \nPermission to make digital or hard copies of all or part of this work for personal \nor classroom  use  is granted  without  fee provided  that  copies  are not made or \ndistributed for profit or commercial advantage and that copies bear this notice \nand the full citation on the first page. Copyrights for components of this work \nowned by others than the author(s) must be honored. Abstracting with credit is \npermitted. To copy otherwise, or republish, to post on servers or to redistribute \nto lists, requires prior specific permission and/or a fee. Request permissions from \npermissions@acm.org. \nCHI 2018, April 21–26, 2018, Montreal, QC, Canada \nCopyright is held by the owner/author(s). Publication rights licensed to ACM. \nACM. ISBN 978-1-4503-5620-6/18/04…$15.00.  \nDOI: https://doi.org/10.1145/3173574.3173673","doi":"10.1145/3173574.3173673","paragraph_after_keyword":"(VR) headsets are being used. A 1:1 mapping of physical to \nvirtual movement requires VEs to fit the constraints of the \nphysical space available. This significantly limits the possi-\nbilities  for  VE  design  and  novel  VR  applications,  reminds \nusers of the physical constraints of their setup and breaks im-\nmersion when physical boundaries interrupt use. \nA number of approaches have been used to overcome physi-\ncal space constraints. Amplified movements allow users to \ntraverse larger virtual spaces by accelerating or amplifying \nthe mapping of physical to virtual movement [22,43,44,57]. \nThis is commonly referred to as translational gain, a multi-\nplier on the x/z position of the VR user, where walking 1m in \nphysical  space can lead  to  walking  2m  (2.0x  gain)  to  50m \n(50.0x  gain)  in  virtual  space.  This  technique  requires  no \ntraining and can enable larger singular virtual rooms whose \nextents  are  fully  reachable  and  spatially  consistent,  with \nevery point in VR mapped to a point in real-world space.  \nTranslational gain has frequently been overlooked in favour \nof both redirected walking and locomotion interfaces. Redi-\nrected walking uses combinations of translational [21], rota-\ntional [27] and curvature [28] gain alongside other perceptual \ntricks to portray infinitely traversable virtual spaces. How-\never, these techniques have significant limitations regarding \nphysical  requirements  (minimum  6m2  [45]  but  realistically \nup room-scale to 22m2 [19]), and they cannot support single-\nroom  virtual  spaces  where  every  visible  corner  is  consist-\nently  reachable  and  fully  explorable.  Virtual  locomotion \ntechniques [54] such as teleportation, walking in-place and \nflight  also  allow  for  infinite  exploration,  but  they  require \ntraining, potentially increase simulator sickness [25] and un-\ndermine the spatial understanding of the virtual space [58]. \nOmni-directional treadmills [12] allow for effortful physical \ntraversal  whilst  remaining stationary  in  reality,  but  require \ncostly additional hardware, and may restrain movement and \ninteraction.  \nConsequently,  this  paper  examines  how  translational  gain \ncan be applied to interactive room-scale VR for the first time. \nTranslational gain has the greatest potential to impact every-\nday  VR  users  across  a  range  of  physical  environments.  It \ncosts  nothing,  is  trivial  to  apply,  and  allows  for  singular, \ncomplex, fully explorable spaces (e.g. small shop, kitchen) \nto be approximated within the limited physical bounds of a \n* These authors contributed equally to this work.","sections":[{"word_count":827,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1556,"figure_citations":{},"section_index":1,"title":"RELATED RESEARCH"},{"word_count":1987,"figure_citations":{"1":["Figure 1) consisted of a warehouse with floor, walls, ceiling lights and structural pillars."],"2":["Figure 2).","Figure 2)."],"3":["Figure 3)."],"4":["Figure 4)."]},"section_index":2,"title":"TRANSLATIONAL GAIN"},{"word_count":2062,"figure_citations":{"6":["Figure 6, where the fixed position of the target evident in Figure 6."],"8":["Figure 8) and on all the TLX subscales, with differences predominantly arising between {1."]},"section_index":3,"title":"RESULTS"},{"word_count":1396,"figure_citations":{},"section_index":4,"title":"DISCUSSION"},{"word_count":135,"figure_citations":{},"section_index":5,"title":"CONCLUSIONS"},{"word_count":15,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":1852,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Object Manipulation in Virtual Reality Under Increasing Levels of Translational Gain","authors":"Graham Wilson, Mark McGill, Matthew Jamieson, Julie R. Williamson, Stephen A. Brewster","abstract":"Room-scale Virtual Reality (VR) has become an affordable consumer reality, with applications ranging from entertainment to productivity. However, the limited physical space available for room-scale VR in the typical home or office environment poses a significant problem. To solve this, physical spaces can be extended by amplifying the mapping of physical to virtual movement (translational gain). Although amplified movement has been used since the earliest days of VR, little is known about how it influences reach-based interactions with virtual objects, now a standard feature of consumer VR. Consequently, this paper explores the picking and placing of virtual objects in VR for the first time, with translational gains of between 1x (a one-to-one mapping of a 3.5m*3.5m virtual space to the same sized physical space) and 3x (10.5m*10.5m virtual mapped to 3.5m*3.5m physical). Results show that reaching accuracy is maintained for up to 2x gain, however going beyond this diminishes accuracy and increases simulator sickness and perceived workload. We suggest gain levels of 1.5x to 1.75x can be utilized without compromising the usability of a VR task, significantly expanding the bounds of interactive room-scale VR.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1: Left: Virtual environment with 1.0x (no) gain. \nRight: Virtual environment with 3.0x gain. \n","bbox":[63.06000000000003,114.26784,272.4825600000001,133.10784],"page":"4"},"2":{"caption":"Figure 2: Left: Highlighted wall for next target. Right: Table. \n","bbox":[315.29999999999995,639.9878400000005,544.88556,648.7478400000005],"page":"4"},"3":{"caption":"Figure 3: Left: User reaching to select target cube, with inset \nreal-world environment. Right: Alignment cube. \n","bbox":[315.42,112.70783999999999,542.93808,131.54783999999998],"page":"4"},"4":{"caption":"Figure 4: The three walking routes. Left: Straight. Middle: \nZig-zag. Right: Curve. Obstacles were scaled based on gain. \n","bbox":[315.4199999999999,657.3878400000007,539.75016,676.3478400000007],"page":"5"},"5":{"caption":"Figure 5. Scatterplot of controller attach point at target \ngrab/release, front-on for Wall, top-down for Table (mean-\ning both are effectively front-on relative to the target). The \nspread increases with increasing gain. \n","bbox":[320.46,284.7878400000001,541.53612,323.66784],"page":"6"},"6":{"caption":"Figure 6. Side-on view of reach trajectory in real-world space \nwhen within 1m virtually of the target. \n","bbox":[55.740000000000066,109.70784000000087,287.2054800000002,128.42784000000086],"page":"7"},"7":{"caption":"Figure 7. Real-world walking velocity whilst navigating ob-\nstacles mid-trial. \n","bbox":[313.49999999999994,109.34794000000002,534.6637199999999,128.06794000000002],"page":"7"},"8":{"caption":"Figure 8. Overall NASA TLX workload score across different \nlevels of gain. \n","bbox":[318.9,614.4279399999999,551.09256,633.26794],"page":"8"}},"crops":{"1":{"crop_coord":[876.5,267.70541846461055,1331.4999914583332,521.7401859059055],"bbox":[317.34,655.9200439,477.53999692499997,743.760040225],"page":"4"},"2":{"crop_coord":[1329.5000049999999,267.70541846461055,1518.1666920833331,521.7401859059055],"bbox":[480.42000179999997,655.9200439,544.74000915,743.760040225],"page":"4"},"3":{"crop_coord":[170.16667,1720.9124792292018,350.50002375,1954.6110372860542],"bbox":[63.0600012,140.16004937500009,124.38000855000001,220.6800536500001],"page":"4"},"4":{"crop_coord":[347.1666735416667,1720.5791119290132,798.4999781250001,1954.6110372860542],"bbox":[126.780002475,140.16004937500009,285.65999212500003,220.80004877500008],"page":"4"},"5":{"crop_coord":[882.1666479166665,1740.915406117208,1337.4999591666665,1958.9450238853453],"bbox":[319.37999325,138.60003655000003,479.6999853,213.48002620000003],"page":"4"},"6":{"crop_coord":[1335.166610625,1741.9155080177745,1520.1666106249997,1958.9450238853453],"bbox":[482.45997982499995,138.60003655000003,545.4599798249999,213.12004082500005],"page":"4"},"7":{"crop_coord":[871.4999914583332,260.37104156823096,1093.8333316666667,445.3959845556882],"bbox":[315.53999692499997,683.400039625,391.9799994,746.400039625],"page":"5"},"8":{"crop_coord":[1090.4999389583331,261.0378184663032,1308.8332722916666,444.3958403574269],"bbox":[394.37997802499996,683.760040225,469.37997802499996,746.16003415],"page":"5"},"9":{"crop_coord":[1305.4999643750002,261.0378184663032,1523.1666158333333,444.3958403574269],"bbox":[471.77998717500003,683.760040225,546.5399817,746.16003415],"page":"5"},"10":{"crop_coord":[885.5000152083331,388.0559234908009,1526.8333874999996,740.4380652691467],"bbox":[320.58000547499995,577.2000274,547.8600194999999,700.4400328749999],"page":"6"},"11":{"crop_coord":[885.5000152083331,730.4380652691467,1526.8333874999996,1082.8202070474927],"bbox":[320.58000547499995,453.960021925,547.8600194999999,577.2000274],"page":"6"},"12":{"crop_coord":[885.5000152083331,1072.8202495535502,1526.8333874999996,1424.869024031707],"bbox":[320.58000547499995,330.83999627500003,547.8600194999999,453.960006625],"page":"6"},"13":{"crop_coord":[150.1666785416667,1393.8659104393828,805.16667,1591.5593186147141],"bbox":[55.86000427500001,270.8400421,288.0600012,338.400039625],"page":"7"},"14":{"crop_coord":[150.1666785416667,1581.5593611207717,805.16667,1779.252769296103],"bbox":[55.86000427500001,203.28002927499995,288.0600012,270.8400267999999],"page":"7"},"15":{"crop_coord":[150.1666785416667,1769.2528118021605,805.16667,1966.9462199774919],"bbox":[55.86000427500001,135.72001645,288.0600012,203.280013975],"page":"7"},"16":{"crop_coord":[866.1667056249998,1455.5414005028981,1523.8333825,1716.2438102401952],"bbox":[313.620014025,225.96002192499998,546.7800177,316.20002739999995],"page":"7"},"17":{"crop_coord":[866.1667056249998,1706.243810240195,1523.8333825,1966.9462199774919],"bbox":[313.620014025,135.72001645,546.7800177,225.960021925],"page":"7"},"18":{"crop_coord":[879.5000049999999,270.7058512677569,1539.5000049999999,375.7193832037783],"bbox":[318.42000179999997,708.480041425,552.4200017999999,742.68003835],"page":"8"},"19":{"crop_coord":[879.5000049999999,365.7193406977205,1539.5000049999999,470.7328726337419],"bbox":[318.42000179999997,674.2800598,552.4200017999999,708.480056725],"page":"8"},"20":{"crop_coord":[879.5000049999999,460.7329151397994,1539.5000049999999,565.0796701777483],"bbox":[318.42000179999997,640.3200530500001,552.4200017999999,674.2800445],"page":"8"}}}},{"filename":"3173574.3173702","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173702.pdf","paper_id":"3173574.3173702","venue":"CHI 18","keywords":["Weight perception","Virtual reality","Pseudo haptics"],"doi":"10.1145/3173574.3173702","paragraph_containing_keyword":"Author Keywords\nWeight perception; virtual reality; pseudo haptics.","paragraph_after_keyword":"INTRODUCTION\nVirtual reality (VR) Head-Mounted Displays (HMDs) are re-\ncently being released as consumer products (e.g. Playstation\nVR, Oculus VR, HTC VIVE) and are currently strongly pro-\nmoted by the entertainment industry. One of the big advan-\ntages of VR HMDs is the level of presence and immersion\nthey are capable of creating. While prior research on VR has\ntraditionally focused on technical or visual aspects to increase\nthe immersion, haptics has recently been identiﬁed as one of\nthe missing aspects which has also a signiﬁcant impact on im-\nmersion and presence. In this paper we focus on one speciﬁc\naspect of haptics, namely weight.\nCurrently, there are two approaches to simulate weight in VR,\neither using a physical actuator [21, 2, 31] or through visual\nindicators [15, 16, 22, 23]. A drawback of physical actuators\nis that they require a modiﬁcation of the used controllers, and\nthat there is currently no technology or mechanism which is\ncapable of fully and realistically simulating the sensation of\nweight. Software modiﬁcations share the advantage that they\ncan be applied in most of the currently available VR devices,\nbut are limited in their expressiveness in creating a perception\nof weight, since visual cues are used as subtle as possible to\nlet users be unaware of any manipulation. In this paper, we\npresent a software based approach capable of simulating a\nvisual and a haptic perception of weight for tracking-based\nVR devices (e.g. Oculus Rift, HTC VIVE).","sections":[{"word_count":567,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1250,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1480,"figure_citations":{},"section_index":2,"title":"WEIGHT MODELING APPROACH"},{"word_count":798,"figure_citations":{},"section_index":3,"title":"FIRST STUDY"},{"word_count":2478,"figure_citations":{},"section_index":4,"title":"SECOND STUDY"},{"word_count":1061,"figure_citations":{},"section_index":5,"title":"LIMITATIONS OF THE APPROACH"},{"word_count":215,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":31,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGMENTS"},{"word_count":1142,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Breaking the Tracking: Enabling Weight Perception using Perceivable Tracking Offsets","authors":"Michael Rietzler, Florian Geiselhart, Jan Gugenheimer, Enrico Rukzio","abstract":"Virtual reality (VR) technology strives to enable a highly immersive experience for the user by including a wide variety of modalities (e.g. visuals, haptics). Current VR hardware however lacks a sufficient way of communicating the perception of weight of an object, resulting in scenarios where users can not distinguish between lifting a bowling ball or a feather. We propose a solely software based approach of simulating weight in VR by deliberately using perceivable tracking offsets. These tracking offsets nudge users to lift their arm higher and result in a visual and haptic perception of weight. We conducted two user studies showing that participants intuitively associated them with the sensation of weight and accept them as part of the virtual world. We further show that compared to no weight simulation, our approach led to significantly higher levels of presence, immersion and enjoyment. Finally, we report perceptional thresholds and offset boundaries as design guidelines for practitioners.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. We propose a solely software based approach of simulating weight in VR by deliberately using perceivable tracking offsets. These tracking\noffsets nudge users to lift their arm higher and result in a visual and haptic perception of weight.\n","bbox":[53.929,432.75027,564.160430000001,449.68627],"page":"1"},"2":{"caption":"Figure 2. a) When an object is grabbed it is pulled down by the weight\nforce (F(g)). The imaginary force (F(o)) is working against the weight\nforce and increases with the offset between visual and tracked position.\nb) When an object is grabbed, the visual position ﬁrst remains on the\ntracked position. While lifting, the visual position is shifted towards the\nobject one’s. c) The faster an object is moved, the more the visual posi-\ntion is shifted towards the tracked one.\n","bbox":[53.929,203.27426999999992,298.39281000000017,265.04326999999995],"page":"4"},"3":{"caption":"Figure 3. Plot of the participant ratings of different presented offsets as\nwell as the trend line. The green area includes ratings where at least 50%\nof the participants rated the offsets as a good metaphor, the yellow area\nincludes offsets which were accepted by all participants. The red area\nincludes values which were not accepted by all participants.\n","bbox":[53.642,457.82527,297.7949800000002,501.66026999999997],"page":"6"},"4":{"caption":"Figure 4. A screenshot of the bowling scene.\n","bbox":[368.305,578.29927,516.95347,586.26927],"page":"6"},"5":{"caption":"Figure 5. A screenshot of the detection threshold task scene.\n","bbox":[73.946,578.29927,276.9817500000001,586.26927],"page":"7"},"6":{"caption":"Figure 6. Boxplots of the Presence, immersion and enjoyment scores\nsplit by condition.\n","bbox":[321.094,493.99227,564.1630600000001,510.92827],"page":"7"},"7":{"caption":"Figure 7. Boxplots of the results of our own questions regarding weight\nperception and estimation.\n","bbox":[53.929,486.42327,296.99806000000024,503.35927],"page":"8"},"8":{"caption":"Figure 8. Illustration of the participants’ rankings of the two conditions.\n","bbox":[53.929,379.78727,298.3928100000002,387.75726999999995],"page":"8"},"9":{"caption":"Figure 9. The probability and variances of correct estimations as well as\na trend line when comparing two weights using different offsets.\n","bbox":[321.094,536.5232699999999,564.1630600000002,553.4602699999999],"page":"8"},"10":{"caption":"Figure 10. Participants’ weight associations of different presented off-\nsets.\n","bbox":[53.929,485.48327,298.3210800000001,502.42026999999996],"page":"9"}},"crops":{"1":{"crop_coord":[144.80278027777777,557.6008775000004,1572.105272222222,942.9416572222223],"bbox":[53.9290009,454.3410034,564.1578979999999,589.4636840999999],"page":"1"},"2":{"crop_coord":[144.80278027777777,1049.4493358333334,830.0201333333333,1455.8416494444443],"bbox":[53.9290009,269.6970062,297.007248,412.39823909999996],"page":"4"},"3":{"crop_coord":[144.80278027777777,168.22976861111107,830.0100030555556,798.5694377777778],"bbox":[53.9290009,506.3150024,297.00360109999997,729.6372833],"page":"6"},"4":{"crop_coord":[886.9277613888887,168.2111188888888,1572.127778611111,558.0111099999998],"bbox":[321.0939941,592.9160004,564.1660003,729.6439972000001],"page":"6"},"5":{"crop_coord":[144.80278027777777,168.2111188888888,830.0027974999999,558.0111099999998],"bbox":[53.9290009,592.9160004,297.0010071,729.6439972000001],"page":"7"},"6":{"crop_coord":[886.9277613888887,168.2283697222221,1572.1315933333333,772.8250205555556],"bbox":[321.0939941,515.5829926,564.1673736,729.6377869],"page":"7"},"7":{"crop_coord":[144.80278027777777,168.22964138888887,829.9992794444444,793.8499788888889],"bbox":[53.9290009,508.0140076,296.9997406,729.6373291],"page":"8"},"8":{"crop_coord":[144.80278027777777,902.635243611111,829.9986013888888,1114.9666594444445],"bbox":[53.9290009,392.4120026,296.99949649999996,465.2513123],"page":"8"},"9":{"crop_coord":[886.9277613888887,168.22502138888885,1572.1499886111108,654.6805402777775],"bbox":[321.0939941,558.1150055,564.1739958999999,729.6389923],"page":"8"},"10":{"crop_coord":[144.80278027777777,168.22536055555574,830.0157675,796.4583419444446],"bbox":[53.9290009,507.0749969,297.0056763,729.6388701999999],"page":"9"}}}},{"filename":"3173574.3173739","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173739.pdf","paper_id":"3173574.3173739","venue":"CHI 18","keywords":["Virtual reality","Autonomous vehicles","Prototyping","Design evaluation"],"paragraph_containing_keyword":"Author Keywords\nvirtual reality, autonomous vehicles, prototyping, design\nevaluation","paragraph_after_keyword":"INTRODUCTION\nDriving simulators play a critical role in human-centered auto-\nmotive research applications. They allow researchers to create\nsafe and replicable stimuli, thereby enabling rapid and safe\nempirical exploration of how people will respond to various\nroad situations and interface designs. A major challenge for\ndriving simulation is replicating the inertial forces and vehicle\ndynamics that are present in on-road driving—particularly in\nlight of recent psychological studies that point to the impor-\ntance of inertial and vestibular cues to distance perception and\nsteering (please see [22], for a review). In the age of advanced\nautomation, another emerging difﬁculty is that in-lab driving\nsimulators lose much of their ability to generate a sense of im-\nmersion and presence if the person using the simulator is not","doi":"10.1145/3173574.3173739","sections":[{"word_count":545,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1096,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1777,"figure_citations":{},"section_index":2,"title":"IMPLEMENTATION"},{"word_count":498,"figure_citations":{"5":["Figure 5 shows how this was done for the validation study."]},"section_index":3,"title":"REPLICATION SETUP CONSIDERATIONS"},{"word_count":569,"figure_citations":{"4":["Figure 4 for the participant view of the virtual driving simulation environment)."],"5":["Figure 5)."]},"section_index":4,"title":"VALIDATION STUDY"},{"word_count":2417,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":102,"figure_citations":{},"section_index":6,"title":"FUTURE WORK"},{"word_count":147,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":85,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGEMENTS"},{"word_count":1382,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"VR-OOM: Virtual Reality On-rOad driving siMulation","authors":"David Goedicke, Jamy Li, Vanessa Evers, Wendy Ju","abstract":"Researchers and designers of in-vehicle interactions and interfaces currently have to choose between performing evaluation and human factors experiments in laboratory driving simulators or on-road experiments. To enjoy the benefit of customizable course design in controlled experiments with the immediacy and rich sensations of on-road driving, we have developed a new method and tools to enable VR driving simulation in a vehicle as it travels on a road. In this paper, we describe how the cost-effective and flexible implementation of this platform allows for rapid prototyping. A preliminary pilot test (N = 6), centered on an autonomous driving scenario, yields promising results, illustrating proof of concept and indicating that a basic implementation of the system can invoke genuine responses from test participants.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. VR-OOM allows participants to experience the physical sen-\nsations of the real-world with the controlled virtual environments and\nevents. Photo by Arjan Reef.\n","bbox":[321.094,399.74027,565.48608,425.64327],"page":"1"},"2":{"caption":"Figure 2. VR-OOM System Diagram (excluding the wizard driver)\n","bbox":[329.316,611.08227,555.9429500000001,619.05227],"page":"3"},"3":{"caption":"Figure 3. View from inside vehicle with participant on left, Driving Wizard on right, and Interaction Wizard in backseat, top. Participant’s view with\nhand in foreground, left. Bird’s eye view of virtual vehicle in virtual world, right.\n","bbox":[53.929,477.22026999999997,564.1619400000001,494.15626999999995],"page":"4"},"4":{"caption":"Figure 4. Participant view at the start of the experiment (excluding the tracked hands).\n","bbox":[161.641,549.9512699999999,456.45130000000034,557.9212699999999],"page":"5"},"5":{"caption":"Figure 5. The virtual route overlaid on a satellite map. The black box\nis the starting location, conditions one and two are marked by the yel-\nlow cars and the white outline is the shape of the road. The gray boxes\nindicated the approximate locations of buildings in the virtual scene.\n","bbox":[321.094,466.01427,565.4860800000001,500.88426999999996],"page":"6"}},"crops":{"1":{"crop_coord":[903.8111030555555,571.4344530555558,1555.2456666666667,1009.7277661111111],"bbox":[327.1719971,430.2980042,558.08844,584.4835969],"page":"1"},"2":{"crop_coord":[893.6833361111111,168.22964138888887,1565.3829277777777,466.94720805555556],"bbox":[323.526001,625.6990051,561.737854,729.6373291],"page":"3"},"3":{"crop_coord":[286.5333216666667,168.22722555555546,1430.4213119444444,819.4138761111112],"bbox":[104.9519958,498.8110046,513.1516723,729.6381988],"page":"4"},"4":{"crop_coord":[144.80278027777777,168.2202741666666,1572.0937433333331,636.7555744444442],"bbox":[53.9290009,564.5679932,564.1537476,729.6407013],"page":"5"},"5":{"crop_coord":[890.3027936111112,168.20565111111134,1568.764063333333,800.7277933333335],"bbox":[322.3090057,505.5379944,562.9550628,729.6459656],"page":"6"}}}},{"filename":"3173574.3173759","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173759.pdf","paper_id":"3173574.3173759","venue":"CHI 18","keywords":["3D drawing","Design sketching","Augmented reality"],"doi":"10.1145/3173574.3173759","paragraph_containing_keyword":"Author Keywords\n3D drawing, design sketching, augmented reality","paragraph_after_keyword":"INTRODUCTION\nSketching has been a design and conceptualization aid for\ncenturies. While traditional 2D sketching is ingrained into\nour consciousness, abilities, and training from childhood [56],\nthe growth of consumer grade augmented and virtual reality\n(AR/VR) devices has recently enabled us to transcend the\nlimits of the sketch canvas digitally—equipping artists with\nthe unprecedented ability to sketch 3D curves directly in the\nair [22, 35, 51]. 3D sketching is indeed becoming an increas-\ningly popular medium of art and storytelling [20, 4, 1].","sections":[{"word_count":635,"figure_citations":{"2":["Figure 2)."],"4":["Figure 4) of illustration styles, textures, and rendering techniques to depict geometric detail, material, and lighting [26, 42]."],"6":["Figure 6)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1276,"figure_citations":{"3":["Figure 3)."],"4":["Figure 4)."],"14":["Figure 14a), mechanical wing augmentation (Figure 14c), mini car (participant creation), Flintstones’ house (author creation), and large fan (Figure 14d)."]},"section_index":1,"title":"RELATED WORK"},{"word_count":378,"figure_citations":{"6":["Figure 6a).","Figure 6b)."],"8":["Figure 8)."]},"section_index":2,"title":"SYSTEM OVERVIEW AND SETUP"},{"word_count":2184,"figure_citations":{"1":["Figure 1 demonstrates the overall workflow of our system."],"2":["Figure 2)."],"7":["Figure 7c–d)."],"9":["Figure 9)."],"10":["Figure 10).","Figure 10)."],"11":["Figure 11) shows an orthographic projection of the active drawing canvas, a color palette, and other functionality and configuration settings (also see Figure 8).","Figure 11a)."],"12":["Figure 12).","Figure 12)."],"13":["Figure 13)."],"14":["Figure 14a).","Figure 14)."],"15":["Figure 15c), softness (drapery or clothes, see Figure 17), or size and reachability (buildings, ceilings, or cars, Figure 14d)."]},"section_index":3,"title":"COMPONENTS"},{"word_count":834,"figure_citations":{},"section_index":4,"title":"IMPLEMENTATION DETAILS"},{"word_count":1393,"figure_citations":{"14":["Figure 14 shows some of the resulting artifacts."],"15":["Figure 15a)."]},"section_index":5,"title":"USER EVALUATION"},{"word_count":310,"figure_citations":{},"section_index":6,"title":"DISCUSSION"},{"word_count":113,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":38,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGEMENTS"},{"word_count":1769,"figure_citations":{"14":["Figure 14c), war helicopter (Figure 14a), large wall-fan (Figure 14d), and some additional results: CHInosaur as the Toronto Raptor (participant creation) and skirt drawn over a human model (author creation)."]},"section_index":9,"title":"REFERENCES"}],"title":"SymbiosisSketch: Combining 2D & 3D Sketching for Designing Detailed 3D Objects in Situ","authors":"Rahul Arora, Rubaiat Habib Kazi, Tovi Grossman, George Fitzmaurice, Karan Singh","abstract":"We present SymbiosisSketch, a hybrid sketching system that combines drawing in air (3D) and on a drawing surface (2D) to create detailed 3D designs of arbitrary scale in an augmented reality (AR) setting. SymbiosisSketch leverages the complementary affordances of 3D (immersive, unconstrained, life-sized) and 2D (precise, constrained, ergonomic) interactions for in situ 3D conceptual design. A defining aspect of our system is the ongoing creation of surfaces from unorganized collections of 3D curves. These surfaces serve a dual purpose: as 3D canvases to map strokes drawn on a 2D tablet, and as shape proxies to occlude the physical environment and hidden curves in a 3D sketch. SymbiosisSketch users draw interchangeably on a 2D tablet or in 3D within an ergonomically comfortable canonical volume, mapped to arbitrary scale in AR. Our evaluation study shows this hybrid technique to be easy to use in situ and effective in transcending the creative potential of either traditional sketching or drawing in air.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":2},"fig":{"captions":{"1":{"caption":"Figure 1. SymbiosisSketch combines mid-air 3D interactions (a) with constrained sketching. Users can create planar or curved canvases (b), and use a\ntablet (c) to sketch onto them. Designs are created in situ, in context of physical objects in the scene (d), allowing quick post-processing operations to\nseamlessly blend the virtual objects into the real world (e).\n","bbox":[53.929,348.94626999999997,564.16411,374.84826999999996],"page":"1"},"2":{"caption":"Figure 2. Lacking surfaces, Tilt Brush users employ many strokes to de-\npict texture or the illusion of a surface. ©Kamikakushi de Sen et Chihiro\n(left) and Christopher Watson (right). Used under CC-Attribution 3.0.\n","bbox":[320.831,595.5232699999999,565.4860800000001,621.4262699999999],"page":"2"},"3":{"caption":"Figure 3. Design workﬂow using 3D and 2D tools. Rustam Hasanov mod-\neled an underlay of an interior scene in a 3D modeling tool (SketchUp),\nand then drew details over the top of those underlays using a 2D inter-\nface and tools. ©Rustam Hasanov. Used with permission.\n","bbox":[321.094,610.5652699999999,565.4860800000001,645.43427],"page":"3"},"4":{"caption":"Figure 4. Details in 2D drawings: structural details, architectural em-\nbellishments, and textures. © Jake Parker (left) and Johannes Figlhuber\n(right). Used with permission.\n","bbox":[320.831,460.69027,565.4860800000001,486.59326999999996],"page":"3"},"5":{"caption":"Figure 5. Post-processed results. HoloLens hardware limitations restrict us to basic Gouraud shading. With appropriate shading and occlusion,\nSymbiosisSketch designs can seamlessly blend into the real world. (Clockwise from top) war helicopter shooting at Captain America (Figure 14a),\nmechanical wing augmentation (Figure 14c), mini car (participant creation), Flintstones’ house (author creation), and large fan (Figure 14d).\n","bbox":[53.929,388.61327,565.156680000001,414.51527],"page":"4"},"6":{"caption":"Figure 6. Setup: the user puts on the HoloLens and draws with a motion-\ntracked stylus, on a tablet (left), or mid-air (right) using a mouse afﬁxed\nto the back of the tablet.\n","bbox":[321.094,129.46726999999998,565.4860800000001,155.37027],"page":"4"},"7":{"caption":"Figure 7. Strokes drawn using the 2D tablet are projected onto drawing\ncanvases. (a) A planar drawing canvas is a rectangle with the same aspect\nratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users\ncan draw closed curves on canvases to deﬁne (d) solid surfaces which also\nlend occlusion, lighting, and shadows to the design.\n","bbox":[53.929,533.63927,297.00304,577.47527],"page":"5"},"8":{"caption":"Figure 8. 2D UI toolbar. (left to right) save/load, initial canvas, create canvas, explore and select bookmarks, color tools (palette, HSV sliders, indicator),\nﬁll, pencil/ink toggle, symmetry plane, UI and pencil toggles, workspace scaling and reset, clear all and undo stroke. Buttons are shown only if currently\nuseful. For example, selecting the pencil hides color tools. Icons © icons8.com. Used under CC BY-ND 3.0.\n","bbox":[53.928999999999974,672.25527,565.1564700000002,698.15827],"page":"6"},"9":{"caption":"Figure 9. Creating a curved drawing canvas. (a) The user draws a few\nstrokes using the 2D and/or 3D interface. (b) A surface patch is ﬁt to\nthese strokes.\n","bbox":[53.92899999999999,550.34627,297.28310000000005,576.24927],"page":"6"},"10":{"caption":"Figure 10. Widgets for direct 3D manipulation: translation and rotation\n(all canvases), and scaling (planar canvases): shown at the center, edges,\nand corners, respectively.\n","bbox":[53.666000000000004,414.24827,297.99230000000017,440.15126999999995],"page":"6"},"11":{"caption":"Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The\ncorresponding strokes in 3D.\n","bbox":[53.929,77.62127000000001,296.99744,94.55827000000001],"page":"6"},"12":{"caption":"Figure 12. Workspace scaling tool for drawing a car. The user uses a\nlarge workspace to draw the shape of the car in a small, comfortable,\nscale (left). She then deﬁnes a small workspace centered on a headlight\n(right). This “zoom in” effect allows her to capture the details of the\nheadlight’s shape, deﬁne a new canvas on it, and draw highlights.\n","bbox":[53.666000000000004,613.30027,297.9943100000002,657.13527],"page":"7"},"13":{"caption":"Figure 13. Physical planes detected by the HoloLens are automatically\nbookmarked (shown overlaid on the scene, left). With workspace scaling,\nusers can visualize and work even with room-scale planes in an accessi-\nble position and comfortable scale (right).\n","bbox":[53.929,474.28427000000005,298.32108000000017,509.15427],"page":"7"},"14":{"caption":"Figure 14. Sample results showing SymbiosisSketch’s creative potential. Author creations: war helicopter shooting at a Captain America ﬁgure (a), logo\ndrawn on a person’s clothing (b), mechanical wing augmenting a robot ﬁgure (c), and a large wall-fan (d). Participant creations: scary ﬁgure utilizing\na mannequin head (e) and genie emanating out of a teapot (f) with details (g, h).\n","bbox":[53.929,430.99226999999996,564.1604300000013,456.89426999999995],"page":"9"},"15":{"caption":"Figure 15. Task-1: drawing London’s Gherkin building (a). Task-2:\ndrawing a logo (b) on a curved physical surface of a vase (c).\n","bbox":[53.929,282.08627,298.3210800000001,299.02227],"page":"9"},"16":{"caption":"Figure 16. Representative results for the ﬁxed tasks. Task 1: Gherkin\nbuilding, using symbiosis (left) and mid-air only (center-left). Task 2:\nlogo on physical object, symbiosis (center-right), mid-air only (right).\n","bbox":[321.094,269.24926999999997,565.48727,295.15126999999995],"page":"9"},"17":{"caption":"Figure 17. (Clockwise from top-left) close-ups of selected results showing details of designs: mechanical wing (Figure 14c), war helicopter (Figure 14a),\nlarge wall-fan (Figure 14d), and some additional results: CHInosaur as the Toronto Raptor (participant creation) and skirt drawn over a human model\n(author creation).\n","bbox":[53.666,343.55027,565.156680000001,369.45327],"page":"14"},"18":{"caption":"Figure 18. All participants’ drawings for the ﬁxed tasks. Row-wise from top: task 1 (Gherkin building) in symbiosis condition, the in mid-air only\ncondition; task 2 (logo on a physical surface) in symbiosis condition, and in mid-air only condition.\n","bbox":[53.928999999999974,235.34127,564.16478,252.27727000000002],"page":"15"}},"crops":{"1":{"crop_coord":[144.80278027777777,577.565943333333,1572.1239641666666,1150.8249833333332],"bbox":[53.9290009,379.503006,564.1646271,582.2762604000001],"page":"1"},"2":{"crop_coord":[886.9277613888887,168.2273099999998,1572.143630833333,465.88612861111113],"bbox":[321.0939941,626.0809937,564.1717070999999,729.6381684],"page":"2"},"3":{"crop_coord":[886.9277613888887,168.22921750000003,1572.1077727777777,399.19720972222234],"bbox":[321.0939941,650.0890045,564.1587982,729.6374817],"page":"3"},"4":{"crop_coord":[886.9277613888887,539.7495777777775,1572.1360861111111,840.4222191666665],"bbox":[321.0939941,491.2480011,564.168991,595.8901520000001],"page":"3"},"5":{"crop_coord":[144.80278027777777,168.22786111111108,1572.2415841666666,1040.6388938888888],"bbox":[53.9290009,419.1699982,564.2069703,729.63797],"page":"4"},"6":{"crop_coord":[886.9277613888887,1523.9917416666667,1572.1212936111108,1760.4861280555556],"bbox":[321.0939941,160.0249939,564.1636656999999,241.562973],"page":"4"},"7":{"crop_coord":[178.56386833333332,168.22260527777757,796.2601472222223,587.972208611111],"bbox":[66.0829926,582.1300049,284.853653,729.6398621000001],"page":"5"},"8":{"crop_coord":[144.80278027777777,168.22714083333327,1572.1525319444445,252.7416569444444],"bbox":[53.9290009,702.8130035,564.1749115,729.6382293],"page":"6"},"9":{"crop_coord":[144.80278027777777,377.3000163888891,829.9962277777778,591.3777583333334],"bbox":[53.9290009,580.904007,296.998642,654.3719940999999],"page":"6"},"10":{"crop_coord":[144.80278027777777,695.4497527777779,829.9999577777777,969.4305758333334],"bbox":[53.9290009,444.8049927,296.9999848,539.838089],"page":"6"},"11":{"crop_coord":[144.80278027777777,1694.618233611111,830.0347986111111,1929.4083405555555],"bbox":[53.9290009,99.2129974,297.0125275,180.1374359],"page":"6"},"12":{"crop_coord":[144.80278027777777,168.2293869444444,829.9993644444444,366.6944630555557],"bbox":[53.9290009,661.7899933,296.9997712,729.6374207],"page":"7"},"13":{"crop_coord":[144.80278027777777,527.9528383333336,830.001483611111,777.7555591666667],"bbox":[53.9290009,513.8079987,297.0005341,600.1369781999999],"page":"7"},"14":{"crop_coord":[144.80278027777777,168.22120666666672,1572.1596952777777,922.9194555555555],"bbox":[53.9290009,461.548996,564.1774902999999,729.6403656],"page":"9"},"15":{"crop_coord":[144.80278027777777,1047.480824722222,830.0191583333333,1361.452772222222],"bbox":[53.9290009,303.677002,297.006897,413.1069031],"page":"9"},"16":{"crop_coord":[886.9277613888887,1047.4822658333333,1572.1190472222222,1372.205556111111],"bbox":[321.0939941,299.8059998,564.162857,413.1063843],"page":"9"},"17":{"crop_coord":[144.80278027777777,216.27261694444456,1572.2946930555554,1165.811106388889],"bbox":[53.9290009,374.1080017,564.2260895,712.3418578999999],"page":"14"},"18":{"crop_coord":[144.80278027777777,216.28575666666652,1572.1485477777776,1491.299981111111],"bbox":[53.9290009,256.9320068,564.1734772,712.3371276],"page":"15"}}}},{"filename":"3173574.3173778","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173778.pdf","paper_id":"3173574.3173778","venue":"CHI 18","keywords":["Visualization","Assistive technology","Autism","Proximity","Prosody","Immersive VR","Accessibility"],"paragraph_containing_keyword":"ABSTRACT \nnuanced \ncommunication \nSocial \nnonverbal  communication  cues, \nincluding  eye  contact, \ngestures,  facial  expressions,  body  language,  and  tone  of \nvoice. This type  of communication  is central to face-to-face \ninteraction,  but  can  be  challenging  for  children  and  adults \nwith autism. Innovative technologies can provide support by \naugmenting \nautomated \nprompting. Specifically, immersive virtual reality (VR) offers \nan  option \ninterventions  by \nin  real-time  social \ninformation \nconcretizing  nonverbal \ninteractions.  In  this  work,  we  explore  the  design  and \nevaluation of three nonverbal communication applications in \nimmersive  VR.  The  results  of  this  work  indicate  that \ndelivering  real-time  visualizations  of  proximity,  speaker \nvolume,  and  duration  of  one’s  speech  is  feasible  in \nimmersive  VR  and  effective  for  real-time  support  for \nproximity  regulation  for  children  with  autism.  We  conclude \nwith design considerations for therapeutic VR systems. \nAUTHOR KEYWORDS \nVisualization; \nassistive \ntechnology;  autism;  proximity;  prosody;  immersive  VR; \naccessibility.  \nACM Classification Keywords \nK.4.2. [Social Issues] Assistive technologies for persons with \ndisabilities; Artificial, augmented, and virtual realities \nINTRODUCTION \nSocial  communication  is  essential  to  quality  of  life.  The \nability  to  express  one’s  own  needs  and  wants  while \nunderstanding  others  is  central  to  our  connection  to  one \nanother  and  our  ability  to  teach  and  learn.    Challenges  with \nsocial  communication  have  the  ability  to  put  human \nPermission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies \nbear  this  notice  and  the  full  citation  on  the  first  page.  Copyrights  for \ncomponents  of  this  work  owned  by  others  than  ACM  must  be  honored. \nAbstracting  with  credit  is  permitted.  To  copy  otherwise,  or  republish,  to \npost on servers or to redistribute to lists, requires prior specific permission \nand/or a fee. Request permissions from Permissions@acm.org. \nCHI 2018, April 21–26, 2018, Montreal, QC, Canada  \n© 2018 Association for Computing Machinery. \nACM ISBN 978-1-4503-5620-6/18/04…$15.00  \nhttps://doi.org/10.1145/3173574.3173778","doi":"10.1145/3173574.3173778","paragraph_after_keyword":"communication;","sections":[{"word_count":30,"figure_citations":{},"section_index":0,"title":"AUTHOR KEYWORDS"},{"word_count":660,"figure_citations":{},"section_index":1,"title":"INTRODUCTION"},{"word_count":796,"figure_citations":{},"section_index":2,"title":"RELATED WORK"},{"word_count":1458,"figure_citations":{"1":["Figure 1-time spent talking bar)."],"2":["Figure 2B) did not result in changing one’s proximity when needed; and c.","Figure 2A).","Figure 2B), and yellow representing an intermediate warning between the two zones (see Figure 2D).","Figure 2E).","Figure 2D) or ‘step away’ respectively (see Figure 2 E).","Figure 2C)."]},"section_index":3,"title":"SYSTEM DESIGN"},{"word_count":830,"figure_citations":{},"section_index":4,"title":"EVALUATION"},{"word_count":2223,"figure_citations":{"3":["Figure 3, left).","Figure 3, middle), yet at the Paper 204 individual level, three participants improved in the intervention condition (P1, P9, P11), three performed well in baseline and intervention (P2, P6, P8), and three showed reduced performance in the intervention condition (P5, P7, P10).","Figure 3, right)."]},"section_index":5,"title":"RESULTS"},{"word_count":1200,"figure_citations":{},"section_index":6,"title":"DESIGN CONSIDERATIONS FOR THERAPUETIC VR"},{"word_count":351,"figure_citations":{},"section_index":7,"title":"DISCUSSION"},{"word_count":169,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":48,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGMENTS"},{"word_count":1288,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"vrSocial: Toward Immersive Therapeutic VR Systems for Children with Autism","authors":"LouAnne E. Boyd, Saumya Gupta, Sagar B. Vikmani, Carlos M. Gutierrez, Junxiang Yang, Erik Linstead, Gillian R. Hayes","abstract":"Social communication frequently includes nuanced nonverbal communication cues, including eye contact, gestures, facial expressions, body language, and tone of voice. This type of communication is central to face-to-face interaction, but can be challenging for children and adults with autism. Innovative technologies can provide support by augmenting human-delivered cuing and automated prompting. Specifically, immersive virtual reality (VR) offers an option to generalize social skill interventions by concretizing nonverbal information in real-time social interactions. In this work, we explore the design and evaluation of three nonverbal communication applications in immersive VR. The results of this work indicate that delivering real-time visualizations of proximity, speaker volume, and duration of one's speech is feasible in immersive VR and effective for real-time support for proximity regulation for children with autism. We conclude with design considerations for therapeutic VR systems.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1: Interfaces for Voice Condition. On the left, the bar is balanced between speakers and the volume icon is \ngreyed out as the user is not speaking. In the middle, the green speaker indicates the user’s volume is in the correct \nrange, and on the right the larger red speaker indicates the user is speaking too loud. \n","bbox":[85.91808,511.8624,528.0556799999997,541.63008],"page":"4"},"2":{"caption":"Figure 2: Images of the five interfaces for Proximity Condition: A.) Baseline, B.) Proximity correct zone, C.) Too far away zone, D.) A bit \ntoo close for acquaintances zone, E.) Intrusion into personal space. \n","bbox":[49.44,70.54272,576.1017599999999,90.43008],"page":"4"},"3":{"caption":"Figure 3: Split plot graphs depicting percent correct proximity (left), percent correct volume (middle) and time spent talking (right) \nacross four conditions, over five sessions. The purple line represents the combination condition, the green line represents the \nproximity condition, the yellow line represents the volume condition and the blue represents the baseline condition. \n","bbox":[52.56,554.62272,559.2854399999997,584.5900800000001],"page":"8"}},"crops":{"1":{"crop_coord":[-44.076232777777776,223.80083722222233,874.1368105555556,696.9302622222224],"bbox":[-14.0674438,542.9051056,312.8892518,709.6316985999999],"page":"4"},"2":{"crop_coord":[386.5486230555556,224.5266469444446,1289.6030766666665,696.5774875000002],"bbox":[140.9575043,543.0321045,462.4571076,709.3704071],"page":"4"},"3":{"crop_coord":[928.9633516666665,219.1424983333334,1739.5108455555555,680.0808291666667],"bbox":[336.2268066,548.9709015,624.4239044,711.3087006],"page":"4"},"4":{"crop_coord":[-128.61573111111113,1590.5398558333334,485.1653969444444,1939.7304024999999],"bbox":[-44.5016632,95.4970551,172.8595429,217.6056519],"page":"4"},"5":{"crop_coord":[180.2589925,1592.379006388889,791.266496388889,1940.3154077777776],"bbox":[66.6932373,95.2864532,283.0559387,216.94355769999999],"page":"4"},"6":{"crop_coord":[480.7977972222222,1591.963967222222,1084.752816111111,1938.7367588888887],"bbox":[174.887207,95.8547668,388.7110138,217.0929718],"page":"4"},"7":{"crop_coord":[803.102205,1592.2236633333334,1400.5619388888888,1940.4422675],"bbox":[290.9167938,95.2407837,502.402298,216.9994812],"page":"4"},"8":{"crop_coord":[1104.6608480555556,1596.7470041666668,1694.9703133333333,1942.1064502777776],"bbox":[399.4779053,94.6416779,608.3893128,215.3710785],"page":"4"},"9":{"crop_coord":[136.7122311111111,248.6091697222221,555.9794447222223,561.1091527777777],"bbox":[51.0164032,591.800705,198.35260010000002,700.7006989],"page":"8"},"10":{"crop_coord":[522.7772266666667,239.99778722222212,961.3136205555556,560.5533513888888],"bbox":[189.9998016,592.0007935,344.2729034,703.8007966],"page":"8"},"11":{"crop_coord":[914.4786072222223,239.02529388888885,1526.3008458333331,561.0952927777777],"bbox":[331.0122986,591.8056946,547.6683045,704.1508942],"page":"8"}}}},{"filename":"3173574.3173792","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173792.pdf","paper_id":"3173574.3173792","venue":"CHI 18","keywords":["Augmented reality","Virtual reality","3D window management"],"paragraph_containing_keyword":"ABSTRACT \nIn augmented and virtual reality (AR and VR), there may be \nmany 3D planar windows with 2D texts, images, and videos \non them. However, managing the position, orientation, and \nscale of such a window in an immersive 3D workspace can \nbe  difficult.  Projective  Windows  strategically  uses  the \nabsolute and apparent sizes of the window at various stages \nof  the  interaction  to  enable  the  grabbing,  moving,  scaling, \nand releasing of the window in one continuous hand gesture. \nWith  it,  the  user  can  quickly  and  intuitively  manage  and \ninteract  with  windows  in  space  without  any  controller \nhardware  or  dedicated  widget.  Through  an  evaluation,  we \ndemonstrate that our technique is performant and preferable, \nand that projective geometry plays an important role in the \ndesign of spatial user interfaces. \nAuthor Keywords \nAugmented reality; virtual reality; 3D window management  \nACM Classification Keywords \nH.5.2.  Information  interfaces  and  presentation  (e.g.,  HCI): \nUser Interfaces-Interaction styles, Windowing systems \nINTRODUCTION \nWe  imagine  an  immersive  future  of  computing  where \nminimal  gear  worn  over  the  eyes  brings  virtual  interactive \nelements to the space around the user. These virtual elements \nmay  merely  enhance  the  user’s  experience  with  the  reality \n(AR) or may completely overwrite it (VR).","paragraph_after_keyword":"Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies \nbear  this  notice  and  the  full  citation  on  the  first  page.  Copyrights  for \ncomponents  of  this  work  owned  by  others  than  ACM  must  be  honored. \nAbstracting  with  credit  is  permitted.  To  copy  otherwise,  or  republish,  to \npost on servers or to redistribute to lists, requires prior specific permission \nand/or a fee. Request permissions from Permissions@acm.org. \n \nCHI 2018, April 21–26, 2018, Montreal, QC, Canada \n© 2018 Association for Computing Machinery. \nACM ISBN 978-1-4503-5620-6/18/04…$15.00 \nhttps://doi.org/10.1145/3173574.3173792","doi":"10.1145/3173574.3173792","sections":[{"word_count":403,"figure_citations":{"1":["Figure 1)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":61,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1205,"figure_citations":{"2":["Figure 2a).","Figure 2b) the fingers, and completes the selection by making the tips of the fingers touch (Figure 2c)."],"3":["Figure 3a), i.","Figure 3b) while maintaining the same apparent size, thus appearing the same to the user (Figure 3a, b insets) as in the image plane interaction proposed by Pierce et al."],"4":["Figure 4a), or smaller by taking it away from the face (Figure 4b).","Figure 4a, b), whereas the window is erected against a horizontal surface, perpendicular to the user’s gaze, to enforce the best viewing angle (Figure 4c)."],"5":["Figure 5a)."]},"section_index":2,"title":"PROJECTIVE WINDOWS"},{"word_count":115,"figure_citations":{"6":["Figure 6a) for tracking hand movement and posture within the user’s view frustum, the Leap Motion sensor’s front-facing camera for a video feed of the real world, the Unity 3D engine for integrating virtual and physical elements (Figure 6b), and an Intel quad-core i7 3."]},"section_index":3,"title":"IMPLEMENTATION"},{"word_count":428,"figure_citations":{"5":["Figure 5b), when the zoom is large, the slope is steep, and when the zoom is small, the slope is gentle, so the user has finer control when the zoom is smaller."],"7":["Figure 7) to a widget-based ray-casting technique (RC) using the same controller."]},"section_index":4,"title":"EVALUATION"},{"word_count":1487,"figure_citations":{"8":["Figure 8h), and others were move widgets (Figure 8f, g).","Figure 8, 9)."],"9":["Figure 9) to simultaneously compare the discrete levels of various factors in the later statistical analyses: For example, moving a window from A to a target at C requires a linear movement of 4 m and an angular movement of -60°, whereas A to D requires 2 m and 30° respectively.","Figure 9)."],"10":["Figure 10a).","Figure 10b)."],"11":["Figure 11a■, F4,1675 = 45, p < 0.","Figure 11b■, F8,1671 = 11, p < 0.","Figure 11c■, F8,1671 = 17, p < 0.","Figure 11d■, F6,1673 = 54, p < 0.","Figure 11a■, F4,1675 = 19, p < 0.","Figure 11b■, F8,1671 = 13, p < 0.","Figure 11d■, F6,1673 = 23, p < 0."],"12":["Figure 12c■, F8,1671 = 59, p < 0.","Figure 12d■, F6,1673 = 30, p < 0.","Figure 12a■, F4,1675 = 8.","Figure 12b■, F8,1671 = 5.","Figure 12c■, F8,1761 = 15, p < 0.","Figure 12d■, F6,1673 = 51, p < 0.","Figure 12a■, F4,1675 = 15, p < 0.","Figure 12b■, F8,1671 = 17, p < 0."],"13":["Figure 13)."]},"section_index":5,"title":"EXPERIMENT SESSION"},{"word_count":247,"figure_citations":{},"section_index":6,"title":"DISCUSSION"},{"word_count":743,"figure_citations":{"11":["Figure 11d) and with an absolute size ratio of 1 for RC (Figure 11c), irrespective of the technique.","Figure 11d, 12d)."]},"section_index":7,"title":"EXPLORATION SESSION"},{"word_count":415,"figure_citations":{"14":["Figure 14) using our implementation (Figure 6).","Figure 14a), the user can pull picture windows out of a laptop screen and easily scale and place them anywhere on nearby walls for visual reference, just as he or she would sticky notes, but with the ability to freely change the size.","Figure 14a), the user can pick up a window from a laptop screen and place it on a tablet device to quickly change the input from typing to drawing, without having to swap applications.","Figure 14b), the user can perform the grab gesture to instantly scan a notebook page and then generate a projective window from it, to scale and place it anywhere for reference.","Figure 14c), the user can pick up a small movie window from a nearby table, play the preview of the movie by bringing it closer to the face [1], and then start playing the movie by projecting it onto a vertical wall.","Figure 14d), the user can use the entire unbounded scene as a workspace, even projecting windows across large distances."]},"section_index":8,"title":"USER SCENARIOS"},{"word_count":607,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Projective Windows: Bringing Windows in Space to the Fingertip","authors":"Joon Hyub Lee, Sang-Gyun An, Yongkwan Kim, Seok-Hyung Bae","abstract":"In augmented and virtual reality (AR and VR), there may be many 3D planar windows with 2D texts, images, and videos on them. However, managing the position, orientation, and scale of such a window in an immersive 3D workspace can be difficult. Projective Windows strategically uses the absolute and apparent sizes of the window at various stages of the interaction to enable the grabbing, moving, scaling, and releasing of the window in one continuous hand gesture. With it, the user can quickly and intuitively manage and interact with windows in space without any controller hardware or dedicated widget. Through an evaluation, we demonstrate that our technique is performant and preferable, and that projective geometry plays an important role in the design of spatial user interfaces.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. In a mock setup depicting the flow of Projective Windows, (a) the user wishing to adjust the position and scale of an AR \nwindow (b) grabs the window, (c) moves it, (d) makes it bigger by bringing it closer, and (e) projects it to the desired position.\n","bbox":[58.44,536.325,555.645,555.648],"page":"1"},"2":{"caption":"Figure 2. (a) The user creates a big area cursor, (b) specifies a \nwindow in a cluttered situation by closing the fingers and \nmaking the cursor smaller, and (c) selects it by pinching. \n","bbox":[319.2,469.485,557.7270000000001,499.248],"page":"2"},"3":{"caption":"Figure 3. (a) The user makes a grab gesture on a window to (b) \nprojectively bring it to the grabbed point. \n","bbox":[316.8,244.728,560.124,264.168],"page":"2"},"4":{"caption":"Figure 4. (a) The user makes the window appear bigger by \nbringing it closer to the face, and (b) smaller by putting it \naway. The user can project a window (a, b) onto a vertical \nsurface, or (c) make it stand on a horizontal surface. \n","bbox":[62.52,629.802,288.9329999999998,669.888],"page":"3"},"5":{"caption":"Figure 5. (a) When the user grabs a window from a wall, \nmoves it relative to the face, and releases it to another wall, (b) \nthe zoom (ratio of the final to initial widths) is inversely \nproportional to d. \n","bbox":[54.843,171.642,296.5290000000001,211.728],"page":"3"},"6":{"caption":"Figure 6. (a) Implementation hardware. (b) The hands, real \nand virtual objects in the user’s view. \n","bbox":[323.28,413.08500000000004,553.6349999999999,432.408],"page":"3"},"7":{"caption":"Figure 7. With Projective Windows (PW) using a hardware \ncontroller (PWC), (a-b) a finger pinch is substituted with (c-d) \nthe pull of the trigger (highlighted green) on the hardware \ncontroller. Note the similarity between the bare hand gesture \nand the controller-based gesture. \n","bbox":[318.111,82.04699999999998,558.9869999999999,132.888],"page":"3"},"8":{"caption":"Figure 8. The experiment was conducted with PW using a controller (PWC) and ray-casting (RC). Using PWC, the participant (a) \nfirst searched for the target (white), (b) grabbed the window (blue), (c) moved the window while adjusting its apparent size, and (d) \nreleased it onto the target when the apparent sizes matched. Using RC, the participant (e) first searched for the target (white), (f) \ndragged the window (blue) to the target, (g) placed it in the target, and (h) scaled it so that the absolute sizes matched. \n","bbox":[56.87700000000001,545.091,561.4499999999999,585.288],"page":"4"},"9":{"caption":"Figure 9. Windows and targets appeared on A~L. The user \nperformed move and scale tasks using Projective Windows \nand a widget-based ray-casting baseline technique. \n","bbox":[324.477,376.84800000000007,552.573,406.61100000000005],"page":"4"},"14":{"caption":"Figure 14. User scenarios of Projective Windows in a (a) \ndesign studio, (b) study, (c) living room, and (d) VR scene. \n","bbox":[326.14799999999997,614.565,550.8599999999999,633.888],"page":"7"}},"crops":{"1":{"crop_coord":[139.44444444444446,456.11111111111103,1560.5555555555557,660.5555555555557],"bbox":[52,556,560,626],"page":"1"},"2":{"crop_coord":[874.9999661111112,638.9999049999999,1554.999915277778,811.666565],"bbox":[316.7999878,501.6000366,557.9999695,560.1600342],"page":"2"},"3":{"crop_coord":[874.9999661111112,1269.666680277778,1554.6666377777779,1464.6666547222221],"bbox":[316.7999878,266.5200043,557.8799896,333.1199951],"page":"2"},"4":{"crop_coord":[145,165.00003388888908,824.9999491666666,337.6666938888889],"bbox":[54,672.2399902,295.19998169999997,730.7999877999999],"page":"3"},"5":{"crop_coord":[270,995.0000000000001,707.7777777777778,1091.1111111111113],"bbox":[99,401,253,432],"page":"3"},"6":{"crop_coord":[145,1412.9999455555555,824.9999491666666,1608.9999219444442],"bbox":[54,214.5600281,295.19998169999997,281.5200196],"page":"3"},"7":{"crop_coord":[867.2222222222222,795,1560.5555555555557,1005.0000000000001],"bbox":[314,432,560,504],"page":"3"},"8":{"crop_coord":[874.9998813888889,1698.6667291666665,1554.3331911111113,1829.0000405555554],"bbox":[316.7999573,135.3599854,557.7599488000001,178.6799775],"page":"3"},"9":{"crop_coord":[920,721.3334486111111,1509.9999661111112,1069.0001086111113],"bbox":[333,408.9599609,541.7999878,530.5199585],"page":"4"},"10":{"crop_coord":[145,158.8888888888889,1566.111111111111,577.2222222222222],"bbox":[54,586,562,733],"page":"4"},"11":{"crop_coord":[867.2222222222222,503.33333333333337,1227.2222222222222,777.2222222222222],"bbox":[314,514,440,609],"page":"5"},"12":{"crop_coord":[1203.3333333333333,503.33333333333337,1563.3333333333333,777.2222222222222],"bbox":[435,514,561,609],"page":"5"},"13":{"crop_coord":[867.2222222222222,872.7777777777778,1227.2222222222222,1107.7777777777778],"bbox":[314,395,440,476],"page":"5"},"14":{"crop_coord":[1203.3333333333333,872.7777777777778,1563.3333333333333,1107.7777777777778],"bbox":[435,395,561,476],"page":"5"},"15":{"crop_coord":[867.2222222222222,1092.2222222222222,1227.2222222222222,1327.2222222222222],"bbox":[314,316,440,397],"page":"5"},"16":{"crop_coord":[1203.3333333333333,1092.2222222222222,1563.3333333333333,1327.2222222222222],"bbox":[435,316,561,397],"page":"5"},"17":{"crop_coord":[867.2222222222222,1422.7777777777778,1227.2222222222222,1655],"bbox":[314,198,440,278],"page":"5"},"18":{"crop_coord":[1203.3333333333333,1422.7777777777778,1563.3333333333333,1655],"bbox":[435,198,561,278],"page":"5"},"19":{"crop_coord":[867.2222222222222,1639.4444444444443,1227.2222222222222,1874.4444444444443],"bbox":[314,119,440,200],"page":"5"},"20":{"crop_coord":[1203.3333333333333,1639.4444444444443,1563.3333333333333,1874.4444444444443],"bbox":[435,119,561,200],"page":"5"},"21":{"crop_coord":[139.44444444444446,458.88888888888886,832.7777777777778,1116.1111111111109],"bbox":[52,392,298,625],"page":"6"},"22":{"crop_coord":[874.9999661111112,165.00003388888908,1554.999915277778,437.6666938888891],"bbox":[316.7999878,636.2399902,557.9999695,730.7999877999999],"page":"7"}}}},{"filename":"3173574.3173793","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173793.pdf","paper_id":"3173574.3173793","venue":"CHI 18","keywords":["Augmented Reality","IoT","Smart Environment","Spatial Interactions","Context Awareness","Localization","UWB","SLAM"],"doi":"10.1145/3173574.3173793","paragraph_containing_keyword":"Author Keywords\nAugmented Reality; IoT; Smart Environment; Spatial\nInteractions; Context Awareness; Localization; UWB; SLAM","paragraph_after_keyword":"INTRODUCTION\nThe ecology of connected smart devices is being rapidly in-\nterwoven with people’s daily lives and work environments.\nPeople envision that their surrounding physical world will\nlargely be enhanced with ubiquitous computing [31]. How-\never, accessing and interacting with the Internet of Things\n(IoT) remains challenging due to the increasing diversity and\ncomplexity of the connected devices [6]. Traditionally, the\ndigital interfaces of the interactive devices have been realized\nwith a self-equipped touch screen display which has a limited\nadaptability. But now, contemporary IoT devices allow users","sections":[{"word_count":695,"figure_citations":{"1":["Figure 1), an AR system which provides fast estimation of the 3D locations of smart things and exploits the spatial relationships for location aware interactions."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":696,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1484,"figure_citations":{},"section_index":2,"title":"SCENARIOT"},{"word_count":915,"figure_citations":{"3":["Figure 3, IoT controllers are deployed to smart things as well as to the AR device.","Figure 3, the overall size of the board is 100mm × 100mm × 20mm with the units installed in position."]},"section_index":3,"title":"IMPLEMENTATION"},{"word_count":1425,"figure_citations":{"4":["Figure 4, we conducted 9 surveyings to collect the surveying data with r ∈ {2, 3, 5}m and n ∈ {1, 2, 4}."],"5":["Figure 5 (right), we observed a mean error of 0."],"6":["Figure 6 (left), the mean error for {1."],"7":["Figure 7, with the condition of r = 5m, n = 2 and n = 4 presented a larger error (> 0."]},"section_index":4,"title":"TECHNICAL EVALUATION"},{"word_count":1730,"figure_citations":{"1":["Figure 1 and Figure 12, we not only visualize the digital interfaces when the corresponding physical object is located inside the view, but also the ones outside."],"9":["Figure 9, the overall average error decreased to 0.","Figure 9, the average of the localization error over all 8 devices yielded 0.","Figure 9, we suspect that the accuracy degradation was not just caused by the localization accuracy."],"10":["Figure 10, we observed an average of 0."]},"section_index":5,"title":"TASK EVALUATION"},{"word_count":440,"figure_citations":{"14":["Figure 14 (a).","Figure 14 (b)).","Figure 14 (c, d))."]},"section_index":6,"title":"EXAMPLE USE CASES"},{"word_count":248,"figure_citations":{},"section_index":7,"title":"DISCUSSION AND FUTURE WORK"},{"word_count":354,"figure_citations":{},"section_index":8,"title":"LIMITATION"},{"word_count":123,"figure_citations":{},"section_index":9,"title":"CONCLUSION"},{"word_count":65,"figure_citations":{},"section_index":10,"title":"ACKNOWLEDGEMENT"},{"word_count":276,"figure_citations":{},"section_index":11,"title":"REFERENCES"},{"word_count":1196,"figure_citations":{},"section_index":12,"title":"ELECTRICAL ENGINEERING AND COMPUTER"}],"title":"Scenariot: Spatially Mapping Smart Things Within Augmented Reality Scenes","authors":"Ke Huo, Yuanzhi Cao, Sang Ho Yoon, Zhuangying Xu, Guiming Chen, Karthik Ramani","abstract":"The emerging simultaneous localizing and mapping (SLAM) based tracking technique allows the mobile AR device spatial awareness of the physical world. Still, smart things are not fully supported with the spatial awareness in AR. Therefore, we present Scenariot, a method that enables instant discovery and localization of the surrounding smart things while also spatially registering them with a SLAM based mobile AR system. By exploiting the spatial relationships between mobile AR systems and smart things, Scenariot fosters in-situ interactions with connected devices. We embed Ultra-Wide Band (UWB) RF units into the AR device and the controllers of the smart things, which allows for measuring the distances between them. With a one-time initial calibration, users localize multiple IoT devices and map them within the AR scenes. Through a series of experiments and evaluations, we validate the localization accuracy as well as the performance of the enabled spatial aware interactions. Further, we demonstrate various use cases through Scenariot.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. Scenariot is a method for discovering and localizing IoT devices with a SLAM-based AR device. We register the discovered devices spatially\nin the AR scene to enable new spatial aware interactions.\n","bbox":[53.929,369.17227,564.4375800000007,386.10927],"page":"1"},"2":{"caption":"Figure 2. Scenariot localization principle.\n","bbox":[372.709,582.53127,512.55031,590.50127],"page":"3"},"3":{"caption":"Figure 3. Overview of the Scenariot hardware. Deploy IoT controller\nboards (right) to IoT devices and AR device (left).\n","bbox":[53.928999999999974,587.7302699999999,297.17015999999995,604.6662699999999],"page":"5"},"4":{"caption":"Figure 4. Technical evaluation setup.\n","bbox":[113.107,634.6152699999999,237.82156,642.5852699999999],"page":"6"},"5":{"caption":"Figure 5. Effect of sampling space on the localization accuracy:\n(left)assume a cubic volume, (right) varying h and set l = w = 1.6(m).\n","bbox":[320.831,599.8074799999999,565.4860800000001,616.80027],"page":"6"},"6":{"caption":"Figure 6. Effect of sampling Number (m) on the localization accuracy.\n","bbox":[57.368,606.83727,293.5589500000002,614.93479],"page":"7"},"7":{"caption":"Figure 7. Effect of sampling distances (r) and number of devices (n) on\nthe localization accuracy.\n","bbox":[321.09399999999994,599.8632699999999,564.16129,616.92779],"page":"7"},"8":{"caption":"Figure 8. Task evaluation setup with 8 IoT devices in an ofﬁce.\n","bbox":[337.047,309.05127,548.2121500000002,317.02126999999996],"page":"7"},"9":{"caption":"Figure 9. Localization accuracy study with users. Runtime: runtime lo-\ncalization result. Height Correction: results with height correction. Dis-\ntance: the distances of IoT devices to the center of the surveying space.\n","bbox":[53.929,380.97027,298.3210800000002,406.87327],"page":"8"},"10":{"caption":"Figure 10. Distant pointing accuracy and completion time.\n","bbox":[343.614,582.53127,541.64459,590.50127],"page":"8"},"11":{"caption":"Figure 11. Proximity based control accuracy and completion time.\n","bbox":[63.206,582.53127,287.7209000000002,590.50127],"page":"9"},"12":{"caption":"Figure 12. Discoverable World. The digital representations of the discov-\nered IoT devices are visualized within the AR scene with spatial PiPs.\n","bbox":[321.094,578.5962699999999,565.4860800000001,595.5322699999999],"page":"9"},"13":{"caption":"Figure 13. Proximity based Control. While users move closer to the\nmachine (a, b, c), the level of engagement is adjusted accordingly.\n","bbox":[321.094,119.41927,564.1630600000002,136.35527],"page":"9"},"14":{"caption":"Figure 14. Monitoring the IoT assets (a, b) and navigating the user to-\nwards the assets by visualizing the direction on the screen(c, d).\n","bbox":[53.642,607.5412699999999,298.3210800000001,624.47727],"page":"10"},"15":{"caption":"Figure 15. User creates a miniature world of the physical environment\nenhanced by the digital interfaces of IoT devices.\n","bbox":[53.929,212.32926999999998,296.9980600000002,229.26627],"page":"10"}},"crops":{"1":{"crop_coord":[144.81668249999998,593.575645,1572.1138825000003,1064.1972349999999],"bbox":[53.9340057,410.6889954,564.1609977,576.5127678],"page":"1"},"2":{"crop_coord":[886.9277613888887,168.21391638888886,1572.127778611111,515.8139038888888],"bbox":[321.0939941,608.1069946,564.1660003,729.6429901],"page":"3"},"3":{"crop_coord":[144.80278027777777,168.24998638888886,829.8027802777777,482.00000333333315],"bbox":[53.9290009,620.2799988,296.9290009,729.6300049],"page":"5"},"4":{"crop_coord":[144.80278027777777,168.2321419444442,829.96677,371.13612694444436],"bbox":[53.9290009,660.1909943,296.9880372,729.6364289],"page":"6"},"5":{"crop_coord":[886.9277613888887,168.2144249999999,1572.127778611111,448.29444027777754],"bbox":[321.0939941,632.4140015,564.1660003,729.6428070000001],"page":"6"},"6":{"crop_coord":[144.80278027777777,168.2144249999999,830.0027974999999,448.29444027777754],"bbox":[53.9290009,632.4140015,297.0010071,729.6428070000001],"page":"7"},"7":{"crop_coord":[886.9277613888887,168.2144249999999,1572.127778611111,448.29444027777754],"bbox":[321.0939941,632.4140015,564.1660003,729.6428070000001],"page":"7"},"8":{"crop_coord":[886.9277613888887,590.2853586111108,1572.1229613888886,1275.4805586111113],"bbox":[321.0939941,334.6269989,564.1642661,577.6972709],"page":"7"},"9":{"crop_coord":[144.80278027777777,683.8250138888887,830.0027974999999,1031.4250013888889],"bbox":[53.9290009,422.4869995,297.0010071,544.022995],"page":"8"},"10":{"crop_coord":[886.9277613888887,168.21391638888886,1572.127778611111,515.8139038888888],"bbox":[321.0939941,608.1069946,564.1660003,729.6429901],"page":"8"},"11":{"crop_coord":[144.80278027777777,168.21391638888886,830.0027974999999,515.8139038888888],"bbox":[53.9290009,608.1069946,297.0010071,729.6429901],"page":"9"},"12":{"crop_coord":[886.9277613888887,168.21222083333328,1572.127778611111,507.3722330555554],"bbox":[321.0939941,611.1459961,564.1660003,729.6436005],"page":"9"},"13":{"crop_coord":[886.9277613888887,1517.093116111111,1572.0737794444442,1782.8639052777776],"bbox":[321.0939941,151.9689941,564.1465605999999,244.04647820000002],"page":"9"},"14":{"crop_coord":[144.80278027777777,168.23142166666662,829.9487983333333,426.9694350000001],"bbox":[53.9290009,640.0910034,296.9815674,729.6366882],"page":"10"},"15":{"crop_coord":[144.80278027777777,1280.7265856666668,829.9584802777777,1524.7805616666667],"bbox":[53.9290009,244.8789978,296.98505289999997,329.13842916],"page":"10"}}}},{"filename":"3173574.3173863","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173863.pdf","paper_id":"3173574.3173863","venue":"CHI 18","keywords":["Computer-mediated communication","Virtual reality","Embodiment","Social presence"],"paragraph_containing_keyword":"Author Keywords\nComputer-mediated communication, virtual reality,\nembodiment, social presence.","paragraph_after_keyword":"INTRODUCTION\nModern communication is frequently mediated by technol-\nogy. Each technology offers its own set of affordances [12],\nand yet it is difﬁcult to match the immediacy and richness\noffered through the multimodality of face-to-face communi-\ncation – so much so that the Department of Energy estimates\nthat roughly eight percent of US energy is used to support\npassenger transport to enable face-to-face communication [1].\nThis work explores how embodied virtual reality (VR) can\nsupport communication around a spatial task. Embodied vir-\ntual reality means that a person’s movements are tracked and\nthen used to drive an avatar in a shared virtual world. Using\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI 2018, April 21–26, 2018, Montreal, QC, Canada\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nISBN 978-1-4503-5620-6/18/04. . . $15.00\nDOI: https://doi.org/10.1145/3173574.3173863","doi":"10.1145/3173574.3173863","sections":[{"word_count":689,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1196,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1417,"figure_citations":{},"section_index":2,"title":"METHODS"},{"word_count":2274,"figure_citations":{"2":["Figure 2e.","Figure 2f.","Figure 2a).","Figure 2g.","Figure 2c)."]},"section_index":3,"title":"ANNOTATED PARTICIPANT BEHAVIOR"},{"word_count":313,"figure_citations":{},"section_index":4,"title":"SEMANTIC DIFFERENCE MEASURE OF SOCIAL PRESENCE"},{"word_count":475,"figure_citations":{},"section_index":5,"title":"NETWORKED MINDS MEASURE OF SOCIAL PRESENCE"},{"word_count":1783,"figure_citations":{"2":["Figure 2 the interaction."]},"section_index":6,"title":"PARTICIPANT PREFERENCES AND EXIT INTERVIEWS"},{"word_count":342,"figure_citations":{},"section_index":7,"title":"CONCLUSIONS"},{"word_count":15,"figure_citations":{},"section_index":8,"title":"REFERENCES"},{"word_count":906,"figure_citations":{},"section_index":9,"title":"APPEARANCE TO CREATE ENERGY SAVINGS"},{"word_count":311,"figure_citations":{},"section_index":10,"title":"INDUSTRIAL NEGOTIATION AT THE PLANT"}],"title":"Communication Behavior in Embodied Virtual Reality","authors":"Harrison Jesse Smith, Michael Neff","abstract":"Embodied virtual reality faithfully renders users' movements onto an avatar in a virtual 3D environment, supporting nuanced nonverbal behavior alongside verbal communication. To investigate communication behavior within this medium, we had 30 dyads complete two tasks using a shared visual workspace: negotiating an apartment layout and placing model furniture on an apartment floor plan. Dyads completed both tasks under three different conditions: face-to-face, embodied VR with visible full-body avatars, and no embodiment VR, where the participants shared a virtual space, but had no visible avatars. Both subjective measures of users' experiences and detailed annotations of verbal and nonverbal behavior are used to understand how the media impact communication behavior. Embodied VR provides a high level of social presence with conversation patterns that are very similar to face-to-face interaction. In contrast, providing only the shared environment was generally found to be lonely and appears to lead to degraded communication.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. Dyads performed the ﬁrst (A) and second (B) tasks in the face-\nto-face conditions. In virtual reality conditions, avatars appeared across\nthe table from each other (C), but were actually positioned on opposite\nsides of the motion capture stage (D). In the embodVR condition, partic-\nipants were able to see both avatars (E). In the no_embodVR condition,\nparticipants were unable to see their partner and could only see their\nhands in the second task, to assist with furniture manipulation (F).\n","bbox":[53.929,435.69827,298.3210800000002,497.46626999999995],"page":"6"},"2":{"caption":"Figure 2. Sub ﬁgure a shows the average number of gestures performed per minute for each condition. Subﬁgure b shows the percentage of gestures that\nfall into each annotated category (note that, because some gestures ﬁt multiple categories, totals for each condition can add up to over 100%). Subﬁgure\nc shows the rate and percentage of gestures which introduced novel content into the discussions (for example, point at a location while referring to it by\na referential pronoun). Subﬁgure d shows the mean number of conversational turns taken per minute. Subﬁgure e shows the percent of utterances that\nfall in each annotated category (note that, because some gestures ﬁt multiple categories, totals for each condition can add up to over 100%). Subﬁgure\nf shows the frequencies of the manners by which conversational turns were started. Subﬁgure g shows the ratio of gestures performed by the more\nfrequent gesturer and less frequent gesturer in each dyad. Subﬁgure h shows the mean social presence scores, with standard errors of the mean, as\nmeasured by the semantic difference questionaire. Subﬁgure i shows the most and least favorite conditions, as reported by participants at the end of the\nexperiment. All error bars show standard error of the mean.\n","bbox":[53.929,37.44527000000002,564.439380000001,117.14627],"page":"7"}},"crops":{"1":{"crop_coord":[212.32220111111113,168.2069227777776,762.4971938888889,810.2194383333333],"bbox":[78.2359924,502.1210022,272.6989898,729.6455078],"page":"6"},"2":{"crop_coord":[144.80278027777777,168.2679577777779,1572.5827875,1866.6639030555557],"bbox":[53.9290009,121.8009949,564.3298035,729.6235352],"page":"7"}}}},{"filename":"3173574.3173902","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173902.pdf","paper_id":"3173574.3173902","venue":"CHI 18","keywords":["Games","Audio","Virtual reality","Player experience","Ambient noises","Background music","Sound effects"],"paragraph_containing_keyword":"Author Keywords\ngames; audio; virtual reality; player experience; ambient\nnoises; background music; sound effects.","paragraph_after_keyword":"INTRODUCTION\nSince Facebook bought Oculus for $2 billion dollars in 2014,\nvirtual reality (VR) has been centre-stage in commercial appli-\ncations and research because of its promise to provide a highly\nimmersive audio-visual experience [6]. While researchers\nhave long seen the potential of VR technology for educational\napplications [7, 12] and therapeutic beneﬁts [46, 33], they have\nonly limited knowledge about what facilitates these immersive\nVR experiences, and speciﬁcally how audio contributes to\nthese impressions. Thus, we still lack a complete scientiﬁc\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for proﬁt or commercial advantage and that copies bear\nthis notice and the full citation on the ﬁrst page. Copyrights for components\nof this work owned by others than the author(s) must be honored. Abstracting\nwith credit is permitted. To copy otherwise, or republish, to post on servers\nor to redistribute to lists, requires prior speciﬁc permission and/or a fee.\nRequest permissions from Permissions@acm.org.\nCHI 2018, April 21–26, 2018, Montreal, QC, Canada. ©2018 Copyright\nis held by the owner/author(s). Publication rights licensed to ACM.\nDOI: https://doi.org/10.1145/3173574.3173902","doi":"10.1145/3173574.3173902","sections":[{"word_count":1014,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":6060,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":230,"figure_citations":{},"section_index":2,"title":"IEQ"},{"word_count":2,"figure_citations":{},"section_index":3,"title":"VSA"},{"word_count":2,"figure_citations":{},"section_index":4,"title":"VSM"},{"word_count":701,"figure_citations":{},"section_index":5,"title":"VSAM"},{"word_count":268,"figure_citations":{},"section_index":6,"title":"GENERAL DISCUSSION AND DESIGN GUIDELINES"},{"word_count":463,"figure_citations":{},"section_index":7,"title":"FUTURE WORK"},{"word_count":204,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":60,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGMENTS"},{"word_count":1729,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"Vanishing Importance: Studying Immersive Effects of Game Audio Perception on Player Experiences in Virtual Reality","authors":"Katja Rogers, Giovanni Ribeiro, Rina R. Wehbe, Michael Weber, Lennart E. Nacke","abstract":"Sound and virtual reality (VR) are two important output modalities for creating an immersive player experience (PX). While prior research suggests that sounds might contribute to a more immersive experience in games played on screens and mobile displays, there is not yet evidence of these effects of sound on PX in VR. To address this, we conducted a within-subjects experiment using a commercial horror-adventure game to study the effects of a VR and monitor-display version of the same game on PX. Subsequently, we explored, in a between-subjects study, the effects of audio dimensionality on PX in VR. Results indicate that audio has a more implicit influence on PX in VR because of the impact of the overall sensory experience and that audio dimensionality in VR may not be a significant factor contributing to PX. Based on our findings and observations, we provide five design guidelines for VR games.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. Factors of IEQ for the study comparing monitor-display version of the game to the HMD-VR version. Emotional involvement was signiﬁcantly\nhigher for HMD-VR, while challenge was signiﬁcantly higher for monitor-display (p<0.05).\n","bbox":[53.928900000000056,553.1181799999999,564.4434000000001,570.06215],"page":"6"},"2":{"caption":"Figure 2. Factors of IEQ compared by audio condition showed no signiﬁcant differences (p>0.05) for increased audio dimensionality.\n","bbox":[84.396,560.0911799999999,533.6967800000008,568.0611799999999],"page":"9"}},"crops":{"1":{"crop_coord":[361.347345825,-1387.8102047583332,818.6607413888888,458.9073837361109],"bbox":[131.885044497,628.5933418550001,292.9178669,1289.811673713],"page":"6"},"2":{"crop_coord":[480.0906998388889,-1584.8305555999998,978.4788230305556,316.7376007972221],"bbox":[174.632651942,679.774463713,350.452376291,1360.7390000159999],"page":"6"},"3":{"crop_coord":[441.2494417777778,-1326.917601647222,978.4788230305558,462.4853797583333],"bbox":[160.64979904,627.305263287,350.45237629100006,1267.890336593],"page":"6"},"4":{"crop_coord":[613.262711725,-1642.1562744777782,1244.8311806444444,288.0691854638891],"bbox":[222.574576221,690.095093233,446.33922503200006,1381.376258812],"page":"6"},"5":{"crop_coord":[574.4297875055556,-1497.0779807972222,1244.8284026972224,374.7189152194444],"bbox":[208.59472350200002,658.901190521,446.33822497100005,1329.148073087],"page":"6"},"6":{"crop_coord":[654.3346614055556,-1504.239528736111,1404.646484338889,380.09146514722204],"bbox":[237.36047810600002,656.967072547,503.872734362,1331.726230345],"page":"6"},"7":{"crop_coord":[787.5150071333334,-1487.4023906222228,1671.0099537416668,395.8535376861111],"bbox":[285.30540256800003,651.292726433,599.763583347,1325.664860624],"page":"6"},"8":{"crop_coord":[867.4282148750001,-1521.7878213388885,1830.8252574361113,384.3945053944444],"bbox":[314.074157355,655.417978058,657.297092677,1338.0436156819999],"page":"6"},"9":{"crop_coord":[1039.4414848222223,-1661.2574395777779,2097.180392997222,278.52415880833314],"bbox":[375.99893453600004,693.5313028290001,753.184941479,1388.252678248],"page":"6"},"10":{"crop_coord":[1000.6002267611111,-1408.116998952778,2097.1803929972216,435.0225935194445],"bbox":[362.016081634,637.191866333,753.1849414789999,1297.1221196230001],"page":"6"},"11":{"crop_coord":[1080.5134345027777,-1496.4807221444446,2256.995696691667,403.9734774166667],"bbox":[390.784836421,648.36954813,810.718450809,1328.9330599720001],"page":"6"},"12":{"crop_coord":[1213.6854463888887,-1425.1680390027773,2523.3480543055557,422.0579138333334],"bbox":[438.7267607,641.85915102,906.60529955,1303.260494041],"page":"6"},"13":{"crop_coord":[1293.5986541305558,-1478.3935077805554,2683.1661359472228,413.8685254222223],"bbox":[467.49551548700003,644.807330848,964.1398089410002,1322.421662801],"page":"6"},"14":{"crop_coord":[786.2787939352833,-1819.3586144605736,1601.5389957685186,202.658487694069],"bbox":[284.860365816702,720.8429444301352,574.7540384766667,1445.1691012058066],"page":"6"},"15":{"crop_coord":[933.3516487616814,-1819.3586144605736,1895.6819274741863,202.658487694069],"bbox":[337.80659355420534,720.8429444301352,680.645493890707,1445.1691012058066],"page":"6"},"16":{"crop_coord":[361.347345825,-1434.2435925777775,778.7055264916668,418.97994831111123],"bbox":[131.885044497,642.9672186079999,278.533989537,1306.527693328],"page":"9"},"17":{"crop_coord":[401.2942268805555,-1462.303637469444,858.6076224444445,418.97994831111123],"bbox":[146.265921677,642.9672186079999,307.29874408,1316.629309489],"page":"9"},"18":{"crop_coord":[441.2494417777778,-1399.147007372222,938.5208301861113,457.5823029111111],"bbox":[160.64979904,629.070370952,336.06749886700004,1293.892922654],"page":"9"},"19":{"crop_coord":[500.05858447222215,-1686.8478893888885,1018.422926138889,265.7289339027778],"bbox":[181.82109040999998,698.137583795,364.83225341,1397.46524018],"page":"9"},"20":{"crop_coord":[500.05858447222215,-1181.6198501361116,1018.422926138889,518.3443425027776],"bbox":[181.82109040999998,607.196036699,364.83225341,1215.583146049],"page":"9"},"21":{"crop_coord":[481.204656675,-1420.1927355277783,1018.4340379277778,426.00537683611117],"bbox":[175.03367640300002,640.438064339,364.836253654,1301.46938479],"page":"9"},"22":{"crop_coord":[574.4297875055556,-1511.4260782000001,1204.8842995888888,404.95131483888906],"bbox":[208.59472350200002,648.017526658,431.958347852,1334.313388152],"page":"9"},"23":{"crop_coord":[614.38778035,-1497.3946667805556,1284.7863955416667,390.91156957777775],"bbox":[222.979600926,653.071834952,460.723102395,1329.262080041],"page":"9"},"24":{"crop_coord":[654.3346614055556,-1571.0713830083334,1364.6912694416667,347.0588947277777],"bbox":[237.36047810600002,668.858797898,489.488856999,1355.785697883],"page":"9"},"25":{"crop_coord":[694.2898763027778,-1553.5258683527773,1444.601699236111,368.1129567249999],"bbox":[251.744355469,661.2793355790001,518.256611725,1349.469312607],"page":"9"},"26":{"crop_coord":[787.5150071333334,-1563.3486897305554,1631.0519608972222,376.88293610555553],"bbox":[285.30540256800003,658.122143002,585.378705923,1353.005528303],"page":"9"},"27":{"crop_coord":[846.326927775,-1271.4364397250001,1710.9568347972222,473.4360477083334],"bbox":[306.477693999,623.363022825,614.144460527,1247.917118301],"page":"9"},"28":{"crop_coord":[827.4729999777777,-1521.2488995777778,1710.9679465861109,376.88293610555553],"bbox":[299.690279992,658.122143002,614.1484607709999,1337.849603848],"page":"9"},"29":{"crop_coord":[886.2821426722223,-1293.8961430166664,1790.870042538889,462.20480708888886],"bbox":[320.861571362,627.406269448,642.913215314,1256.002611486],"page":"9"},"30":{"crop_coord":[867.4282148750001,-1529.6688576083334,1790.870042538889,368.4629780750001],"bbox":[314.074157355,661.153327893,642.913215314,1340.880788739],"page":"9"},"31":{"crop_coord":[926.2373575694445,-1675.6249826111111,1870.7721384916667,271.34038729166673],"bbox":[335.245448725,696.117460575,671.677969857,1393.42499374],"page":"9"},"32":{"crop_coord":[926.2373575694445,-1653.1680572666667,1870.7721384916667,282.5688499638888],"bbox":[335.245448725,692.075214013,671.677969857,1385.3405006159999],"page":"9"},"33":{"crop_coord":[907.3750959305555,-1526.8603529666664,1870.7721384916667,360.0430200444446],"bbox":[328.455034535,664.1845127839999,671.677969857,1339.869727068],"page":"9"},"34":{"crop_coord":[1019.4624884,-1162.9048197000002,2057.2224001527775,527.7004687472222],"bbox":[368.806495824,603.827831251,738.800064055,1208.845735092],"page":"9"},"35":{"crop_coord":[1000.6002267611111,-1462.300859522222,2057.2224001527775,404.95131483888906],"bbox":[362.016081634,648.017526658,738.800064055,1316.628309428],"page":"9"},"36":{"crop_coord":[1040.5582196055557,-1415.528562141667,2137.138385841666,447.0511049916667],"bbox":[376.400959058,632.861602203,767.5698189029999,1299.790282371],"page":"9"},"37":{"crop_coord":[1099.3673623,-1742.984646855556,2217.0404817944445,237.6605551694443],"bbox":[397.572250428,708.242200139,796.334573446,1417.674472868],"page":"9"},"38":{"crop_coord":[1080.5134345027777,-1509.0814907444446,2217.0404817944445,376.88293610555553],"bbox":[390.784836421,658.122143002,796.334573446,1333.469336668],"page":"9"},"39":{"crop_coord":[1120.4603155583334,-1499.728142447222,2296.942577747222,404.95131483888906],"bbox":[405.165713601,648.017526658,825.099327989,1330.1021312809999],"page":"9"},"40":{"crop_coord":[1232.5477080277778,-1662.7908664444446,2483.4011732500003,277.757445375],"bbox":[445.51717489,693.807319665,892.2244223700001,1388.80471192],"page":"9"},"41":{"crop_coord":[1213.6854463888887,-1474.3265930472223,2483.3928394083337,392.9255813138888],"bbox":[438.7267607,652.346790727,892.221422187,1320.957573497],"page":"9"},"42":{"crop_coord":[1253.6434392333333,-1450.2751259972215,2563.3060471500003,406.95421478611127],"bbox":[453.111638124,647.296482677,920.9901769740001,1312.2990453589998],"page":"9"},"43":{"crop_coord":[1293.5986541305558,-1494.386149938889,2643.2192548916664,418.97994831111123],"bbox":[467.49551548700003,642.9672186079999,949.758931761,1328.179013978],"page":"9"},"44":{"crop_coord":[1333.5538690277776,-1482.3493046249996,2723.1213508444444,406.95421478611127],"bbox":[481.87939285,647.296482677,978.523686304,1323.8457496649999],"page":"9"},"45":{"crop_coord":[790.8929641157915,-1819.3586144605736,1610.764558182406,202.658487694069],"bbox":[286.52146708168493,720.8429444301352,578.0752409456662,1445.1691012058066],"page":"9"},"46":{"crop_coord":[836.3401791387383,-1819.3586144605736,1701.6589882283001,202.658487694069],"bbox":[302.8824644899458,720.8429444301352,610.797235762188,1445.1691012058066],"page":"9"},"47":{"crop_coord":[893.7547903914441,-1819.3586144605736,1816.4882107337114,202.658487694069],"bbox":[323.55172454091985,720.8429444301352,652.1357558641361,1445.1691012058066],"page":"9"},"48":{"crop_coord":[954.1473609659466,-1819.3586144605736,1937.2733518827167,202.658487694069],"bbox":[345.29304994774077,720.8429444301352,695.618406677778,1445.1691012058066],"page":"9"}}}},{"filename":"3173574.3173917","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173917.pdf","paper_id":"3173574.3173917","venue":"CHI 18","keywords":["Virtual reality","Design","Lucid dreaming","Introspection","Wellbeing","Positive technologies"],"paragraph_containing_keyword":"ABSTRACT \nVirtual  reality  (VR)  is  resurging  in  popularity  with  the \nadvancement  of  low-cost  hardware  and  more  realistic \ngraphics.  How  might  this  technology  help  others? That  is, \nto increase mental well-being? The ultimate VR might look \nlike  lucid  dreaming,  the  phenomenon  of  knowing  one  is \ndreaming while in the dream. Lucid dreaming can be used \nas  an  introspective  tool  and,  ultimately,  increase  mental \nwell-being.  What  these  introspective  experiences  are  like \nfor  lucid  dreamers  might  be  key  in  determining  specific \ndesign guidelines for future creation of a technological tool \nused  for  helping  people  examine  their  own  thoughts  and \nemotions.  This  study  describes  nine  active  and  proficient \nlucid  dreamers’  representations  of \nintrospective \nexperiences  gained  through  phenomenological  interviews. \nFour  major  themes  emerged:  sensations  and  feelings, \nactions  and  practices,  influences  on  experience,  and \nmeaning  making.  This  knowledge  can  help  design  a  VR \nsystem  that  is  grounded \nin  genuine  experience  and \npreserving the human condition.  \nAuthor Keywords \nVirtual reality, design, lucid dreaming, introspection, well-\nbeing, positive technologies. \nACM Classification Keywords \nH.5.m. Information interfaces and presentation (e.g., HCI): \nMiscellaneous  \nINTRODUCTION \nTechnology  is  becoming  more  and  more  ingrained  in  our \nevery day lives, yet it is only recently that HCI researchers, \ntechnologists and designers have paused to think about how \nthese  technologies  could  be  used  to  better  our  lives  and \nincrease  our  well-being  [2,23,53,61].  While  many  mobile \napplications  designed  for  this  purpose  exist  [43],  one  area \nthat  is  still  largely  unexplored  is  virtual  reality  (VR).  VR \nlow-cost  and  high  quality  simulation  of \noffers  a","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for personal \nor  classroom  use  is  granted  without  fee  provided  that  copies  are  not  made  or \ndistributed for profit or commercial advantage and that copies bear this notice \nand the full citation on the first page. Copyrights for components of this work \nowned  by  others  than  ACM  must  be  honored.  Abstracting  with  credit  is \npermitted. To copy otherwise, or republish, to post on servers or to redistribute \nto  lists,  requires  prior  specific  permission  and/or  a  fee.  Request  permissions \nfrom permissions@acm.org. \nCHI 2018, April 21–26, 2018, Montréal, QC, Canada.  \n© 2018 ACM ISBN 978-1-4503-5620-6/18/04…$15.00  \nDOI: https://doi.org/10.1145/3173574.3173917","doi":"10.1145/3173574.3173917","sections":[{"word_count":591,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1031,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1053,"figure_citations":{},"section_index":2,"title":"STUDY METHODOLOGY"},{"word_count":4916,"figure_citations":{},"section_index":3,"title":"RESULTS"},{"word_count":1453,"figure_citations":{},"section_index":4,"title":"DISCUSSION"},{"word_count":166,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":12,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGEMENTS"},{"word_count":1805,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Are You Dreaming?: A Phenomenological Study on Understanding Lucid Dreams as a Tool for Introspection in Virtual Reality","authors":"Alexandra Kitson, Thecla Schiphorst, Bernhard E. Riecke","abstract":"Virtual reality (VR) is resurging in popularity with the advancement of low-cost hardware and more realistic graphics. How might this technology help others? That is, to increase mental well-being? The ultimate VR might look like lucid dreaming, the phenomenon of knowing one is dreaming while in the dream. Lucid dreaming can be used as an introspective tool and, ultimately, increase mental well-being. What these introspective experiences are like for lucid dreamers might be key in determining specific design guidelines for future creation of a technological tool used for helping people examine their own thoughts and emotions. This study describes nine active and proficient lucid dreamers' representations of their introspective experiences gained through phenomenological interviews. Four major themes emerged: sensations and feelings, actions and practices, influences on experience, and meaning making. This knowledge can help design a VR system that is grounded in genuine experience and preserving the human condition.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{},"crops":{}}},{"filename":"3173574.3173919","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173919.pdf","paper_id":"3173574.3173919","venue":"CHI 18","keywords":["Virtual reality","Text entry","Physical keyboard","Hands"],"paragraph_containing_keyword":"Author Keywords\nVirtual reality, Text entry, Physical keyboard, Hands","paragraph_after_keyword":"INTRODUCTION\nHead-mounted displays (HMDs) for Virtual Reality (VR) are\nﬁnally available for the consumer market. Today, consumers\nmainly use VR for entertainment applications including 3D\nmovies and games [16]. A large ﬁeld of view (FOV), high\nvisual ﬁdelity as well as the visual and auditory encapsulation\ncan create truly immersive experiences with almost unlimited\nopportunities. Gamepads and novel tracked input devices are\nused to interact with the application or game.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI 2018, April 21–26, 2018, Montreal, QC, Canada\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nISBN 978-1-4503-5620-6/18/04. . . $15.00\nDOI: https://doi.org/10.1145/3173574.3173919","doi":"10.1145/3173574.3173919","sections":[{"word_count":698,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":477,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":280,"figure_citations":{"1":["Figure 1: Side by side illustration of the real environment (left) and the virtual reality replica (right)."]},"section_index":2,"title":"REALIZING TYPING IN VIRTUAL REALITY"},{"word_count":513,"figure_citations":{"2":["Figure 2: Hand with 23 retroreflective markers (left) and the hardware setup for finger and keyboard tracking (right)."]},"section_index":3,"title":"IMPLEMENTATION"},{"word_count":708,"figure_citations":{"3":["Figure 3: Pictures of the eight hand visualizations used in the study."]},"section_index":4,"title":"METHOD"},{"word_count":404,"figure_citations":{},"section_index":5,"title":"RESULTS"},{"word_count":595,"figure_citations":{"4":["Figure 4: Mean values of words per minute and corrected error rate for each condition."]},"section_index":6,"title":"WPM"},{"word_count":8,"figure_citations":{},"section_index":7,"title":"TRANSPARENCY"},{"word_count":5,"figure_citations":{},"section_index":8,"title":"REAL WORLD"},{"word_count":66,"figure_citations":{},"section_index":9,"title":"M"},{"word_count":1204,"figure_citations":{"5":["Figure 5: Subjective assessments of task load and presence."]},"section_index":10,"title":"SD"},{"word_count":801,"figure_citations":{},"section_index":11,"title":"DISCUSSION"},{"word_count":36,"figure_citations":{},"section_index":12,"title":"ACKNOWLEDGMENTS"},{"word_count":132,"figure_citations":{},"section_index":13,"title":"REFERENCES"},{"word_count":839,"figure_citations":{},"section_index":14,"title":"CONCLUSION"}],"title":"Physical Keyboards in Virtual Reality: Analysis of Typing Performance and Effects of Avatar Hands","authors":"Pascal Knierim, Valentin Schwind, Anna Maria Feit, Florian Nieuwenhuizen, Niels Henze","abstract":"Entering text is one of the most common tasks when interacting with computing systems. Virtual Reality (VR) presents a challenge as neither the user's hands nor the physical input devices are directly visible. Hence, conventional desktop peripherals are very slow, imprecise, and cumbersome. We developed a apparatus that tracks the user's hands, and a physical keyboard, and visualize them in VR. In a text input study with 32 participants, we investigated the achievable text entry speed and the effect of hand representations and transparency on typing performance, workload, and presence. With our apparatus, experienced typists benefited from seeing their hands, and reach almost outside-VR performance. Inexperienced typists profited from semi-transparent hands, which enabled them to type just 5.6 WPM slower than with a regular desktop setup. We conclude that optimizing the visualization of hands in VR is important, especially for inexperienced typists, to enable a high typing performance.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":2},"fig":{"captions":{"1":{"caption":"Figure 1: Side by side illustration of the real environment (left)\nand the virtual reality replica (right).\n","bbox":[321.094,75.153992,564.8457691,96.075992],"page":"2"},"2":{"caption":"Figure 2: Hand with 23 retroreﬂective markers (left) and the\nhardware setup for ﬁnger and keyboard tracking (right).\n","bbox":[53.929,259.18799199999995,297.004819402,280.108992],"page":"3"},"3":{"caption":"Figure 3: Pictures of the eight hand visualizations used in the study. Realistic, abstract, ﬁngertips with no transparency and real\nhands (1st row) as well as 50% transparency and no hands (2nd row).\n","bbox":[53.929,543.308992,564.1887475359997,564.230992],"page":"4"},"4":{"caption":"Figure 4: Mean values of words per minute and corrected error rate for each condition. Error bars show standard error of the\nmean (SE). Exact values are also listed in Table 1.\n","bbox":[53.929,365.29799199999997,564.1862368599994,386.219992],"page":"5"},"5":{"caption":"Figure 5: Subjective assessments of task load and presence. Error bars show standard error of the mean (SE).\n","bbox":[91.719,375.25999199999995,526.3935839999998,385.222992],"page":"7"}},"crops":{"1":{"crop_coord":[886.9277613888887,1450.4671816666664,1572.1477422222222,1910.8389027777778],"bbox":[321.0939941,105.897995,564.1731872,268.0318146],"page":"2"},"2":{"crop_coord":[144.80278027777777,1029.8730724999998,439.9216886111111,1399.6333144444443],"bbox":[53.9290009,289.9320068,156.5718079,419.44569390000004],"page":"3"},"3":{"crop_coord":[436.82501888888896,1030.0173102777776,806.4410230555557,1399.6333144444443],"bbox":[159.0570068,289.9320068,288.51876830000003,419.39376830000003],"page":"3"},"4":{"crop_coord":[159.74723805555556,168.22836972222234,516.9822183333333,386.56945972222246],"bbox":[59.3090057,654.6349945,184.3135986,729.6377868999999],"page":"4"},"5":{"crop_coord":[513.9027575,168.22836972222234,871.1377377777776,386.56945972222246],"bbox":[186.8049927,654.6349945,311.8095856,729.6377868999999],"page":"4"},"6":{"crop_coord":[868.0583191666666,168.22836972222234,1225.2932994444445,386.56945972222246],"bbox":[314.3009949,654.6349945,439.3055878,729.6377868999999],"page":"4"},"7":{"crop_coord":[1222.2138808333332,168.22836972222234,1579.4488611111112,386.56945972222246],"bbox":[441.7969971,654.6349945,566.80159,729.6377868999999],"page":"4"},"8":{"crop_coord":[159.74723805555556,382.93114555555553,516.9822183333333,601.2722355555554],"bbox":[59.3090057,577.3419952,184.3135986,652.3447876],"page":"4"},"9":{"crop_coord":[513.9027575,382.93114555555553,871.1377377777776,601.2722355555554],"bbox":[186.8049927,577.3419952,311.8095856,652.3447876],"page":"4"},"10":{"crop_coord":[868.0583191666666,382.93114555555553,1225.2932994444445,601.2722355555554],"bbox":[314.3009949,577.3419952,439.3055878,652.3447876],"page":"4"},"11":{"crop_coord":[1222.2138808333332,382.93474833333323,1579.4430541666668,601.2722355555554],"bbox":[441.7969971,577.3419952,566.7994995,652.3434906],"page":"4"},"12":{"crop_coord":[144.80278027777777,168.22821043750005,1572.2075195013888,612.0999994444445],"bbox":[53.9290009,573.4440002,564.1947070205,729.6378442425],"page":"5"},"13":{"crop_coord":[144.80278027777777,604.8639694444443,1572.2099802777777,1146.3916694444445],"bbox":[53.9290009,381.098999,564.1955929,572.448971],"page":"5"},"14":{"crop_coord":[144.80278027777777,168.22821043750005,1572.2075195013888,612.0999994444445],"bbox":[53.9290009,573.4440002,564.1947070205,729.6378442425],"page":"7"},"15":{"crop_coord":[144.80278027777777,604.8639694444443,1572.2099802777777,1146.3916694444445],"bbox":[53.9290009,381.098999,564.1955929,572.448971],"page":"7"}}}},{"filename":"3173574.3173927","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503 Design Project 1/_PaperVis/papers/CHI 18/3173574.3173927.pdf","paper_id":"3173574.3173927","venue":"CHI 18","keywords":["H.5.2. Information interfaces and presentation: Input devices and strategies","Interaction styles"],"paragraph_containing_keyword":"ABSTRACT\nThe latest generations of smartphones with built-in AR ca-\npabilities enable a new class of mobile apps that merge dig-\nital and real-world content depending on a user’s task, con-\ntext, and preference. But even experienced mobile app de-\nsigners face signiﬁcant challenges: creating 2D/3D AR con-\ntent remains difﬁcult and time-consuming, and current mobile\nprototyping tools do not support AR views. There are sepa-\nrate tools for this; however, they require signiﬁcant technical\nskill. This paper presents ProtoAR which supplements rapid\nphysical prototyping using paper and Play-Doh with new mo-\nbile cross-device multi-layer authoring and interactive cap-\nture tools to generate mobile screens and AR overlays from\npaper sketches, and quasi-3D content from 360◦ captures of\nclay models. We describe how ProtoAR evolved over four\ndesign jams with students to enable interactive prototypes of\nmobile AR apps in less than 90 minutes, and discuss the ad-\nvantages and insights ProtoAR can give designers.\nAuthor Keywords\nmobile augmented reality; physical-digital prototyping;\nquasi-3D 360◦ captures.\nACM Classiﬁcation Keywords\nH.5.2. Information interfaces and presentation: Input devices\nand strategies, Interaction styles.\nINTRODUCTION\nThis work is driven by the vision of a future in which design-\ners, not only experienced developers, will be able to create\naugmented reality (AR) interfaces. Previously, special hard-\nware and extensive instrumentation of user and environment\nwere required, but the latest technologies like Apple ARKit\nand Google ARCore enable AR on existing smartphones.\nThis facilitates a new generation of mobile interfaces that are\nno longer limited to apps living in a purely digital form on\nthe device. Instead, the device becomes a lens into the virtual\nworld, making use of a user’s physical environment to aug-\nment their view with digital content. For example, IKEA’s\nPlace app gives users previews of how new furniture ﬁts into\ntheir homes before committing to the purchase and assembly.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for components of this work owned by others than\nACM must be honored. Abstracting with credit is permitted. To copy otherwise,\nor republish, to post on servers or to redistribute to lists, requires prior speciﬁc\npermission and/or a fee. Request permissions from permissions@acm.org.\nCHI 2018, April 21–26, 2018, Montr´eal, QC, Canada.\nCopyright is held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-5620-6/18/04...$15.00.\nhttps://doi.org/10.1145/3173574.3173927","doi":"10.1145/3173574.3173927","paragraph_after_keyword":"This research considers the signiﬁcance of these develop-\nments and the new need for more AR designers. The main\nproblem is that interaction designers, who in today’s world\nplay a key role in user experience design, are not equipped\nwith the toolbox to design tomorrow’s AR interfaces. A\nsigniﬁcant part of the problem is that existing research has\nmostly focused on technical aspects of AR as a new technol-\nogy [2, 3, 25, 40]. AR as a new medium for interaction de-\nsigners has received much less attention [24]. These design-\ners are guided by principles such as affordances, mappings,\nand constraints [30]. But in this new design world that mixes\nthe physical and the digital, there are no established design\nrules and only a limited understanding of the kinds of interac-\ntions that users would ﬁnd intuitive and natural [32]. We are\nnot the ﬁrst to recognize this need for AR tools to empower\ninteraction designers, but other than DART [24] there are not\nmany good examples. Even advanced commercial tools—\nInVision, Sketch, and Adobe XD—do not provide access to\nphone cameras as a basic requirement for mobile AR.\nExisting research has focused on new digital tools like DART\nto create AR interfaces without programming. However,\ncreating 2D/3D digital content for AR remains difﬁcult and\ntime-consuming [10]. When designing mobile apps, paper\nprototypes [33, 35] are typically the starting point, but this\nseems too limiting for AR [13]. To address this, we explore\nhow working with modeling compounds like Play-Doh could\ncomplement paper prototyping to create props that could later\nbe substituted with higher quality digital 3D content.\nThis paper presents the ProtoAR tool designed to investi-\ngate lightweight and creative ways to quickly transition from\nphysical to digital prototyping of mobile AR apps. There are\ntwo key innovations in ProtoAR: (i) cross-device multi-layer\nauthoring tools for live editing of mobile AR apps on phones;\n(ii) interactive capture tools to generate mobile screens or AR\noverlays from paper sketches, and 3D models from Play-Doh.\nWe offer three main contributions with ProtoAR:\n• the design and study of ProtoAR as a mobile AR live proto-\ntyping tool that extends cross-device authoring techniques\n[6, 11, 12, 27, 28, 29] to AR interface design;\n• simple and fast techniques for digitizing paper mockups\nand Play-Doh models without the need for special hard-\nware or expensive algorithms for 3D scanning [14];\n• initial explorations of the design space of mobile AR apps\nthat can be developed using a tool like ProtoAR and how it\ncould expand to more advanced prototypes in the future."},"fig":null},{"filename":"3173574.3173937","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173937.pdf","paper_id":"3173574.3173937","venue":"CHI 18","keywords":["Mixed Reality","Spatial Augmented Reality","Virtual Reality","Evaluation","Quantitative Methods"],"paragraph_containing_keyword":"Author Keywords\nMixed Reality; Spatial Augmented Reality; Virtual Reality;\nEvaluation; Quantitative Methods\n*email: FirstName[-MiddleName].LastName@inria.fr","paragraph_after_keyword":"Publication rights licensed to ACM. ACM acknowledges that this contribution was\nauthored or co-authored by an employee, contractor or afﬁliate of a national govern-\nment. As such, the Government retains a nonexclusive, royalty-free right to publish or\nreproduce this article, or to allow others to do so, for Government purposes only.\nCHI’18, April 21–26, 2018, Montreal, QC, Canada\nACM ISBN 978-1-4503-5620-6/18/04. . . $15.00\nDOI: 10.1145/3173574.3173937","doi":"10.1145/3173574.3173937","sections":[{"word_count":493,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":739,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1377,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2)."],"3":["Figure 3-left), and to confirm the estimation by pressing controller’s trigger (using the soft \"hair-trigger\" trigger mode to mitigate unintended movement when clicking).","Figure 3-right)."],"7":["Figure 7)."]},"section_index":2,"title":"STUDY DESIGN"},{"word_count":23,"figure_citations":{},"section_index":3,"title":"CONDITION"},{"word_count":4050,"figure_citations":{"3":["Figure 3)."],"4":["Figure 4)."],"5":["Figure 5 presents the error distribution per region and condition."],"6":["Figure 6).","Figure 6)."],"7":["Figure 7).","Figure 7, 7-Likert scale) and the comments obtained during the interview were similar."],"8":["Figure 8-left).","Figure 8-left).","Figure 8)."],"9":["Figure 9-top).","Figure 9bottom)."],"10":["Figure 10).","Figure 10 and Figure 6: both studies present similar distributions from the egocentric perspective, yet there is a shift towards zero for the Study 2."],"11":["Figure 11)."],"12":["Figure 12).","Figure 12).","Figure 12) shows that both conditions present positive mean scores (MR: 1."]},"section_index":4,"title":"SAR"},{"word_count":898,"figure_citations":{"13":["Figure 13 show three cases of distances between target and estimation.","Figure 13)."]},"section_index":5,"title":"DISCUSSION"},{"word_count":388,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":15,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGMENTS"},{"word_count":1445,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Understanding Users' Capability to Transfer Information between Mixed and Virtual Reality: Position Estimation across Modalities and Perspectives","authors":"Joan Sol Roo, Jean Basset, Pierre-Antoine Cinquin, Martin Hachet","abstract":"Mixed Reality systems combine physical and digital worlds, with great potential for the future of HCI. It is possible to design systems that support flexible degrees of virtuality by combining complementary technologies. In order for such systems to succeed, users must be able to create unified mental models out of heterogeneous representations. In this paper, we present two studies focusing on the users' accuracy on heterogeneous systems using Spatial Augmented Reality (SAR) and immersive Virtual Reality (VR) displays, and combining viewpoints (egocentric and exocentric). The results show robust estimation capabilities across conditions and viewpoints.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. Setup and Tasks involved in the studies presented in this paper. Participants interacted with a spatially augmented mock-up and its virtual\ncounterpart (left). They were iteratively presented with a spherical target from either egocentric (Study 1) and exocentric perspectives (Study 2), and\nthen asked to estimate the position of the target (right) using an estimator attached to a controller. The objective of this protocol is to quantitatively\nmeasure the participants’ capability to transfer information between physical and virtual spaces, and between egocentric and exocentric perspectives.\n","bbox":[53.929,406.38727,564.4437100000002,441.25726999999995],"page":"1"},"2":{"caption":"Figure 2. Experiment setup: HTC Vive lighthouses were placed in front\nof the participant, while the Optitrack cameras and the LG projector\nwhere placed around and over the participant.\n","bbox":[320.807,452.59527,564.3384000000001,478.49827],"page":"3"},"3":{"caption":"Figure 3. Controller with estimation attached to it (left). Controller\nused to align Optitrack and HTC Vive spaces, markers placed to easily\nidentify origin and orientation of the controller (right).\n","bbox":[321.094,86.96227000000002,564.4420100000001,112.86527000000001],"page":"3"},"4":{"caption":"Figure 4. 48 target locations (left), 6 regions (center), and the 48 targets\nclustered by region (right). The participant (not represented) would be\nplaced at the bottom of the picture.\n","bbox":[53.929,86.96227000000002,296.9980600000001,112.86527000000001],"page":"5"},"5":{"caption":"Figure 5. Absolute error: The effect of region by condition. The charts\npresent both mean with conﬁdence 95% intervals at one sample per-\nparticipant (black), and distribution at one sample per-trial (colored).\n","bbox":[321.094,598.35727,565.4860800000001,624.26027],"page":"5"},"6":{"caption":"Figure 6. Depth error: A positive error in depth implies that the partici-\npants estimated closer to themselves (presented this way for clarity).\n","bbox":[321.094,281.58727,565.4860800000001,298.52326999999997],"page":"5"},"7":{"caption":"Figure 7. Subjective experience based on a 7-Likert scale questionnaire,\nvalues between -3 and 3. Error bars indicate conﬁdence intervals.\n","bbox":[53.730000000000004,588.19827,297.9943100000002,605.13527],"page":"6"},"8":{"caption":"Figure 8. target locations (48) and 6 POVs, regions (6).\n","bbox":[81.84,520.55327,266.61648000000014,528.52327],"page":"8"},"9":{"caption":"Figure 9. Estimation error per condition in comparison with Study 1\n(top), and the tendency towards order effect (bottom).\n","bbox":[53.666,86.96227,297.59594000000016,103.89927],"page":"8"},"10":{"caption":"Figure 10. Depth error: The error per condition for Study 2, computed\nfrom both the egocentric (left) and exocentric (right) viewpoints. Note\nthe distribution around zero, in contrast with Study 1 (see Figure 6).\n","bbox":[321.094,503.77627,564.1630600000002,529.67927],"page":"8"},"11":{"caption":"Figure 11. Workload results from the NASA-TLX questionnaire. Both\nthe results for the ﬁrst and second study are presented.\n","bbox":[321.094,86.96227,564.1630600000002,103.89927],"page":"8"},"12":{"caption":"Figure 12. Results obtained for the subjective experience questionnaire\nfor the second study, using a 7-Likert scale. The results are presented on\na scale between -3 to 3 for clarity.\n","bbox":[53.929,514.73627,296.9980600000001,540.63927],"page":"9"},"13":{"caption":"Figure 13. Three instances of estimation: touching the center of the\ntarget (A), touching the target (B), and the failure threshold (C).\n","bbox":[321.094,86.96227,564.16306,103.89927],"page":"9"}},"crops":{"1":{"crop_coord":[228.62778555555553,582.3768275,1488.2732730555556,966.3583458333334],"bbox":[84.1060028,445.9109955,533.9783783,580.5443421],"page":"1"},"2":{"crop_coord":[893.6833361111111,505.57150083333335,1565.348722777778,862.9083336111112],"bbox":[323.526001,483.1529999,561.7255402000001,608.1942597],"page":"3"},"3":{"crop_coord":[886.9277613888887,1649.604593888889,1572.1377391666665,1878.555543611111],"bbox":[321.0939941,117.5200043,564.1695861,196.3423462],"page":"3"},"4":{"crop_coord":[148.17777,1641.2742274999998,826.6164483333333,1878.555543611111],"bbox":[55.1439972,117.5200043,295.7819214,199.3412781],"page":"5"},"5":{"crop_coord":[890.3027936111112,168.22455499999998,1568.7534669444442,458.0139075],"bbox":[322.3090057,628.9149933,562.9512480999999,729.6391602],"page":"5"},"6":{"crop_coord":[890.3027936111112,1087.9009330555552,1568.7734305555555,1362.8389061111109],"bbox":[322.3090057,303.1779938,562.958435,398.55566410000006],"page":"5"},"7":{"crop_coord":[148.17777,168.22531805555545,826.6401841666667,511.1389075000001],"bbox":[55.1439972,609.7899933,295.7904663,729.6388855],"page":"6"},"8":{"crop_coord":[161.68332416666664,503.68068249999993,813.0828349999999,718.4166716666667],"bbox":[60.0059967,535.1699982,290.9098206,608.8749543],"page":"8"},"9":{"crop_coord":[148.17777,1415.9061516666668,826.5993669444446,1657.5750055555554],"bbox":[55.1439972,197.072998,295.77577210000004,280.4737854],"page":"8"},"10":{"crop_coord":[195.4416572222222,1650.339516388889,779.3680741666667,1903.461108611111],"bbox":[72.1589966,108.5540009,278.7725067,196.0777741],"page":"8"},"11":{"crop_coord":[920.6916469444444,457.42518972222234,1538.3638508333333,720.7388900000001],"bbox":[333.2489929,534.3339996,552.0109863,625.5269317],"page":"8"},"12":{"crop_coord":[890.3027936111112,1660.9172905555554,1568.7257891666666,1903.461108611111],"bbox":[322.3090057,108.5540009,562.9412841,192.26977540000001],"page":"8"},"13":{"crop_coord":[148.17777,168.2300652777777,826.6418797222221,690.2944269444446],"bbox":[55.1439972,545.2940063,295.79107669999996,729.6371765],"page":"9"},"14":{"crop_coord":[954.447225,1533.3614433333335,1504.5967441666667,1903.461108611111],"bbox":[345.401001,108.5540009,539.8548279,238.1898804],"page":"9"}}}},{"filename":"3173574.3173982","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3173982.pdf","paper_id":"3173574.3173982","venue":"CHI 18","keywords":["Feedforward","Exergame","Virtual reality (VR)","Performance","Intrinsic motivation"],"doi":"10.1145/3173574.3173982","paragraph_containing_keyword":"ACM Classiﬁcation Keywords\nApplied computing: Computer games; Applied computing:\nConsumer health; Human-centered computing: Virtual reality\nAuthor Keywords\nFeedforward, exergame, virtual reality (VR), performance,\nintrinsic motivation\nINTRODUCTION\nPhysical inactivity has been identiﬁed as the fourth leading\nIt is well established that a\ncause of death globally [50].\nsedentary lifestyle increases the risk of developing diseases\nsuch as type 2 diabetes and cardiovascular disease [87] which\naccount for 30% of global mortality. The American College\nof Sports Medicine (ACSM) recommends adults should do\nat least 150 minutes of moderate exercise or 75 minutes of\nvigorous exercise per week [63]. However, when people begin\nphysical activity regimens, 40% to 65% are predicted to drop\nout within 3 to 6 months [2, 3, 17]. Intrinsic motivation, i.e.\nmotivation derived from enjoyment and satisfaction gained\nfrom an activity, has been identiﬁed as an important predictor\nof adherence to an exercise program [1, 30, 72]. Indeed, lack\nof time and maintaining motivation are the most commonly\ncited barriers to continuing exercise [26] so tackling these two\nchallenges is key to improving global health.\nHigh-intensity interval training (HIIT) – short intermittent\nbouts of vigorous activity, interspersed with periods of rest or\nlow-intensity exercise [32] – can reduce the time required for\na healthy exercise regime. Studies show that HIIT is equally\nbeneﬁcial or superior to traditional aerobic exercise in many\nﬁtness and health related measures [33, 49, 61]. Participants","sections":[{"word_count":1368,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1119,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":693,"figure_citations":{"1":["Figure 1a.","Figure 1c).","Figure 1d).","Figure 1 b-d), similar to [74].","Figure 1b)."]},"section_index":2,"title":"EXERGAME DESIGN"},{"word_count":1712,"figure_citations":{},"section_index":3,"title":"EXPERIMENTAL DESIGN"},{"word_count":1064,"figure_citations":{"2":["Figure 2 top-left).","Figure 2 top-right).","Figure 2 bottom-left).","Figure 2 bottom-right) showed that SC led to a significantly higher performance, t(22) = 3."],"3":["Figure 3 top-left)."]},"section_index":4,"title":"RESULTS"},{"word_count":26,"figure_citations":{},"section_index":5,"title":"VA"},{"word_count":13,"figure_citations":{},"section_index":6,"title":"NA"},{"word_count":15,"figure_citations":{},"section_index":7,"title":"FA"},{"word_count":8,"figure_citations":{},"section_index":8,"title":"NSC"},{"word_count":823,"figure_citations":{"3":["Figure 3: IMI Interest/Enjoyment (left column) and IMI Pressure/Tension (right column) scores for the baseline (B), equal challenge (E) and harder challenge (H) conditions for 1) self competition (SC) framing in different levels of resistance awareness (top row) and 2) without resistance awareness (NA) in self competition (SC) vs.","Figure 3 bottom-left).","Figure 3 bottomright)."],"4":["Figure 4 top-left).","Figure 4: FSQ Balance of Challenges and Skills (left column) and FSQ Absorption in the Task (right column) scores for the baseline (B), equal challenge (E) and harder challenge (H) conditions for 1) self competition (SC) framing in different levels of resistance awareness (top row) and 2) without resistance awareness (NA) in self competition (SC) vs.","Figure 4 top-right).","Figure 4 bottom-left).","Figure 4 bottom-right)."],"5":["Figure 5: IEQ scores in different levels of resistance awareness (left) and competition framing (right).","Figure 5 left).","Figure 5 right) showed that there was a significant difference between SC and NSC, t(22) = 2."]},"section_index":9,"title":"H"},{"word_count":21,"figure_citations":{},"section_index":10,"title":"E"},{"word_count":375,"figure_citations":{"2":["Figure 2: ∆Power (difference from baseline B) for the equal challenge (E) and harder challenge (H) conditions in different levels of resistance awareness (top-left) and in non-self (NSC) vs."],"3":["Figure 3 top-right)."]},"section_index":11,"title":"SC"},{"word_count":2,"figure_citations":{},"section_index":12,"title":"B"},{"word_count":8,"figure_citations":{},"section_index":13,"title":"IEQ"},{"word_count":1049,"figure_citations":{},"section_index":14,"title":"DISCUSSION"},{"word_count":142,"figure_citations":{},"section_index":15,"title":"CONCLUSION"},{"word_count":77,"figure_citations":{},"section_index":16,"title":"ACKNOWLEDGEMENTS"},{"word_count":2744,"figure_citations":{},"section_index":17,"title":"REFERENCES"}],"title":"Interactive Feedforward for Improving Performance and Maintaining Intrinsic Motivation in VR Exergaming","authors":"Soumya C. Barathi, Daniel J. Finnegan, Matthew Farrow, Alexander Whaley, Pippa Heath, Jude Buckley, Peter W. Dowrick, Burkhard C. Wuensche, James L. J. Bilzon, Eamonn O'Neill, Christof Lutteroth","abstract":"Exergames commonly use low to moderate intensity exercise protocols. Their effectiveness in implementing high intensity protocols remains uncertain. We propose a method for improving performance while maintaining intrinsic motivation in high intensity VR exergaming. Our method is based on an interactive adaptation of the feedforward method: a psychophysical training technique achieving rapid improvement in performance by exposing participants to self models showing previously unachieved performance levels. We evaluated our method in a cycling-based exergame. Participants competed against (i) a self model which represented their previous speed; (ii) a self model representing their previous speed but increased resistance therefore requiring higher performance to keep up; or (iii) a virtual competitor at the same two levels of performance. We varied participants' awareness of these differences. Interactive feedforward led to improved performance while maintaining intrinsic motivation even when participants were aware of the interventions, and was superior to competing against a virtual competitor.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":2},"fig":{"captions":{"1":{"caption":"Figure 1: a) The exergame is based on a computer-controlled stationary exercycle and played while wearing a head-mounted display.\nb) A “self modelling cue” helps the player to identify a “ghost” avatar with their own previous performance. c) Low-intensity\ncycling and avoiding trucks during warm-up, recovery and cool-down phases. d) High-intensity race against the “ghost.”\n","bbox":[53.929,440.680992,565.9297618599994,472.560992],"page":"1"},"2":{"caption":"Figure 2: ∆Power (difference from baseline B) for the equal\nchallenge (E) and harder challenge (H) conditions in different\nlevels of resistance awareness (top-left) and in non-self (NSC)\nvs. self competition (SC) framing (bottom-left). ∆Power for\nH in different levels of resistance awareness (top-right) and\ncompetition framing (bottom-right).\n","bbox":[53.68,451.162992,297.679254724,515.919992],"page":"8"},"3":{"caption":"Figure 3: IMI Interest/Enjoyment (left column) and IMI Pres-\nsure/Tension (right column) scores for the baseline (B), equal\nchallenge (E) and harder challenge (H) conditions for 1) self\ncompetition (SC) framing in different levels of resistance\nawareness (top row) and 2) without resistance awareness (NA)\nin self competition (SC) vs. non-self competition (NSC) fram-\ning (bottom row).\n","bbox":[321.094,440.20399199999997,565.833441142,515.919992],"page":"8"},"4":{"caption":"Figure 4: FSQ Balance of Challenges and Skills (left column)\nand FSQ Absorption in the Task (right column) scores for the\nbaseline (B), equal challenge (E) and harder challenge (H)\nconditions for 1) self competition (SC) framing in different\nlevels of resistance awareness (top row) and 2) without resis-\ntance awareness (NA) in self competition (SC) vs. non-self\ncompetition (NSC) framing (bottom row).\n","bbox":[53.929,440.20399199999997,298.66117811500004,515.919992],"page":"9"},"5":{"caption":"Figure 5: IEQ scores in different levels of resistance awareness\n(left) and competition framing (right).\n","bbox":[320.766,611.018992,564.17207104,631.940992],"page":"9"}},"crops":{"1":{"crop_coord":[162.0972188888889,578.1111061111111,304.3888855555556,855.8194394444445],"bbox":[60.1549988,485.7050018,107.7799988,582.0800018],"page":"1"},"2":{"crop_coord":[301.30832250000003,578.1099616666668,577.7840847222222,855.8194394444445],"bbox":[110.2709961,485.7050018,206.2022705,582.0804138],"page":"1"},"3":{"crop_coord":[574.700003611111,578.0994508919446,1071.8768267936111,855.8194394444445],"bbox":[208.6920013,485.7050018,384.0756576457,582.0841976789],"page":"1"},"4":{"crop_coord":[1068.794428611111,578.1066401136112,1554.8287583705555,855.8194394444445],"bbox":[386.5659943,485.7050018,557.9383530134,582.0816095591],"page":"1"},"5":{"crop_coord":[162.2500186111111,222.7967411111111,527.7607686111112,469.01388388888887],"bbox":[60.2100067,624.9550018,188.1938767,709.9931732],"page":"8"},"6":{"crop_coord":[516.5805477777778,222.79768388888888,823.6403144444444,469.01388388888887],"bbox":[187.7689972,624.9550018,294.7105132,709.9928338],"page":"8"},"7":{"crop_coord":[162.2500186111111,498.3840383333332,525.7907477777778,744.6027883333333],"bbox":[60.2100067,525.7429962,187.4846692,610.7817462],"page":"8"},"8":{"crop_coord":[516.5805477777778,498.3856455555555,803.088908888889,744.6027883333333],"bbox":[187.7689972,525.7429962,287.31200720000004,610.7811676],"page":"8"},"9":{"crop_coord":[906.6083358333334,222.79513388888876,1270.1490649999998,469.01388388888887],"bbox":[328.1790009,624.9550018,455.4536634,709.9937518],"page":"8"},"10":{"crop_coord":[1260.9389072222223,222.79237861111085,1624.479636388889,469.01112861111096],"bbox":[455.7380066,624.9559937,583.0126691,709.9947437000001],"page":"8"},"11":{"crop_coord":[906.6083358333334,498.3840383333332,1270.1490649999998,744.6027883333333],"bbox":[328.1790009,525.7429962,455.4536634,610.7817462],"page":"8"},"12":{"crop_coord":[1260.9389072222223,498.3840383333332,1624.479636388889,744.6027883333333],"bbox":[455.7380066,525.7429962,583.0126691,610.7817462],"page":"8"},"13":{"crop_coord":[164.06666222222222,222.7967411111111,529.5774122222223,469.01388388888887],"bbox":[60.8639984,624.9550018,188.8478684,709.9931732],"page":"9"},"14":{"crop_coord":[518.397233888889,222.7967411111111,883.9079838888889,469.01388388888887],"bbox":[188.4230042,624.9550018,316.4068742,709.9931732],"page":"9"},"15":{"crop_coord":[164.06666222222222,498.3856455555555,529.5774122222223,744.6027883333333],"bbox":[60.8639984,525.7429962,188.8478684,610.7811676],"page":"9"},"16":{"crop_coord":[518.397233888889,498.3856455555555,883.9079838888889,744.6027883333333],"bbox":[188.4230042,525.7429962,316.4068742,610.7811676],"page":"9"},"17":{"crop_coord":[899.0027619444444,168.22838777777787,1215.969224166667,422.32220111111116],"bbox":[325.4409943,641.7640076,435.94892070000003,729.6377804],"page":"9"},"18":{"crop_coord":[1259.174991111111,168.23326474361104,1553.1319199786112,422.32220111111116],"bbox":[455.1029968,641.7640076,557.3274911923,729.6360246923],"page":"9"}}}},{"filename":"3173574.3174020","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3174020.pdf","paper_id":"3173574.3174020","venue":"CHI 18","keywords":["Games","Mixed reality","EMS"],"paragraph_containing_keyword":"ABSTRACT \nWe  present  a  mobile  system  that  enhances  mixed  reality \nexperiences  and  games  with  force  feedback  by  means  of \nelectrical  muscle  stimulation  (EMS).  The  benefit  of  our \napproach  is  that  it  adds  physical  forces  while  keeping  the \nusers’ hands free to interact unencumbered—not only with \nvirtual objects, but also with physical objects, such as props \nand  appliances.  We  demonstrate  how  this  supports  three \nclasses  of  applications  along  the  mixed-reality  continuum: \n(1)  entirely  virtual  objects,  such  as  furniture  with  EMS \nfriction  when  pushed  or  an  EMS-based  catapult  game.  (2) \nVirtual  objects  augmented  via  passive  props  with  EMS-\nconstraints, such as a light control panel made tangible by \nmeans of a physical cup or a balance-the-marble game with \nan  actuated  tray.  (3)  Augmented  appliances  with  virtual \nbehaviors,  such  as  a  physical  thermostat  dial  with  EMS-\ndetents  or  an  escape-room  that  repurposes  lamps  as  levers \nwith detents. We present a user-study in which participants \nrated the EMS-feedback as significantly more realistic than \na no-EMS baseline. \nAuthor Keywords: games; mixed reality; EMS; \nACM  Classification  Keywords:  H5.2  [Information  inter-\nfaces  and  presentation]:  User  Interfaces.  -  Graphical  user \ninterfaces. \nINTRODUCTION \nAugmented Reality and Mixed Reality (AR/MR) interfaces \nallow  displaying  virtual  information  to  the  human  senses \nwhile  users  explore  the  real  world [16,44].  Researchers \nexplored  MR  to  overlay  data  [63],  assist  in  maintenance \ntasks [49,20] and virtually recreate physical games [31]. \nAs  the  next  step  towards  realism  and  immersion,  many \nresearchers argue that MR systems should also support the \nhaptic sense in order to convey the physicality of the virtual \nworld  [64],  to  better  “blend”  both  realities  [44,  22]  and  to \nincrease the user’s sense of agency [42].","paragraph_after_keyword":"Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies \nbear  this  notice  and  the  full  citation  on  the  first  page.  Copyrights  for \ncomponents  of  this  work  owned  by  others  than  the  author  must  be  hon-\nored.  Abstracting  with  credit  is  permitted.  To  copy  otherwise,  or  repub-\nlish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior  specific \npermission and/or a fee. Request permissions from Permissions@acm.org.  \n \nCHI 2018, April 21–26, 2018, Montreal, QC, Canada \n© 2018 Copyright is held by the owner/author(s). Publication rights \nlicensed to ACM. \nACM 978-1-4503-5620-6/18/04…$15.00  \nhttps://doi.org/10.1145/3173574.3174020","doi":"10.1145/3173574.3174020","sections":[{"word_count":595,"figure_citations":{"1":["Figure 1: (a) In this Mixed Reality game that uses a physical tray as prop, our mobile system renders shifts in the tray’s center of gravity as the marble moves.","Figure 1 illustrates this at the example of our Mixed Reality balance marble game using a physical tray as a game prop, which our approach augments via EMS-based force feedback."],"3":["Figure 3 shows the user’s view through the HoloLens."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1298,"figure_citations":{"1":["Figure 1a shows the system she is wearing, i."],"2":["Figure 2 shows a user wearing our EMS for Mixed Reality system.","Figure 2: Using a regular cup as an impromptu tangible brightness dial."],"3":["Figure 3: The previous scene through the HoloLens."],"4":["Figure 4: (a) This user physically drags the couch and feels the simulated friction against the floor.","Figure 4 shows the user exploring different placements in the room by pushing a couch with her two hands.","Figure 4a).","Figure 4b)."],"5":["Figure 5, the user now explores a lamp from the catalog.","Figure 5b).","Figure 5: Turning on the virtual lamp."],"6":["Figure 6: The user configures the intensity of the desired light bulb using a cup as a stand-in for a dial (passive prop).","Figure 6, the user picks up a cup to serve as a tangible brightness dial.","Figure 6b shows how our system also adds detents to the dial."],"7":["Figure 7a).","Figure 7: The user manipulates two cups to control the light temperature and intensity simultaneously.","Figure 7b).","Figure 7c, for example, the user chooses a “colder” light, causing the system to switch to a less intense bulb by actuating the user’s wrist as to reach that option (Figure 7d)."],"8":["Figure 8: Here, our system enhances a fully functional thermostat with detents."],"10":["Figure 10 shows a simple MR game featuring a virtual catapult that appears in the user’s physical surroundings."]},"section_index":1,"title":"WALKTHROUGH OF A MIXED REALITY EXPERIENCE"},{"word_count":793,"figure_citations":{"1":["Figure 1 depicted a classic MR marble maze, which was in fact inspired by that of Ohan and Feiner [48] and complemented with EMS-based force feedback for added realism."],"9":["Figure 9: Walkthrough examples mapped to the realityvirtuality continuum by Milgram et al."],"10":["Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult’s spring."],"11":["Figure 11: (a) At the start of the game the marble falls from the sky."],"12":["Figure 12 illustrates the user solving this room’s puzzle.","Figure 12, the user finds that when moved the lamps have detents, rendered using our system, hence they can be only in one of three positions."]},"section_index":2,"title":"SUMMARY OF WALKTHROUGH"},{"word_count":1037,"figure_citations":{"12":["Figure 12: (a) These gooseneck lamps are repurposed as levers, with force feedback, allowing the user to input the secret combination to (b) unlock the door."]},"section_index":3,"title":"RELATED WORK"},{"word_count":1174,"figure_citations":{"3":["Figure 3b."],"13":["Figure 13: The hardware components and electrode placement (one arm only).","Figure 13 details how 10 electrodes are placed on the user’s right arm and shoulder; the user’s left arm is equipped the same way."],"14":["Figure 14: Stimulation parameters per haptic effect at the example of one study participant: amplitude (in mA), pulse-width (in µs) and duration (in ms)."]},"section_index":4,"title":"IMPLEMENTATION"},{"word_count":497,"figure_citations":{"13":["Figure 13, which allowed for untethered use."],"15":["Figure 15 shows participant’s average ratings in both conditions regarding perceived realism."]},"section_index":5,"title":"USER STUDY"},{"word_count":1014,"figure_citations":{"15":["Figure 15: Participants rated their experience as more realistic in the EMS conditions."],"16":["Figure 16, participants rated the enjoyment significantly higher when in the EMS condition for the furniture and the catapult tasks.","Figure 16: Participants rated their experience as more enjoyable in most of the EMS conditions."],"17":["Figure 17 summarizes participants’ preferences for each of the interface conditions.","Figure 17: Most participants preferred the EMS to the no-EMS interface condition across tasks."],"18":["Figure 18) polarized participants in that only 7 of them expressed a preference for experiencing it with EMS.","Figure 18: Participant balancing the marble (image from the study, with consent of the participant)."]},"section_index":6,"title":"EMS"},{"word_count":419,"figure_citations":{},"section_index":7,"title":"CONCLUSIONS AND OUTLOOK"},{"word_count":1976,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation","authors":"Pedro Lopes, Sijing You, Alexandra Ion, Patrick Baudisch","abstract":"We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered-not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMS-constraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMS-detents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":2},"fig":{"captions":{"1":{"caption":"Figure 1: (a) In this Mixed Reality game that uses a \nphysical tray as prop, our mobile system renders shifts \nin the tray’s center of gravity as the marble moves. \n(b) Our system creates the necessary forces by applying \nelectrical muscle stimulation to users’ triceps muscles. \n(c) Our approach leaves users’ hands free at all times, \nallowing the user to interact with the tray.  \n","bbox":[319.5419892241395,135.0443954459206,558.628721161458,213.76439279125947],"page":"1"},"3":{"caption":"Figure 3: The previous scene through the HoloLens. \n","bbox":[327.0216889719029,375.2843873443481,551.1466814137736,385.12438701251546],"page":"2"},"4":{"caption":"Figure 4: (a) This user physically drags the couch and \nfeels the simulated friction against the floor. (b) As the \ncouch collides with a real wall, the system stops the user \nby pushing the user’s shoulders and wrists backwards.     \n","bbox":[318.83068924812636,116.08439608530551,566.7359808880583,160.24439459610556],"page":"2"},"5":{"caption":"Figure 5: Turning on the virtual lamp. Here our EMS \nsystem renders the forces of the button’s mechanism.  \n","bbox":[59.13969800564181,350.0843881941635,291.8556901577997,371.4443874738439],"page":"3"},"6":{"caption":"Figure 6: The user configures the intensity of the de-\nsired light bulb using a cup as a stand-in for a dial (pas-\nsive prop). Using EMS force feedback, our system aug-\nments the tangible with constraints and detents.  \n","bbox":[319.2139892352005,469.1243841797977,556.4414612352185,513.524382682504],"page":"3"},"7":{"caption":"Figure 7: The user manipulates two cups to control the \nlight temperature and intensity simultaneously.  \n","bbox":[321.05298917318436,87.04439706461649,557.1074412127599,108.40439634429684],"page":"3"},"8":{"caption":"Figure 8: Here, our system enhances a fully functional \nthermostat with detents.  \n","bbox":[58.44969802891071,317.684389286783,291.6964501631699,339.04438856646334],"page":"4"},"9":{"caption":"Figure 9: Walkthrough examples mapped to the reality-\nvirtuality continuum by Milgram et al. [43].  \n","bbox":[54.560698160058756,134.56439546210768,293.09541011599293,155.6843947498815],"page":"4"},"10":{"caption":"Figure 10: While the user pulls the lever of this virtual \ncatapult, our system provides force feedback simulating \nthe catapult’s spring.  \n","bbox":[318.8359892479478,104.08439648997955,559.3329811377084,136.9643953811729],"page":"4"},"11":{"caption":"Figure 11: (a) At the start of the game the marble falls \nfrom the sky. (b) As it hits the tray, the EMS pulls the \nuser’s arms down quickly so as to represent the impact.  \n","bbox":[55.524698127550074,328.96438890638956,297.1049899807787,361.8443877975829],"page":"5"},"12":{"caption":"Figure 12: (a) These gooseneck lamps are repurposed as \nlevers, with force feedback, allowing the user to input \nthe secret combination to (b) unlock the door.  \n","bbox":[54.421698164746395,74.32439749357107,295.6758300289742,107.20439638476441],"page":"5"},"13":{"caption":"Figure 13: The hardware components and electrode \nplacement (one arm only).  \n","bbox":[63.309697865017775,396.40438663212194,286.8541903264646,417.76438591180226],"page":"7"},"14":{"caption":"Figure 14: Stimulation parameters per haptic effect at \nthe example of one study participant: amplitude (in \nmA), pulse-width (in µs) and duration (in ms).   \n","bbox":[322.4659891255339,201.04439322021386,555.7029812601222,233.92439211140723],"page":"7"},"15":{"caption":"Figure 15: Participants rated their experience as more \nrealistic in the EMS conditions.  \n","bbox":[58.19969803734142,491.20438343519754,291.9680301540115,512.5643827148779],"page":"9"},"16":{"caption":"Figure 16: Participants rated their experience as more \nenjoyable in most of the EMS conditions.  \n","bbox":[58.19969803734142,294.8843900556636,291.96013015427786,316.24438933534395],"page":"9"},"17":{"caption":"Figure 17: Most participants preferred the EMS to the \nno-EMS interface condition across tasks. \n","bbox":[57.48869806131845,122.56439586678168,292.63143013163983,146.03999507511801],"page":"9"},"18":{"caption":"Figure 18: Participant balancing the marble (image \nfrom the study, with consent of the participant). \n","bbox":[328.14798893392083,203.44439313927913,550.0219814517017,224.80439241895948],"page":"9"}},"crops":{"1":{"crop_coord":[-5252.638687462441,392.7861825794326,1592.7777389232026,3019.852749458057],"bbox":[-1889.1499274864786,-293.34698980490055,571.5999860123529,648.7969742714042],"page":"1"},"2":{"crop_coord":[886.205523220943,1256.0472691270468,1559.841601496923,1639.0000351636581],"bbox":[320.8339883595395,203.75998734108313,559.7429765388923,338.0229831142632],"page":"1"},"3":{"crop_coord":[876.5194137931965,857.5972370187413,1550.7610540617607,1612.713889695478],"bbox":[317.34698896555074,213.2229997096279,556.4739794622338,481.4649946732531],"page":"1"},"4":{"crop_coord":[1237.0388526931142,859.2528155862074,1591.1860777266807,1284.3111489057576],"bbox":[447.13398696952106,331.4479863939273,571.0269879816051,480.86898638896537],"page":"1"},"5":{"crop_coord":[854.2194149095714,859.2500181377953,1169.3916250566915,1284.691686144891],"bbox":[309.3189893674457,331.31099298783926,419.18098502040897,480.86999347039364],"page":"1"},"6":{"crop_coord":[144.66667497140867,1694.66669783575,820.6666362965703,2064.000020220307],"bbox":[53.880002989707116,50.759992720689546,293.63998906676534,180.11998877912995],"page":"1"},"7":{"crop_coord":[148.76361731409222,1309.8730695228194,821.2913565410697,2099.1980611142817],"bbox":[55.3549022330732,38.08869799885853,293.8648883547851,318.645694971785],"page":"2"},"8":{"crop_coord":[477.93052671525476,919.1889130762565,1150.4305034154881,1708.4833446925632],"bbox":[173.85498961749173,178.74599591067727,412.35498122957574,459.2919912925476],"page":"2"},"9":{"crop_coord":[878.1388374886833,447.97785648285173,1557.5832584130644,1119.0889443549231],"bbox":[317.929981495926,390.92798003222765,558.9299730287032,628.9279716661733],"page":"2"},"10":{"crop_coord":[879.7360935820287,1444.3889542690636,1132.1249586787035,1744.4305920909028],"bbox":[318.5049936895303,165.80498684727496,405.7649851243332,270.2199764631371],"page":"2"},"11":{"crop_coord":[1146.1166061910453,1461.02005889901,1550.3305019107925,1747.7583643226274],"bbox":[414.4019782287763,164.60698884385423,556.3189806878853,264.23277879635634],"page":"2"},"12":{"crop_coord":[144.66667497140867,714.6222613827606,823.5555488150405,1157.40003241097],"bbox":[53.880002989707116,377.13598833205083,294.6799975734146,532.9359859022062],"page":"3"},"13":{"crop_coord":[878.1388374886833,320.21118819147694,1557.5832584130644,763.5445063004354],"bbox":[317.929981495926,518.9239777318433,558.9299730287032,674.9239722510683],"page":"3"},"14":{"crop_coord":[878.1388374886833,1099.908367068312,1557.027711332315,1886.991673091962],"bbox":[317.929981495926,114.48299768689365,558.7299760796334,394.2329878554077],"page":"3"},"15":{"crop_coord":[144.66667497140867,938.2139520606246,824.11109589579,1247.241702363001],"bbox":[53.880002989707116,344.7929871493197,294.87999452248437,452.4429772581751],"page":"4"},"16":{"crop_coord":[880.1527432738872,1116.2972384507093,1551.6804808378172,1807.8250092200271],"bbox":[318.6549875785994,142.98299668079025,556.8049731016142,388.3329941577447],"page":"4"},"17":{"crop_coord":[247.3747249469705,808.5167079557527,953.3635886780414,1477.6472474661878],"bbox":[90.85490098090938,261.84699091217243,341.4108919240949,499.13398513592904],"page":"5"},"18":{"crop_coord":[519.1721922756252,1471.066699567821,1328.4571627404212,1956.2233397715897],"bbox":[188.70198921922508,89.55959768222769,476.44457858655164,260.61598815558455],"page":"5"},"19":{"crop_coord":[-25.188564010620915,1469.6917131987684,756.2839159401822,1940.6916732588334],"bbox":[-7.26788304382353,95.15099762681997,270.46220973846556,261.1109832484434],"page":"5"},"20":{"crop_coord":[518.8944399170982,1471.0361395926648,1328.1791136971308,1956.189982348022],"bbox":[188.60199837015534,89.57160635471203,476.3444809309671,260.6269897466407],"page":"5"},"21":{"crop_coord":[-41.528607248016364,497.69450751358005,7205.582251142254,1027.294488568],"bbox":[-13.150298609285892,423.97398411552007,2592.2096104112115,611.0299772951112],"page":"7"},"22":{"crop_coord":[571.0388489764778,497.69450751358005,927.0749425017722,1027.294488568],"bbox":[207.373985631532,423.97398411552007,331.94697930063796,611.0299772951112],"page":"7"},"23":{"crop_coord":[153.46777463907483,497.69450751358005,509.51221811812974,1027.294488568],"bbox":[57.04839887006693,423.97398411552007,181.62439852252672,611.0299772951112],"page":"7"},"24":{"crop_coord":[-4.374389673431167,591.8167338691804,473.62282694324875,913.6333834690514],"bbox":[0.22521971756478001,464.8919819511415,168.70421769956957,577.145975807095],"page":"7"},"25":{"crop_coord":[878.1388374886833,1309.8722641959198,1557.3054636908423,1564.4555973969839],"bbox":[317.929981495926,230.59598493708583,558.8299669287032,318.6459848894689],"page":"9"}}}},{"filename":"3173574.3174039","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3174039.pdf","paper_id":"3173574.3174039","venue":"CHI 18","keywords":["Augmented reality","Microsurgery","Surgical simulator","Surgical training","Cataract","Instrument tracking","Dexterous input"],"paragraph_containing_keyword":"ABSTRACT\nWe propose CatAR, a novel stereoscopic augmented reality\n(AR) cataract surgery training system. It provides dexterous\ninstrument  tracking  ability  using  a  specially  designed\ninfrared  optical  system  with  2  cameras  and  1  reflective\nmarker. The tracking accuracy on the instrument tip is 20 µm,\nmuch higher than previous simulators. Moreover, our system\nallows  trainees  to  use  and  to  see  real  surgical  instruments\nwhile practicing. Five training modules with 31 parameters\nwere designed and 28 participants were enrolled to conduct\nefficacy  and  validity  tests.  The  results  revealed  significant\ndifferences  between  novice  and  experienced  surgeons.\nImprovements in surgical skills after practicing with CatAR\nwere also significant.\nAuthor Keywords\nAugmented  reality;  Microsurgery;  Surgical  simulator;\nSurgical  training;  Cataract;  Instrument  tracking;  Dexterous\ninput.\nPermission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear  this  notice  and  the  full  citation  on  the  first  page.  Copyrights for\ncomponents  of  this  work  owned  by  others  than  the  author(s)  must  be\nhonored.  Abstracting  with  credit  is  permitted.  To  copy  otherwise,  or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from Permissions@acm.org.\nCHI 2018, April 21–26, 2018, Montreal, QC, Canada\n© 2018  Copyright  is  held  by  the  owner/author(s).  Publication  rights\nlicensed to ACM.\nACM 978-1-4503-5620-6/18/04…$15.00\nhttps://doi.org/10.1145/3173574.3174039","doi":"10.1145/3173574.3174039","paragraph_after_keyword":"ACM Classification Keywords\nH.5.2.  Information  interfaces  and  presentation  (e.g.,  HCI):\nMultimedia  Information  Systems  - Artificial,  augmented,\nand virtual realities\nBACKGROUND AND RELATED WORK\nCataract  is  a  clouding  of  the  lens  in  the  eye  that  occludes\nvision.  According  to  a  recent  assessment  by  the  World\nHealth Organization [26], cataract is responsible for 51% of\nblindness, representing 20 million people worldwide, which\nmakes it the current leading cause of blindness. Although the\nopaque lens material can be removed through microsurgery\nprocedures, it is very difficult for surgeons to master those\nskills because of following reasons:\n1. Absence of force feedback\nThe lens is suspended behind the iris by a ring of fibrous\nstrands  called  the  zonule  of  Zinn.  The  diameter  of  the\nzonule is 1 to 2 micrometers, and it can be dehisced if the\nlens is pushed excessively during surgery. The lens capsule\nis  only  2  to  28  micrometers  thick  and  can  be  easily torn\napart  without  any  resistance.  Due  to  these  anatomical\nproperties  of  the  lens,  cataract  surgery  mainly  relies  on\nvisual feedback rather than the force feedback utilized in\nthe other surgeries [7].\n2. Difficult to reproduce subtle movements\nThe surgical field in cataract surgery is approximately 10\nmm  in  both  diameter  and  depth.  Every  movement  is\nextremely  delicate  and  the  result  can  be  significantly","sections":[{"word_count":924,"figure_citations":{},"section_index":0,"title":"BACKGROUND AND RELATED WORK"},{"word_count":1655,"figure_citations":{"1":["Figure 1b, 2) which helps meet user requirements for using real surgical instruments instead of fake props."],"3":["Figure 3a), which helped users judge the depth of the tip with greater ease.","Figure 3c) and dot markers on 3 different planes were designed for CatAR calibration (Figure 3b) to solve the extrinsic and intrinsic camera parameters."],"4":["Figure 4), which provided a realistic visual environment for microsurgery training."],"5":["Figure 5b).","Figure 5a, 5b).","Figure 5c).","Figure 5d)."]},"section_index":1,"title":"IMPLEMENTATION"},{"word_count":369,"figure_citations":{"6":["Figure 6)."],"7":["Figure 7a)."]},"section_index":2,"title":"SYSTEM EVALUATION"},{"word_count":294,"figure_citations":{"7":["Figure 7b).","Figure 7c)."]},"section_index":3,"title":"RMSE"},{"word_count":445,"figure_citations":{"1":["Figure 1c, 7e)."],"7":["Figure 7d).","Figure 7f)."]},"section_index":4,"title":"TRAINING MODULE DESIGN"},{"word_count":1096,"figure_citations":{},"section_index":5,"title":"USER STUDY"},{"word_count":1419,"figure_citations":{"9":["Figure 9)."]},"section_index":6,"title":"RESULT"},{"word_count":633,"figure_citations":{"8":["Figure 8) in our study."],"9":["Figure 9)."],"10":["Figure 10)."],"11":["Figure 11a), the difference was not statistically significant.","Figure 11b)."]},"section_index":7,"title":"DISCUSSION"},{"word_count":134,"figure_citations":{},"section_index":8,"title":"CONCLUSION AND FUTURE WORK"},{"word_count":187,"figure_citations":{"11":["Figure 11c)."]},"section_index":9,"title":"ACKNOWLEDGMENTS"},{"word_count":599,"figure_citations":{},"section_index":10,"title":"REFERENCES"},{"word_count":694,"figure_citations":{},"section_index":11,"title":"OF SURGICAL INSTRUMENTS IN THE EYE"}],"title":"CatAR: A Novel Stereoscopic Augmented Reality Cataract Surgery Training System with Dexterous Instruments Tracking Technology","authors":"Yu-Hsuan Huang, Hao-Yu Chang, Wan-ling Yang, Yu-Kai Chiu, Tzu-Chieh Yu, Pei-Hsuan Tsai, Ming Ouhyoung","abstract":"We propose CatAR, a novel stereoscopic augmented reality (AR) cataract surgery training system. It provides dexterous instrument tracking ability using a specially designed infrared optical system with 2 cameras and 1 reflective marker. The tracking accuracy on the instrument tip is 20 µm, much higher than previous simulators. Moreover, our system allows trainees to use and to see real surgical instruments while practicing. Five training modules with 31 parameters were designed and 28 participants were enrolled to conduct efficacy and validity tests. The results revealed significant differences between novice and experienced surgeons. Improvements in surgical skills after practicing with CatAR were also significant.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. (a)  A surgeon operating a CatAR  system.  (b) System  overview: AR microscope platform, dual  4K displays, tracking  area  and surgical\nmannequin are shown. (c) A real surgical instrument interacting with the virtual object in a training module. The iris, blue guidance curve, and white\nrectangle are virtual objects overlaid on a real scene.\n","bbox":[53.5798,416.67427599999996,558.2990627999998,443.0854608],"page":"1"},"2":{"caption":"Figure 2. CatAR system structure diagram.\n","bbox":[363.2577,78.75352559999999,511.6391842,86.7469256],"page":"2"},"3":{"caption":"Figure 3. (a) Components and structure of the lower camera module.\n(b) Dot markers on 3 different plans. (c) Three different types of micro\ncalibration boards.\n","bbox":[53.5798,126.7688264,294.97248659999985,153.2439608],"page":"3"},"4":{"caption":"Figure 4. Stereoscopic display system in CatAR. The distance between\nthe eye and screen is 32 cm, the viewing angle is 50 degrees and has 2160\npixels in this field (43 PPD).\n","bbox":[53.5798,301.910576,295.2122886,328.32176080000005],"page":"4"},"5":{"caption":"Figure 5. (a) Spatula and (b) Capsule forceps used in cataract surgery.\nArrow: reflective tracker. (c) Standard hand posture on the face model\nwhile holding forceps. (d) Dimensions of eye model, the soft part is made\nusing EVA material. (e) The soft part allows the forceps to be tilted in\nthe small artificial wound. (f) The elasticity will constrain the opening\ndistance of forceps while tilting laterally.\n","bbox":[316.6587,283.55444879999993,558.4030961999999,337.5918608],"page":"4"},"6":{"caption":"Figure 6. This spatula is fixed on the linear translation stage and its tip\nis placed inside the eye model through the wound. * Micrometer drive.\n","bbox":[53.5798,499.0562184,295.26024899999993,516.2586608],"page":"5"},"7":{"caption":"Figure 7. Instructor’s 3D view (black star) and user’s AR view (white star) of 5 modules. (a) Antitremor module, 6 blue virtual balls are placed in the\npupil area, and turn to red when touched. Arrow: insertion point of the instrument. (b) Anterior chamber navigation module. Arrow: starting point\nin  the  pupil  center.  (c)  Circular  tracing  module,  green  line  represents the  curve  drawn by  the user  along  the white  reference circle.  (d) Forceps\ntraining module: six blue balls are dragged to the small white point in the pupil center. Two white balls beside the blue ball represent the forceps tips.\nArrow: wound touch warning indicators, the lower one is touched and turned to red. (e) Capsulorhexis module: white thin box represents the proximal\nend of the capsule flap. The box is dragged along the blue curve to the white point. (f) Capsulorhexis in the human eye.\n","bbox":[53.5798,71.8315441,558.3949835999998,126.1167608],"page":"6"},"8":{"caption":"Figure  8.  Wound  touch  counts  and  accumulated  time  of  every\nparticipants’ pre- and post-intervention in the forceps training module.\nThe IDs are sorted in ascending order by their total training experience.\n","bbox":[316.8461,161.9376256,558.4386215999999,188.2848608],"page":"9"},"9":{"caption":"Figure  9.  Correlation  between  experience  and  search  time  in  the\nanterior  chamber  navigation  module.  Linear  regression  lines  are\npresented.\n","bbox":[53.5798,544.043276,295.1963017999999,570.4544608],"page":"10"},"10":{"caption":"Figure  10.  3D  presentation  of  the  results  in  circular  tracing  module.\nNovice: R2, Experienced: Attending physician\n","bbox":[316.8461,559.3737679999999,558.2947403999998,576.5122607999999],"page":"10"},"11":{"caption":"Figure  11. Tracking  plots  on  the  XY  and  XZ  planes  in  3  different  modules  (unit  of  axes:  mm).  (a)  Pre-intervention  results  of  novice  (R2)  and\nexperienced surgeons (R4) in the antitremor module. (b) Pre-intervention results of novice (R2) and experienced surgeons (Fellow) in the forceps\ntraining module. The pink circles represent the position of virtual balls. (c) Pre- and post-intervention results of the same novice participant (R2).\nThe circles in 4 different colors represent the starting and releasing point of 4 tears. The curves connecting 2 circles are the guidance tracts for the\ntrainee to follow.\n","bbox":[53.5798,71.89236689999998,558.4124403999999,117.9680537],"page":"10"}},"crops":{"1":{"crop_coord":[1095.7191127777778,632.6157972222221,1473.4290991666664,963.2891675000001],"bbox":[396.2588806,447.0158997,528.6344756999999,562.458313],"page":"1"},"2":{"crop_coord":[606.1258188888888,622.8308866666667,1087.865821111111,970.3275383333332],"bbox":[220.0052948,444.4820862,389.8316956,565.9808808],"page":"1"},"3":{"crop_coord":[228.63083722222223,632.6157972222221,601.5341355555555,963.2891675000001],"bbox":[84.1071014,447.0158997,214.7522888,562.458313],"page":"1"},"4":{"crop_coord":[226.5708161111111,910.0291441666666,311.41750777777776,953.332468611111],"bbox":[83.3654938,450.6003113,110.31030279999999,462.5895081],"page":"1"},"5":{"crop_coord":[646.6391330555556,907.9691655555556,732.1725125,951.2725322222223],"bbox":[234.5900879,451.3418884,261.7821045,463.3311004],"page":"1"},"6":{"crop_coord":[1093.487515,907.9691655555556,1177.475815,951.2725322222223],"bbox":[395.4555054,451.3418884,422.0912934,463.3311004],"page":"1"},"7":{"crop_coord":[874.2691550000002,1612.317538888889,1555.8291458333335,1703.3442094444445],"bbox":[316.5368958,180.5960846,558.2984925000001,209.76568600000002],"page":"2"},"8":{"crop_coord":[874.2691550000002,1693.3442094444445,1555.8291458333335,1784.37088],"bbox":[316.5368958,151.4264832,558.2984925000001,180.5960846],"page":"2"},"9":{"crop_coord":[874.2691550000002,1774.37088,1555.8291458333335,1865.397423611111],"bbox":[316.5368958,122.2569275,558.2984925000001,151.4264832],"page":"2"},"10":{"crop_coord":[874.2691550000002,1855.3974658333334,1555.8291458333335,1946.2525600000001],"bbox":[316.5368958,93.1490784,558.2984925000001,122.2569123],"page":"2"},"11":{"crop_coord":[148.46247361111114,1428.2908627777779,471.5824891666666,1582.1474624999998],"bbox":[55.2464905,224.2269135,167.9696961,276.0152894],"page":"3"},"12":{"crop_coord":[148.46247361111114,1572.1474202777777,471.5824891666666,1725.8324855555554],"bbox":[55.2464905,172.5003052,167.9696961,224.22692870000003],"page":"3"},"13":{"crop_coord":[471.0241613888889,1402.1975708333334,825.5591413888889,1753.4708575],"bbox":[171.3686981,162.5504913,295.4012909,285.4088745],"page":"3"},"14":{"crop_coord":[471.0241613888889,1701.2408447222224,554.4974772222222,1747.977523888889],"bbox":[171.3686981,164.5280914,197.8190918,177.75329589999998],"page":"3"},"15":{"crop_coord":[143.82748916666665,1701.2408447222222,228.50247694444442,1747.4625397222221],"bbox":[53.5778961,164.7134857,80.46089169999999,177.7532959],"page":"3"},"16":{"crop_coord":[1068.767513611111,785.0558047222223,1555.4859075000002,1257.1824305555558],"bbox":[386.5563049,341.214325,558.1749267,507.57991029999994],"page":"4"},"17":{"crop_coord":[1054.6908569444442,786.4291805555555,1138.1641727777776,833.1658597222221],"bbox":[381.4887085,493.8602905,407.9391022,507.08549500000004],"page":"4"},"18":{"crop_coord":[1296.3975355555556,1055.602527222222,1381.930915,1102.339206388889],"bbox":[468.5031128,396.9578857,495.6951294,410.18309020000004],"page":"4"},"19":{"crop_coord":[858.990783611111,1010.7975088888891,1072.0724655555555,1257.1824305555558],"bbox":[311.0366821,341.214325,384.1460876,426.3128968],"page":"4"},"20":{"crop_coord":[858.990783611111,786.4291383333333,1072.0724655555555,1015.4758369444445],"bbox":[311.0366821,428.2286987,384.1460876,507.0855102],"page":"4"},"21":{"crop_coord":[1296.3975355555556,1153.109156111111,1381.930915,1199.8458352777777],"bbox":[468.5031128,361.8554993,495.6951294,375.08070380000004],"page":"4"},"22":{"crop_coord":[1054.6908569444442,1207.1841852777777,1140.0525324999999,1253.9208644444445],"bbox":[381.4887085,342.3884888,408.61891169999996,355.6136933],"page":"4"},"23":{"crop_coord":[210.26247666666669,920.8441077777778,763.5873922222223,1289.6275330555554],"bbox":[77.4944916,329.5340881,273.0914612,458.6961212],"page":"4"},"24":{"crop_coord":[642.0041486111112,1111.565916111111,752.7725133333334,1158.9892408333333],"bbox":[232.9214935,376.5638733,269.1981048,390.0362702],"page":"4"},"25":{"crop_coord":[273.26415166666663,1133.0242663888891,400.16919444444443,1180.4475911111113],"bbox":[100.1750946,368.8388672,142.26091,382.31126409999996],"page":"4"},"26":{"crop_coord":[153.78414583333333,1018.1791177777777,201.20747055555555,1210.4891797222222],"bbox":[57.1622925,358.0238953,70.6346894,423.6555176],"page":"4"},"27":{"crop_coord":[220.56249833333328,435.88579805555554,748.1375291666667,766.3874647222221],"bbox":[81.2024994,517.9005127,267.5295105,633.2811127],"page":"5"},"28":{"crop_coord":[142.45419833333332,1294.562496666667,1556.1725108333333,1834.1541205555554],"bbox":[53.0835114,133.5045166,558.4221039,324.15750119999996],"page":"6"},"29":{"crop_coord":[602.5208452777778,1573.1775580555554,688.0542247222222,1619.742491111111],"bbox":[218.7075043,210.6927032,245.8995209,223.85607910000002],"page":"6"},"30":{"crop_coord":[1072.0291647222223,1306.5792591666668,1157.5625441666666,1354.002583888889],"bbox":[387.7304993,306.3590698,414.9225159,319.83146669999996],"page":"6"},"31":{"crop_coord":[1072.0291647222223,1574.3791875,1157.5625441666666,1621.1158666666665],"bbox":[387.7304993,210.198288,414.9225159,223.42349249999998],"page":"6"},"32":{"crop_coord":[131.12416583333334,1305.3775447222222,216.48584138888887,1352.114223888889],"bbox":[49.0046997,307.0388794,76.1349029,320.2640839],"page":"6"},"33":{"crop_coord":[131.12416583333334,1573.1775580555554,216.48584138888887,1619.742491111111],"bbox":[49.0046997,210.6927032,76.1349029,223.85607910000002],"page":"6"},"34":{"crop_coord":[517.7174972222223,1395.5026074999998,593.1224738888889,1448.2475619444444],"bbox":[188.178299,272.4308777,211.7240906,287.8190613],"page":"6"},"35":{"crop_coord":[436.3475036111111,1508.9741261111112,512.4391683333333,1561.7192077777777],"bbox":[158.8851013,231.5810852,182.6781006,246.9693146],"page":"6"},"36":{"crop_coord":[925.0825330555556,1309.325841111111,1503.4707724999998,1679.3108536111113],"bbox":[334.8297119,189.2480927,539.4494781,318.8426972],"page":"9"},"37":{"crop_coord":[143.1408436111111,270.2276102777777,833.6274802777779,604.6775308333333],"bbox":[53.3307037,576.1160889,298.3058929,692.9180603],"page":"10"},"38":{"crop_coord":[147.08914027777777,1421.424111111111,603.9374375,1870.8907572222222],"bbox":[54.7520905,120.2793274,215.6174775,278.48732],"page":"10"},"39":{"crop_coord":[1097.435811388889,1424.8575508333333,1545.529251111111,1875.0108847222223],"bbox":[396.8768921,118.7960815,554.5905304,277.2512817],"page":"10"},"40":{"crop_coord":[629.3008677777779,1411.4674969444443,1069.3258413888889,1871.5774875],"bbox":[228.3483124,120.0321045,383.1573029,282.07170110000004],"page":"10"},"41":{"crop_coord":[1091.4274936111112,1814.197506388889,1174.9008094444446,1860.9341855555556],"bbox":[394.7138977,123.8636932,421.1642914,137.0888977],"page":"10"},"42":{"crop_coord":[145.71580666666668,1814.197506388889,231.24914361111112,1860.9341855555556],"bbox":[54.2576904,123.8636932,81.4496917,137.0888977],"page":"10"},"43":{"crop_coord":[623.9791530555556,1814.197506388889,709.3408286111112,1860.9341855555556],"bbox":[226.4324951,123.8636932,253.56269830000002,137.0888977],"page":"10"},"44":{"crop_coord":[955.1242066666667,260.2709111111111,1482.0125072222222,600.0425888888889],"bbox":[345.6447144,577.784668,531.7245026,696.502472],"page":"10"}}}},{"filename":"3173574.3174057","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3174057.pdf","paper_id":"3173574.3174057","venue":"CHI 18","keywords":["Virtual Reality","User Experience"],"paragraph_containing_keyword":"ABSTRACT \nResearch  on  virtual  reality  (VR)  has  studied  users’ \nexperience of immersion, presence, simulator sickness, and \nlearning  effects.  However,  the  momentary  experience  of \nexiting  VR  and  transitioning  back  to  the  real-world  is  not \nwell  understood.  Do  users  become  self-conscious  of  their \nactions upon exit? Are users nervous of their surroundings? \nUsing  explicitation  interviews,  we  explore  the  moment  of \nexit  from  VR  across  four  applications.  Analysis  of  the \ninterviews  reveals  five  components  of  experience:  space, \ncontrol, sociality, time, and sensory adaptation. Participants \ndescribed  spatial  disorientation,  for  example,  regardless  of \nthe complexity of the VR scene. Participants also described \na window across which they exit VR, for example mentally \nfirst and then physically. We present six designs for easing \nor  heightening  the  exit  experience,  as  described  by  the \nparticipants. Based on these findings, we further discuss the \n‘moment of exit’ as an opportunity for designing engaging \nand enhanced VR experiences.  \nAuthor Keywords \nVirtual Reality; User Experience \nACM Classification Keywords \nH.5.m. Information interfaces and presentation (e.g., HCI): \nMiscellaneous.  \nINTRODUCTION \nVirtual  Reality  (VR)  is  being  employed  to  provide  a  wide \nrange  of  experiences.  Recent  advances  in  commercial  VR \nheadsets  have  led  to  new  industry  attention  on  VR  games. \nIn  research,  VR  has  a  long  tradition  as  a  tool  for  studying \ncomplex phenomena (different perspectives and fears) [26] \nand,  increasingly,  as  a  technology  for  exploring  novel \nexperiences [7,9] and collaborations [18].  \nWhile  the  user  is  in  VR,  the  user  experience  is  well \nunderstood.  Research  has  explored  both  the  technical \nfactors  of  immersion  (e.g.,  [11,39])  and  the  experiential \nfactors  of  presence  (e.g.,  [36]).  After  exit,  the  mid-term \naftereffects  of  VR  have  also  been  explored  [1,38],  such  as","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies \nbear this notice and the full citation on the first page. Copyrights for \ncomponents of this work owned by others than the author(s) must be \nhonored. Abstracting with credit is permitted. To copy otherwise, or \nrepublish, to post on servers or to redistribute to lists, requires prior \nspecific permission and/or a fee. Request permissions \nfrom Permissions@acm.org. \n \nCHI 2018, April 21–26, 2018, Montreal, QC, Canada \n© 2018 Copyright is held by the owner/author(s). Publication rights \nlicensed to ACM. \nACM 978-1-4503-5620-6/18/04…$15.00  \nhttps://doi.org/10.1145/3173574.3174057","doi":"10.1145/3173574.3174057","sections":[{"word_count":667,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":985,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":4083,"figure_citations":{"1":["Figure 1, top left).","Figure 1, bottom right).","Figure 1, bottom left)."],"2":["Figure 2)."]},"section_index":2,"title":"EXPLORING THE MOMENT OF EXIT"},{"word_count":2,"figure_citations":{},"section_index":3,"title":"EXPERIENCE OF EXITING VR"},{"word_count":2,"figure_citations":{},"section_index":4,"title":"LESSENING"},{"word_count":1144,"figure_citations":{},"section_index":5,"title":"THE"},{"word_count":1122,"figure_citations":{},"section_index":6,"title":"DISCUSSION"},{"word_count":185,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":20,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGEMENTS"},{"word_count":1430,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"The Dream is Collapsing: The Experience of Exiting VR","authors":"Jarrod Knibbe, Jonas Schjerlund, Mathias Petraeus, Kasper Hornbæk","abstract":"Research on virtual reality (VR) has studied users' experience of immersion, presence, simulator sickness, and learning effects. However, the momentary experience of exiting VR and transitioning back to the real-world is not well understood. Do users become self-conscious of their actions upon exit? Are users nervous of their surroundings? Using explicitation interviews, we explore the moment of exit from VR across four applications. Analysis of the interviews reveals five components of experience: space, control, sociality, time, and sensory adaptation. Participants described spatial disorientation, for example, regardless of the complexity of the VR scene. Participants also described a window across which they exit VR, for example mentally first and then physically. We present six designs for easing or heightening the exit experience, as described by the participants. Based on these findings, we further discuss the ?moment of exit' as an opportunity for designing engaging and enhanced VR experiences.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":3},"fig":{"captions":{"1":{"caption":"Figure 1. The four VR scenarios. Top: Gaming and Illusions scenarios. Bottom: Cognitive and Perception scenarios. \n","bbox":[81.12,480.57912000000005,528.3000000000001,490.41912],"page":"3"},"2":{"caption":"Figure  2.  Illustration  of  the  Illusion  Scenario.  The  player \nstarts  in  the  green  location  in  both  the  virtual  and  real \nenvironments. By slightly varying the player’s rotation in VR, \nthe player ends up in one corner of the room in VR (orange), \nbut  in  the  opposite  corner,  facing  the  opposite  direction  in \nreality (blue). \n","bbox":[54,225.04128000000006,297.6228000000002,285.75384],"page":"4"},"3":{"caption":"Figure  3.  Illustrating  Abrupt  vs  Open  Endings.  Left:  an \nabrupt  ending  (fade  to  black),  forces  the  player  to  exit  VR. \nRight:  the  game  world  remains  visible  while  suggesting  an \nexit, leaving the player in control. \n","bbox":[54,549.7581599999999,297.5872800000002,589.59384],"page":"8"},"4":{"caption":"Figure  4.  Illustrating  Soft  Transitions.  From  left  to  right,  the \ngame  world  softly  fades  into  the  real  world,  allowing  the \nplayer to re-orient themselves before removing the headset. \n","bbox":[53.99999999999997,185.91672000000003,297.42744000000005,215.43384],"page":"8"},"5":{"caption":"Figure  5.  Spatial  Alignment  illustration.  Towards  the  end  of \nthe game, objects of focus are moved to the extremities of the \nreal-space,  helping  the  player  adapt  to  the  real-world \nproportions before exiting VR. \n","bbox":[316.8,491.6784,560.2452000000001,531.75384],"page":"8"},"6":{"caption":"Figure  6.  Illustration  of  Ability  Dashboards.  A  heads-up-\ndisplay  shows  the  participant  special  capabilities  that  are \nenabled in VR (left) and shows them disabled before removing \nthe headset (right). \n","bbox":[316.8,165.5184,560.39616,205.59384],"page":"8"},"7":{"caption":"Figure  7.  Spatial  Reconfiguration  illustration.  Left:  the  room \nlayout as the player enters VR. Right: The re-arranged layout \n(where furnishings have moved) as the player exits VR. \n","bbox":[54,566.0767199999999,297.45408000000003,595.59384],"page":"9"},"8":{"caption":"Figure  8.  Illustration  of  Social  Setting  Changes.  Left:  the \nsocial setting of the space as the player enters VR. Right: The \nsocial  setting  upon  exiting  VR,  where  multiple  people  are \npresent. \n","bbox":[54,202.7184,297.44520000000006,242.79384],"page":"9"}},"crops":{"1":{"crop_coord":[145.66668194444446,1670.9999591666667,821.6666666666666,2070.3333027777776],"bbox":[54.2400055,48.480011,294,188.6400147],"page":"1"},"2":{"crop_coord":[131.66665805555556,820.3333199999998,1554.9999575000002,884.3333266666666],"bbox":[49.1999969,475.4400024,557.9999847,494.88000480000005],"page":"3"},"3":{"crop_coord":[205.66665638888887,145.66663944444446,1515.0001016666668,815.6666563888889],"bbox":[75.8399963,500.1600037,543.6000366000001,737.7600098],"page":"3"},"4":{"crop_coord":[207.66666,961.6666666666667,761.6666922222222,1387.666651388889],"bbox":[76.5599976,294.2400055,272.4000092,444],"page":"4"},"5":{"crop_coord":[145,306.3333808333332,824.3333519444444,542.3334249999998],"bbox":[54,598.559967,294.9600067,679.9199829],"page":"8"},"6":{"crop_coord":[145,1351.00001,846.9999863888888,1583.0000049999999],"bbox":[54,223.9199982,303.1199951,303.8399964],"page":"8"},"7":{"crop_coord":[874.9999661111112,451.0000950000001,1552.9999541666666,703.0000813888889],"bbox":[316.7999878,540.7199707,557.2799835,627.8399658],"page":"8"},"8":{"crop_coord":[874.9999661111112,1384.333343611111,1549.6666294444444,1611.0000102777778],"bbox":[316.7999878,213.8399963,556.0799866,291.8399963],"page":"8"},"9":{"crop_coord":[145,306.4566380555557,815.6666563888889,527.0746358333334],"bbox":[54,604.0531311,291.8399963,679.8756103],"page":"9"},"10":{"crop_coord":[171.66668361111113,1259.000023611111,796.3333469444445,1507.6666852777778],"bbox":[63.6000061,251.0399933,284.8800049,336.9599915],"page":"9"}}}},{"filename":"3173574.3174088","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503 Design Project 1/_PaperVis/papers/CHI 18/3173574.3174088.pdf","paper_id":"3173574.3174088","venue":"CHI 18","keywords":["Dementia","Care","Virtual reality","Augmented reality","Creativity","Experience","Expression"],"paragraph_containing_keyword":"ABSTRACT \nDespite  indications  that  recreational  virtual  reality  (VR) \nexperiences  could  be  beneficial  for  people  with  dementia, \nthis area remains unexplored in contrast to the body of work \non neurological rehabilitation through VR in dementia. With \nrecreational  VR  applications  coming  to  the  market  for \ndementia, we must consider how VR experiences for people \nwith  dementia  can  be  sensitively  designed  to  provide \ncomfortable and enriching experiences. Working  with  seven \nparticipants  from  a  local  dementia  care  charity,  we  outline \nsome  of  the  opportunities  and  challenges  inherent  to  the \ndesign and use of VR experiences with people with dementia \nand  their  carers  through  an  inductive  thematic  analysis.  We \nalso provide a series of future directions for work in VR and \ndementia:  1)  careful  physical  design,  2)  making  room  for \nsharing,  3)  utilizing  all  senses,  4)  personalization,  and  5) \npositioning the person with dementia as an active participant. \nAuthor Keywords \nDementia; care; virtual reality; augmented reality; creativity; \nexperience; expression.  \nACM Classification Keywords \nH.5.m. Information interfaces and presentation (e.g., HCI). \nINTRODUCTION \nAn initial focus in dementia and design research has been the \ntackling  of  cognitive  deficits  and  other  behavioural \nsymptoms  in  people  with  dementia  [37];  however,  more \nrecent  research  has  followed  more  social  and  interpersonal \nlines,  by  paying  attention  to  networks  of  care  and  aesthetic \nexperience  in  dementia  [39].  A  similar  initial  focus  on \nmedicalisation has been faced by the development of virtual \nreality  for  clinical  populations  [12].  Virtual  reality  refers  to \nthe  computer-generated  simulation  of  a  three-dimensional \nenvironment  that  can  be  interacted  with  by  a  person  using \nequipment  such  as  head-mounted  displays  (HMDs)  or  VR \ngloves  [54,  32].  For  those  facing  neurological  impairments \nsuch  as  acquired  brain  injury  through  to  learning  disorders; \nneurological  rehabilitation  through  VR  has  been  a  well-","paragraph_after_keyword":"CHI 2018, April 21–26, 2018, Montréal, QC, Canada. \nACM ISBN 978-1-4503-5620-6/18/04. \nhttps://doi.org/10.1145/3173574.3174088.","doi":"10.1145/3173574.3174088."},"fig":{"captions":{"1":{"caption":"Figure 1: From sketching to final environment \n","bbox":[316.85000000000014,413.20600000000076,493.1510000000002,422.20600000000076],"page":"5"},"2":{"caption":"Figure 2: creating the concert venue for Janet \n","bbox":[316.8499999999999,357.88600000000093,490.27099999999996,366.88600000000093],"page":"7"}},"crops":{"1":{"crop_coord":[147.60488722222223,1737.895168888889,287.48464805555557,1793.648266388889],"bbox":[54.9377594,148.0866241,101.6944733,164.5577392],"page":"1"},"2":{"crop_coord":[950.4999880555555,373.30556222222236,1479.3610805555554,677.8889041666669],"bbox":[343.9799957,549.7599945,530.769989,655.8099976],"page":"5"},"3":{"crop_coord":[875.0000086111113,684.5555283333333,1555,1011.4999897222222],"bbox":[316.8000031,429.6600037,558,543.7600098],"page":"5"},"4":{"crop_coord":[875.0000086111113,821.8888683333333,1555,1165.3610991666667],"bbox":[316.8000031,374.2700043,558,494.3200074],"page":"7"}}}},{"filename":"3173574.3174151","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3174151.pdf","paper_id":"3173574.3174151","venue":"CHI 18","keywords":["Multisensory VR","Virtual Reality","Multimodal Interaction"],"paragraph_containing_keyword":"ABSTRACT\nIn the same way that we experience the real-world through a\nrange of senses, experiencing a virtual environment through\nmultiple sensory modalities may augment both our presence\nwithin a scenario and our reaction to it. In this paper, we\npresent Season Traveller, a multisensory virtual reality (VR)\nnarration of a journey through four seasons within a mystical\nrealm. By adding olfactory and haptic (thermal and wind)\nstimuli, we extend traditional audio-visual VR technologies\nto achieve enhanced sensory engagement within interactive\nexperiences. Using both subjective measures of presence and\nelicited physiological responses, we evaluated the impact of\ndifferent modalities on the virtual experience. Our results in-\ndicate that 1) the addition of any singular modality improves\nsense of presence with respect to traditional audio-visual ex-\nperiences and 2) providing a combination of these modalities\nproduces a further signiﬁcant enhancement over the aforemen-\ntioned improvements. Furthermore, insights into participants’\npsychophysiology were extrapolated from electrodermal activ-\nity (EDA) and heart rate (HR) measurements during each of\nthe VR experiences.\nACM Classiﬁcation Keywords\nH.5.m. Information Interfaces and Presentation (e.g. HCI):\nMiscellaneous\nAuthor Keywords\nMultisensory VR; Virtual Reality; Multimodal Interaction\nINTRODUCTION\nAs the human perceptual system has been conditioned by the\nperception of real-world multisensory stimuli (e.g. auditory,\nvisual, haptic and olfactory) it is crucial to appeal to these same\nperceptual mechanisms when aiming to simulate an immersive","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a\nfee. Request permissions from permissions@acm.org.\nCHI 2018, April 21–26, 2018, Montreal, QC, Canada\n© 2018 ACM. ISBN 978-1-4503-5620-6/18/04. . . $15.00\nDOI: https://doi.org/10.1145/3173574.3174151","doi":"10.1145/3173574.3174151","sections":[{"word_count":906,"figure_citations":{"1":["Figure 1: Season Traveller is a multisensory VR experience integrated with Samsung Gear VR HMD."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1029,"figure_citations":{},"section_index":1,"title":"BACKGROUND AND RELATED WORK"},{"word_count":2372,"figure_citations":{"1":["Figure 1) that aims to distribute the weight of the system evenly between the front and the back of the user’s head.","Figure 1).","Figure 1)."],"2":["Figure 2, the chamber contained the smell emitting mechanisms, a control module, a fume extractor for clearing scented air between participant exposures, and a nose rest that ensured participants maintained a consistent distance from the stimuli.","Figure 2: The experimental setup of the smell chamber used to compare olfactory delivery methods."],"3":["Figure 3: Average sensation scores based on Labeled Magnitude Scale (LMS)."],"4":["Figure 4: Main modules of Season Traveller: (a) Control Module, (b) Olfactory Simulation Module, (c) Front Casing and Wind Simulation Module, (d) Thermal Simulation Module participants (10 males, average age = 26, SD = 2."]},"section_index":2,"title":"DESIGN"},{"word_count":615,"figure_citations":{"1":["Figure 1, both the Control Module and all of the Simulation Modules were mounted on the Samsung Gear VR Paper 577 CHI 2018, April 21–26, 2018, Montréal, QC, Canada (a) Spring (b) Summer (c) Autumn (d) Winter Figure 5: The four seasons and their stimuli: a) Spring (Jasmine scent, medium wind strength and no thermal stimulation), b) Summer (Lemon scent, mild wind strength and heating stimulation), c) Autumn (Cinnamon scent, strong wind strength and no thermal stimulation) and d) Winter (Mint scent, medium wind strength and cooling stimulation)."],"4":["Figure 4)."],"5":["Figure 5)."]},"section_index":3,"title":"SYSTEM IMPLEMENTATION"},{"word_count":1854,"figure_citations":{"6":["Figure 6 for study setup).","Figure 6: The experimental setup of Season Traveller."],"7":["Figure 7: Normalized average Sensory sub-factors scores for five system configurations (Error bars represent 95% CI, n = 20)."],"8":["Figure 8: Normalized average Sensory Factors scores for five system configurations (Error bars represent 95% CI, n = 20)."],"9":["Figure 9: Normalized average scores for item 30 (richness of experience) (Error bars represent 95% CI, n = 20).","Figure 9, post-hoc tests further indicate a significant improvement (p = 0."],"10":["Figure 10 presents the baseline adjusted HR of all the participants recorded through the different system configurations."],"11":["Figure 11 presents the baseline adjusted electrical skin conductivity of all the participants."]},"section_index":4,"title":"EVALUATION"},{"word_count":685,"figure_citations":{"10":["Figure 10: Baseline adjusted HR of all the participants recorded through the five different system configurations."],"11":["Figure 11: Baseline adjusted EDA of all the participants recorded through the five different system configurations."]},"section_index":5,"title":"DISCUSSION AND CONCLUSION"},{"word_count":29,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":42,"figure_citations":{},"section_index":7,"title":"REFERENCES"},{"word_count":1496,"figure_citations":{},"section_index":8,"title":"ESSEX CORP WESTLAKE VILLAGE CA HUMAN"}],"title":"Season Traveller: Multisensory Narration for Enhancing the Virtual Reality Experience","authors":"Nimesha Ranasinghe, Pravar Jain, Nguyen Thi Ngoc Tram, Koon Chuan Raymond Koh, David Tolley, Shienny Karwita, Lin Lien-Ya, Yan Liangkun, Kala Shamaiah, Chow Eason Wai Tung, Ching Chiuan Yen, Ellen Yi-Luen Do","abstract":"In the same way that we experience the real-world through a range of senses, experiencing a virtual environment through multiple sensory modalities may augment both our presence within a scenario and our reaction to it. In this paper, we present Season Traveller, a multisensory virtual reality (VR) narration of a journey through four seasons within a mystical realm. By adding olfactory and haptic (thermal and wind) stimuli, we extend traditional audio-visual VR technologies to achieve enhanced sensory engagement within interactive experiences. Using both subjective measures of presence and elicited physiological responses, we evaluated the impact of different modalities on the virtual experience. Our results indicate that 1) the addition of any singular modality improves sense of presence with respect to traditional audio-visual experiences and 2) providing a combination of these modalities produces a further significant enhancement over the aforementioned improvements. Furthermore, insights into participants' psychophysiology were extrapolated from electrodermal activity (EDA) and heart rate (HR) measurements during each of the VR experiences.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":2},"fig":{"captions":{"1":{"caption":"Figure 1: Season Traveller is a multisensory VR experience\nintegrated with Samsung Gear VR HMD. It enhances partici-\npants’ sense of presence using visual, auditory, wind, thermal,\nand olfactory stimuli.\n","bbox":[321.094,322.875992,565.8298444989999,365.71599199999997],"page":"1"},"2":{"caption":"Figure 2: The experimental setup of the smell chamber used\nto compare olfactory delivery methods.\n","bbox":[53.929,471.226992,297.01195291,492.148992],"page":"5"},"3":{"caption":"Figure 3: Average sensation scores based on Labeled Magni-\ntude Scale (LMS). (Error bars represent 95% CI, n = 15).\n","bbox":[321.094,510.331992,565.832683954,531.2539919999999],"page":"5"},"4":{"caption":"Figure 4: Main modules of Season Traveller: (a) Control\nModule, (b) Olfactory Simulation Module, (c) Front Casing\nand Wind Simulation Module, (d) Thermal Simulation Module\n","bbox":[53.929,522.3069919999999,297.01025920000006,554.1879919999999],"page":"6"},"5":{"caption":"Figure 5: The four seasons and their stimuli: a) Spring (Jas-\nmine scent, medium wind strength and no thermal stimula-\ntion), b) Summer (Lemon scent, mild wind strength and heat-\ning stimulation), c) Autumn (Cinnamon scent, strong wind\nstrength and no thermal stimulation) and d) Winter (Mint scent,\nmedium wind strength and cooling stimulation).\n","bbox":[321.094,494.54599199999996,565.83170758,559.3029919999999],"page":"6"},"6":{"caption":"Figure 6: The experimental setup of Season Traveller. Physio-\nlogical signals were recorded during each participant’s experi-\nence with the system.\n","bbox":[321.094,502.150992,565.827303934,534.031992],"page":"7"},"7":{"caption":"Figure 7: Normalized average Sensory sub-factors scores for ﬁve system conﬁgurations (Error bars represent 95% CI, n = 20).\nSigniﬁcant differences are highlighted by asterisks.\n","bbox":[53.929,547.049992,565.9296423039998,567.971992],"page":"8"},"8":{"caption":"Figure 8: Normalized average Sensory Factors scores for ﬁve\nsystem conﬁgurations (Error bars represent 95% CI, n = 20).\n","bbox":[53.929,620.8159919999999,297.01020938500005,641.7379920000001],"page":"9"},"9":{"caption":"Figure 9: Normalized average scores for item 30 (richness of\nexperience) (Error bars represent 95% CI, n = 20).\n","bbox":[321.094,622.906992,564.1750001620001,643.828992],"page":"9"},"10":{"caption":"Figure 10: Baseline adjusted HR of all the participants\nrecorded through the ﬁve different system conﬁgurations.\nSolid lines represent the means and the ﬁlled areas represent\ntheir respective standard deviations.\n","bbox":[53.929,495.962992,298.7480056600001,538.8019919999999],"page":"10"},"11":{"caption":"Figure 11: Baseline adjusted EDA of all the participants\nrecorded through the ﬁve different system conﬁgurations.\nSolid lines represent the means and the ﬁlled areas represent\ntheir respective standard deviations.\n","bbox":[321.094,495.962992,565.9130056600001,538.8019919999999],"page":"10"}},"crops":{"1":{"crop_coord":[907.1833377777776,698.7347155555556,1551.8333349999998,1175.6722258333334],"bbox":[328.3860016,370.5579987,556.8600005999999,538.6555023999999],"page":"1"},"2":{"crop_coord":[168.4361013888889,168.2268863888888,806.3640933333334,824.4694519444444],"bbox":[62.4369965,496.9909973,288.4910736,729.6383209],"page":"5"},"3":{"crop_coord":[893.6833361111111,168.22790361111134,1565.3686863888888,715.8472188888892],"bbox":[323.526001,536.0950012,561.7327271,729.6379546999999],"page":"5"},"4":{"crop_coord":[144.80278027777777,168.22222388888878,829.9760522222223,652.138892222222],"bbox":[53.9290009,559.0299988,296.9913788,729.6399994000001],"page":"6"},"5":{"crop_coord":[901.8722194444445,168.23044666666675,1235.925538611111,351.90276249999994],"bbox":[326.473999,667.1150055,443.1331939,729.6370392],"page":"6"},"6":{"crop_coord":[1255.8527713888889,168.23044666666675,1589.9060905555557,351.90276249999994],"bbox":[453.9069977,667.1150055,570.5661926,729.6370392],"page":"6"},"7":{"crop_coord":[901.8722194444445,398.0804527777778,1235.925538611111,581.752768611111],"bbox":[326.473999,584.3690033,443.1331939,646.891037],"page":"6"},"8":{"crop_coord":[1255.8527713888889,398.0804527777778,1589.9060905555557,581.752768611111],"bbox":[453.9069977,584.3690033,570.5661926,646.891037],"page":"6"},"9":{"crop_coord":[903.8111030555555,168.2269288888891,1555.2700805555555,708.1277975],"bbox":[327.1719971,538.8739929,558.097229,729.6383056],"page":"7"},"10":{"crop_coord":[144.80278027777777,168.23226944444428,1572.10328,613.8527680555555],"bbox":[53.9290009,572.8130035,564.1571808,729.636383],"page":"8"},"11":{"crop_coord":[159.74723805555556,681.340077777778,1587.0724061111111,1145.7972125000001],"bbox":[59.3090057,381.3130035,569.5460662,544.917572],"page":"8"},"12":{"crop_coord":[144.80278027777777,168.22781888888892,829.9735516666667,408.9444394444444],"bbox":[53.9290009,646.5800018,296.9904786,729.6379852],"page":"9"},"13":{"crop_coord":[886.9277613888887,168.23150638888882,1572.1294741666666,403.13889388888873],"bbox":[321.0939941,648.6699982,564.1666107,729.6366577],"page":"9"},"14":{"crop_coord":[144.80278027777777,168.20450666666682,829.9841055555556,694.8777855555558],"bbox":[53.9290009,543.6439972,296.994278,729.6463775999999],"page":"10"},"15":{"crop_coord":[886.9277613888887,168.20450666666682,1572.1090866666666,694.8777855555558],"bbox":[321.0939941,543.6439972,564.1592711999999,729.6463775999999],"page":"10"}}}},{"filename":"3173574.3174221","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/CHI 18/3173574.3174221.pdf","paper_id":"3173574.3174221","venue":"CHI 18","keywords":["Virtual reality","Text entry","Pointing","Mid-air","User experience","Task performance"],"paragraph_containing_keyword":"ABSTRACT\nIn recent years, Virtual Reality (VR) and 3D User Interfaces\n(3DUI) have seen a drastic increase in popularity, especially\nin terms of consumer-ready hardware and software. While\nthe technology for input as well as output devices is market\nready, only a few solutions for text input exist, and empirical\nknowledge about performance and user preferences is lacking.\nIn this paper, we study text entry in VR by selecting characters\non a virtual keyboard. We discuss the design space for assess-\ning selection-based text entry in VR. Then, we implement six\nmethods that span different parts of the design space and eval-\nuate their performance and user preferences. Our results show\nthat pointing using tracked hand-held controllers outperforms\nall other methods. Other methods such as head pointing can be\nviable alternatives depending on available resources. We sum-\nmarize our ﬁndings by formulating guidelines for choosing\noptimal virtual keyboard text entry methods in VR.\nACM Classiﬁcation Keywords\nH.5.m. Information Interfaces and Presentation (e.g. HCI):\nMiscellaneous\nAuthor Keywords\nVirtual reality; text entry; pointing; mid-air; user experience;\ntask performance.\nINTRODUCTION\nAs text-based communication is rarely studied in VR research\nregarding text entry performance [14, 35, 36, 43], there is a","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a\nfee. Request permissions from permissions@acm.org.\nCHI 2018, April 21–26, 2018, Montr´eal, QC, Canada.\nCopyright is held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-5620-6/18/04 ...$15.00.\nhttps://doi.org/10.1145/3173574.3174221","doi":"10.1145/3173574.3174221","sections":[{"word_count":13,"figure_citations":{},"section_index":0,"title":"NG"},{"word_count":2,"figure_citations":{},"section_index":1,"title":"HAND"},{"word_count":2,"figure_citations":{},"section_index":2,"title":"T"},{"word_count":215,"figure_citations":{},"section_index":3,"title":"NUOUSCURSOR"},{"word_count":508,"figure_citations":{},"section_index":4,"title":"INTRODUCTION"},{"word_count":2892,"figure_citations":{"1":["Figure 1): • Head Pointing (HP) — the participant selects a character by pointing to it with her head."]},"section_index":5,"title":"X"},{"word_count":1092,"figure_citations":{"1":["Figure 1).","Figure 1).","Figure 1).","Figure 1)."]},"section_index":6,"title":"EVALUATED TEXT INPUT TECHNIQUES"},{"word_count":35,"figure_citations":{},"section_index":7,"title":"EMPIRICAL STUDY"},{"word_count":13,"figure_citations":{},"section_index":8,"title":"HP"},{"word_count":2,"figure_citations":{},"section_index":9,"title":"CP"},{"word_count":2,"figure_citations":{},"section_index":10,"title":"CT"},{"word_count":2,"figure_citations":{},"section_index":11,"title":"FH"},{"word_count":2,"figure_citations":{},"section_index":12,"title":"CC"},{"word_count":6,"figure_citations":{},"section_index":13,"title":"DC"},{"word_count":2,"figure_citations":{},"section_index":14,"title":"WHO"},{"word_count":3,"figure_citations":{},"section_index":15,"title":"WANTS TO"},{"word_count":2,"figure_citations":{},"section_index":16,"title":"VER"},{"word_count":2,"figure_citations":{},"section_index":17,"title":"TS TO LIVE"},{"word_count":746,"figure_citations":{"3":["Figure 3)."]},"section_index":18,"title":"FO"},{"word_count":508,"figure_citations":{},"section_index":19,"title":"S"},{"word_count":174,"figure_citations":{"4":["Figure 4)."]},"section_index":20,"title":"RESULTS"},{"word_count":486,"figure_citations":{"5":["Figure 5)."]},"section_index":21,"title":"WPM"},{"word_count":961,"figure_citations":{},"section_index":22,"title":"DISCUSSION"},{"word_count":2,"figure_citations":{},"section_index":23,"title":"START"},{"word_count":11,"figure_citations":{},"section_index":24,"title":"YES"},{"word_count":2,"figure_citations":{},"section_index":25,"title":"HEAD POINTING"},{"word_count":10,"figure_citations":{},"section_index":26,"title":"NO"},{"word_count":138,"figure_citations":{},"section_index":27,"title":"L"},{"word_count":14,"figure_citations":{},"section_index":28,"title":"H"},{"word_count":97,"figure_citations":{},"section_index":29,"title":"CURSOR CONTROL"},{"word_count":2,"figure_citations":{},"section_index":30,"title":"POINTING"},{"word_count":290,"figure_citations":{"7":["Figure 7)."]},"section_index":31,"title":"FREEHAND"},{"word_count":287,"figure_citations":{},"section_index":32,"title":"CONCLUSION AND OUTLOOK"},{"word_count":1719,"figure_citations":{},"section_index":33,"title":"REFERENCES"}],"title":"Selection-based Text Entry in Virtual Reality","authors":"Marco Speicher, Anna Maria Feit, Pascal Ziegler, Antonio Krüger","abstract":"In recent years, Virtual Reality (VR) and 3D User Interfaces (3DUI) have seen a drastic increase in popularity, especially in terms of consumer-ready hardware and software. While the technology for input as well as output devices is market ready, only a few solutions for text input exist, and empirical knowledge about performance and user preferences is lacking. In this paper, we study text entry in VR by selecting characters on a virtual keyboard. We discuss the design space for assessing selection-based text entry in VR. Then, we implement six methods that span different parts of the design space and evaluate their performance and user preferences. Our results show that pointing using tracked hand-held controllers outperforms all other methods. Other methods such as head pointing can be viable alternatives depending on available resources. We summarize our findings by formulating guidelines for choosing optimal virtual keyboard text entry methods in VR.","publication":{"venue":"CHI '18","venue_full":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","year":"2018","date":"2018/4/21"},"version":2},"fig":{"captions":{"1":{"caption":"Figure 1. This ﬁgure illustrates our implemented selection-based text entry candidates for VR. From left to right: Head Pointing (HP, red), Controller\nPointing (CP, yellow), Controller Tapping (CT, blue), Freehand (FH, green), Discrete (DC, orange) and Continuous Cursor (CC, light blue).\n","bbox":[53.929,500.82627,564.335770000001,517.7622700000001],"page":"1"},"2":{"caption":"Figure 2. This ﬁgure illustrates the experimental setup. (1) HTC Vive\noptical tracker (at 2.5m) and tracking space with 4 × 4m2. (2) Virtual\nkeyboard, stimulus and text input ﬁeld. (3) Participant wearing HTC\nVive and tracked hand-held controllers. (4) PC for experiment control\nand ﬁlling out questionnaires.\n","bbox":[53.642,544.44927,296.9980600000002,588.28527],"page":"6"},"3":{"caption":"Figure 3. This ﬁgure shows the virtual environment including stimulus\n(purple), text input ﬁeld, and the virtual keyboard.\n","bbox":[320.831,573.91427,564.1630600000001,590.85027],"page":"6"},"4":{"caption":"Figure 4. The upper ﬁgure show the speed measurements, given in words\nper minute (WPM). Below, the corrected error rate measurements, given\nin percent (%).\n","bbox":[321.094,77.62527000000001,564.1630600000001,103.52827],"page":"7"},"5":{"caption":"Figure 5. User Experience Questionnaire (UEQ) ratings with respect to\ncomparison benchmarks.\n","bbox":[53.929,77.62527,296.9980600000002,94.56227],"page":"8"},"6":{"caption":"Figure 6. This ﬁgure shows the NASA subscales with signiﬁcant differences between the six text input methods. Non-signiﬁcant subscales (mental\ndemand, temporal demand, and effort) are not shown for better clarity.\n","bbox":[53.929,547.1302699999999,564.1604300000007,564.0662699999999],"page":"9"},"7":{"caption":"Figure 7. Decision support tool for VR text input on a virtual keyboard.\nDiscrete Cursor is not considered, because of the bad results across all\nmeasurements. Controller Tapping performed slightly worse than Point-\ning, so it is not considered due to its higher technical requirements.\n","bbox":[321.094,377.58927,565.5578100000001,412.45826999999997],"page":"10"}},"crops":{"1":{"crop_coord":[144.80278027777777,466.27660944444443,1572.1911036111107,753.8416461111111],"bbox":[53.9290009,522.4170074,564.1887972999999,622.3404206],"page":"1"},"2":{"crop_coord":[144.80305651813885,119.64968820694438,829.9958398514723,557.9444377777778],"bbox":[53.92910034652999,592.9400024,296.99850234653,747.1261122455],"page":"6"},"3":{"crop_coord":[886.9277613888887,168.23108249999999,1572.1029830555556,550.8194308333334],"bbox":[321.0939941,595.5050049,564.1570739,729.6368103],"page":"6"},"4":{"crop_coord":[886.9277613888887,919.9370575000002,1572.131127222222,1904.4916702777778],"bbox":[321.0939941,108.1829987,564.1672057999999,459.0226593],"page":"7"},"5":{"crop_coord":[144.80278027777777,1229.7078961111113,829.9992794444444,1929.3972355555557],"bbox":[53.9290009,99.2169952,296.9997406,347.5051574],"page":"8"},"6":{"crop_coord":[144.80278027777777,168.22599638888875,1572.1487597222224,625.2194638888888],"bbox":[53.9290009,568.720993,564.1735535,729.6386413],"page":"9"},"7":{"crop_coord":[886.9276230335057,131.00399062530585,1602.5596238391724,1046.3524373033888],"bbox":[321.093944292062,417.11312257078004,575.121464582102,743.0385633748899],"page":"10"}}}},{"filename":"3313831.3376195","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376195.pdf","paper_id":"3313831.3376195","venue":"chi2020","keywords":["Haptics","Temperature","Thermal Feedback","Virtual Reality"],"paragraph_containing_keyword":"Author Keywords \nHaptics, Temperature, Thermal Feedback, Virtual Reality","paragraph_after_keyword":"CCS Concepts \n•Human-centered computing → Human computer inter-\naction (HCI); Haptic devices; User studies;","doi":"10.1145/3313831.3376195","sections":[{"word_count":760,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1376,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1117,"figure_citations":{"2":["Figure 2a).","Figure 2b)."],"3":["Figure 3c depicts a closeup perspective of the actuator tubes, as well as an example thermal camera view at 43 ◦C.","Figure 3b shows a user wearing two actuators on the right arm and the abdomen."]},"section_index":2,"title":"THERMINATOR CONCEPTS AND SYSTEM"},{"word_count":2192,"figure_citations":{"1":["Figure 1a) shows a participant while exposed to the snow visualization."],"3":["Figure 3a."]},"section_index":3,"title":"METHODOLOGY"},{"word_count":1447,"figure_citations":{"5":["Figure 5a.","Figure 5b.","Figure 5c, the median rating as well as the minimum and maximum ratings show differences with regards to the visuals."]},"section_index":4,"title":"RESULTS"},{"word_count":895,"figure_citations":{"5":["Figure 5a), the thermal stimuli have the highest impact on the perceived temperature.","Figure 5b and Figure 5c), the involvement resulted in higher medians for both stimuli.","Figure 5a), they only slightly affect the involvement if there is no visual stimulus displayed."],"6":["Figure 6)."]},"section_index":5,"title":"DISCUSSION"},{"word_count":433,"figure_citations":{"1":["Figure 1d).","Figure 1c), discovered on a treasure map.","Figure 1b)."]},"section_index":6,"title":"EXAMPLE APPLICATIONS"},{"word_count":421,"figure_citations":{},"section_index":7,"title":"LIMITATIONS AND FUTURE WORK"},{"word_count":105,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":41,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGEMENTS"},{"word_count":2593,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"Therminator: Understanding the Interdependency of Visual and On-Body Thermal Feedback in Virtual Reality","authors":"Sebastian Günther, Florian Müller, Dominik Schön, Omar Elmoghazy, Max Mühlhäuser, Martin Schmitz","abstract":"Recent advances have made Virtual Reality (VR) more realistic than ever before. This improved realism is attributed to today's ability to increasingly appeal to human sensations, such as visual, auditory or tactile. While research also examines temperature sensation as an important aspect, the interdependency of visual and thermal perception in VR is still underexplored. In this paper, we propose Therminator, a thermal display concept that provides warm and cold on-body feedback in VR through heat conduction of flowing liquids with different temperatures. Further, we systematically evaluate the interdependency of different visual and thermal stimuli on the temperature perception of arm and abdomen with 25 participants. As part of the results, we found varying temperature perception depending on the stimuli, as well as increasing involvement of users during conditions with matching stimuli.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1.  Therminator concepts and example VR applications showing (a) a user during our experiment with a snow visual stimulus, (b) a cold game \nenvironment with a user throwing snowballs, (c) a warm tropical islands, and (d) a ﬁreﬁghting simulation with a user extinguishing ﬂames. \n","bbox":[53.929,456.59902,566.15293,473.53526999999997],"page":"1"},"2":{"caption":"Figure 2.  Concept of our Therminator system.  a) We use heat conducting tubes which can be ﬂexibly formed to ﬁt various shapes of different body \nparts.  We let liquids with adjustable temperatures ﬂow through the tubes which are then emitting their temperature.  b) We have four parameters to \nadjust the temperature, arrangement and number, shape, and size of each tube. \n","bbox":[53.929,611.6157700000001,566.15293,637.51827],"page":"3"},"4":{"caption":"Figure 4. Visual stimuli based on their expected temperature from cold to hot: snow, rain cloud, neutral / no visualization, heating lamp, ﬁre. \n","bbox":[70.384,618.15027,549.6997999999996,626.12027],"page":"5"},"6":{"caption":"Figure 6. Comfort rating of the participants with regards to the thermal \nstimuli (from very uncomfortable to very comfortable). Each temperature \nlevel is split into both body parts. \n","bbox":[53.929,77.62177,298.99056,103.52427],"page":"8"}},"crops":{"1":{"crop_coord":[144.80278027777777,649.2481316666666,1572.1023052777775,876.6944377777778],"bbox":[53.9290009,478.1900024,564.1568298999999,556.4706726000001],"page":"1"},"2":{"crop_coord":[144.80278027777777,195.90090444444436,849.3191105555555,392.0944552777775],"bbox":[53.9290009,652.6459961,303.9548798,719.6756744],"page":"3"},"3":{"crop_coord":[867.6305644444444,195.90090444444436,1572.146894722222,392.0944552777775],"bbox":[314.1470032,652.6459961,564.1728820999999,719.6756744],"page":"3"},"4":{"crop_coord":[144.80278027777777,208.43078611111136,792.6132966666665,585.2833133333334],"bbox":[53.9290009,583.0980072,283.5407868,715.164917],"page":"4"},"5":{"crop_coord":[803.8527680555557,216.22790027777774,1210.7034811111112,585.2833133333334],"bbox":[291.1869965,583.0980072,434.05325320000003,712.3579559],"page":"4"},"6":{"crop_coord":[1221.9610936111112,195.90243027777794,1572.1354083333333,585.2833133333334],"bbox":[441.7059937,583.0980072,564.168747,719.6751251],"page":"4"},"7":{"crop_coord":[144.80278027777777,168.228708888889,1572.139477222222,447.3138936111112],"bbox":[53.9290009,632.7669983,564.1702118,729.6376647999999],"page":"5"},"8":{"crop_coord":[144.80278027777777,195.9025572222223,622.5148011111111,531.6305711111112],"bbox":[53.9290009,602.4129944,222.3053284,719.6750794],"page":"7"},"9":{"crop_coord":[619.6055602777777,195.9025572222223,1097.3175811111112,531.6305711111112],"bbox":[224.8580017,602.4129944,393.2343292,719.6750794],"page":"7"},"10":{"crop_coord":[1094.4055430555557,195.9025572222223,1572.1175638888888,531.6305711111112],"bbox":[395.7859955,602.4129944,564.162323,719.6750794],"page":"7"},"11":{"crop_coord":[144.80278027777777,1497.4934552777777,816.4849802777777,1904.5027752777778],"bbox":[53.9290009,108.1790009,292.1345929,251.1023561],"page":"8"}}}},{"filename":"3313831.3376228","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376228.pdf","paper_id":"3313831.3376228","venue":"chi2020","keywords":["Virtual reality","Notification systems","Interruptibility","Receptivity","Eye-tracking"],"paragraph_containing_keyword":"ABSTRACT \nVirtual  reality  (VR)  platforms  provide  their  users  with \nimmersive  virtual environments, but disconnect  them  from \nreal-world events. The increasing length of VR sessions can \ntherefore  be  expected  to  boost  users’  needs  to  obtain \ninformation  about  external  occurrences  such  as  message \narrival.  Yet,  how  and  when  to  present  these  real-world \nnotifications  to  users  engaged  in  VR  activities  remains \nunderexplored. We conducted an experiment to investigate \nindividuals’ receptivity during four VR activities (Loading, \n360  Video,  Treasure  Hunt,  Rhythm  Game)  to  message \nnotifications delivered using three types of displays (head-\nmounted,  controller,  and  movable  panel).  While  higher \nthat \nengagement  generally \nnotifications were ill-timed and/or disruptive, the suitability \nof  notification  displays  to  VR  activities  was  influenced  by \nthe  time-sensitiveness  of  VR  content,  overlapping  use  of \nmodalities for delivering alerts, the display locations, and a \nrequirement that the display be moved for notifications to be \nseen. Specific design suggestions are also provided. \nAuthor Keywords \nVirtual reality; notification systems; interruptibility; \nreceptivity; eye-tracking \nCCS Concepts \n• Human-centered computing~Laboratory experiments; \nVirtual reality \nINTRODUCTION \nIn recent years, a variety of immersive  virtual reality (VR) \napplications  have  been  developed  and  popularized.  All \nleverage  various  modalities  to  provide  their  users  with \ndiverse  experiences in immersive virtual environments.  By \ntheir nature, these immersive experiences can result in users \nlosing  connectedness  with  the  real  world,  including  by \ndimming their awareness of incoming calls, text messages, \nand other phone notifications. Far from wanting to fully drop \nout of reality, however, VR users may desire to be notified \nabout real-world events such as messages, and in some cases \nPermission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advantage and that copies bear this notice and the full citation \non the first page. Copyrights for components of this work owned by others than the \nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or \nrepublish, to post on servers or to redistribute to lists, requires prior specific permission \nand/or a fee. Request permissions from Permissions@acm.org. \n \nCHI '20, April 25–30, 2020, Honolulu, HI, USA \n©  2020 Copyright is held by the owner/author(s). Publication rights licensed to \nACM. ACM 978-1-4503-6708-0/20/04...$15.00 \nhttps://doi.org/10.1145/3313831.3376228","doi":"10.1145/3313831.3376228","paragraph_after_keyword":"to","sections":[{"word_count":646,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":525,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1922,"figure_citations":{"1":["Figure 1, top left) was inspired by the Steam VR Home.","Figure 1, top right) is a common VR activity for individuals wearing HMDs.","Figure 1 bottom left) is a popular VR gaming format.","Figure 1, bottom right) was inspired by a popular VR game called Beat Saber1."],"2":["Figure 2).","Figure 2a) was fixed to the upper left corner in the user’s field of view: specifically, in the near-peripheral region, about 25 degrees from the line of sight [61].","Figure 2b) was inspired by NotifiVR [20].","Figure 2).","Figure 2c) was inspired by Facebook Space’s2 information pad."],"3":["Figure 3, top).","Figure 3, bottom), in each of which participant experienced one of the three VR activities (360 Video, Treasure Hunt, and Rhythm Game) preceded by a Loading activity to simulate system loading."]},"section_index":2,"title":"THE EXPERIMENT"},{"word_count":273,"figure_citations":{},"section_index":3,"title":"DATA ANALYSIS"},{"word_count":2191,"figure_citations":{"4":["Figure 4 indicates, they were the most engaged in Rhythm Game (M=6."],"5":["Figure 5, showing the percentages of notifications the participants actually looked at, indicates that they failed to see a significant portion of notifications in Rhythm Game, especially when alerts were sent via the controller (59."],"6":["Figure 6, top left), followed by Treasure Hunter (M=3.","Figure 6, bottom left), all the previously noted differences held true, but in Rhythm Game, alerts presented via controller display (M=5.","Figure 6, top center): i.","Figure 6, top right).","Figure 6, bottom right), we observed a significant increase in the recall of notifications for all displays (HMD: 75%=> 77."]},"section_index":4,"title":"RESULTS"},{"word_count":729,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":1096,"figure_citations":{},"section_index":6,"title":"DESIGN RECOMMENDATIONS"},{"word_count":390,"figure_citations":{},"section_index":7,"title":"LIMITATIONS"},{"word_count":266,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":56,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGMENTS"},{"word_count":2205,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"Bridging the Virtual and Real Worlds: A Preliminary Study of Messaging Notifications in Virtual Reality","authors":"Ching-Yu Hsieh, Yi-Shyuan Chiang, Hung-Yu Chiu, Yung-Ju Chang","abstract":"Virtual reality (VR) platforms provide their users with immersive virtual environments, but disconnect them from real-world events. The increasing length of VR sessions can therefore be expected to boost users' needs to obtain information about external occurrences such as message arrival. Yet, how and when to present these real-world notifications to users engaged in VR activities remains underexplored. We conducted an experiment to investigate individuals' receptivity during four VR activities (Loading, 360 Video, Treasure Hunt, Rhythm Game) to message notifications delivered using three types of displays (head-mounted, controller, and movable panel). While higher engagement generally led to higher perceptions that notifications were ill-timed and/or disruptive, the suitability of notification displays to VR activities was influenced by the time-sensitiveness of VR content, overlapping use of modalities for delivering alerts, the display locations, and a requirement that the display be moved for notifications to be seen. Specific design suggestions are also provided.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"3":{"caption":"Figure 3. A sample of the study procedure (top) and the timeline \nof each block (bottom). \none  11-20  times.  The  remaining  11  (27.5%)  had  used  VR \nmore than 20 times. The participants were balanced in terms \nof  their  self-reported  receptivity  to  message  notifications, \nwith  10  individuals  fitting  into  each  of  the  following  four \ncategories: 1) tend to ignore notifications; 2) tend not to deal \nwith notifications immediately, but check and respond later; \n3) tend to check notifications immediately, but respond later; \nand 4) tend to check and respond immediately.  \n","bbox":[54,337.6186400000001,297.7410801599999,450.88599999999997],"page":"4"},"4":{"caption":"Figure 4. Means and SDs of engagement, by VR activity. \n","bbox":[64.464,597.556,283.34,606.556],"page":"5"},"6":{"caption":"Figure 6. All notifications’ means and SDs of the outcome variables: Perceived Disruptiveness (top left), Perceived Timeliness (top \ncenter) and Recall (top right); seen notifications’ means and SDs of the outcome variables: Perceived Disruptiveness (bottom left), \nPerceived Timeliness (bottom center) and Recall (bottom right). \n","bbox":[54,422.566,560.227,452.20599999999996],"page":"6"}},"crops":{"1":{"crop_coord":[875.0000086111113,1494.3027580555554,1207.7777777777778,1677.6111094444445],"bbox":[316.8000031,189.8600006,433,252.2510071],"page":"2"},"2":{"crop_coord":[1218.8889058333332,1494.3028005555557,1551.2500255555556,1679.2777930555553],"bbox":[440.6000061,189.2599945,556.6500092,252.2509918],"page":"2"},"3":{"crop_coord":[875.0000086111113,1685.9722222222222,1207.7777777777778,1869.027786388889],"bbox":[316.8000031,120.9499969,433,183.25],"page":"2"},"4":{"crop_coord":[1218.611111111111,1685.9750197222222,1551.5277777777778,1869.027786388889],"bbox":[440.5,120.9499969,556.75,183.24899290000002],"page":"2"},"5":{"crop_coord":[145,165.008341388889,501.2499830555556,434.6944511111112],"bbox":[54,637.3099976,178.6499939,730.7969971],"page":"3"},"6":{"crop_coord":[498.1944275,165.008341388889,854.4444105555556,434.6944511111112],"bbox":[181.1499939,637.3099976,305.7999878,730.7969971],"page":"3"},"7":{"crop_coord":[851.3888975,165.008341388889,1207.6388805555557,434.6944511111112],"bbox":[308.3000031,637.3099976,432.94999700000005,730.7969971],"page":"3"},"8":{"crop_coord":[1197.6388802777776,165.19445611111118,1537.4999913888887,434.72221361111104],"bbox":[432.9499969,637.3000031,551.6999969,730.7299958],"page":"3"},"9":{"crop_coord":[228.33333333333334,165.00003388888882,742.2222052777778,590.0000169444445],"bbox":[84,581.3999939,265.3999939,730.7999878],"page":"4"},"10":{"crop_coord":[228.33333333333334,581.6666580555556,742.1111127777779,939.7222222222222],"bbox":[84,455.5,265.36000060000003,580.8000031],"page":"4"},"11":{"crop_coord":[297.27779805555554,164.99999166666692,662.3333738888889,513.4722308333335],"bbox":[108.8200073,608.9499969,236.6400146,730.800003],"page":"5"},"12":{"crop_coord":[941.7222425,164.99999166666692,1482.9722425000002,513.4722308333335],"bbox":[340.8200073,608.9499969,532.0700073,730.800003],"page":"5"},"13":{"crop_coord":[188.05555555555554,165.41664111111103,1511.6111247222223,943.6110941666667],"bbox":[69.5,454.1000061,542.3800049,730.6500092],"page":"6"}}}},{"filename":"3313831.3376243","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376243.pdf","paper_id":"3313831.3376243","venue":"chi2020","keywords":["VR","Virtual reality","Jumping","Super human","Immersion","Hyper realism"],"doi":"10.1145/3313831.3376243","paragraph_containing_keyword":"parameters for its hyper-realistic scaling. We discuss design \nimplications for VR experiences and research. \nAuthor Keywords \nVR; virtual reality; jumping; super human; immersion; hyper \nrealism. \nCCS Concepts \n•Human-centered  computing  →  Interaction  techniques; \nEmpirical studies in HCI; \nINTRODUCTION \nModern virtual reality (VR) provides an environment with \ntheoretically unlimited possibilities. We can assume different \nroles,  gain  new  abilities  and  explore  ﬁctional  worlds,  in  a \ntechnologically  and  emotionally  immersive  world  overlaid \non top of real life.  While a large body of research focuses \non achieving increasingly “realistic” experiences in VR such \nas feeling haptic feedback when touching virtual walls [4], \ngetting hit by  objects [38],  or climbing physical steps [32, \n23], there is no reason to restrict our imagination to physical \nlimitations set by the world we know. As children, we are often \ninspired by superheroes, dreaming of gaining similar abilities \none day [24]. In growing older, children learn to distinguish \nbetween  fact  and  ﬁction,  but  we  argue  that  a  yearning  for","sections":[{"word_count":476,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1216,"figure_citations":{"1":["Figure 1c) [3]."]},"section_index":1,"title":"RELATED WORK"},{"word_count":154,"figure_citations":{},"section_index":2,"title":"RESEARCH FOCUS"},{"word_count":717,"figure_citations":{},"section_index":3,"title":"IMPLEMENTATION"},{"word_count":1026,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2b).","Figure 2a).","Figure 2a), MoveInPlace was also enabled.","Figure 2a."],"3":["Figure 3 (all shown jumps were executed with a mean airtime of 250ms)."]},"section_index":4,"title":"EVALUATION"},{"word_count":770,"figure_citations":{},"section_index":5,"title":"RESULTS"},{"word_count":2,"figure_citations":{},"section_index":6,"title":"C ONDITION PAIR"},{"word_count":116,"figure_citations":{},"section_index":7,"title":"SSQ"},{"word_count":25,"figure_citations":{},"section_index":8,"title":"IMI"},{"word_count":281,"figure_citations":{},"section_index":9,"title":"IPQ"},{"word_count":3,"figure_citations":{},"section_index":10,"title":"C ONDITION"},{"word_count":3,"figure_citations":{},"section_index":11,"title":"J UMPS"},{"word_count":1981,"figure_citations":{},"section_index":12,"title":"FALLS"},{"word_count":105,"figure_citations":{},"section_index":13,"title":"CONCLUSION"},{"word_count":24,"figure_citations":{},"section_index":14,"title":"ACKNOWLEDGEMENTS"},{"word_count":1485,"figure_citations":{},"section_index":15,"title":"REFERENCES"}],"title":"JumpVR: Jump-Based Locomotion Augmentation for Virtual Reality","authors":"Dennis Wolf, Katja Rogers, Christoph Kunder, Enrico Rukzio","abstract":"One of the great benefits of virtual reality (VR) is the implementation of features that go beyond realism. Common \"unrealistic\" locomotion techniques (like teleportation) can avoid spatial limitation of tracking, but minimize potential benefits of more realistic techniques (e.g. walking). As an alternative that combines realistic physical movement with hyper-realistic virtual outcome, we present JumpVR, a jump-based locomotion augmentation technique that virtually scales users' physical jumps. In a user study (N=28), we show that jumping in VR (regardless of scaling) can significantly increase presence, motivation and immersion compared to teleportation, while largely not increasing simulator sickness. Further, participants reported higher immersion and motivation for most scaled jumping variants than forward-jumping. Our work shows the feasibility and benefits of jumping in VR and explores suitable parameters for its hyper-realistic scaling. We discuss design implications for VR experiences and research.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: We use physical jumps to augment locomotion in VR, by applying a scaling factor to extend the natural jumping parabola \nby forward motion (a). The range of the previous jump is indicated to users by a radius indicator (b). We compared this scaled \njumping to a teleportation baseline (c). \n","bbox":[53.929,366.55799199999996,566.7196616000002,398.438992],"page":"1"},"2":{"caption":"Figure 2:  Topviews of the course (a) in the main study (all conditions except forward-jumping) and (b) the smaller second \ncourse used only for the forward-jumping condition. The brown blocks in the main-study course were checkpoints; upon falling, \nparticipants were re-set to the last passed checkpoint. \n","bbox":[53.929,564.602992,567.9019199999997,596.713141],"page":"4"},"3":{"caption":"Figure 3: The different scaled jumping conditions in comparison, visualizing the range of each scaling for an average jump on the \nyellow square to the left. \n","bbox":[53.68,599.733992,566.6405319999999,620.6559920000001],"page":"5"},"4":{"caption":"Figure 4:  The main states the system is able to detect:  On-\nGround (a), KneesBent (b), Rising/Falling (c), Landing (d). \nThe dashed line symbolises the measured baseline. \n","bbox":[53.57331949999997,442.705992,301.2997065999999,474.816141],"page":"5"},"5":{"caption":"Figure 5: Immersion, interest/enjoyment, and presence were rated higher for the scaled jumping conditions than compared to \nare highlighted with ∗ (p<.05), ∗∗ (p<.01) \nteleportation (and largely also compared to forward-jumping). Signiﬁcant differences \nand ∗ ∗ ∗ (p<.001). \n","bbox":[53.929,527.806029,567.332067,559.696992],"page":"8"},"6":{"caption":"Figure 6: Results of participants’ most preferred condition. \n","bbox":[57.842,376.567992,295.5890689999999,386.530992],"page":"8"},"7":{"caption":"Figure 7: Comfort with HMD. \n","bbox":[248.281,549.728992,372.310387,559.691992],"page":"9"}},"crops":{"1":{"crop_coord":[144.80278027777777,813.7839083333332,1571.9585758333333,1098.6139],"bbox":[53.9290009,398.298996,564.1050872999999,497.237793],"page":"1"},"2":{"crop_coord":[147.82499527777776,195.90199897444444,832.9816118861111,548.4889052777777],"bbox":[55.0169983,596.3439941,298.073380279,719.6752803692],"page":"4"},"3":{"crop_coord":[878.3694458333332,195.90199897444444,1563.5260624416667,548.4889052777777],"bbox":[318.0130005,596.3439941,561.069382479,719.6752803692],"page":"4"},"4":{"crop_coord":[144.80278027777777,168.22669167083345,1572.1171473283332,481.341671388889],"bbox":[53.9290009,620.5169983,564.1621730382,729.6383909985],"page":"5"},"5":{"crop_coord":[144.80278027777777,582.0249855555556,830.0136483333332,887.0916494444446],"bbox":[53.9290009,474.4470062,297.00491339999996,580.6710052],"page":"5"},"6":{"crop_coord":[32.10960217599999,1531.0936267672223,834.2875185348888,1975.882198762778],"bbox":[13.359456783359995,82.48240844539998,298.54350667256,239.00629436379998],"page":"8"},"7":{"crop_coord":[143.75281977750998,159.47719187262783,1598.0899224552877,649.2259413934614],"bbox":[53.551015119903596,560.0786610983539,573.5123720839035,732.788210925854],"page":"8"},"8":{"crop_coord":[140.41443820409998,168.22795614312247,1572.1856274679888,655.0772254070109],"bbox":[52.349197753476,557.972198853476,564.1868258884759,729.6379357884759],"page":"9"}}}},{"filename":"3313831.3376260","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376260.pdf","paper_id":"3313831.3376260","venue":"chi2020","keywords":["Virtual reality","VR","User studies","In-VR questionnaires","InVRQs","Research methods"],"paragraph_containing_keyword":"Author Keywords \nVirtual reality; VR; user studies; in-VR questionnaires; \ninVRQs; research methods.","paragraph_after_keyword":"CCS Concepts \n•Human-centered computing → Virtual reality;  HCI de-\nsign  and  evaluation  methods;  Empirical  studies  in  HCI; \nUser studies;","doi":"10.1145/3313831.3376260","sections":[{"word_count":867,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":818,"figure_citations":{},"section_index":1,"title":"STATE OF THE ART"},{"word_count":390,"figure_citations":{},"section_index":2,"title":"LITERATURE REVIEW"},{"word_count":868,"figure_citations":{"1":["Figure 1 depicts 7 different realizations of IN VRQ S."]},"section_index":3,"title":"HUD"},{"word_count":1590,"figure_citations":{},"section_index":4,"title":"EXPERT SURVEY"},{"word_count":1018,"figure_citations":{},"section_index":5,"title":"DESIGN STUDY"},{"word_count":538,"figure_citations":{},"section_index":6,"title":"USER STUDY"},{"word_count":989,"figure_citations":{"5":["Figure 5b."]},"section_index":7,"title":"UMUX"},{"word_count":2,"figure_citations":{},"section_index":8,"title":"INV"},{"word_count":1085,"figure_citations":{},"section_index":9,"title":"REAL"},{"word_count":595,"figure_citations":{},"section_index":10,"title":"CONCLUSION"},{"word_count":67,"figure_citations":{},"section_index":11,"title":"ACKNOWLEDGEMENTS"},{"word_count":7378,"figure_citations":{},"section_index":12,"title":"REFERENCES"}],"title":"Examining Design Choices of Questionnaires in VR User Studies","authors":"Dmitry Alexandrovsky, Susanne Putze, Michael Bonfert, Sebastian Höffner, Pitt Michelmann, Dirk Wenig, Rainer Malaka, Jan David Smeddinck","abstract":"Questionnaires are among the most common research tools in virtual reality (VR) user studies. Transitioning from virtuality to reality for giving self-reports on VR experiences can lead to systematic biases. VR allows to embed questionnaires into the virtual environment which may ease participation and avoid biases. To provide a cohesive picture of methods and design choices for questionnaires in VR (inVRQ), we discuss 15 inVRQ studies from the literature and present a survey with 67 VR experts from academia and industry. Based on the outcomes, we conducted two user studies in which we tested different presentation and interaction methods of inVRQs and evaluated the usability and practicality of our design. We observed comparable completion times between inVRQs and questionnaires outside VR (nonVRQs) with higher enjoyment but lower usability for \\inVRQs. These findings advocate the application of inVRQs and provide an overview of methods and considerations that lay the groundwork for inVRQ design.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Examples of different realizations of INVRQ: (a) and (b) present the questionnaire using a HUD, (c)-(f) use a world-referenced questionnaire, \nand (g) presents the questionnaire attached to the body. \n","bbox":[53.92890999999997,527.61902,567.14909,544.55527],"page":"4"},"2":{"caption":"Figure  2.  Ratings  of  the  usefulness  of  INVRQS  (Q17,  scale:  0  to  5,  5 \nbeing highest) separated by INVRQ experience (Yes/No). \n","bbox":[321.094,586.51202,566.15936,603.44827],"page":"5"},"3":{"caption":"Figure 3.  Screenshots of the archery task.  The world space-anchored \nINVRQ is ﬁlled out using the HTC Vive controller as a laser pointer. \n","bbox":[321.094,468.94027,566.1555599999999,485.87726999999995],"page":"7"},"4":{"caption":"Figure 4.  Ratings of usability on UMUX [155] (left) and of completion \ntimes (right) for both conditions. \n","bbox":[53.929,600.29302,298.99056,617.2292699999999],"page":"8"},"5":{"caption":"Figure 5. Boxplots of presence and workload split by questionnaire type \n","bbox":[187.181,525.5752699999999,432.90407,533.54527],"page":"9"}},"crops":{"1":{"crop_coord":[276.7166647222222,195.9081097222224,578.4369913888888,394.87778555555576],"bbox":[101.4179993,651.6439972,206.43731689999998,719.6730805],"page":"4"},"2":{"crop_coord":[582.2500102777777,195.916671666667,836.2797972222222,394.87778555555576],"bbox":[211.4100037,651.6439972,299.260727,719.6699981999999],"page":"4"},"3":{"crop_coord":[840.1222313888888,195.91332333333352,1134.2466905555555,394.87778555555576],"bbox":[304.2440033,651.6439972,406.5288086,719.6712035999999],"page":"4"},"4":{"crop_coord":[1138.0694580555555,195.9041680555557,1440.2152930555558,394.87778555555576],"bbox":[411.5050049,651.6439972,516.6775055,719.6744994999999],"page":"4"},"5":{"crop_coord":[428.94165027777774,451.3227761111112,726.3971877777778,650.2722338888891],"bbox":[156.2189941,559.7019958,259.70298760000003,627.7238006],"page":"4"},"6":{"crop_coord":[730.2472094444445,451.2909022222223,1023.7192283333333,650.2722338888891],"bbox":[264.6889954,559.7019958,366.7389222,627.7352751999999],"page":"4"},"7":{"crop_coord":[1027.5472088888887,451.2973872222223,1287.995817222222,650.2722338888891],"bbox":[371.7169952,559.7019958,461.8784942,627.7329406],"page":"4"},"8":{"crop_coord":[971.3277688888888,168.2273288888889,1487.7242888888889,515.8250088888888],"bbox":[351.4779968,608.1029968,533.780744,729.6381616],"page":"5"},"9":{"crop_coord":[890.3611161111112,168.2194686111111,1231.0777791666665,508.93613166666654],"bbox":[322.3300018,610.5829926,441.3880005,729.6409913],"page":"7"},"10":{"crop_coord":[1227.986111111111,168.2194686111111,1568.7027741666668,508.93613166666654],"bbox":[443.875,610.5829926,562.9329987,729.6409913],"page":"7"},"11":{"crop_coord":[890.3611161111112,501.6944544444445,1231.0777791666665,842.4111175000002],"bbox":[322.3300018,490.5319977,441.3880005,609.5899964],"page":"7"},"12":{"crop_coord":[1227.986111111111,501.6944544444445,1568.7027741666668,842.4111175000002],"bbox":[443.875,490.5319977,562.9329987,609.5899964],"page":"7"},"13":{"crop_coord":[148.09168499999998,168.22781834488714,488.9418067817882,477.5444369444444],"bbox":[55.1130066,621.8840027,174.21905044144376,729.6379853958406],"page":"8"},"14":{"crop_coord":[485.86112972222224,175.2342819317855,826.7074145405415,477.5444369444444],"bbox":[176.7100067,621.8840027,295.81466923459493,727.1156585045572],"page":"8"},"15":{"crop_coord":[165.92501333333334,195.9019541666664,813.7179333333332,631.0972341666666],"bbox":[61.5330048,566.6049957,291.13845599999996,719.6752965000001],"page":"9"},"16":{"crop_coord":[863.8416800000001,195.9019541666664,1511.6346,631.0972341666666],"bbox":[312.7830048,566.6049957,542.388456,719.6752965000001],"page":"9"}}}},{"filename":"3313831.3376265","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376265.pdf","paper_id":"3313831.3376265","venue":"chi2020","keywords":["Accessibility","Games","Virtual reality"],"paragraph_containing_keyword":"Author Keywords \nAccessibility, games, virtual reality.","paragraph_after_keyword":"CCS Concepts \n•Applied computing → Computer games;","doi":"10.1145/3313831.3376265","sections":[{"word_count":599,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":3556,"figure_citations":{"2":["Figure 2)."],"3":["Figure 3)."]},"section_index":1,"title":"RELATED WORK"},{"word_count":147,"figure_citations":{},"section_index":2,"title":"PENS"},{"word_count":2588,"figure_citations":{},"section_index":3,"title":"TLX"},{"word_count":836,"figure_citations":{},"section_index":4,"title":"DISCUSSION"},{"word_count":189,"figure_citations":{},"section_index":5,"title":"LIMITATIONS AND FUTURE WORK"},{"word_count":144,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":41,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGEMENTS"},{"word_count":1056,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Virtual Reality Games for People Using Wheelchairs","authors":"Kathrin Gerling, Patrick Dickinson, Kieran Hicks, Liam Mason, Adalberto L. Simeone, Katta Spiel","abstract":"Virtual Reality (VR) holds the promise of providing engaging embodied experiences, but little is known about how people with disabilities engage with it. We explore challenges and opportunities of VR gaming for wheelchair users. First, we present findings from a survey that received 25 responses and gives insights into wheelchair users' motives to (non-) engage with VR and their experiences. Drawing from this survey, we derive design implications which we tested through implementation and qualitative evaluation of three full-body VR game prototypes with 18 participants. Our results show that VR gaming engages wheelchair users, though nuanced consideration is required for the design of embodied immersive experiences for minority bodies, and we illustrate how designers can create meaningful, positive experiences.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Screenshot of Karamaisu Slope. \n","bbox":[104.858,578.29427,248.06296000000003,586.26427],"page":"5"},"2":{"caption":"Figure 2. Screenshot of Dungeon Spell. \n","bbox":[376.6669,578.29427,510.57882000000006,586.26427],"page":"5"},"3":{"caption":"Figure 3. Screenshot of Space Travel. \n","bbox":[379.547,402.60727,507.7046000000001,410.57727],"page":"5"},"4":{"caption":"Figure 4. Sketch of a player engaging with Space Travel. \n","bbox":[347.005,400.09727,540.2456199999999,408.06726999999995],"page":"6"}},"crops":{"1":{"crop_coord":[144.80278027777777,168.22502138888885,830.0027974999999,558.0250124999999],"bbox":[53.9290009,592.9109955,297.0010071,729.6389923],"page":"5"},"2":{"crop_coord":[886.9277613888887,168.22502138888885,1572.127778611111,558.0250124999999],"bbox":[321.0939941,592.9109955,564.1660003,729.6389923],"page":"5"},"3":{"crop_coord":[886.9277613888887,656.2416586111112,1572.127778611111,1046.0416497222222],"bbox":[321.0939941,417.2250061,564.1660003,553.9530029],"page":"5"},"4":{"crop_coord":[886.9277613888887,536.6138966666665,1572.127778611111,1053.0138991666668],"bbox":[321.0939941,414.7149963,564.1660003,597.0189972000001],"page":"6"}}}},{"filename":"3313831.3376286","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376286.pdf","paper_id":"3313831.3376286","venue":"chi2020","keywords":["Force feedback","Haptic feedback","Virtual reality","VR","Robotics","Cleaning robot","Human-robot interaction"],"paragraph_containing_keyword":"Author Keywords \nForce feedback, haptic feedback, virtual reality, VR, robotics, \ncleaning robot, human-robot interaction.","paragraph_after_keyword":"CCS Concepts \n•Human-centered  computing  →  Human  computer \ninteraction  (HCI);  Virtual  reality;  Haptic  devices; \nInteraction devices; •Hardware → Haptic devices;","doi":"10.1145/3313831.3376286","sections":[{"word_count":518,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":610,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1206,"figure_citations":{"1":["Figure 1 shows the setup of our prototype."],"3":["Figure 3 B).","Figure 3 A and 3 C).","Figure 3 D shows, users can tie shared proxies (strings, ropes or sewing threads) to the robot to simulate force feedback such as tension.","Figure 3 E indicates.","Figure 3 F shows.","Figure 3 H shows.","Figure 3 I shows, users can attach the VR controller with tape onto a user-driving proxy simulating a stick or a ﬁshing rod.","Figure 3 J shows.","Figure 3 D)."],"4":["Figure 4A shows."]},"section_index":2,"title":"MOVEVR IMPLEMENTATION"},{"word_count":3883,"figure_citations":{"4":["Figure 4A shows, different levels of tensile strength can be simulated through changing the distance between the user and MoveVR: the further they are apart, the stronger the tension."],"5":["Figure 5 A.","Figure 5 C illustrates.","Figure 5 D illustrates.","Figure 5 D illustrates, the robot bumps into the user to simulate a dog rubbing up against his/her leg or a strike on his/her shoulder.","Figure 5 B).","Figure 5 B simulates two virtual objects built with different materials.","Figure 5B."],"6":["Figure 6 shows the accuracy and confusion matrix of each benchmark study.","Figure 6 A), 96.","Figure 6 B), 99.","Figure 6 C), 97.","Figure 6 D), and 96.","Figure 6 E)."],"7":["Figure 7, the user has a joint experience with a dog leading her/him to the front door."],"8":["Figure 8, the user notices a box blocking the entry."],"9":["Figure 9 B where an enemy escapes from the user’s hit."],"10":["Figure 10 shows that participants perceived the virtual world as more realistic and enjoyable in the MoveVR condition compared to the barehand (p = 0."],"11":["Figure 11A indicates.","Figure 11 indicates."]},"section_index":3,"title":"MOVEVR FORCE EXPRESSIONS"},{"word_count":888,"figure_citations":{},"section_index":4,"title":"DISCUSSION AND FUTURE WORK"},{"word_count":143,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":249,"figure_citations":{},"section_index":6,"title":"REFERENCES"},{"word_count":1511,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGEMENT"}],"title":"MoveVR: Enabling Multiform Force Feedback in Virtual Reality using Household Cleaning Robot","authors":"Yuntao Wang, Zichao (Tyson) Chen, Hanchuan Li, Zhengyi Cao, Huiyi Luo, Tengxiang Zhang, Ke Ou, John Raiti, Chun Yu, Shwetak Patel, Yuanchun Shi","abstract":"Haptic feedback can significantly enhance the realism and immersiveness of virtual reality (VR) systems. In this paper, we propose MoveVR, a technique that enables realistic, multiform force feedback in VR leveraging commonplace cleaning robots. MoveVR can generate tension, resistance, impact and material rigidity force feedback with multiple levels of force intensity and directions. This is achieved by changing the robot's moving speed, rotation, position as well as the carried proxies. We demonstrated the feasibility and effectiveness of MoveVR through interactive VR gaming. In our quantitative and qualitative evaluation studies, participants found that MoveVR provides more realistic and enjoyable user experience when compared to commercially available haptic solutions such as vibrotactile haptic systems.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure  1.  Overview  of  MoveVR.  A:  Hardware  setup.  B:  Adjusting \ntension  intensities  and  directions  by  conﬁguring  the  relative  position \nbetween the robot and the user. C: Multi-level resistance force feedback \nby conﬁguring the robot’s direction of motion.  D: Simulating material \nrigidity through rotating different material proxies within reach of users. \nE: Applying different levels of impact force to users by varying the speed. \n","bbox":[321.094,272.18602,567.55031,324.98726999999997],"page":"1"},"2":{"caption":"Figure 2. MoveVR system architecture. \n","bbox":[108.679,231.72727,244.24073,239.69727],"page":"3"},"3":{"caption":"Figure  3.  Fabricating  and  assembling  MoveVR.  A:  Example  of \nfabrication tools.  B: Example of everyday objects as proxies.  C: Touch \nfasteners or double-sided tape to ﬁx the carry-on proxy. D: Attaching the \nshared-proxy onto the robot.  E: Fixing the carry-on proxy using touch \nfasteners. F: Fixing the carry-on proxy using tapes. H - I: Attaching the \nVR controller with the user-driving proxy.  G/J: Examples of MoveVR \ncarrying proxies. \n","bbox":[53.64208,468.9727700000001,298.9905600000001,530.74027],"page":"4"},"4":{"caption":"Figure  4.  Force  expression  explanation.  A:  Tension  force  through \nan  elastic  string.  B:  Reaction  force  simulating  material  rigidity.  C: \nResistance  force  against  user’s  force.  D:  Impact  force  that  the  robot \nactively applies to the user. \n","bbox":[53.929,188.09452,300.3135799999999,222.96327],"page":"4"},"5":{"caption":"Figure 5. MoveVR can simulate a variety of force feedback with everyday \nobjects as proxies.  A: Tension using string/rope proxies between users \nand  robots.  B:  Reaction  with  proxies  built  by  different  materials.  C: \nResistance by conﬁguring the wheels’ status.  D: Impact from the robot \ncrashing on users with varying speeds. \n","bbox":[53.921029999999995,509.0292700000001,300.3056099999999,552.8642699999999],"page":"5"},"6":{"caption":"Figure 6. Results of force perception accuracy. A: Tension force intensity. \nB:  Resistive  force  intensity.  C:  Impact  force  intensity.  D:  Material \nrigidity. E: Tension force direction. \n","bbox":[321.09399999999994,197.81277,567.55031,223.71527],"page":"6"},"7":{"caption":"Figure 7. Dog walking VR scenario with four haptic conditions. \n","bbox":[67.909,287.80927,285.01179999999994,295.77927],"page":"7"},"8":{"caption":"Figure 8. Box Pushing VR scenario with four haptic conditions. \n","bbox":[335.07,487.86327,552.1807699999999,495.83326999999997],"page":"7"},"9":{"caption":"Figure  9.  Enemy  hitting  VR  scenario  with  four  haptic  conditions.  A: \nThe user strikes the enemy successfully.  B: The enemy avoids a strike \nfrom the user. C: The enemy attacks the user and the user striking back. \n","bbox":[53.66599,327.18577,300.38530999999995,353.08826999999997],"page":"8"},"10":{"caption":"Figure 10.  Overall Likert score of four conditions.  Error bar indicates \nthe  standard  error  of  the  mean.  *  indicates  signiﬁcant  difference \nbetween two conditions with p <0.05. \n","bbox":[321.094,268.71477,566.1555599999999,294.61726999999996],"page":"8"},"11":{"caption":"Figure 11.  A: Likert scores of continuous pulling force feedback (dog \nleading  scenario).  B:  Likert  scores  of  resistive  force  feedback  (box \npushing scenario).  C: Likert scores of reaction force feedback (enemy \nhitting  scenario).  D:  Likert  scores  of  impact  force  feedback  (enemy \nattacking scenario). Error bar indicates the standard error of the mean. \n* indicates signiﬁcant difference between two conditions with p <0.05. \n","bbox":[53.132000000000005,285.70702,300.38530999999995,338.50827],"page":"9"}},"crops":{"1":{"crop_coord":[886.9277613888887,842.6353113888888,1572.1026441666666,1289.3277825],"bbox":[321.0939941,329.6419983,564.1569519,486.8512879],"page":"1"},"2":{"crop_coord":[178.56386833333332,1144.7461108333332,796.2038591666668,1520.7111275],"bbox":[66.0829926,246.3439941,284.8333893,378.0914001],"page":"3"},"3":{"crop_coord":[178.56386833333332,168.22065555555542,796.1888547222222,717.7916547222222],"bbox":[66.0829926,535.3950043,284.8279877,729.640564],"page":"4"},"4":{"crop_coord":[178.56386833333332,1190.9797752777777,796.2288666666667,1572.7277883333334],"bbox":[66.0829926,227.6179962,284.842392,361.4472809],"page":"4"},"5":{"crop_coord":[191.98054,168.22646250000022,775.9057277777779,656.3361188888889],"bbox":[70.9129944,557.5189972,277.526062,729.6384734999999],"page":"5"},"6":{"crop_coord":[900.3499858333332,1141.1760627777778,1551.7994691666668,1570.6389025],"bbox":[325.9259949,228.3699951,556.8478089,379.3766174],"page":"6"},"7":{"crop_coord":[191.98054,1010.5757902777779,775.9005569444445,1364.9277919444446],"bbox":[70.9129944,302.4259949,277.5242005,426.3927155],"page":"7"},"8":{"crop_coord":[934.1083188888888,454.8674350000001,1518.0283358333336,809.2194366666668],"bbox":[338.0789948,502.4810028,544.6902009,626.4477234],"page":"7"},"9":{"crop_coord":[191.98054,168.21345027777787,775.9005569444445,1211.269455],"bbox":[70.9129944,357.7429962,277.5242005,729.6431579],"page":"8"},"10":{"crop_coord":[967.8694491666666,1042.7214813888888,1484.2637294444446,1373.68888],"bbox":[350.2330017,299.2720032,532.5349426,414.8202667],"page":"8"},"11":{"crop_coord":[191.98054,765.3973133333333,775.9008111111111,1251.76946],"bbox":[70.9129944,343.1629944,277.524292,514.6569672],"page":"9"}}}},{"filename":"3313831.3376358","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376358.pdf","paper_id":"3313831.3376358","venue":"chi2020","keywords":["Virtual Reality","Shape Display","Haptic Device","Handheld Device"],"doi":"10.1145/3313831.3376358","paragraph_containing_keyword":"Author Keywords \nVirtual Reality; Shape Display; Haptic Device; Handheld \nDevice","paragraph_after_keyword":"CCS Concepts \n•Human-centered  computing  →  Human  computer  inter-\naction (HCI); Haptic devices; Virtual reality;","sections":[{"word_count":708,"figure_citations":{"1":["Figure 1)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":784,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1766,"figure_citations":{"2":["Figure 2 shows a system schematic of PoCoPo."],"3":["Figure 3)."],"4":["Figure 4)."],"6":["Figure 6)."],"7":["Figure 7 demonstrates how the user can hold several objects on the table as seen in VR."],"8":["Figure 8)."]},"section_index":2,"title":"DESIGN AND IMPLEMENTATION"},{"word_count":2791,"figure_citations":{"10":["Figure 10) were chosen based on the previous studies [41, 46]."],"11":["Figure 11)."],"12":["Figure 12)."]},"section_index":3,"title":"INFORMATION"},{"word_count":818,"figure_citations":{},"section_index":4,"title":"LIMITATION AND FUTURE WORK"},{"word_count":412,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":27,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":2317,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality","authors":"Shigeo Yoshida, Yuqian Sun, Hideaki Kuzuoka","abstract":"We introduce PoCoPo, the first handheld pin-based shape display that can render various 2.5D shapes in hand in realtime. We designed the display small enough for a user to hold it in hand and carry it around, thereby enhancing the haptic experiences in a virtual environment. PoCoPo has 18 motor-driven pins on both sides of a cuboid, providing the sensation of skin contact on the user's palm and fingers. We conducted two user studies to understand the capability of PoCoPo. The first study showed that the participants were generally successful in distinguishing the shapes rendered by PoCoPo with an average success rate of 88.5%. In the second study, we investigated the acceptable visual size of a virtual object when PoCoPo rendered a physical object of a certain size. The result led to a better understanding of the acceptable differences between the perceptions of visual size and haptic size.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1.  a:  PoCoPo is a handheld pin-based shape display that has 18 pins on two opposite faces of the device (36 pins in total).  The display area \nwhere the pins move up and down touch the user’s ﬁngers and palm base.  By controlling the length of the 36 pins, PoCoPo can render various shapes, \nincluding rectangular and curved shapes (e.g., b-1 and b-2 show a user holding a glass).  Moreover, PoCoPo can render dynamic transformations of \ngraspable objects (e.g., c-1 and c-2 show the pins moving up and down to represent the heartbeat of a small animal). \n","bbox":[53.64208,403.69151999999997,567.1491799999995,438.56026999999995],"page":"1"},"2":{"caption":"Figure 2. Top and side views with the dimensions of PoCoPo. \n","bbox":[339.593,219.36127,547.65782,227.33127],"page":"3"},"3":{"caption":"Figure 3. Worm gear mechanism used to change the force direction and \nincrease the power. By actuating the worm gear (blue: worm, red: worm \nwheel with lead screw) by a DC motor (black), the pin moves with up-\ndown motion. \n","bbox":[320.80708,563.3875200000001,566.1555599999999,598.25627],"page":"4"},"4":{"caption":"Figure 4. Variations in pin length depending on their position: 3, 9, and \n14 mm. A rendered CG image of the worm gear mechanism is overlaid. \n","bbox":[320.49625,365.28202,566.1555599999999,382.21826999999996],"page":"4"},"5":{"caption":"Figure  5.  Overview  of  the  software  and  hardware  architecture  of \nPoCoPo.  Four  types  of  custom  two-layer  PCBs  were  designed  for \nPoCoPo.  The microcontroller (Teensy 3.6) and a Bluetooth module are \nattached  to  the  main  board.  The  motor  driver  board  is  connected  to \nthe main board via I2C. The board with two multiplexers (multiplexer \nboard)  is  used  to  switch  the  optical  encoders  to  read  by  time  division. \nThe  encoder  board  is  placed  at  the  bottom  of  the  pin  to  measure  the \nlength of its extrusion. \n","bbox":[53.66599,429.32452,300.3853099999999,500.05827],"page":"5"},"6":{"caption":"Figure  6.  Shape  surface  represented  by  the  blue  areas  (a  circle  here). \nEach pin length is determined so that the midpoint on the outer edge of \neach pin is close to the surface of the target shape. \n","bbox":[321.094,558.11877,567.5503099999999,584.02127],"page":"5"},"7":{"caption":"Figure 7.  Example applications of PoCoPo regarding the rendering of \nstatic objects.  The user can freely hold a:  a glass (linear surface), b:  a \nmatryoshka  (convex  surface),  or  c:  a  trophy  (concave  surface)  on  the \ntable in VR. \n","bbox":[321.094,392.13851999999997,566.1555599999999,427.00726999999995],"page":"5"},"8":{"caption":"Figure 8.  Example applications of PoCoPo regarding the rendering of \ndynamic objects. The user can freely hold animals in VR. a: Popping up \nof the pins when holding a hedgehog.  b:  Expanding or shrinking of the \ndevice representing hamster pulse. c: Movement of the pins correspond-\ning to the movement of a snake. \n","bbox":[53.929,507.53727000000015,298.99056,551.37227],"page":"6"},"9":{"caption":"Figure 9.  Example setup of Study 1.  The image on the right side shows \nhow the paricipant held the device. \n","bbox":[321.094,549.35002,566.1555599999999,566.28627],"page":"6"},"10":{"caption":"Figure 10. Shapes used in the shape prediction study. The images at the \nbottom show examples of how the shapes appeared when rendered by \nPoCoPo. \n","bbox":[321.094,309.69977,566.4345099999999,335.60227],"page":"6"},"11":{"caption":"Figure  11.  Confusion  matrix  summarizing  the  results  from  the  shape \nprediction study (N=10). Each row shows the total number of predicted \nobjects in the ﬁnal two sets of the study. \n","bbox":[321.094,471.34177,566.1555599999998,497.24427],"page":"7"},"12":{"caption":"Figure 12.  Shapes used in the visual size acceptance range study.  The \nimages at the bottom show examples of how the shapes appeared when \nrendered by PoCoPo. \n","bbox":[321.094,484.58377,566.15556,510.48627],"page":"8"}},"crops":{"2":{"crop_coord":[144.80278027777777,585.2714625,1572.1350269444442,973.8472325],"bbox":[53.9290009,443.2149963,564.1686096999999,579.5022735],"page":"1"},"3":{"crop_coord":[143.1788636111111,1773.199598611111,772.2793072222222,1914.100002777778],"bbox":[53.3443909,104.723999,276.22055059999997,151.8481445],"page":"1"},"4":{"crop_coord":[159.84921777777777,1755.0079005555556,414.2936622222223,1851.1190116666667],"bbox":[59.3457184,127.3971558,147.3457184,158.3971558],"page":"1"},"7":{"crop_coord":[886.9277613888887,979.0448591666666,1572.131084722222,1555.0583225],"bbox":[321.0939941,233.9790039,564.1671905,437.7438507],"page":"3"},"9":{"crop_coord":[954.447225,168.23074333333335,1504.6082305555556,530.2499897222224],"bbox":[345.401001,602.9100037,539.858963,729.6369324],"page":"4"},"10":{"crop_coord":[886.9277613888887,658.3184222222221,1572.1188352777779,1130.3527747222224],"bbox":[321.0939941,386.8730011,564.1627807,553.205368],"page":"4"},"12":{"crop_coord":[144.80278027777777,168.22277499999984,830.0082230555555,803.0194516666667],"bbox":[53.9290009,504.7129974,297.0029603,729.639801],"page":"5"},"13":{"crop_coord":[886.9277613888887,168.22527555555567,1572.1240063888888,569.7889030555557],"bbox":[321.0939941,588.6759949,564.1646423,729.6389008],"page":"5"},"14":{"crop_coord":[886.9277613888887,674.4317880555556,1572.1392652777777,1005.9388816666668],"bbox":[321.0939941,431.6620026,564.1701355,547.4045563],"page":"5"},"16":{"crop_coord":[144.80278027777777,168.2199775,830.0142841666666,660.4805755555554],"bbox":[53.9290009,556.0269928,297.0051423,729.6408081],"page":"6"},"17":{"crop_coord":[886.9277613888887,322.7077144444443,1572.1239638888887,619.0527936111109],"bbox":[321.0939941,570.9409943,564.164627,674.0252228],"page":"6"},"18":{"crop_coord":[886.9277613888887,711.2693447222223,1572.1607122222224,1259.8416561111112],"bbox":[321.0939941,340.2570038,564.1778564,534.1430359],"page":"6"},"20":{"crop_coord":[886.9277613888887,264.5190663983333,1572.1254625777779,810.8361052777777],"bbox":[321.0939941,501.8990021,564.165166528,694.9731360966],"page":"7"},"22":{"crop_coord":[954.447225,168.22451277777782,1504.618148888889,774.0527597222222],"bbox":[345.401001,515.1410065,539.8625336,729.6391754],"page":"8"},"24":{"crop_coord":[144.80278027777777,168.22701333333316,1572.1623655555554,423.32500027777775],"bbox":[53.9290009,641.4029999,564.1784516,729.6382752000001],"page":"9"}}}},{"filename":"3313831.3376438","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376438.pdf","paper_id":"3313831.3376438","venue":"chi2020","keywords":["Virtual reality","Occlusion","Eye tracking","Smooth pursuits"],"doi":"10.1145/3313831.3376438","paragraph_containing_keyword":"Author Keywords \nVirtual reality, Occlusion, Eye tracking, Smooth pursuits","paragraph_after_keyword":"CCS Concepts \n•Human-centered  computing  →  Interaction  techniques; \nVirtual reality;","sections":[{"word_count":700,"figure_citations":{"1":["Figure 1, the Page 1 CHI 2020 Paper concept is to display the outlines of objects that lie in the direction in which the user points, and to generate a distinct motion around each of the outlines."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":722,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1019,"figure_citations":{"2":["Figure 2 shows the cone casting technique."]},"section_index":2,"title":"OUTLINE PURSUITS"},{"word_count":1355,"figure_citations":{"5":["Figure 5 illustrates Controller-based Outline Pursuits in a room planner setting."]},"section_index":3,"title":"TECHNIQUES"},{"word_count":307,"figure_citations":{},"section_index":4,"title":"USER STUDIES"},{"word_count":3736,"figure_citations":{},"section_index":5,"title":"ANOVA"},{"word_count":714,"figure_citations":{},"section_index":6,"title":"DISCUSSION"},{"word_count":162,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":1993,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Outline Pursuits: Gaze-assisted Selection of Occluded Objects in Virtual Reality","authors":"Ludwig Sidenmark, Christopher Clarke, Xuesong Zhang, Jenny Phu, Hans Gellersen","abstract":"In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1.  Outline Pursuits support selection in occluded 3D scenes.  A: The user points at an object of interest but the selection is ambiguous due to \nocclusion by other objects.  B: Potential targets are outlined, with each outline presenting a moving stimulus that the user can follow with their gaze. \nC: Matching of the user’s smooth pursuit eye movement completes the selection. Note that outline pursuits can augment manual pointing as shown, or \nsupport hands-free input using the head or gaze for initial pointing. \n","bbox":[53.929,402.14816160000004,567.5620944999997,437.01734910000005],"page":"1"},"2":{"caption":"Figure 2. Cone casting with a primary pointer is used to select Nc  candi-\ndate targets within a visual angle radius rc. In this example, with Nc = 2, \nfour objects are within the cone (A) and objects 2 and 3 are selected as \nthey are closest to the centre of the cone. \n","bbox":[53.92469139999997,611.8465241000001,299.9855887,646.8438707],"page":"3"},"3":{"caption":"Figure 3.  Movement paths for occluded objects.  A: Whole:  the target \nmoves along the whole outline.  B: Shared:  the target moves along the \nvisible part of the object.  C: Cut:  the target moves along the shortest \npath  to  the  next  visible  part.  D:  Jump:  the  target  jumps  to  the  next \nvisible part. \n","bbox":[320.8947475,627.9197991000002,566.1666048999999,671.7553491],"page":"3"},"4":{"caption":"Figure 4. Hands-free Outline Pursuits in an interactive virtual cityscape \nenvironment.  A: The user selects candidate buildings by pointing with \ntheir head; B: The user follows the outline motion with their gaze (red) \nto select the building.  The outline shows feedback of selection progress \nvia colour change.  C: The building is selected and contextual feedback \nis displayed. \n","bbox":[320.8947475,569.6774366000002,566.6926314999998,622.4793490999999],"page":"4"},"5":{"caption":"Figure 5.  Controller-based Outline Pursuits in a Room Planner setting. \nA: The user wants to move a partially occluded book to a different shelf \nbut  does  not  know  which  book  is  which.  B:  The  user  points  with  the \ncontroller to pre-select the book and other nearby items, showing which \nbooks  are  available  for  interaction  via  their  outline  and  stimuli  move-\nments.  C: The user follows the motion of a book with their eyes, which \nin turn highlights it.  D: The user selects the book via a button click and \ncan now manipulate it in 3D space, moving it to another shelf. \n","bbox":[320.80707639999997,545.2577116000003,567.5534022999999,615.9923491],"page":"5"},"6":{"caption":"Figure  6.  A:  Participants  were  tasked  to  select  the  yellow  object.  B: \nParticipants were tasked to select the yellow object at a higher level of \nocclusion. C: The user was tasked to fnish the memory game. \n","bbox":[53.929,576.0555241000001,300.3166713999999,601.9583491],"page":"6"},"7":{"caption":"Figure  7.  Study  2  task  performance  at  different  object  densities  and \nlevels of occlusion. Error bars represents mean 95% confdence interval. \n","bbox":[53.929,567.7238866,300.38840229999994,584.6603491],"page":"8"},"8":{"caption":"Figure 8.  Study 2 average translational head movements during trials. \nError bars represents 95% confdence interval for means. \n","bbox":[53.929,628.4508866,300.3884022999999,645.3873490999999],"page":"9"},"9":{"caption":"Figure 9. Median scores on a 5-point Likert scale with error bars repre-\nsenting interquartile ranges. \n","bbox":[321.094,578.5768866,565.4891463999999,595.5133490999999],"page":"9"}},"crops":{"1":{"crop_coord":[144.96668499999998,665.5195955555555,622.6700169444445,978.1333413888889],"bbox":[53.9880066,441.6719971,222.3612061,550.6129456],"page":"1"},"2":{"crop_coord":[619.6055602777777,665.5195955555555,1097.308892222222,978.1333413888889],"bbox":[224.8580017,441.6719971,393.2312012,550.6129456],"page":"1"},"3":{"crop_coord":[1094.2416805555556,665.7138483333334,1571.9475130555554,978.1333413888889],"bbox":[395.727005,441.6719971,564.1011047,550.5430146],"page":"1"},"4":{"crop_coord":[144.80278027777777,168.22451277777782,830.0168272222222,395.6389024999999],"bbox":[53.9290009,651.3699951,297.0060578,729.6391754],"page":"3"},"5":{"crop_coord":[890.0472088888888,168.2246397222224,1062.0976680555555,326.08332305555575],"bbox":[322.2169952,676.4100037,380.5551605,729.6391296999999],"page":"3"},"6":{"crop_coord":[1059.0166811111112,168.2246397222224,1231.067140277778,326.08332305555575],"bbox":[383.0460052,676.4100037,441.38417050000004,729.6391296999999],"page":"3"},"7":{"crop_coord":[1227.986111111111,168.2246397222224,1400.0365702777779,326.08332305555575],"bbox":[443.875,676.4100037,502.2131653,729.6391296999999],"page":"3"},"8":{"crop_coord":[1396.955541111111,168.2246397222224,1569.0060002777777,326.08332305555575],"bbox":[504.7039948,676.4100037,563.0421600999999,729.6391296999999],"page":"3"},"9":{"crop_coord":[886.9277613888887,168.23087055555558,1572.1048905555554,462.9611036111111],"bbox":[321.0939941,627.1340027,564.1577606,729.6368866],"page":"4"},"10":{"crop_coord":[886.9277613888887,168.21946888888922,1572.0744152777777,480.98054666666667],"bbox":[321.0939941,620.6470032,564.1467895,729.6409911999999],"page":"5"},"11":{"crop_coord":[144.80278027777777,168.22777638888888,829.9757133333333,519.9638705555558],"bbox":[53.9290009,606.6130066,296.9912568,729.6380005],"page":"6"},"12":{"crop_coord":[144.80278027777777,168.2301924999999,830.0044930555556,371.7388916666664],"bbox":[53.9290009,659.973999,297.0016175,729.6371307000001],"page":"8"},"13":{"crop_coord":[144.80278027777777,364.50798027777796,830.0044930555556,568.0166794444447],"bbox":[53.9290009,589.3139954,297.0016175,658.9771271],"page":"8"},"14":{"crop_coord":[144.80278027777777,168.22896333333318,830.0044930555556,399.3277572222222],"bbox":[53.9290009,650.0420074,297.0016175,729.6375732],"page":"9"},"15":{"crop_coord":[886.9277613888887,168.2295563888888,1572.1085780555554,537.866668611111],"bbox":[321.0939941,600.1679993,564.1590881,729.6373597],"page":"9"}}}},{"filename":"3313831.3376470","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376470.pdf","paper_id":"3313831.3376470","venue":"chi2020","keywords":["Virtual Reality","Haptics","Force Feedback","String-Driven","Touch","Grasp"],"paragraph_containing_keyword":"ABSTRACT  \nToday's virtual reality (VR) systems allow users to explore \n \n \nimmersive  new  worlds  and  experiences  through  sight.  Un-\n \nfortunately,  most  VR  systems  lack  haptic  feedback,  and \n \n \neven  high-end  consumer  systems  use  only  basic  vibration \n \n \nmotors. This clearly precludes realistic physical interactions \n \n \nwith virtual objects. Larger obstacles, such as walls, railings, \n \nand  furniture  are  not  simulated  at  all.  In  response,  we  de-\n \nveloped Wireality, a self-contained worn system that allows \n \n \nfor  individual  joints  on  the  hands  to  be  accurately  arrested \n \n \n \n \n \nin 3D space through the use of retractable wires that can be \n \nprogrammatically locked. This allows for convincing tangi-\nble interactions with complex geometries, such as wrapping \n \n \nfingers around a  railing. Our approach  is lightweight, low-\ncost,  and  low-power,  criteria  important  for  future,  worn \n \nconsumer  uses.  In  our  studies,  we  further  show  that  our \n \n \nsystem is fast-acting, spatially-accurate, high-strength, com-\n  \nfortable, and immersive. \n \n \nAuthor Keywords \n \n \n \nVirtual Reality; Haptics; Force Feedback; String-Driven; \n \n \n \nTouch; Grasp. \n \nCSS Concepts \n \n \nHuman-centered computing → Human computer interac-\n \n \n \ntion (HCI) → Interaction devices → Haptic devices. \n \nINTRODUCTION  \nVirtual reality (VR) systems, such as the Oculus Quest [15] \n \n \n \nand HTC Vive [28], use controllers for tracking the hands, \n \ncapturing  buttoned  input,  and  delivering  basic  vibrotactile \n \n \nhaptic  feedback.  The  latter  is  insufficient  to  produce  im-\n \n \nmersive  physical  interactions  with  virtual  objects.  More \n \n \ncritically, large obstacles like walls, railings, and furniture – \n \nkey elements in most VR worlds – are not simulated at all. \n \n \n \nThe  current  state-of-the-art  in  consumer  VR  systems  is  a \n \n \nvibration  alert  when  a  hand  intersects  a  virtual  object  or \n  \n \n \nobstacle – falling far short of any reality.","paragraph_after_keyword":"","doi":"10.1145/3313831.3376470","sections":[{"word_count":572,"figure_citations":{"1":["Figure 1).","Figure 1)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":560,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1002,"figure_citations":{"2":["Figure 2)."],"3":["Figure 3)."],"4":["Figure 4)."]},"section_index":2,"title":"WIREALITY IMPLEMENTATION"},{"word_count":1302,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2), we included a 10-turn potentiometer that allowed us to precisely track a point’s distance from the module, but not the azimuth or altitude."],"6":["Figure 6)."]},"section_index":3,"title":"EVALUATION"},{"word_count":427,"figure_citations":{"7":["Figure 7 shows our main results."]},"section_index":4,"title":"QUALITATIVE STUDY"},{"word_count":968,"figure_citations":{"8":["Figure 8, A-D)."],"9":["Figure 9, A-D)."],"10":["Figure 10 shows four example scenarios: an ATM screen, a button, a lever, and a piano."],"11":["Figure 11, A-D)."]},"section_index":5,"title":"EXAMPLE USES"},{"word_count":127,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":1975,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics","authors":"Cathy Fang, Yang Zhang, Matthew Dworman, Chris Harrison","abstract":"Today's virtual reality (VR) systems allow users to explore immersive new worlds and experiences through sight. Unfortunately, most VR systems lack haptic feedback, and even high-end consumer systems use only basic vibration motors. This clearly precludes realistic physical interactions with virtual objects. Larger obstacles, such as walls, railings, and furniture are not simulated at all. In response, we developed Wireality, a self-contained worn system that allows for individual joints on the hands to be accurately arrested in 3D space through the use of retractable wires that can be programmatically locked. This allows for convincing tangible interactions with complex geometries, such as wrapping fingers around a railing. Our approach is lightweight, low-cost, and low-power, criteria important for future, worn consumer uses. In our studies, we further show that our system is fast-acting, spatially-accurate, high-strength, comfortable, and immersive.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure   1.   Wireality   enables   strong,   whole-hand  haptic   feed-\nback for  complex  objects  in VR  experiences.  \n","bbox":[317.56,437.64312,557.430112,456.60192],"page":"1"},"2":{"caption":"Figure  2.  An  earlier  prototype  using  a  motor  for  string  retrac-\ntion  and  a  potentiometer  for  tracking  hand  positions.  \n","bbox":[54.76,604.4433599999999,294.729568,623.6419199999999],"page":"3"},"3":{"caption":"Figure  3.  Exploded  illustration  of  one  haptic  module. \n","bbox":[336.76,585.96192,541.800976,594.84192],"page":"3"},"4":{"caption":"Figure 4. Locking mechanism. Our driver board actuates the \n \nsolenoid  pawl  for  40  milliseconds,  which  locks  the  ratchet \ngear,  stopping  a  joint  from  further  forward  movement  (A). \nString tension keeps the gear latched even after the solenoid is \nnot powered (B). When the user pulls back from touching the \n \nvirtual  surface,  slack  is  created  in  the  string,  which  then  re-\n \n \nleases the ratchet (C). \n \n","bbox":[318.28,71.88192,560.522848,142.92192],"page":"3"},"5":{"caption":"Figure  5.  Example  Wireality  setup  with  seven  haptic  modules.  \n","bbox":[54.76,623.6419199999999,294.3424,632.5219199999999],"page":"4"},"6":{"caption":"Figure 6. Exemplary objects used in the study. Left to right:  a \nwall, tilted flat surface, sphere, pole, and irregular object. \n \n \n","bbox":[318.28,75.00192,560.15344,94.20192],"page":"5"},"8":{"caption":"Figure   8.   Our   system   restrains   a   user’s   hand  collided  with   a \nwall  (A),  railings  (B),  a  roadblock  (C),  and  a  fire  hydrant  (D).  \n","bbox":[54.52,81.7236,296.54972800000013,101.16192000000001],"page":"6"},"9":{"caption":"Figure  9.  Our  system  provides   force   feedback  when   touching \na virtual  sofa (A),  stereo (B),  sculpture  (C),  and  car  (D).  \n","bbox":[318.04,82.20312,560.0981440000003,101.16192000000001],"page":"6"},"10":{"caption":"Figure  10.   Our   system  provides   force  feedback  when  a  user’s \nhand interacts  with an ATM   touchscreen  (A),  a  button  (B),  a \nlever  (C),  and  a  piano  (D).  \n","bbox":[54.76,562.2047999999999,296.9850880000001,591.72192],"page":"7"},"11":{"caption":"Figure  11.  Our  system  provides  haptic  feedback  when  a  user’s \nhand  interacts   with  virtual   characters:   high  five   (A),   tap  on \nshoulder  (B),  shaking  hands (C),  and  face  slapping  (D).  \n","bbox":[318.28,562.2047999999999,560.5761279999999,591.72192],"page":"7"}},"crops":{"1":{"crop_coord":[147.1110958333333,1721.999876111111,823.1110805555556,1994.666536111111],"bbox":[54.7599945,75.720047,294.519989,170.2800446],"page":"1"},"2":{"crop_coord":[877.1111044444443,495.3333197222225,1553.777771111111,1034.0000066666666],"bbox":[317.5599976,421.5599976,557.5599976,611.8800048999999],"page":"1"},"3":{"crop_coord":[877.777786388889,496.00000166666683,1550.4444463888888,913.3333333333333],"bbox":[317.8000031,465,556.3600007,611.6399994],"page":"1"},"4":{"crop_coord":[879.1111077777776,1577.3332044444444,1555.7777744444443,2005.9998744444445],"bbox":[318.2799988,71.6400452,558.2799987999999,222.36004640000002],"page":"3"},"5":{"crop_coord":[147.1110958333333,167.3333572222222,823.7777625,562.0000119444444],"bbox":[54.7599945,591.4799957,294.7599945,729.9599914],"page":"3"},"6":{"crop_coord":[222.44445805555557,167.99999666666653,745.7778,451.3333130555554],"bbox":[81.8800049,631.3200073,266.680008,729.7200012000001],"page":"3"},"7":{"crop_coord":[879.1111077777776,167.33335694444457,1555.7777744444443,608.0000050000001],"bbox":[318.2799988,574.9199982,558.2799987999999,729.9599915],"page":"3"},"8":{"crop_coord":[879.7777897222222,167.99999666666676,1555.1111349999999,530.6666819444447],"bbox":[318.5200043,602.7599945,558.0400086,729.7200012],"page":"3"},"9":{"crop_coord":[879.7777897222222,1611.3333469444444,1555.1111349999999,1786.0000102777776],"bbox":[318.5200043,150.8399963,558.0400086,210.11999509999998],"page":"3"},"10":{"crop_coord":[147.1110958333333,167.33335694444457,823.7777625,498.6666699999998],"bbox":[54.7599945,614.2799988,294.7599945,729.9599915],"page":"4"},"11":{"crop_coord":[147.77777777777777,167.99999666666653,823.1111230555555,427.33331472222216],"bbox":[55,639.9600067,294.5200043,729.7200012000001],"page":"4"},"12":{"crop_coord":[879.1111077777776,1699.9998813888888,1555.7777744444443,2004.6665530555556],"bbox":[318.2799988,72.1200409,558.2799987999999,178.2000427],"page":"5"},"13":{"crop_coord":[147.1110958333333,167.33335694444457,1553.777771111111,708.000005],"bbox":[54.7599945,538.9199982,557.5599976,729.9599915],"page":"5"},"14":{"crop_coord":[147.77777777777777,167.99999666666653,1553.777771111111,623.3333502777778],"bbox":[55,569.3999939,557.5599976,729.7200012000001],"page":"5"},"15":{"crop_coord":[879.7777897222222,1717.3333147222222,1554.4444530555554,1921.9999863888888],"bbox":[318.5200043,101.8800049,557.8000030999999,171.9600067],"page":"5"},"16":{"crop_coord":[878.4444258333333,1496.666556388889,1555.1110925,2005.3332349999998],"bbox":[318.0399933,71.8800354,558.0399933,251.4000397],"page":"6"},"17":{"crop_coord":[146.44445638888888,1496.666556388889,823.1111230555555,2005.9998744444445],"bbox":[54.5200043,71.6400452,294.5200043,251.4000397],"page":"6"},"18":{"crop_coord":[147.1110958333333,167.33331472222216,823.7777625,647.9999880555555],"bbox":[54.7599945,560.5200043,294.7599945,729.9600067],"page":"6"},"19":{"crop_coord":[147.77777777777777,167.99999666666653,821.1111197222223,590.0000169444445],"bbox":[55,581.3999939,293.8000031,729.7200012000001],"page":"6"},"20":{"crop_coord":[147.1110958333333,1530.6666566666668,825.7777658333334,1902.6666769444446],"bbox":[54.7599945,108.8399963,295.4799957,239.16000359999998],"page":"6"},"21":{"crop_coord":[879.1111077777776,1530.6666566666668,1557.7777777777778,1902.6666769444446],"bbox":[318.2799988,108.8399963,559,239.16000359999998],"page":"6"},"22":{"crop_coord":[147.1110958333333,167.3333569444448,823.7777625,692.0000202777779],"bbox":[54.7599945,544.6799927,294.7599945,729.9599914999999],"page":"7"},"23":{"crop_coord":[879.1111077777776,167.3333569444448,1555.7777744444443,692.0000202777779],"bbox":[318.2799988,544.6799927,558.2799987999999,729.9599914999999],"page":"7"},"24":{"crop_coord":[147.77777777777777,167.99999666666653,826.4444477777779,540.0000169444443],"bbox":[55,599.3999939,295.7200012,729.7200012000001],"page":"7"},"25":{"crop_coord":[879.7777897222222,167.99999666666653,1558.4444597222223,540.0000169444443],"bbox":[318.5200043,599.3999939,559.2400055,729.7200012000001],"page":"7"}}}},{"filename":"3313831.3376481","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376481.pdf","paper_id":"3313831.3376481","venue":"chi2020","keywords":["Haptics","Robot arm","Immersive environments","Virtual reality","User study","Perception","Presence","Emotion"],"doi":"10.1145/3313831.3376481","paragraph_containing_keyword":"Author Keywords \nHaptics; robot arm; immersive environments; virtual reality; \nuser study; perception; presence; emotion","paragraph_after_keyword":"CCS Concepts \n•Human-centered  computing  →  Human  computer  inter-\naction (HCI); Haptic devices; User studies; \nINTRODUCTION AND MOTIVATION \nOver the last decade, Virtual Reality (VR) systems have been \nmassively improved, in particular driven by the gaming indus-\ntry but increasingly also other industry sectors. Predominantly, \nadvances have been made in providing affordable yet high \nquality visual displays.  However, non-visual cues can be a \nkey factor in immersive systems, for example to improve over-\nall simulation and perceptual ﬁdelity [36] or to invoke emo-\ntional reactions [14, 27]. While rendering audio cues is well \nsupported, haptic feedback is still challenging, and foremost \ntargeted towards the hands [31]. In this paper, we look at how \nhaptic feedback can be provided towards the face rather then \nthe hands.  We explore how this feedback could be of value \nin an immersive environment while wearing a head-mounted \ndisplay (HMD). The reason why we choose the face is that it is \nhighly sensitive to haptic cues and can perceive different kinds \nof haptic feedback well, as other areas of the body are often","sections":[{"word_count":1409,"figure_citations":{"1":["Figure 1 and Figure 2)."]},"section_index":0,"title":"INTRODUCTION AND MOTIVATION"},{"word_count":759,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":792,"figure_citations":{"2":["Figure 2) consists of a custom-made robot arm attached to a commercial HMD, currently an Oculus Rift CV1."]},"section_index":2,"title":"FACEHAPTICS SYSTEM"},{"word_count":5156,"figure_citations":{"3":["Figure 3 and highest for the condition with oscillating head and static wind (M = 14.","Figure 3 (middle)): The standard deviation of the signed point error differed signiﬁcantly between movement conditions (F(3, 45) = 3.","Figure 3 (right)."],"4":["Figure 4) but from a static location.","Figure 4) that contained 16 events along a 3 minute pre-deﬁned walkthrough."]},"section_index":3,"title":"USER STUDIES"},{"word_count":512,"figure_citations":{},"section_index":4,"title":"TECHNICAL CONSIDERATIONS AND LIMITATIONS"},{"word_count":734,"figure_citations":{"5":["Figure 5), underlining the easy extensibility of the system, but also the need for a reloading mechanism or dispensing system (e."]},"section_index":5,"title":"CONCLUSION AND FUTURE WORK"},{"word_count":19,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":2386,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"FaceHaptics: Robot Arm based Versatile Facial Haptics for Immersive Environments","authors":"Alexander Wilberz, Dominik Leschtschow, Christina Trepkowski, Jens Maiero, Ernst Kruijff, Bernhard Riecke","abstract":"This paper introduces FaceHaptics, a novel haptic display based on a robot arm attached to a head-mounted virtual reality display. It provides localized, multi-directional and movable haptic cues in the form of wind, warmth, moving and single-point touch events and water spray to dedicated parts of the face not covered by the head-mounted display.The easily extensible system, however, can principally mount any type of compact haptic actuator or object. User study 1 showed that users appreciate the directional resolution of cues, and can judge wind direction well, especially when they move their head and wind direction is adjusted dynamically to compensate for head rotations. Study 2 showed that adding FaceHaptics cues to a VR walkthrough can significantly improve user experience, presence, and emotional responses.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1.  The FaceHaptics system, showing a side and frontal view of the setup for face haptic feedback, affording various sensations including touch, \ntexture, warmth, air ﬂow, or wetness. The left image depict one of many possible touch/texture feedback elements, which can easily be exchanged. \n","bbox":[53.929,362.95202,567.1491799999998,379.88827],"page":"1"},"2":{"caption":"Figure  2.  System  overview:  elements  of  the  robot  arm  with  different \nfeedback  elements.  The  close  up  shows  a  frontal  view  of  lower  robot \narm  with  fan,  heat  wire  in  front  of  fan,  and  spray  nozzle.  The  blue \noverlay over the face shows the approximate area that can be reached \nusing touch events (wind can be sensed over the whole face). \n","bbox":[321.094,570.2162700000001,566.15556,614.0512699999999],"page":"3"},"3":{"caption":"Figure 3. Mean performance for the different conditions, averaged over \nthe two repetitions. Gray dots indicate participant mean data, whiskers \nindicate 95% conﬁdence intervals. \n","bbox":[53.929,457.25477,299.1658999999999,483.15727],"page":"6"},"4":{"caption":"Figure 4.  Feedback elements with sample events: ventilator depicts fan, \nwire for heat, leaf is the rubber tip and spray the water-air spray. \n","bbox":[320.80708,499.12802,567.1518099999998,516.06427],"page":"6"},"5":{"caption":"Figure 5.  Adding biting functionality to FaceHaptics - showing the po-\ntential of extensibility, but also the limitations regarding reloading. \n","bbox":[321.094,587.65402,565.4860799999999,604.5902699999999],"page":"9"}},"crops":{"1":{"crop_coord":[144.80278027777777,741.8249850000001,1572.052798888889,1136.8250019444445],"bbox":[53.9290009,384.5429993,564.1390076,523.1430054],"page":"1"},"2":{"crop_coord":[886.9277613888887,168.22595361111084,1572.105441388889,486.37223972222205],"bbox":[321.0939941,618.7059937,564.1579589,729.6386567000001],"page":"3"},"3":{"crop_coord":[151.37993724230557,-223.39531413833333,485.9485107461944,849.9683429816666],"bbox":[56.29677740723001,487.8113965266,173.14146386863,870.6223130898001],"page":"6"},"4":{"crop_coord":[374.3667290891111,-282.4600356033338,718.6981408480002,849.9700481855556],"bbox":[136.57202247208,487.8107826532,256.93133070528006,891.8856128172001],"page":"6"},"5":{"crop_coord":[597.3521680756389,-223.39531413833333,931.9207415795279,849.9683429816666],"bbox":[216.84678050723002,487.8113965266,333.69146696863004,870.6223130898001],"page":"6"},"6":{"crop_coord":[886.9277613888887,168.2295991666667,1572.131466111111,758.5611047222224],"bbox":[321.0939941,520.7180023,564.1673278,729.6373443],"page":"6"},"7":{"crop_coord":[886.9277613888887,168.21879083333357,1572.0507641666668,512.652791388889],"bbox":[321.0939941,609.2449951,564.1382751,729.6412353],"page":"9"}}}},{"filename":"3313831.3376523","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376523.pdf","paper_id":"3313831.3376523","venue":"chi2020","keywords":["Haptic interfaces","Room-scale haptics","Virtual reality","Swarm robots"],"paragraph_containing_keyword":"Author Keywords \n \nhaptic interfaces; room-scale haptics; virtual reality; swarm \n \n \n \nrobots","paragraph_after_keyword":"","doi":"10.1145/3313831.3376523","sections":[{"word_count":809,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":2552,"figure_citations":{"3":["Figure 3 illustrates the mechanical design of each RoomShift robot."],"5":["Figure 5) ranges from 3.","Figure 5 illustrates various static props that the RoomShift robot can actuate."],"6":["Figure 6 depicts the space and mounted cameras on the ceiling (left) and tracking software (right)."],"7":["Figure 7).","Figure 7)."],"9":["Figure 9 illustrates the schematic of RoomShift’s circuit."]},"section_index":1,"title":"RELATED WORK"},{"word_count":9,"figure_citations":{},"section_index":2,"title":"MCU"},{"word_count":432,"figure_citations":{"8":["Figure 8)."]},"section_index":3,"title":"GND"},{"word_count":718,"figure_citations":{"10":["Figure 10 illustrates the architecture of the RoomShift software."]},"section_index":4,"title":"UDP"},{"word_count":920,"figure_citations":{"1":["Figure 1)."],"12":["Figure 12)."],"13":["Figure 13)."],"14":["Figure 14)."]},"section_index":5,"title":"INTERACTION WITH ROOMSHIFT"},{"word_count":294,"figure_citations":{},"section_index":6,"title":"DISCUSSION"},{"word_count":559,"figure_citations":{},"section_index":7,"title":"LIMITATIONS AND FUTURE WORK"},{"word_count":110,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":43,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGEMENTS"},{"word_count":1939,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots","authors":"Ryo Suzuki, Hooman Hedayati, Clement Zheng, James L. Bohn, Daniel Szafir, Ellen Yi-Luen Do, Mark D. Gross, Daniel Leithinger","abstract":"RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1.  RoomShift is composed of a swarm of shape-changing robots for haptic feedback in VR. RoomShift robots move beneath a piece of furniture \nto lift, move and place it.  Multiple robots move furniture to construct a physical haptic environment collectively.  The corresponding virtual scene is \nshown, with a human silhouette added for a reference. \n","bbox":[53.929,455.98877,566.1529299999999,481.89126999999996],"page":"1"},"2":{"caption":"Figure 2. A RoomShift robot drives beneath a desk, lifts it by extending \nthe scissor structure, and moves it. \n","bbox":[53.929,254.89502,298.99055999999996,271.83126999999996],"page":"3"},"3":{"caption":"Figure 3. Mechanical design of the robot and the scissor structure. \n","bbox":[330.149,467.64827,557.1027199999999,475.61827],"page":"3"},"4":{"caption":"Figure 4. Each robot can extend from 30 cm to 100 cm to lift objects. \n","bbox":[326.175,128.16327,561.0748099999998,136.13327],"page":"3"},"5":{"caption":"Figure 5. Different types of furniture moved by the system. \n","bbox":[75.64,300.12627,277.28099999999995,308.09626999999995],"page":"4"},"6":{"caption":"Figure 6. Photo of tracked space and screenshot of tracking software. \n","bbox":[325.103,443.49427000000003,562.1467399999999,451.46427],"page":"4"},"7":{"caption":"Figure 7.  Retro-reﬂective markers mounted to parallel lift bars,  high-\nlighted in pink. \n","bbox":[321.094,238.51902,565.4860799999999,255.45527],"page":"4"},"8":{"caption":"Figure  8.  The  system  ﬁrst  navigates  the  robot  to  a  user-deﬁned  entry \npoint to avoid the collision with the legs of furniture. \n","bbox":[53.929,324.59302,299.26951,341.52927],"page":"5"},"9":{"caption":"Figure 9. Hardware schematic of the robot. \n","bbox":[369.178,472.34727,518.07354,480.31726999999995],"page":"5"},"10":{"caption":"Figure 10. The communication software. \n","bbox":[373.689,155.28327000000002,513.5625,163.25327000000001],"page":"5"},"11":{"caption":"Figure 11. The interaction design space of RoomShift. \n","bbox":[217.559,562.73627,402.52676,570.70627],"page":"6"},"12":{"caption":"Figure 12. Simulating a larger table by moving a smaller surface. \n","bbox":[64.908,288.38927,288.0122099999999,296.35927],"page":"7"},"13":{"caption":"Figure  13.  When  teleporting,  the  robots  move  furniture  to  match  the \nnew scene position. \n","bbox":[321.094,452.89702,566.1555599999998,469.83326999999997],"page":"7"},"14":{"caption":"Figure 14. Pointing and moving with a gesture. \n","bbox":[363.061,152.61627000000001,524.1904899999998,160.58627],"page":"7"}},"crops":{"1":{"crop_coord":[147.88056277777778,618.2964721944443,766.4508765763889,853.4833188888889],"bbox":[55.0370026,486.5460052,274.1223155675,567.6132700100001],"page":"1"},"2":{"crop_coord":[763.3666652777779,618.2873283333333,1569.0592363888888,853.4833188888889],"bbox":[276.6119995,486.5460052,563.0613251,567.6165618],"page":"1"},"3":{"crop_coord":[144.80278027777777,1041.6152616666666,830.0020347222221,1436.9833544444446],"bbox":[53.9290009,276.4859924,297.00073249999997,415.2185058],"page":"3"},"4":{"crop_coord":[887.1499888888889,536.4790683333333,1231.190482222222,865.3722041666667],"bbox":[321.173996,482.2660065,441.4285736,597.0675354],"page":"3"},"5":{"crop_coord":[1228.1083508333334,536.4754740486665,1571.9056618013333,865.3722041666667],"bbox":[443.9190063,482.2660065,564.08603824848,597.0688293424801],"page":"3"},"6":{"crop_coord":[886.9277613888887,1482.9421233333333,1572.119767777778,1808.3860947222222],"bbox":[321.0939941,142.7810059,564.1631164,256.3408356],"page":"3"},"7":{"crop_coord":[144.80278027777777,985.191226388889,829.8783536111112,1330.7111019444446],"bbox":[53.9290009,314.7440033,296.9562073,435.53115849999995],"page":"4"},"8":{"crop_coord":[886.9277613888887,694.9784680555556,1572.1126894444442,932.4666680555556],"bbox":[321.0939941,458.1119995,564.1605682,540.0077515],"page":"4"},"9":{"crop_coord":[886.9277613888887,1235.069665,1572.1375272222222,1482.4722205555554],"bbox":[321.0939941,260.1100006,564.1695098,345.5749206],"page":"4"},"10":{"crop_coord":[144.80278027777777,863.2297361622225,830.0117451977778,1243.3805591666667],"bbox":[53.9290009,346.1829987,297.0042282712,479.43729498159996],"page":"5"},"11":{"crop_coord":[886.9277613888887,476.2046651447222,1572.134642416389,852.3222097222223],"bbox":[321.0939941,486.9640045,564.1684712699,618.7663205479],"page":"5"},"12":{"crop_coord":[886.9277613888887,1393.9131586977778,1572.1368309744441,1733.0555725],"bbox":[321.0939941,169.8999939,564.1692591507999,288.3912628688],"page":"5"},"13":{"crop_coord":[144.80278027777777,168.22615041622268,1572.0332982869443,601.2388780555557],"bbox":[53.9290009,577.3540039,564.1319873832999,729.6385858501599],"page":"6"},"14":{"crop_coord":[144.80278027777777,1163.447706388889,829.96677,1363.3138783333334],"bbox":[53.9290009,303.0070038,296.9880372,371.3588257],"page":"7"},"15":{"crop_coord":[886.9277613888887,504.24411361111095,1572.1038308333332,886.9777594444444],"bbox":[321.0939941,474.4880066,564.1573791,608.6721191],"page":"7"},"16":{"crop_coord":[886.9277613888887,1350.7316250000001,1572.091751111111,1740.4638841666667],"bbox":[321.0939941,167.2330017,564.1530304,303.93661499999996],"page":"7"}}}},{"filename":"3313831.3376550","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376550.pdf","paper_id":"3313831.3376550","venue":"chi2020","keywords":["Mixed Reality","Augmented Reality","Virtual Reality","Remote collaboration","3D panorama","Scene reconstruction","Eye gaze","Hand gesture"],"paragraph_containing_keyword":"Author Keywords \nMixed Reality; Augmented Reality; Virtual Reality; remote \ncollaboration; 3D panorama; scene reconstruction; eye gaze; \nhand gesture.","paragraph_after_keyword":"CCS Concepts \n•Human-centered computing → Mixed / augmented real-\nity; Collaborative interaction; Computer supported cooper-\native work;","doi":"10.1145/3313831.3376550","sections":[{"word_count":669,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1483,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1668,"figure_citations":{"1":["Figure 1 shows an overview of our system."],"2":["Figure 2a).","Figure 2a.","Figure 2b with the red frames."],"3":["Figure 3) that could be shared from the remote VR mode to the local AR mode: • Eye Gaze A virtual raycast line of the remote user’s eye gaze overlaid onto the local user’s AR view from a third-person perspective."],"4":["Figure 4)."]},"section_index":2,"title":"SYSTEM OVERVIEW"},{"word_count":1347,"figure_citations":{"5":["Figure 5b).","Figure 5a.","Figure 5c.","Figure 5a."],"6":["Figure 6a).","Figure 6b)."]},"section_index":3,"title":"USER STUDY"},{"word_count":1797,"figure_citations":{"7":["Figure 7 shows the average CP rating."],"8":["Figure 8 shows the average rating results of each condition for TLX."],"9":["Figure 9)."]},"section_index":4,"title":"RESULTS"},{"word_count":1918,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":254,"figure_citations":{},"section_index":6,"title":"CONCLUSIONS AND FUTURE WORK"},{"word_count":2057,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing","authors":"Huidong Bai, Prasanth Sasikumar, Jing Yang, Mark Billinghurst","abstract":"Supporting natural communication cues is critical for people to work together remotely and face-to-face. In this paper we present a Mixed Reality (MR) remote collaboration system that enables a local worker to share a live 3D panorama of his/her surroundings with a remote expert. The remote expert can also share task instructions back to the local worker using visual cues in addition to verbal communication. We conducted a user study to investigate how sharing augmented gaze and gesture cues from the remote expert to the local worker could affect the overall collaboration performance and user experience. We found that by combing gaze and gesture cues, our remote collaboration system could provide a significantly stronger sense of co-presence for both the local and remote users than using the gaze cue alone. The combined cues were also rated significantly higher than the gaze in terms of ease of conveying spatial actions.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure  1.  The  3D  panorama  unit  reconstructs  the  local  environment, \nand then streams the stitched point-cloud data to the remote VR expert \nvia the network. The eye gaze and hand gesture information are shared \nback to the local AR worker from the remote VR expert synchronously. \n","bbox":[320.89475,180.35052,567.1518099999998,215.21927],"page":"3"},"2":{"caption":"Figure 2.  a) The sensor cluster with eight depth camera units;  b) The \ncalibration result of the sensor cluster. \n","bbox":[53.929,400.60302,298.9905599999999,417.53927],"page":"4"},"3":{"caption":"Figure 3.  Natural visual cues shared from the remote to the local user: \nthe purple gaze raycast line,  and the grey hand mesh in a) the remote \nVR mode and b) the local AR mode. \n","bbox":[320.80708,592.73977,567.4785799999997,618.6422699999999],"page":"4"},"4":{"caption":"Figure  4.  Auxiliary  awareness  cues  for  the  local  worker:  the  simple \navatar has the purple sphere head and half-transparent yellow view frus-\ntum, and the pin arrow in orange always points to the head sphere. \n","bbox":[321.094,296.57277,566.1555599999997,322.47526999999997],"page":"4"},"5":{"caption":"Figure  5.  Experiment  environment:  a)  The  local  section  with  Lego \nbricks placed around desks and the live 3D panorama capture unit in-\nstalled on the ceiling; b) The remote part with the VR headset installed; \nc) A screenshot of the reconstructed live 3D panorama in VR. \n","bbox":[53.929,447.04852,299.7875599999999,481.91727],"page":"6"},"6":{"caption":"Figure 6. a) Lego bricks on the tags; b) The overlaid cubes with numbers \nto indicate the brick for picking up, and the overlaid cubes with numbers \nand arrows to indicate the target position and orientation of the Lego \nbrick with the same pickup number. \n","bbox":[53.929,238.79352,298.99056,273.66227],"page":"6"},"7":{"caption":"Figure  7.  Results  of  Co-presence  questionnaires  (7-point  Likert  scale \nfrom 1 to 7, the higher the better). \n","bbox":[321.094,532.04602,566.1555599999998,548.98227],"page":"7"},"8":{"caption":"Figure  8.  Results  of  the  TLX  questionnaire  (100-points  range  with  5-\npoint steps, 0: very low~100: very high, the lower the better). \n","bbox":[53.929,248.23602,298.32108,265.17226999999997],"page":"8"},"9":{"caption":"Figure 9. User preference based ranking results (Rank1 is the most pre-\nferred, *: statistically signiﬁcant). \n","bbox":[321.094,135.41902,565.4860799999999,152.35527],"page":"8"}},"crops":{"1":{"crop_coord":[886.9277613888887,1204.4389175,1572.127778611111,1594.2389086111111],"bbox":[321.0939941,219.8739929,564.1660003,356.6019897],"page":"3"},"2":{"crop_coord":[144.80278027777777,726.8484911111111,829.9872247222222,1032.2416855555555],"bbox":[53.9290009,422.1929932,296.9954009,528.5345432],"page":"4"},"3":{"crop_coord":[886.9277613888887,168.22625805555555,1572.1122058333333,473.6194525],"bbox":[321.0939941,623.2969971,564.1603941,729.6385471],"page":"4"},"4":{"crop_coord":[886.9277613888887,990.9123474999999,1572.1122058333333,1296.3055419444445],"bbox":[321.0939941,327.1300049,564.1603941,433.4715549],"page":"4"},"5":{"crop_coord":[144.80278027777777,168.22664944444463,829.9872247222222,853.411093888889],"bbox":[53.9290009,486.5720062,296.9954009,729.6384062],"page":"6"},"6":{"crop_coord":[144.80278027777777,1126.5040241666666,829.9872247222222,1431.897218611111],"bbox":[53.9290009,278.3170013,296.9954009,384.6585513],"page":"6"},"7":{"crop_coord":[886.9277613888887,263.24011833333327,1572.1494947222225,667.1194627777776],"bbox":[321.0939941,553.6369934,564.1738181000001,695.4335574],"page":"7"},"8":{"crop_coord":[144.80278027777777,1051.6012227777776,830.0245136111112,1455.4805672222224],"bbox":[53.9290009,269.8269958,297.00882490000004,411.62355980000007],"page":"8"},"9":{"crop_coord":[886.9277613888887,1333.2141663888885,1585.6481897222222,1768.8611263888888],"bbox":[321.0939941,157.0099945,569.0333483,310.24290010000004],"page":"8"}}}},{"filename":"3313831.3376574","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376574.pdf","paper_id":"3313831.3376574","venue":"chi2020","keywords":["Virtual reality","Navigation techniques","Locomotion user interface","Virtual walking"],"paragraph_containing_keyword":"ABSTRACT \nWe introduce VR Strider, a novel locomotion user interface \n(LUI) for seated virtual reality (VR) experiences, which maps \ncycling  biomechanics  of  the  user’s  legs  to  virtual  walking \nmovements.  The core idea is to translate the motion of ped-\naling on a mini exercise bike to a corresponding walking ani-\nmation of a virtual avatar while providing audio-based tactile \nfeedback on virtual ground contacts. We conducted an experi-\nment to evaluate the LUI and our novel anchor-turning rotation \ncontrol method regarding task performance, spatial cognition, \nVR sickness, sense of presence, usability and comfort in a \npath-integration task.  The results show that VR Strider has \na signiﬁcant positive effect on the participants’ angular and \ndistance estimation, sense of presence and feeling of comfort \ncompared to other established locomotion techniques, such as \nteleportation and joystick-based navigation. A conﬁrmatory \nstudy further indicates the necessity of synchronized avatar \nanimations for virtual vehicles that rely on pedalling. \nAuthor Keywords \nVirtual reality, navigation techniques, locomotion user \ninterface, virtual walking \nCCS Concepts \n•Human-centered  computing  →  Human  computer  inter-\naction (HCI); •Human computer interaction (HCI) → In-\nteraction techniques; •Interaction paradigms → Virtual re-\nality; \nINTRODUCTION \nReal walking is usually considered as the most intuitive way \nof locomotion in real and virtual worlds, and has been found \nto be more presence-enhancing compared to other forms of \nlocomotion such as ﬂying or joystick-based navigation [46]. \nFurthermore,  walking  has  been  shown  to  be  superior  over \nother techniques for complex spatial tasks [32], cognitive map \nbuilding [33], and cognitive demands [25].  However, there \nare limitations of using walking as locomotion user interface","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed \nfor proﬁt or commercial advantage and that copies bear this notice and the full citation \non the ﬁrst page.  Copyrights for components of this work owned by others than the \nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or \nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission \nand/or a fee. Request permissions from permissions@acm.org. \nCHI ’20, April 25–30, 2020, Honolulu, HI, USA. \n© 2020 Copyright is held by the owner/author(s). Publication rights licensed to ACM. \nACM ISBN 978-1-4503-6708-0/20/04 ...$15.00. \nhttp://dx.doi.org/10.1145/3313831.3376574","doi":"10.1145/3313831.3376574","sections":[{"word_count":623,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":576,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":2456,"figure_citations":{"1":["Figure 1): 1."],"2":["Figure 2 A.","Figure 2 B).","Figure 2 C).","Figure 2 D), Page 3 CHI 2020 Paper CHI 2020, April 25–30, 2020, Honolulu, HI, USA Figure 2: A-F: Iterations of the IK layer of the walking animation.","Figure 2 E).","Figure 2 F).","Figure 2 G as four discrete phases of a single step, where the ﬁlled and the blank pedals represent the right and left foot respectively."]},"section_index":2,"title":"VR STRIDER"},{"word_count":1649,"figure_citations":{"3":["Figure 3)."]},"section_index":3,"title":"USER STUDY"},{"word_count":1168,"figure_citations":{"4":["Figure 4(left).","Figure 4(center).","Figure 4(right)."],"5":["Figure 5(left).","Figure 5(center).","Figure 5(right)."],"6":["Figure 6 illustrates the distribution of responses for items with signiﬁcant differences."]},"section_index":4,"title":"RESULTS"},{"word_count":1612,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":815,"figure_citations":{"7":["Figure 7)."]},"section_index":6,"title":"CONFIRMATORY STUDY"},{"word_count":343,"figure_citations":{},"section_index":7,"title":"CONCLUSION AND FUTURE WORK"},{"word_count":66,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGEMENTS"},{"word_count":1477,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Walking by Cycling: A Novel In-Place Locomotion User Interface for Seated Virtual Reality Experiences","authors":"Jann Philipp Freiwald, Oscar Ariza, Omar Janeh, Frank Steinicke","abstract":"We introduce VR Strider, a novel locomotion user interface (LUI) for seated virtual reality (VR) experiences, which maps cycling biomechanics of the user's legs to virtual walking movements. The core idea is to translate the motion of pedaling on a mini exercise bike to a corresponding walking animation of a virtual avatar while providing audio-based tactile feedback on virtual ground contacts. We conducted an experiment to evaluate the LUI and our novel anchor-turning rotation control method regarding task performance, spatial cognition, VR sickness, sense of presence, usability and comfort in a path-integration task. The results show that VR Strider has a significant positive effect on the participants' angular and distance estimation, sense of presence and feeling of comfort compared to other established locomotion techniques, such as teleportation and joystick-based navigation. A confirmatory study further indicates the necessity of synchronized avatar animations for virtual vehicles that rely on pedalling.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: The VR Strider device including mini exercise bike and electronics for pressure tracking and vibrotactile feedback. \n","bbox":[60.337,604.034992,560.270377,613.9979920000001],"page":"2"},"2":{"caption":"Figure 2: A-F: Iterations of the IK layer of the walking animation. G: The result superimposed on the reference diagram [43]. \n","bbox":[58.654,645.6289919999999,561.954871,655.591992],"page":"4"},"3":{"caption":"Figure 3: A view to the VE for the path-integration task. \n","bbox":[330.327,599.0899919999999,557.4335849999999,609.052992],"page":"5"},"4":{"caption":"Figure 4: Task Time (left), Distance Error (center), and Angular Error (right). Lower is better in all plots. \n","bbox":[97.178,596.752992,523.405103,606.9451409999999],"page":"7"},"5":{"caption":"Figure 5: Results for SSQ (left, lower is better), IPQ (center, higher is better), and SUS (right, higher is better). \n","bbox":[88.128,597.0659919999999,532.4777999999997,607.028992],"page":"8"},"6":{"caption":"Figure 6: Results for the Device Assessment (DAQ). \n","bbox":[70.619,71.83099200000001,282.810974,81.793992],"page":"8"},"7":{"caption":"Figure 7: A translation of real pedalling to a virtual GoKart. \n","bbox":[322.935,573.322992,564.826677,583.2859920000001],"page":"9"}},"crops":{"1":{"crop_coord":[144.80278027777777,168.20505777777788,1572.0775097222222,472.163891388889],"bbox":[53.9290009,623.8209991,564.1479035,729.6461792],"page":"2"},"2":{"crop_coord":[144.80278027777777,168.23587194444463,1572.02279,356.62501861111116],"bbox":[53.9290009,665.4149933,564.1282044,729.6350861],"page":"4"},"3":{"crop_coord":[886.9277613888887,168.22328361111084,1572.1331191666666,485.8999802777777],"bbox":[321.0939941,618.8760071,564.1679229,729.6396179000001],"page":"5"},"4":{"crop_coord":[144.80278,183.46367774827786,591.2869126308334,478.5555438888888],"bbox":[53.929000800000004,621.5200042,211.0632885471,724.15307601062],"page":"7"},"5":{"crop_coord":[634.9611155555556,182.59433678544454,1081.434734175222,478.55554361111115],"bbox":[230.38600160000001,621.5200043,387.51650430307996,724.46603875724],"page":"7"},"6":{"crop_coord":[1125.1166533333335,183.41794637202798,1571.59357136525,478.55554361111115],"bbox":[406.84199520000004,621.5200043,563.97368569149,724.16953930607],"page":"7"},"7":{"crop_coord":[144.80278,182.5914683038335,591.2869126308334,477.6833344444444],"bbox":[53.929000800000004,621.8339996,211.0632885471,724.46707141062],"page":"8"},"8":{"crop_coord":[634.9611155555556,182.62064317352804,1081.4404107162222,477.6833344444444],"bbox":[230.3860016,621.8339996,387.51854785783996,724.4565684575299],"page":"8"},"9":{"crop_coord":[1125.1166533333333,182.5914683038335,1571.5987261163334,477.6833344444444],"bbox":[406.8419952,621.8339996,563.97554140188,724.46707141062],"page":"8"},"10":{"crop_coord":[144.80278027777777,1102.3143346949998,828.943993365,1950.5083211111112],"bbox":[53.9290009,91.6170044,296.6198376114,393.3668395098],"page":"8"},"11":{"crop_coord":[886.9277613888887,168.22709833333323,1572.1272277777778,557.4750180555557],"bbox":[321.0939941,593.1089935,564.165802,729.6382446],"page":"9"}}}},{"filename":"3313831.3376582","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376582.pdf","paper_id":"3313831.3376582","venue":"chi2020","keywords":["Alienation","Cinematic VR","Distancing Effect","Estrangement","Immersive Storytelling","Reﬂexivity","Virtual Reality"],"paragraph_containing_keyword":"Author Keywords \n \nAlienation; Cinematic VR; Distancing Effect; Estrangement; \n \n \n \nImmersive Storytelling; Reﬂexivity; Virtual Reality.","paragraph_after_keyword":"","doi":"10.1145/3313831.3376582","sections":[{"word_count":934,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":3011,"figure_citations":{"2":["Figure 2 displays the diorama."]},"section_index":1,"title":"RELATED WORK"},{"word_count":488,"figure_citations":{},"section_index":2,"title":"METHOD"},{"word_count":1896,"figure_citations":{},"section_index":3,"title":"FINDINGS"},{"word_count":1318,"figure_citations":{},"section_index":4,"title":"DISCUSSION AND IMPLICATIONS"},{"word_count":132,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":92,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGEMENTS"},{"word_count":1700,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Reflexive VR Storytelling Design Beyond Immersion: Facilitating Self-Reflection on Death and Loneliness","authors":"Sojung Bahng, Ryan M. Kelly, Jon McCormack","abstract":"This research examines the reflexive dimensions of cinematic virtual reality (CVR) storytelling. We created Anonymous, an interactive CVR piece that employs a reflexive storytelling method. This method is based on distancing effects and is used to elicit audience awareness and self-reflection about loneliness and death. To understand the audience's experiences, we conducted in-depth interviews to study which design factors and elements prompted reflexive thoughts and feelings. Our findings highlight how the audience experience was impacted by four reflexive dimensions: abstract and minimal aesthetics, everyday materials and textures, the restriction of control, and multiple, disembodied points of view. We use our findings to discuss how these dimensions can inform the design of VR storytelling experiences that provoke self and social reflection.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Scenes from Anonymous, an interactive cinematic virtual reality story about death and loneliness. \n","bbox":[95.615,417.630992,524.9904109999999,427.823141],"page":"1"},"2":{"caption":"Figure 2: Initial diorama of Anonymous using physical card-\nboard. \n","bbox":[53.929,516.942992,298.66269900000003,538.0941409999999],"page":"4"},"3":{"caption":"Figure 3: Objects in Anonymous. \n","bbox":[376.887,432.279992,510.869424,442.472141],"page":"4"},"4":{"caption":"Figure 4: The virtual environment of Anonymous. \n","bbox":[76.442,594.966992,276.97726399999993,605.159141],"page":"5"},"5":{"caption":"Figure 5: The widower character from Anonymous. \n","bbox":[340.504,539.076992,547.2462130000001,549.269141],"page":"5"},"6":{"caption":"Figure 6: Screenshot of the viewer inhabiting an object within \nAnonymous. In this image, the viewer’s perspective is shown \nfrom inside the main character’s telephone. The foreground of \nthe image shows the telephone’s number pad and handset, and \nthe main character appears in the background. \n","bbox":[53.321,399.03899199999995,299.51342529999994,452.837992],"page":"6"},"7":{"caption":"Figure 7:  Screenshots of becoming the butterﬂy.  The upper \nhalf of the image shows the viewer’s perspective when gazing \nat the butterﬂy. The lower half shows the viewer’s perspective \nwhen they are embodied as the butterﬂy. \n","bbox":[320.736,409.195992,566.8895501,452.035992],"page":"6"},"8":{"caption":"Figure 8: Screenshots of the ﬁnal scene in Anonymous. \n","bbox":[199.589,530.576992,420.99674899999997,540.769141],"page":"7"}},"crops":{"1":{"crop_coord":[169.38613055555555,621.3265819444447,817.1211158333333,989.9555544444446],"bbox":[62.779007,437.4160004,292.3636017,566.5224304999999],"page":"1"},"2":{"crop_coord":[899.752773611111,621.2605455555556,1547.432785,989.9555544444446],"bbox":[325.7109985,437.4160004,555.2758026,566.5462036],"page":"1"},"3":{"crop_coord":[144.80278027777777,168.2255724999999,830.0030941666666,683.6444347222222],"bbox":[53.9290009,547.6880035,297.0011139,729.6387939],"page":"4"},"4":{"crop_coord":[886.9277613888887,168.21696805555575,1572.1169280555555,949.2611269444444],"bbox":[321.0939941,452.0659943,564.1620941,729.6418914999999],"page":"4"},"5":{"crop_coord":[144.80278027777777,168.2255300000001,829.9947866666668,497.35276111111096],"bbox":[53.9290009,614.753006,296.9981232,729.6388092],"page":"5"},"6":{"crop_coord":[886.9277613888887,168.22909027777803,1572.112986111111,652.6027594444445],"bbox":[321.0939941,558.8630066,564.160675,729.6375274999999],"page":"5"},"7":{"crop_coord":[144.80278027777777,168.22430083333364,829.9965244444444,919.8305680555555],"bbox":[53.9290009,462.6609955,296.9987488,729.6392516999999],"page":"6"},"8":{"crop_coord":[893.6833361111111,168.23502444444458,1565.3666519444444,922.0583513888888],"bbox":[323.526001,461.8589935,561.7319947,729.6353912],"page":"6"},"9":{"crop_coord":[144.80278027777777,168.2299805555555,1572.1494380555555,676.2166680555554],"bbox":[53.9290009,550.3619995,564.1737977,729.637207],"page":"7"}}}},{"filename":"3313831.3376614","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376614.pdf","paper_id":"3313831.3376614","venue":"chi2020","keywords":["ARchitect","Virtual reality","Affordance","Passive haptics","Asymmetric"],"paragraph_containing_keyword":"Author Keywords \nARchitect; virtual reality; affordance; passive haptics; \nasymmetric","paragraph_after_keyword":"CCS Concepts \n•Human-centered computing → Virtual reality; Graphical \nuser interfaces; \n*Denotes equal contribution \nPermission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed \nfor proﬁt or commercial advantage and that copies bear this notice and the full citation \non the ﬁrst page.  Copyrights for components of this work owned by others than the \nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or \nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission \nand/or a fee. Request permissions from permissions@acm.org. \nCHI’20, April 25–30, 2020, Honolulu, HI, USA \n© 2020 Copyright held by the owner/author(s).  Publication rights licensed to ACM. \nISBN 978-1-4503-6708-0/20/04. . . $15.00 \nDOI: https://doi.org/10.1145/3313831.3376614","doi":"10.1145/3313831.3376614","sections":[{"word_count":1062,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":648,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1730,"figure_citations":{"2":["Figure 2)."],"3":["Figure 3a-e)."],"4":["Figure 4)."],"5":["Figure 5a).","Figure 5b).","Figure 5c), rotated by two-ﬁnger twirl (Figure 5d), and resized by two-ﬁnger pinch (Figure 5e) [2].","Figure 5f)."]},"section_index":2,"title":"DESIGN AND IMPLEMENTATION"},{"word_count":1284,"figure_citations":{"3":["Figure 3)."],"6":["Figure 6)."],"7":["Figure 7)."]},"section_index":3,"title":"USER STUDY"},{"word_count":1804,"figure_citations":{"8":["Figure 8 shows an overview for comparison between ARchitect and the baseline."]},"section_index":4,"title":"EXPERIMENTAL RESULT AND ANALYSIS"},{"word_count":1412,"figure_citations":{"9":["Figure 9a).","Figure 9b-d)."],"10":["Figure 10b).","Figure 10a).","Figure 10c)."],"11":["Figure 11a).","Figure 11b).","Figure 11c-d)."]},"section_index":5,"title":"DESIGN GUIDELINES AND EXAMPLE EXPERIENCES"},{"word_count":395,"figure_citations":{},"section_index":6,"title":"LIMITATIONS AND FURTHER WORK"},{"word_count":105,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":69,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGMENTS"},{"word_count":1744,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"ARchitect: Building Interactive Virtual Experiences from Physical Affordances by Bringing Human-in-the-Loop","authors":"Chuan-en Lin, Ta Ying Cheng, Xiaojuan Ma","abstract":"Automatic generation of Virtual Reality (VR) worlds which adapt to physical environments have been proposed to enable safe walking in VR. However, such techniques mainly focus on the avoidance of physical objects as obstacles and overlook their interaction affordances as passive haptics. Current VR experiences involving interaction with physical objects in surroundings still require verbal instruction from an assisting partner. We present ARchitect, a proof-of-concept prototype that allows flexible customization of a VR experience with human-in-the-loop. ARchitect brings in an assistant to map physical objects to virtual proxies of matching affordances using Augmented Reality (AR). In a within-subjects study (9 user pairs) comparing ARchitect to a baseline condition, assistants and players experienced decreased workload and players showed increased VR presence and trust in the assistant. Finally, we defined design guidelines of ARchitect for future designers and implemented three demonstrative experiences.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. (a) ARchitect allows physical objects to be mapped to virtual proxies offering matching affordances (e.g. both “chair” and “tree stump” afford \na “sitting” interaction). A physical scene (b) may be translated using ARchitect’s user interface (c) to an interactive virtual experience (d). \n","bbox":[53.929,487.97802,566.1529299999999,504.91427],"page":"1"},"2":{"caption":"Figure 2. Overview of the ARchitect system \n","bbox":[368.728,547.68027,518.52415,555.65027],"page":"3"},"3":{"caption":"Figure 3. Components of the virtual world being added by the Assistant: (a) scene, (b) barriers, (c) obstacles, (d) interactables, (e) game objects. \n","bbox":[65.403,661.5432699999999,554.6813,669.5132699999999],"page":"4"},"4":{"caption":"Figure 4. The Affordance Recommender detects predeﬁned interactable \nclasses (e.g. “chair” class corresponds to “sitting on” interactable). \n","bbox":[321.094,503.81701999999996,566.1555599999999,520.7532699999999],"page":"4"},"5":{"caption":"Figure 5.  Operations for conﬁguring a virtual proxy over the physical \nscene using ARchitect:  (a) scan the ﬂoor, (b) place the virtual proxy, (c) \ntranslate  the  virtual  proxy,  (d)  rotate  the  virtual  proxy,  (e)  resize  the \nvirtual proxy, (f) remove the virtual proxy. \n","bbox":[53.72975,496.03652000000005,299.52454999999986,530.90527],"page":"5"},"6":{"caption":"Figure 6. The virtual view of the Player in baseline (a) and in ARchitect \n(b) while bending down to collect virtual mushrooms (c). \n","bbox":[320.83099,658.86602,566.1555599999999,675.80227],"page":"6"},"7":{"caption":"Figure 7. The virtual view of the Player in baseline (a) and in ARchitect \n(b) while sitting down to take a rest (c). \n","bbox":[320.83099,579.39802,566.1555599999999,596.33427],"page":"6"},"8":{"caption":"Figure 8.  Means and standard deviations of presence, trust, workload \n(Assistant), and workload (Player) scores comparing ARchitect and the \nbaseline. Error bars are standard errors. \n","bbox":[53.66599,542.00377,298.99055999999996,567.90627],"page":"7"},"9":{"caption":"Figure 9.  (a) In LavaEscape, the Assistant ﬁrst maps physical furniture \nto rocks of various shapes and sizes (and heights). The Player steps onto \nthe lowest piece of furniture (b) and continues moving onto higher furni-\nture as the lava plane rises (c) until reaching the higher ground (d). \n","bbox":[53.929,555.94752,298.99055999999996,590.8162699999999],"page":"9"},"10":{"caption":"Figure 10.  A Player attempting to locate the treasure chest (3D audio \nsource)  (a)  while  playing  Maze.  The  virtual  maze  corresponds  to  the \nphysical room conﬁguration (walls and table) with the addition of a fake \nvirtual wall (b). (c) shows an example completion path that may be taken \nby the Player. \n","bbox":[320.89475,544.6712700000002,566.15556,588.50627],"page":"9"},"11":{"caption":"Figure 11. A Player playing with a skateboard simulator conﬁgured with \nSandbox (a) using a physical skateboard controller (b).  The experience \nmay be reconﬁgured to a different theme (e.g. aquatic (c) or space (d)). \n","bbox":[53.70584,495.02876999999995,298.99055999999996,520.9312699999999],"page":"10"}},"crops":{"1":{"crop_coord":[144.80278027777777,497.26173388888895,1572.0805613888888,789.3000199999999],"bbox":[53.9290009,509.6519928,564.1490021,611.1857758],"page":"1"},"2":{"crop_coord":[886.9277613888887,168.22866638888897,1572.127778611111,644.1166686111111],"bbox":[321.0939941,561.9179993,564.1660003,729.6376801],"page":"3"},"3":{"crop_coord":[144.80278027777777,168.2284544444443,1572.1493108333332,327.83333666666647],"bbox":[53.9290009,675.7799988,564.1737519,729.6377564000001],"page":"4"},"4":{"crop_coord":[886.9277613888887,377.886081111111,1572.127778611111,746.5860916666667],"bbox":[321.0939941,525.029007,564.1660003,654.1610108],"page":"4"},"5":{"crop_coord":[144.80278027777777,168.2289208333334,830.0027974999999,718.3889091666668],"bbox":[53.9290009,535.1799927,297.0010071,729.6375885],"page":"5"},"6":{"crop_coord":[886.9277613888887,168.2283700000002,1572.012574722222,315.8944363888888],"bbox":[321.0939941,680.0780029,564.1245269,729.6377868],"page":"6"},"7":{"crop_coord":[886.9277613888887,388.5115391666667,1572.1339669444442,536.6416847222222],"bbox":[321.0939941,600.6089935,564.1682281,650.3358459],"page":"6"},"8":{"crop_coord":[144.80278027777777,168.2287936111112,830.0027974999999,615.608333888889],"bbox":[53.9290009,572.1809998,297.0010071,729.6376343],"page":"7"},"9":{"crop_coord":[144.80278027777777,168.2389238888889,829.9569363888888,551.969435],"bbox":[53.9290009,595.0910034,296.9844971,729.6339874],"page":"9"},"10":{"crop_coord":[886.9277613888887,168.22790361111112,1572.104000277778,558.3833397222221],"bbox":[321.0939941,592.7819977,564.1574401,729.6379547],"page":"9"},"11":{"crop_coord":[144.80278027777777,168.23578722222194,829.9799941666665,746.0944619444443],"bbox":[53.9290009,525.2059937,296.99279789999997,729.6351166000001],"page":"10"}}}},{"filename":"3313831.3376626","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376626.pdf","paper_id":"3313831.3376626","venue":"chi2020","keywords":["Virtual Reality","Locomotion","Foot-based input"],"doi":"10.1145/3313831.3376626","paragraph_containing_keyword":"Author Keywords \nVirtual Reality, Locomotion, Foot-based input \nCCS Concepts \n•Human-centered computing → Virtual reality; User stud-\nies; Interaction devices; \nINTRODUCTION \nWhile Virtual Reality (VR) allows for inﬁnitely large spaces, \nthe real-world space the user’s physical body resides in is usu-\nally limited. This discrepancy needs to be overcome using ar-\ntiﬁcial locomotion. Current approaches in standard consumer \napplications rely mainly on third party controllers or the two \ncontrollers present with most VR devices on the market. Lo-\ncomotion is either realized via direct motion or teleportation, \nboth rely on button inputs and controller or head-mounted \ndisplay (HMD) positions. \nHowever, the hands are usually used for interaction, the head \nfor exploration, and feet for locomotion. While hands and head \ncan naturally be used for their real world purposes in VR, feet \nare still a neglected input modality. In this paper we explored \npossible input modalities relying on feet for locomotion input, \nto more naturally distribute task to the users’ interaction habits. \nAn established group of approaches utilizing feet for loco-\nmotion in VR is redirected walking. Locomotion relying on \nredirected walking lets the user explore the world around them \nfreely by walking around. However, the user’s walking path is","sections":[{"word_count":458,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":2047,"figure_citations":{"2":["Figure 2 a).","Figure 2 b).","Figure 2 c)."],"3":["Figure 3 a) shows a schematic representation of this input modality.","Figure 3 b)).","Figure 3 c) further illustrates how this input method works."],"4":["Figure 4 shows the internals of the prototype."],"5":["Figure 5), with a light blue line ending in a circle with upwards fading walls and a downward pointing arrow in the center."]},"section_index":1,"title":"RELATED WORK"},{"word_count":1241,"figure_citations":{"5":["Figure 5)."],"8":["Figure 8)."]},"section_index":2,"title":"METHODOLOGY"},{"word_count":2266,"figure_citations":{"6":["Figure 6 depicts the measured mean errors for all conditions."],"7":["Figure 7a depicts the measured mean TCTs for all conditions.","Figure 7b depicts the measured mean numbers of teleports for individual conditions.","Figure 7c depicts the measured mean RTLX values the individual conditions."],"8":["Figure 8 depicts all answers of the participants.","Figure 8 depicts all answers of the participants.","Figure 8 depicts all answers given."]},"section_index":3,"title":"RESULTS"},{"word_count":1756,"figure_citations":{"7":["Figure 7a, we found no signiﬁcant differences between directional input modalities."],"8":["Figure 8)."]},"section_index":4,"title":"DISCUSSION"},{"word_count":180,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":22,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGEMENT"},{"word_count":2410,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Podoportation: Foot-Based Locomotion in Virtual Reality","authors":"Julius von Willich, Martin Schmitz, Florian Müller, Daniel Schmitt, Max Mühlhäuser","abstract":"Virtual Reality (VR) allows for infinitely large environments. However, the physical traversable space is always limited by real-world boundaries. This discrepancy between physical and virtual dimensions renders traditional locomotion methods used in real world unfeasible. To alleviate these limitations, research proposed various artificial locomotion concepts such as teleportation, treadmills, and redirected walking. However, these concepts occupy the user's hands, require complex hardware or large physical spaces. In this paper, we contribute nine VR locomotion concepts for foot-based locomotion, relying on the 3D position of the user's feet and the pressure applied to the sole as input modalities. We evaluate our concepts and compare them to state-of-the-art point &amp; teleport technique in a controlled experiment with 20 participants. The results confirm the viability of our approaches for foot-based and engaging locomotion. Further, based on the findings, we contribute a wireless hardware prototype implementation.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. A person in Virtual Reality, using their feet for teleportation while simultaneously using their hands for interaction. \n","bbox":[96.805,399.91927,523.2796999999997,407.88926999999995],"page":"1"},"2":{"caption":"Figure 2.  Illustration of the different direction input methods.  a) inter \nfeet direction b) foot direction c) Point and lean \n","bbox":[53.928999999999974,620.05126,298.99055999999996,637.0512699999999],"page":"3"},"3":{"caption":"Figure 3. Illustration of the different distance input methods. a) forefoot \nlift b) inter feet distance c) intra foot pressure \n","bbox":[321.094,619.64726,566.15556,636.6472699999999],"page":"3"},"4":{"caption":"Figure 4.  The inside of our prototype with two visible sensors (left), the \ntop sole with the four pins (middle) and an example of how the prototype \nis worn (right) \n","bbox":[53.929,584.20177,298.99056,610.1042699999999],"page":"4"},"5":{"caption":"Figure 5. Screenshot of our testing environment, showing a target and a \nteleport being performed. \n","bbox":[53.929,536.67302,298.99055999999996,553.6092699999999],"page":"5"},"6":{"caption":"Figure 6. Error in position, measured between the target center and the \nparticipants’ active foot. \n","bbox":[53.929,526.93802,298.9905599999999,543.87427],"page":"6"},"7":{"caption":"Figure 7. Efﬁciency and convenience metric of all presented input modalities including point & teleport as baseline. \n","bbox":[113.662,557.28927,506.4235999999998,565.25927],"page":"7"},"8":{"caption":"Figure 8. The results of our custom questionnaire on a 5-point Likert scale (1:fully disagree, 5:fully agree, percentage of answers) \n","bbox":[90.122,565.84727,529.9623899999999,573.81727],"page":"8"}},"crops":{"1":{"crop_coord":[144.80278027777777,642.1915349999999,1572.0877669444444,1045.2055527777777],"bbox":[53.9290009,417.526001,564.1515961,559.0110474],"page":"1"},"2":{"crop_coord":[144.80278027777777,168.2283697222221,830.0027974999999,392.04168527777784],"bbox":[53.9290009,652.6649933,297.0010071,729.6377869],"page":"3"},"3":{"crop_coord":[886.9277613888887,191.84893277777783,1572.0946755555553,393.16388444444453],"bbox":[321.0939941,652.2610016,564.1540832,721.1343842],"page":"3"},"4":{"crop_coord":[144.80278027777777,168.22294444444447,829.9913536111111,466.89443805555567],"bbox":[53.9290009,625.7180023,296.99688729999997,729.63974],"page":"4"},"5":{"crop_coord":[144.80278027777777,168.2235380555558,830.010215,623.8277775000003],"bbox":[53.9290009,569.2220001,297.0036774,729.6395262999999],"page":"5"},"6":{"crop_coord":[144.80278027777777,168.2256683333336,830.0041802777778,650.8666483333336],"bbox":[53.9290009,559.4880066,297.0015049,729.6387593999999],"page":"6"},"7":{"crop_coord":[144.80278027777777,200.12964722222227,537.4870402777779,592.8139072222222],"bbox":[53.9290009,580.3869934,191.6953345,718.153327],"page":"7"},"8":{"crop_coord":[577.0860969444444,200.12964722222227,969.7703569444445,592.8139072222222],"bbox":[209.5509949,580.3869934,347.31732850000003,718.153327],"page":"7"},"9":{"crop_coord":[1009.369456111111,195.89982722222203,1572.1038561111113,592.8139072222222],"bbox":[365.1730042,580.3869934,564.1573882,719.6760622],"page":"7"},"10":{"crop_coord":[144.80278027777777,168.2249788888888,1572.177793888889,592.5999875],"bbox":[53.9290009,580.4640045,564.1840058,729.6390076],"page":"8"}}}},{"filename":"3313831.3376628","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376628.pdf","paper_id":"3313831.3376628","venue":"chi2020","keywords":["Sketching","Pen and tablet","Mid-air painting","Virtual reality","Interaction metaphors","Design space"],"doi":"10.1145/3313831.3376628","paragraph_containing_keyword":"Author Keywords \nsketching; pen and tablet; mid-air painting; \nvirtual reality; interaction metaphors; design space;","paragraph_after_keyword":"CCS Concepts \n•Human-centered  computing  →  Virtual  reality;  Gra­\nphics  input  devices;  Interaction  techniques;  Walkthrough \nevaluations;","sections":[{"word_count":639,"figure_citations":{"1":["Figure 1b) and the combination of pen and tablet as 6DoF-tracked 3D input devices (e.","Figure 1d/8a)."],"7":["Figure 7a) and sketch on them using pen on tablet (see Figure 1c/7b)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1309,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":901,"figure_citations":{},"section_index":2,"title":"PRELIMINARY EXPERT INTERVIEWS"},{"word_count":2439,"figure_citations":{"2":["Figure 2) known as Zwicky box [64], which is a common design space tool (e.","Figure 2/3).","Figure 2).","Figure 2).","Figure 2).","Figure 2).","Figure 2)."],"3":["Figure 3), we will show how to place an interaction metaphor from SymbiosisSketch [3].","Figure 3 for implementation and avoided functional gaps (2).","Figure 3; see [23]).","Figure 3 are not complete and were created and positioned during joint brainstorming sessions."],"5":["Figure 5a)."],"7":["Figure 7a)."],"8":["Figure 8a)."],"9":["Figure 9a) that allows users to see distant and hidden objects inside the space.","Figure 9b) in the direction of one axis [13]."]},"section_index":3,"title":"DESIGN SPACE"},{"word_count":1880,"figure_citations":{"3":["Figure 3).","Figure 3), we implemented drawing surfaces as drawing aids that can be placed in space by the tablet position and orientation.","Figure 3).","Figure 3) that it is constrained to 2D surfaces.","Figure 3)."],"4":["Figure 4a) on the pen to increase and the left to decrease.","Figure 4a).","Figure 4a).","Figure 4b).","Figure 4c).","Figure 4a/5a)."],"5":["Figure 5a).","Figure 5b)."],"6":["Figure 6a).","Figure 6b) and scaling are supported as well."],"7":["Figure 7a).","Figure 7b)."],"8":["Figure 8a).","Figure 8b)."],"9":["Figure 9a).","Figure 9b)."],"10":["Figure 10).","Figure 10).","Figure 10).","Figure 10).","Figure 10)."]},"section_index":4,"title":"THE VRSKETCHIN SYSTEM"},{"word_count":133,"figure_citations":{},"section_index":5,"title":"RESULTS"},{"word_count":1114,"figure_citations":{},"section_index":6,"title":"USABILITY WALKTHROUGH"},{"word_count":337,"figure_citations":{},"section_index":7,"title":"DISCUSSION AND FUTURE WORK"},{"word_count":196,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":26,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGMENTS"},{"word_count":2129,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality","authors":"Tobias Drey, Jan Gugenheimer, Julian Karlbauer, Maximilian Milo, Enrico Rukzio","abstract":"Sketching in virtual reality (VR) enhances perception and understanding of 3D volumes, but is currently a challenging task, as spatial input devices (e.g., tracked controllers) do not provide any scaffolding or constraints for mid-air interaction. We present VRSketchIn, a VR sketching application using a 6DoF-tracked pen and a 6DoF-tracked tablet as input devices, combining unconstrained 3D mid-air with constrained 2D surface-based sketching. To explore what possibilities arise from this combination of 2D (pen on tablet) and 3D input (6DoF pen), we present a set of design dimensions and define the design space for 2D and 3D sketching interaction metaphors in VR. We categorize prior art inside our design space and implemented a subset of metaphors for pen and tablet sketching in our prototype. To gain a deeper understanding which specific sketching operations users perform with 2D and which with 3D metaphors, we present findings of usability walkthroughs with six participants.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure  1.  VRSketchIn  is  an  (a)  immersive  sketching  application  combining  (b)  unconstrained  3D  mid-air  sketching  with  a  pen  and  (c)  constrained \nsurface-based sketching with pen on tablet. We created a design space and describe multiple interaction metaphors such as drawing surfaces (c) or World \nIn Miniature (d) to enable a combination of 2D and 3D mid-air sketching. \n","bbox":[53.92899999999999,346.7725241,566.1593569,372.6753491],"page":"1"},"2":{"caption":"Figure 2. A design space for pen- and tablet-based VR sketching applica­\ntions. Interaction metaphors are categorized into cells of this space by se­\nlecting the appropriate input devices (D1) and sketching operations (D2) \nparameters (P). This matrix was created during a morphological analysis. \n","bbox":[53.929,598.4111616000001,300.38840229999994,633.2803491],"page":"5"},"3":{"caption":"Figure 3. A populated version of our design space for pen- and tablet-based sketching that shows (1) the classiﬁcation of interaction metaphors, (2) the \nclassiﬁcation of prior art, (3) the deﬁned interaction metaphor groups, and (4) the interaction metaphors implemented in VRSketchIn. Due to space \nconstraints, we had to split the design space into two parts. This is only a visualization. \n","bbox":[53.928999999999974,418.0125241,566.1593569,443.9153491],"page":"6"},"4":{"caption":"Figure 4. Our setup (c) consisted of an HMD, a tablet, and a pen (a). We \n3D  printed  a  pen  case  and  combined  the  graphical  pen  with  wireless \nbuttons (b). All devices were motion tracked using OptiTrack [42]. \n","bbox":[53.929,547.9855241,298.9936348,573.8883490999999],"page":"7"},"5":{"caption":"Figure  5.  3D  Mid-Air  Sketching.  a)  Pen-based  mid-air  drawing  in  the \nimmersive environment. b) Pen-based mid-air object selection. \n","bbox":[321.094,438.9158866,566.1586348,455.8523491],"page":"7"},"6":{"caption":"Figure  6.  Gizmos.  a)  The  translation  gizmo  activated  for  the  selected \nstroke. b) The rotation gizmo for the same stroke. \n","bbox":[321.094,302.33788660000005,566.1586347999998,319.27434910000005],"page":"7"},"7":{"caption":"Figure 7. Drawing Surfaces. a) New drawing surfaces can be created by \nplacing them in space with the tablet position and orientation. The scale \nand the distance are set by sliding the pen on the tablet. A preview of the sli­\nced objects is provided on the tablet. b) Sketching on the tablet creates sto­\nkes in mid-air that lie on the deﬁned surface. Gridlines can be activated. \n","bbox":[53.929,464.5427991,299.27258830000005,508.37834910000004],"page":"8"},"8":{"caption":"Figure 8. World In Miniature. a) WIM provides a miniaturized view of \nthe whole space in the vicinity of the user and attaches it to the tablet. \nb) It is possible to use all interaction metaphors inside WIM such as the \nrotation gizmo. \n","bbox":[321.094,597.9731616000001,567.5534022999999,632.8423491],"page":"8"},"9":{"caption":"Figure 9. a) Portals can be places in space and provide a virtual camera \nthat is visible on the tablet. b) Primitives can be extruded to create solid \nobjects. \n","bbox":[321.094,317.95152410000003,566.1586348,343.85434910000004],"page":"8"},"10":{"caption":"Figure  10.  Conﬁguration  Menu.  Besides  the  permanently  available \nphysical tablet with two hardware buttons and the ﬂoating buttons, all \nother  controls  are  accessible  via  the  conﬁguration  menu  on  the  tablet. \nIcons © icons8.com. Used under CC BY-ND 3.0. [26] \n","bbox":[53.929,516.5361616000001,300.3884023,551.4053491],"page":"9"},"11":{"caption":"Figure 11. Islands with beach houses created by our participants during the usability walkthrough (P2: (a), P1: (b), P5: (c)). \n","bbox":[103.281,621.2732490999999,516.8096384999998,629.2433490999999],"page":"10"}},"crops":{"1":{"crop_coord":[144.80278027777777,585.2738783333336,1572.1684266666666,1156.8611061111112],"bbox":[53.9290009,377.3300018,564.1806336,579.5014037999999],"page":"1"},"2":{"crop_coord":[144.80278027777777,168.2275989444447,830.0036845555555,432.9583400000001],"bbox":[53.9290009,637.9349976,297.00132644,729.63806438],"page":"5"},"3":{"crop_coord":[144.80278027777777,168.227323888889,1572.117785833333,958.9749994444444],"bbox":[53.9290009,448.5690002,564.1624029,729.6381633999999],"page":"6"},"4":{"crop_coord":[144.80278027777777,168.22502138888885,829.9653288888888,597.9361130555554],"bbox":[53.9290009,578.5429993,296.9875184,729.6389923],"page":"7"},"5":{"crop_coord":[886.9277613888887,659.8778280555555,1572.0984055555557,925.8166758333333],"bbox":[321.0939941,460.5059967,564.155426,552.6439819],"page":"7"},"6":{"crop_coord":[886.9277613888887,1039.2583719444444,1572.0984055555557,1305.1972197222224],"bbox":[321.0939941,323.9290009,564.155426,416.0669861],"page":"7"},"7":{"crop_coord":[144.80278027777777,513.9694722222223,829.9734244444445,779.90832],"bbox":[53.9290009,513.0330048,296.9904328,605.17099],"page":"8"},"8":{"crop_coord":[886.9277613888887,168.23616888888887,1572.0984055555557,434.17501666666686],"bbox":[321.0939941,637.496994,564.155426,729.6349792],"page":"8"},"9":{"crop_coord":[886.9277613888887,970.9805891666667,1572.0984055555557,1236.9194369444444],"bbox":[321.0939941,348.5090027,564.155426,440.6469879],"page":"8"},"10":{"crop_coord":[178.56386833333332,294.01080666666684,796.2165325000001,660.3916508333334],"bbox":[66.0829926,556.0590057,284.8379517,684.3561096],"page":"9"},"11":{"crop_coord":[144.80278027777777,168.23383750000028,1572.0814091666666,438.63889055555575],"bbox":[53.9290009,635.8899994,564.1493073,729.6358184999999],"page":"10"}}}},{"filename":"3313831.3376639","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376639.pdf","paper_id":"3313831.3376639","venue":"chi2020","keywords":["Throwing","Virtual Reality","User Study"],"paragraph_containing_keyword":"Author Keywords \nThrowing; Virtual Reality; User Study","paragraph_after_keyword":"CCS Concepts \n•Human-centered computing → Virtual reality; User stud-\nies; •Software and its engineering → Interactive games;","doi":"10.1145/3313831.3376639","sections":[{"word_count":411,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":563,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":435,"figure_citations":{},"section_index":2,"title":"PHYSICAL BACKGROUND"},{"word_count":380,"figure_citations":{},"section_index":3,"title":"IMPLEMENTATION OF THROWING IN VR"},{"word_count":711,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2 illustrates the accuracy of each hit point for the three participants over the course of the session."]},"section_index":4,"title":"LEARNING CURVE"},{"word_count":729,"figure_citations":{"1":["Figure 1 shows the three stations in the real world and a throw in VR.","Figure 1)."]},"section_index":5,"title":"USER STUDY"},{"word_count":1228,"figure_citations":{"3":["Figure 3)."],"4":["Figure 4, the precision is not only more than 2-3 times higher in the real world than in VR, it is also more stable between participants, namely they have more similar precision levels in real world, than in VR."],"5":["Figure 5) highlight, that while the scattering of hit points is generally higher in all VR throwing styles, the hit points are also much more scattered along the vertical/longitudal axis, than the horizontal."],"6":["Figure 6 shows the “darts-like” throw (left) and the underhand throw (right) for a representative participant."],"7":["Figure 7 we can observe Page 5 CHI 2020 Paper CHI 2020, April 25–30, 2020, Honolulu, HI, USA Figure 7."]},"section_index":6,"title":"RESULTS"},{"word_count":341,"figure_citations":{},"section_index":7,"title":"DISCUSSION"},{"word_count":81,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":1080,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Performance and Experience of Throwing in Virtual Reality","authors":"Tim Zindulka, Myroslav Bachynskyi, Jörg Müller","abstract":"Throwing is a fundamental movement in many sports and games. Given this, accurate throwing in VR applications today is surprisingly difficult. In this paper we explore the nature of the difficulties of throwing in VR in more detail. We present the results of a user study comparing throwing in VR and in the physical world. In a short pre-study with 3 participants we determine an optimal number of throwing repetitions for the main study by exploring the learning curve and subjective fatigue of throwing in VR. In the main study, with 12 participants, we find that throwing precision and accuracy in VR are lower particularly in the distance and height dimensions. It also requires more effort and exhibits different kinematic patterns.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1.  We show that throwing in reality (2-4) is more than twice as accurate compared to VR (1).  We investigate overhand throws at short range \nwith vertical targets (1, 2), underhand throws at short range with horizontal targets (3), and overhand throws at long range (4). \n","bbox":[53.64208,486.02002,566.1529299999999,502.95626999999996],"page":"1"},"2":{"caption":"Figure 2.  Throwing accuracy for three participants shows no improve-\nment after the initial 10-15 throws. \n","bbox":[53.929,529.13302,298.32108,546.06927],"page":"4"},"3":{"caption":"Figure 3. Throwing accuracy in the real world is almost twice as high as \nin VR. \n","bbox":[321.094,585.23802,566.1555599999999,602.17427],"page":"5"},"4":{"caption":"Figure 4.  Throwing precision in the real world is 2-3 times higher, and \nmore stable between the participants compared to VR. \n","bbox":[321.094,435.57702,566.1555599999998,452.51327],"page":"5"},"5":{"caption":"Figure  5.  Scatter  plots  of  hit  points  with  kernel  density  estimator.  A \nsmall  number  of  virtual  hit  points  are  located  outside  the  viewport \nshown  here.  The  ﬁrst  row  shows  the  real  setting  and  the  second  row \nVR. Stations 1, 2, and 3 are shown from left to right.  The black circle \nrepresents the outline of the target (All units are meters). \n","bbox":[53.64208,520.6462700000002,299.2774799999999,564.48127],"page":"6"},"6":{"caption":"Figure 6.  Comparison of kinematic patterns for Station 1 (left) and 2 \n(right).  The real movement data recorded with OptiTrack is displayed \nin red.  Data points recorded in VR with Unity are shown in blue.  The \nthrowing  direction  followed  the  green  arrow  along  the  red  axis  to  the \nright. \n","bbox":[53.66599,372.30926999999997,298.99853,416.14426999999995],"page":"6"},"7":{"caption":"Figure 7.  Boxplot of the average perceived workload according to the \nRaw NASA TLX scores. \n","bbox":[321.0914,581.14202,566.1529599999999,598.07827],"page":"6"}},"crops":{"1":{"crop_coord":[144.80278027777777,529.935167222222,494.93131861111107,794.9694655555556],"bbox":[53.9290009,507.6109924,176.3752747,599.4233398],"page":"1"},"2":{"crop_coord":[503.85556527777777,529.935167222222,853.9841036111111,794.9694655555556],"bbox":[183.1880035,507.6109924,305.6342773,599.4233398],"page":"1"},"3":{"crop_coord":[862.9083505555554,529.935167222222,1213.0368888888888,794.9694655555556],"bbox":[312.4470062,507.6109924,434.89328,599.4233398],"page":"1"},"4":{"crop_coord":[1221.9610936111112,529.935167222222,1572.0896319444444,794.9694655555556],"bbox":[441.7059937,507.6109924,564.1522675,599.4233398],"page":"1"},"5":{"crop_coord":[161.68332416666664,168.22277499999984,813.1215327777777,675.2111138888887],"bbox":[60.0059967,550.723999,290.9237518,729.639801],"page":"4"},"6":{"crop_coord":[890.8138783333334,169.6354844444445,1135.1093630555556,519.3639033333333],"bbox":[322.4929962,606.8289948,406.8393707,729.1312256],"page":"5"},"7":{"crop_coord":[1132.0249855555553,169.33052055555564,1351.3328044444445,519.3639033333333],"bbox":[409.3289948,606.8289948,484.6798096,729.2410126],"page":"5"},"8":{"crop_coord":[1348.2527922222223,168.22548749999984,1568.240771666667,519.3639033333333],"bbox":[487.1710052,606.8289948,562.7666778,729.6388245],"page":"5"},"9":{"crop_coord":[887.1027713888888,594.929682222222,1131.3899908333335,935.0888908333333],"bbox":[321.1569977,457.1679993,405.5003967,576.0253144000001],"page":"5"},"10":{"crop_coord":[1128.3166758333332,594.8562283333333,1351.6793483333333,935.0888908333333],"bbox":[407.9940033,457.1679993,484.8045654,576.0517578],"page":"5"},"11":{"crop_coord":[1348.5916647222223,599.6508788888888,1571.9543372222222,935.0888908333333],"bbox":[487.2929993,457.1679993,564.1035614,574.3256836],"page":"5"},"12":{"crop_coord":[151.3805475,168.22866666666684,377.45928444444445,399.7639041666668],"bbox":[56.2969971,649.8849945,134.0853424,729.6376799999999],"page":"6"},"13":{"crop_coord":[374.36668388888893,168.22866666666684,600.4454208333333,399.7639041666668],"bbox":[136.5720062,649.8849945,214.3603515,729.6376799999999],"page":"6"},"14":{"crop_coord":[597.3527780555555,168.22866666666684,823.4315150000001,399.7639041666668],"bbox":[216.8470001,649.8849945,294.6353454,729.6376799999999],"page":"6"},"15":{"crop_coord":[151.3805475,392.5314247222223,377.45928444444445,624.0666622222224],"bbox":[56.2969971,569.1360016,134.0853424,648.8886871],"page":"6"},"16":{"crop_coord":[374.36668388888893,392.5314247222223,600.4454208333333,624.0666622222224],"bbox":[136.5720062,569.1360016,214.3603515,648.8886871],"page":"6"},"17":{"crop_coord":[597.3527780555555,392.5314247222223,823.4315150000001,624.0666622222224],"bbox":[216.8470001,569.1360016,294.6353454,648.8886871],"page":"6"},"18":{"crop_coord":[148.09168499999998,777.9872555555554,488.9271969444444,1036.1139],"bbox":[55.1130066,420.798996,174.2137909,510.124588],"page":"6"},"19":{"crop_coord":[485.86112972222224,777.9872555555554,826.6966416666667,1036.1139],"bbox":[176.7100067,420.798996,295.810791,510.124588],"page":"6"},"20":{"crop_coord":[886.9277613888887,168.22510611111127,1572.1407486111111,530.7389069444445],"bbox":[321.0939941,602.7339935,564.1706695,729.6389618],"page":"6"}}}},{"filename":"3313831.3376642","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376642.pdf","paper_id":"3313831.3376642","venue":"chi2020","keywords":["Shared experience","Virtual reality","Social","Replay","Shared experience","Presence","Immersion"],"paragraph_containing_keyword":"Author Keywords \n \nshared experience, virtual reality, social, replay, shared \n \n \n \n \n \nexperience, presence, immersion","paragraph_after_keyword":"","doi":"10.1145/3313831.3376642","sections":[{"word_count":615,"figure_citations":{"1":["Figure 1, ReliveInVR allows users to relive the virtual experience together."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":453,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":900,"figure_citations":{"2":["Figure 2)."],"3":["Figure 3, All events were different on each island."]},"section_index":2,"title":"PROTOTYPES AND VR ARCHERY GAME"},{"word_count":1420,"figure_citations":{"5":["Figure 5 (similar to co-watching 360-degree video in Facebook Spaces)."],"6":["Figure 6, we implemented a VR networked environment and a state-based replay system with SteamVR plugins and Photon Networking framework in Unity."]},"section_index":3,"title":"USER STUDY"},{"word_count":2132,"figure_citations":{"7":["Figure 7 (a), the ANOVA for the linear mixed model of Shared Cognition yielded a signiﬁcant effect of the sharing condition, F(2, 102) = 13.","Figure 7(c) shows a signiﬁcant effect of the sharing condition, F(2, 102) = 47.","Figure 7 (e) shows that the the Paper 513 ANOVA for the linear mixed model of Conversation Engagement yielded a signiﬁcant effect of the sharing condition, F(2, 102) = 19.","Figure 7 (f).","Figure 7 (g) and (h), when asked about most prefered sharing tool for sharing VR experience remotely together, 96% of the participants chose ReliveInVR, 2% of the participants chose Co-watchVR and 2% chose Co-watchDT."],"8":["Figure 8, the means and standard deviations for the sharing time ratios for each sharing condition are: Co-watchDT=1."],"9":["Figure 9, a post-hoc test using Wilcoxon signed-rank with Bonferroni correction showed signiﬁcant differences between ReliveInVR and Co-watchDT Spaces (p < 0."]},"section_index":4,"title":"RESULTS"},{"word_count":1048,"figure_citations":{"10":["Figure 10 (a) demonstrates the percentage of where participants viewed their replayed avatars during Review session in 4 different areas around the replayed avatar."]},"section_index":5,"title":"DISCUSSION"},{"word_count":586,"figure_citations":{"10":["Figure 10(b), participants only had average 18."]},"section_index":6,"title":"DESIGN IMPLICATIONS"},{"word_count":206,"figure_citations":{},"section_index":7,"title":"LIMITATIONS AND FUTURE WORK"},{"word_count":136,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":1292,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Again, Together: Socially Reliving Virtual Reality Experiences When Separated","authors":"Cheng Yao Wang, Mose Sakashita, Upol Ehsan, Jingjin Li, Andrea Stevenson Won","abstract":"To share a virtual reality (VR) experience remotely together, users usually record videos from an individual's point of view and then co-watch these videos. However, co-watching recorded videos limits users to reliving their memories from the perspective from which the video was captured. In this paper, we describe ReliveInVR, a new time-machine-like VR experience sharing method. ReliveInVR allows multiple users to immerse themselves in the relived experience together and independently view the experience from any perspective. We conducted a 1x3 within-subject study with 26 dyads to compare ReliveInVR with (1) co-watching 360-degree videos on desktop, and (2) co-watching 360-degree videos in VR. Our results suggest that participants reported higher levels of immersion and social presence in ReliveInVR. Participants in ReliveInVR also understood the shared experience better, discovered unnoticed things together and found the sharing experience more fulfilling. We discuss the design implications for sharing VR experiences over time and space.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Three prototypes for sharing virtual reality experiences over distance: (1) Co-watching 360-degrees videos on desktop; (2) Co-watching \n360-degrees videos in VR; (3) ReliveInVR: fully recreating the experience to relive it socially. \n","bbox":[63.283,455.48552,556.8013399999996,474.41427],"page":"1"},"2":{"caption":"Figure  2.  The  basic  interactions  in  the  archery  game;  (1)  picking  up \na  bow,  (2)  grabbing  an  arrow,  releasing  an  arrow,  and  (3)  teleporting. \nReliveInVR provides controls such as play, pause, seek to certain time \nstamp (4). \n","bbox":[53.929,593.0955200000001,300.38530999999995,627.9642699999999],"page":"3"},"3":{"caption":"Figure 3. All the surprising events in the both islands. \n","bbox":[84.546,467.38627,268.37405,475.35627],"page":"3"},"4":{"caption":"Figure 4. Co-watching 360-degree videos on desktop (Co-watchDT) pro-\ntotype:  (a)  Sharer  can  have  the  video  control.  (b)  Sharee  can  view  a \nsynchronized  video  with  the  sharer  over  distance.  Sharer  and  sharee \ncan have different perspectives of the 360-degree video \n","bbox":[321.094,612.1735200000002,566.1555599999999,647.04227],"page":"3"},"5":{"caption":"Figure 5. Co-watching 360-degree videos in VR prototype (CowatchVR). \nUsers  can  watch  the  360-degree  video  together  and  see  each  other’s \navatar at the same time. \n","bbox":[53.929,602.66177,300.38530999999995,628.56427],"page":"4"},"6":{"caption":"Figure  6.  Reliving  experience  in  VR  prototype  (ReliveInVR).(a)  P2’s \npoint of view when P1 and P2 relive P1’s experience through ReliveInVR. \n(b)  P1  and  P2  can  see  each  other  and  P1’s  replayed  avatar.  (c)  VR \nnetworked environment and record-replay technique in the ReliveInVR \nprototype. \n","bbox":[53.66599,441.94227,300.38530999999995,485.77727],"page":"4"},"7":{"caption":"Figure 7.  (a)-(e) boxplots for each factor across sharing conditions.  (f) Self-reported emotion ratings for each condition.  (g) Most preferred sharing \ntools. (h) Least preferred sharing tools. \n","bbox":[53.929,529.83602,566.15293,546.7722699999999],"page":"6"},"8":{"caption":"Figure 8.  The result of normalized sharing time consists of the Review \nsession and the Exploration session. \n","bbox":[53.929,582.21202,299.2774799999999,599.14827],"page":"8"},"9":{"caption":"Figure 9. Head orientation result across all sharing conditions. \n","bbox":[336.557,603.26227,550.6949599999999,611.23227],"page":"8"},"10":{"caption":"Figure  10.  (a)  The  distribution  of  relative  positions  between  partici-\npant’s avatar and replayed avatar when the participant viewed the re-\nplayed avatar in the ReliveInVR condition.  (b) The percentage of time \nwhen two participants were within the area of social space during the \nExploration session in the ReliveInVR condition. \n","bbox":[320.80708,590.1122700000002,566.1555599999998,633.94727],"page":"9"}},"crops":{"1":{"crop_coord":[283.07500194444447,545.0550166666668,1426.9616783333333,874.2527772222221],"bbox":[103.7070007,479.0690002,511.9062042,593.980194],"page":"1"},"2":{"crop_coord":[246.08333166666662,168.22095222222228,728.6999936111112,447.7249908333333],"bbox":[90.3899994,632.6190033,260.53199770000003,729.6404572],"page":"3"},"3":{"crop_coord":[212.32220111111113,584.0226574999999,762.4558680555556,866.102761111111],"bbox":[78.2359924,482.003006,272.6841125,579.9518433000001],"page":"3"},"4":{"crop_coord":[937.5666808333334,168.23002305555553,1521.4744483333334,394.73053833333313],"bbox":[339.3240051,651.6970062,545.9308014000001,729.6371917],"page":"3"},"5":{"crop_coord":[212.32220111111113,168.22900583333322,762.4821894444445,446.0583497222222],"bbox":[78.2359924,633.2189941,272.6935882,729.6375579],"page":"4"},"6":{"crop_coord":[178.56386833333332,561.8089972222222,796.2329358333334,842.68887],"bbox":[66.0829926,490.4320068,284.8438569,587.948761],"page":"4"},"7":{"crop_coord":[215.67222583333333,168.2216727777777,1501.288468888889,673.2583277777778],"bbox":[79.4420013,551.427002,538.6638488,729.6401978],"page":"6"},"8":{"crop_coord":[246.08333166666662,168.23197250000007,728.7058427777778,527.7694616666666],"bbox":[90.3899994,603.8029938,260.5341034,729.6364899],"page":"8"},"9":{"crop_coord":[954.447225,168.22281722222223,1504.613528888889,488.6694505555556],"bbox":[345.401001,617.8789978,539.8608704000001,729.6397858],"page":"8"},"10":{"crop_coord":[954.447225,168.2275644444447,1504.599668888889,431.1055416666667],"bbox":[345.401001,638.602005,539.8558808,729.6380767999999],"page":"9"}}}},{"filename":"3313831.3376652","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376652.pdf","paper_id":"3313831.3376652","venue":"chi2020","keywords":["Interactive Spaces","Spatial Awareness","Interaction Design","Virtual Reality","Design tools"],"paragraph_containing_keyword":"ABSTRACT \nWe  propose  using  virtual  reality  (VR)  as  a  design  tool  for \nsketching and simulating spatially-aware interactive spaces. \nUsing VR, designers can quickly experience their envisioned \nspaces and interactions by simulating technologies such as \nmotion tracking, multiple networked devices, or unusual form \nfactors such as spherical touchscreens or bezel-less display \ntiles. Design ideas can be rapidly iterated without restrictions \nby the number, size, or shape and availability of devices or \nsensors in the lab. To understand the potentials and challenges \nof designing in VR, we conducted a user study with 12 in-\nteraction designers.  As their tool, they used a custom-built \nvirtual design environment with ﬁnger tracking and physics \nsimulations for natural interactions with virtual devices and ob-\njects. Our study identiﬁed the designers’ experience of space \nin relation to their own bodies and playful design explorations \nas key opportunities. Key challenges were the complexities of \nbuilding a usable yet versatile VR-based \"World Editor\". \nAuthor Keywords \nInteractive Spaces; Spatial Awareness; Interaction Design; \nVirtual Reality; Design tools. \nCCS Concepts \n•Human-centered computing → Systems and tools for in-\nteraction design; Ubiquitous and mobile computing design \nand evaluation methods;","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed \nfor proﬁt or commercial advantage and that copies bear this notice and the full citation \non the ﬁrst page. Copyrights for components of this work owned by others than ACM \nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish, \nto post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a \nfee. Request permissions from permissions@acm.org. \nCHI ’20, April 25–30, 2020, Honolulu, HI, USA. \n© 2020 Association of Computing Machinery. \nACM ISBN 978-1-4503-6708-0/20/04 ...$15.00. \nhttp://dx.doi.org/10.1145/3313831.3376652","doi":"10.1145/3313831.3376652","sections":[{"word_count":1106,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1499,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1761,"figure_citations":{"2":["Figure 2)."]},"section_index":2,"title":"DESIGN AND IMPLEMENTATION"},{"word_count":1446,"figure_citations":{"4":["Figure 4)."],"5":["Figure 5, A and B).","Figure 5, A), we allow users to conﬁgure the VE with displays of different shapes, as well as real-world and abstract objects.","Figure 5 B), only objects with collision status turned on remain graspable and can be rescaled or moved around in virtual space."]},"section_index":3,"title":"USER STUDY"},{"word_count":2687,"figure_citations":{"6":["Figure 6)."],"7":["Figure 7).","Figure 7)."]},"section_index":4,"title":"OBSERVED OPPORTUNITIES AND CHALLENGES"},{"word_count":461,"figure_citations":{},"section_index":5,"title":"LIMITATIONS AND FUTURE WORK"},{"word_count":116,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":40,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGEMENTS"},{"word_count":3693,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"\"In VR, everything is possible!\": Sketching and Simulating Spatially-Aware Interactive Spaces in Virtual Reality","authors":"Hans-Christian Jetter, Roman Rädle, Tiare Feuchtner, Christoph Anthes, Judith Friedl, Clemens Nylandsted Klokmose","abstract":"We propose using virtual reality (VR) as a design tool for sketching and simulating spatially-aware interactive spaces. Using VR, designers can quickly experience their envisioned spaces and interactions by simulating technologies such as motion tracking, multiple networked devices, or unusual form factors such as spherical touchscreens or bezel-less display tiles. Design ideas can be rapidly iterated without restrictions by the number, size, or shape and availability of devices or sensors in the lab. To understand the potentials and challenges of designing in VR, we conducted a user study with 12 interaction designers. As their tool, they used a custom-built virtual design environment with finger tracking and physics simulations for natural interactions with virtual devices and objects. Our study identified the designers' experience of space in relation to their own bodies and playful design explorations as key opportunities. Key challenges were the complexities of building a usable yet versatile VR-based \"World Editor\".","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1.  We created a virtual environment for designers, in which they can generate and arrange an arbitrary number of devices that execute real-\nworld  web  applications  (A).  This  allows  simulation  of  existing  interactive  spaces  and  multi-device  systems  (B,  C)  [71],  as  well  as  sketching  of  new \ninteractions with diverse tracking systems or futuristic devices, e.g., a cylindrical touch screen (D). \n","bbox":[53.64208,479.16877,566.43985,505.07126999999997],"page":"1"},"2":{"caption":"Figure 2. Our experimental design tool uses ﬁnger and hand tracking to \nlet users naturally interact with devices and objects in the VE. \n","bbox":[321.094,596.11202,566.1555599999999,613.04827],"page":"4"},"3":{"caption":"Figure 3.  Simulations of existing systems in VR (right) and the original \nsystem (left).  1) Window Manager by Klokmose et al.  [49] for 75 LCD \ntiles.  2)  VisTiles  by  Langner  et  al.  [51].  3)  Peephole  navigation  with \nHuddleLamp by Rädle et al. [73]. 4) Edge Bubbles by Rädle et al. [74]. \n","bbox":[53.61020000000002,431.55676,298.99056,466.48927],"page":"5"},"4":{"caption":"Figure 4.  VR-based design tools can enable designers to experience fu-\nturistic  touchscreen  devices,  e.g.,  Google  Maps  on  a  cylindrical  touch-\nscreen or drawing on a spherical display. \n","bbox":[53.929,574.1417700000001,298.32108,600.04427],"page":"6"},"5":{"caption":"Figure 5.  In design mode (A), users can manipulate device properties and resize them.  In use mode (B), users can draw on devices and interact with \nreal-world applications using (multi-)touch. \n","bbox":[53.929,634.06502,566.15293,651.00127],"page":"7"},"6":{"caption":"Figure 6. P6 tiptoes (left) and P9 jumps (right) to reach overhead targets. \n","bbox":[321.094,630.49627,567.5503099999997,638.46627],"page":"8"},"7":{"caption":"Figure  7.  P1  throwing  a  ball  at  the  targets  on  the  display  (left).  P6 \npicking up a tablet she dropped (right). \n","bbox":[53.929,201.12502,298.9905599999999,218.06127],"page":"9"}},"crops":{"1":{"crop_coord":[144.80278027777777,529.6584319444446,1572.052798888889,789.0944502777778],"bbox":[53.9290009,509.7259979,564.1390076,599.5229645],"page":"1"},"2":{"crop_coord":[886.9277613888887,168.21849416666672,1572.1499886111108,489.1583252777777],"bbox":[321.0939941,617.7030029,564.1739958999999,729.6413421],"page":"4"},"3":{"crop_coord":[144.80278027777777,168.23036194444455,829.9975416666667,896.2694294444445],"bbox":[53.9290009,471.1430054,296.999115,729.6370697],"page":"5"},"4":{"crop_coord":[144.80278027777777,168.21633250000014,830.0250075,525.2805413888889],"bbox":[53.9290009,604.6990051,297.0090027,729.6421203],"page":"6"},"5":{"crop_coord":[144.80278027777777,168.2320572222225,1572.052798888889,383.73331694444454],"bbox":[53.9290009,655.6560059,564.1390076,729.6364593999999],"page":"7"},"6":{"crop_coord":[886.9277613888887,168.22311388888886,1572.0110913888889,418.5527716666667],"bbox":[321.0939941,643.1210022,564.1239929,729.639679],"page":"8"},"7":{"crop_coord":[144.80278027777777,1336.0147772222222,829.8861102777778,1586.344435],"bbox":[53.9290009,222.7160034,296.9589997,309.2346802],"page":"9"}}}},{"filename":"3313831.3376687","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376687.pdf","paper_id":"3313831.3376687","venue":"chi2020","keywords":["Interaction techniques","Ergonomics","Input re-mapping"],"paragraph_containing_keyword":"Author Keywords \ninteraction techniques; ergonomics; input re-mapping","paragraph_after_keyword":"INTRODUCTION \nThe large movements typically required in virtual reality (VR) \noften become fatiguing, cumbersome, or even impractical in \nconstrained environments. For example, large arm swinging \nor reaching movements in VR when seated at a desk could \nresult in damage to equipment or personal injury. Furthermore, \nextensive movement in VR may be uncomfortable or even \nimpossible for users with mobility issues. \nA typical remedy to this problem is input amplifcation: trans-\nforming smaller, more comfortable movements into the larger, \nmore dramatic movements that the user expects.  However, \nmany of these techniques come at a cost. Typical amplifcation \ntechniques allow the user to manipulate objects at distances \nmuch further than a typical arm’s reach [3,15,25]. This unreal-\nistic increase in reach comes at the expense of body ownership \n— the psychological mapping of one’s real body to a virtual \nbody [29] — detracting from the user’s feeling of presence","doi":"10.1145/3313831.3376687","sections":[{"word_count":688,"figure_citations":{"1":["Figure 1)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":2670,"figure_citations":{"2":["Figure 2a).","Figure 2b."],"3":["Figure 3 illustrates the curve confgurations used for Experiment 1.","Figure 3)."],"4":["Figure 4)."],"5":["Figure 5a), and within each layout, the three amplifcation levels produced similar times."]},"section_index":1,"title":"BACKGROUND AND RELATED WORK"},{"word_count":2,"figure_citations":{},"section_index":2,"title":"NONE"},{"word_count":2,"figure_citations":{},"section_index":3,"title":"LOW"},{"word_count":948,"figure_citations":{"5":["Figure 5b), with HIGH amplifcation reducing RULA more than LOW.","Figure 5c).","Figure 5d)."]},"section_index":4,"title":"HIGH"},{"word_count":1000,"figure_citations":{"6":["Figure 6)."],"7":["Figure 7)."]},"section_index":5,"title":"AMPLIFICATION"},{"word_count":1777,"figure_citations":{"8":["Figure 8a).","Figure 8b).","Figure 8c).","Figure 8d).","Figure 8e)."],"9":["Figure 9)."],"10":["Figure 10 shows responses for questions 1 to 4."]},"section_index":6,"title":"TANCES"},{"word_count":1066,"figure_citations":{},"section_index":7,"title":"GENERAL DISCUSSION"},{"word_count":168,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":36,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGEMENTS"},{"word_count":1198,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"Improving Virtual Reality Ergonomics Through Reach-Bounded Non-Linear Input Amplification","authors":"Johann Wentzel, Greg d'Eon, Daniel Vogel","abstract":"Input amplification enables easier movement in virtual reality (VR) for users with mobility issues or in confined spaces. However, current techniques either do not focus on maintaining feelings of body ownership, or are not applicable to general VR tasks. We investigate a general purpose non-linear transfer function that keeps the user's reach within reasonable bounds to maintain body ownership. The technique amplifies smaller movements from a user-definable neutral point into the expected larger movements using a configurable Hermite curve. Two experiments evaluate the approach. The first establishes that the technique has comparable performance to the state-of-the-art, increasing physical comfort while maintaining task performance and body ownership. The second explores the characteristics of the technique over a wide range of amplification levels. Using the combined results, design and implementation recommendations are provided with potential applications to related VR transfer functions.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. The physical controller position (green), relative to a calibrated \nneutral position, is amplifed using a non-linear function so the virtual \ncontroller (blue) appears farther away.  The transfer function keeps the \nphysical-to-virtual hand offset small near the body,  but maximum vir-\ntual reach can be achieved with the real controller moving 30% less. \n","bbox":[321.09399999999994,448.3847991,566.1586348,492.2203491],"page":"1"},"2":{"caption":"Figure 2. Key geometric points captured in calibration and used for the \namplifcation technique: (a) the user’s maximum reach Pmax, at distance \nrmax  from their shoulder point PS;  (b) P∗ \nH  is calculated from the user’s \nhand position PH . \n","bbox":[53.92697069999997,574.9003232,298.9936348,610.4523491],"page":"3"},"3":{"caption":"Figure 3. The Low and High amplifcation functions used in Experiment \n1.  These functions modify the relationship between the physical offset r \nand the virtual offset  f (r) from PN . \n","bbox":[320.51218270000004,530.5513232000001,566.1666048999999,557.1373490999999],"page":"3"},"4":{"caption":"Figure 4.  The  ERGONOMIC,  LIMITS, and  FIXED  target layouts used in \nExperiment 1. \n","bbox":[321.09350789999996,613.2008866,566.1581427,630.1373490999999],"page":"4"},"5":{"caption":"Figure 5. (a) Time, (b) Comfort, (c) Physical Path Length, and (d) Virtual Path Length by AMPLIFICATION and LAYOUT. Error bars are 95% CI. \n","bbox":[65.227,610.2364883,552.8667611,618.2703491],"page":"5"},"6":{"caption":"Figure 6. Levels of AMPLIFICATION used in Experiment 2. These curves \nmodify the relationship between the physical offset r and the virtual off-\nset  f (r) from PN . \n","bbox":[53.92052309999997,81.2473232,298.99312800000007,107.83334909999999],"page":"6"},"7":{"caption":"Figure  7.  The  target  layout  used  in  Experiment  2,  coloured  by  DIS-\nTANCE,  from the side (a) and the front (b).  Purple targets are  CLOSE, \nblue are MID, green are FAR. Only two targets are visible at a time (c). \n","bbox":[321.062,626.0728866000001,567.15405,651.9763491],"page":"6"},"8":{"caption":"Figure 8. (a) Time, (b) Comfort, (c) Error, (d) Physical Path Length, and (e) Virtual Path Length by AMPLIFICATION. Error bars are 95% CI. \n","bbox":[71.321,611.2974883,546.7737611,619.3313491],"page":"8"},"9":{"caption":"Figure 9. The proportion of participants who noticed that amplifcation \nwas taking place at each level. \n","bbox":[53.6420764,480.00688660000003,298.9936348,496.94334910000003],"page":"8"},"10":{"caption":"Figure 10. Proportion of questionnaire answers by AMPLIFICATION. Answers were inverted for Q2 for visual comparison. \n","bbox":[101.977,612.3082491,518.11033,620.2783491],"page":"9"}},"crops":{"1":{"crop_coord":[800.3880688888889,564.9661111111113,1454.3436244444445,875.5522222222222],"bbox":[289.9397048,478.6012,521.7637048,586.8122],"page":"1"},"2":{"crop_coord":[163.50556277777778,168.22065583333352,811.3199786111111,496.3694594444444],"bbox":[60.6620026,615.1069946,290.2751923,729.6405639],"page":"3"},"3":{"crop_coord":[972.7981794444445,152.42106722222223,1490.5498461111113,644.4649561111111],"bbox":[352.0073446,561.7926158,534.7979446,735.3284158],"page":"3"},"4":{"crop_coord":[905.630543888889,168.22870916666687,1553.3888836111112,441.6888683333333],"bbox":[327.8269958,634.7920074,557.4199981,729.6376647],"page":"4"},"5":{"crop_coord":[217.05426222222223,153.3422147222222,1505.9987066666667,469.1177702777779],"bbox":[79.9395344,624.9176027,540.3595344,734.9968027],"page":"5"},"6":{"crop_coord":[182.66945722222223,1396.64067,792.1369572222223,1892.5351144444444],"bbox":[67.5610046,112.4873588,283.3693046,287.4093588],"page":"6"},"7":{"crop_coord":[905.630543888889,168.22408888888899,1553.3937580555555,381.02500916666673],"bbox":[327.8269958,656.6309967,557.4217529,729.639328],"page":"6"},"8":{"crop_coord":[157.01112527777778,168.22778666666656,1559.9194586111112,466.1722311111111],"bbox":[58.3240051,625.9779968,559.7710051,729.6379968],"page":"8"},"9":{"crop_coord":[220.3416697222222,546.4768133333336,754.4611141666667,811.6722022222223],"bbox":[81.1230011,501.5980072,269.8060011,593.4683471999999],"page":"8"},"10":{"crop_coord":[217.08858916666665,152.1403872222222,1503.5449780555557,463.5423316666666],"bbox":[79.9518921,626.9247606,539.4761921,735.4294606],"page":"9"}}}},{"filename":"3313831.3376698","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376698.pdf","paper_id":"3313831.3376698","venue":"chi2020","keywords":["Virtual Reality","Pen input","FInger and wrist dexterity","Grip postures","Handheld controller","Spatial target selection"],"paragraph_containing_keyword":"Author Keywords \nVirtual Reality; pen input; ﬁnger and wrist dexterity; grip \npostures; handheld controller; spatial target selection.","paragraph_after_keyword":"CCS Concepts \n•Human-centered computing → Virtual reality; User stud-\nies;","doi":"10.1145/3313831.3376698","sections":[{"word_count":876,"figure_citations":{"1":["Figure 1)."],"7":["Figure 7b)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1048,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":4817,"figure_citations":{"2":["Figure 2 illustrates the ﬁve grip postures that were investigated in this study, their descriptions follow: Tripod at Front End (TFE): This is the most common grip posture for precise writing and drawing on a surface (Figure 2a).","Figure 2c).","Figure 2d).","Figure 2e)."],"3":["Figure 3a.","Figure 3b)."],"5":["Figure 5)."],"6":["Figure 6b and Figure 7b.","Figure 6b and 7b) that ran the experimental (Unity) application."],"9":["Figure 9 revealed a strong correlation between the trial time and ID for Tilt gestures, with both R2 values above 0."],"10":["Figure 10)."]},"section_index":2,"title":"AND FINGER MOTION"},{"word_count":272,"figure_citations":{"11":["Figure 11)."]},"section_index":3,"title":"INTERACTION TECHNIQUES AND APPLICATIONS"},{"word_count":888,"figure_citations":{"12":["Figure 12), we show that the gestures can be utilized as interface widgets, which trigger scrolling or zooming operations on a web view with tilting and poking the pen respectively."],"13":["Figure 13a, the user can select an object which is hidden by another object he can’t see with the Palm Grip, leading to misinterpretation.","Figure 13b, the larger object is stacked below the target object."]},"section_index":4,"title":"DISCUSSION AND FUTURE WORK"},{"word_count":134,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":74,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":2413,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen","authors":"Nianlong Li, Teng Han, Feng Tian, Jin Huang, Minghui Sun, Pourang Irani, Jason Alexander","abstract":"The use of Virtual Reality (VR) in applications such as data analysis, artistic creation, and clinical settings requires high precision input. However, the current design of handheld controllers, where wrist rotation is the primary input approach, does not exploit the human fingers' capability for dexterous movements for high precision pointing and selection. To address this issue, we investigated the characteristics and potential of using a pen as a VR input device. We conducted two studies. The first examined which pen grip allowed the largest range of motion---we found a tripod grip at the rear end of the shaft met this criterion. The second study investigated target selection via 'poking' and ray-casting, where we found the pen grip outperformed the traditional wrist-based input in both cases. Finally, we demonstrate potential applications enabled by VR pen input and grip postures.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. A user grips a pen controller to perform (a) poke gestures, and \n(b) tilt gestures. \n","bbox":[320.83099,404.24602,566.1555599999999,421.18226999999996],"page":"1"},"2":{"caption":"Figure 2.  Candidate grip postures:  (a) tripod at front end, (b) tripod at \nrear end, (c) quadropod at rear end, (d) pinch, and (e) overhand. \n","bbox":[321.094,571.67602,566.1555599999999,588.61227],"page":"3"},"3":{"caption":"Figure 3. (a) The 3D printed pen was used in the studies; (b) A user took \nparticipant in Study 1. \n","bbox":[53.929,600.77802,299.2137199999999,617.7142699999999],"page":"4"},"4":{"caption":"Figure 4. The results of Study 1: (a) Pen tip travelled distance (mm); (b) \nPen shaft tilted angle (◦). \n","bbox":[321.094,563.90527,566.6895499999997,580.84227],"page":"4"},"5":{"caption":"Figure  5.  Participant  responses  to  the  workload  of  different  grip  pos-\ntures.  Graphs are centered around the neutral response, with the pro-\nportion of positive and negative responses on the right and left side, re-\nspectively. \n","bbox":[321.094,498.14652000000007,565.48608,533.01527],"page":"5"},"6":{"caption":"Figure 6. (a) The user interface for selection with the poke; (b) A partic-\nipant uses the Pen Grip and selects with the poke in Study 2. \n","bbox":[53.929,610.19526,298.32108,627.1952699999999],"page":"6"},"7":{"caption":"Figure 7.  (a) The user interface for selection with tilt; (b) A participant \nuses the Palm Grip and selects with tilt in Study 2. \n","bbox":[321.094,610.19526,566.1555599999998,627.1952699999999],"page":"6"},"8":{"caption":"Figure 8. Mean selection time for poke and tilt. \n","bbox":[362.909,568.7632699999999,524.3413499999999,576.73327],"page":"7"},"9":{"caption":"Figure 9.  Fitts’ law models for the poke (solid line) and tilt (dash line) \ntasks. \n","bbox":[53.929,581.57802,299.52454999999986,598.51427],"page":"8"},"10":{"caption":"Figure  10.  Participant  responses  to  the  workload  of  poke  and  tilt. \nGraphs are centered around the neutral response, with the proportion \nof positive and negative responses on the right and left side, respectively. \n","bbox":[321.094,515.25377,567.5503099999999,541.15627],"page":"8"},"11":{"caption":"Figure 11.  (a) A participant ﬁrst selected a furniture using Palm Grip, \n(b) then changed its color using Pen Grip. \n","bbox":[53.673959999999994,588.46426,299.99478,605.4642699999999],"page":"9"},"12":{"caption":"Figure 12.  (a) A participant used tilt gesture to scroll the web page, (b) \nand double poked to zoom in or zoom out. \n","bbox":[53.929,425.30002,299.52455,442.23627],"page":"9"},"13":{"caption":"Figure 13.  (a) A participant can select an invisible object; (b) A partici-\npant can see but cannot select the target. \n","bbox":[321.094,588.28602,565.4860799999999,605.22227],"page":"9"}},"crops":{"1":{"crop_coord":[886.9277613888887,736.9575586111112,1571.8921152777777,1022.1194288888888],"bbox":[321.0939941,425.8370056,564.0811615,524.8952789],"page":"1"},"2":{"crop_coord":[886.9277613888887,168.25422500000008,1571.998206111111,557.0361158333334],"bbox":[321.0939941,593.2669983,564.1193542,729.628479],"page":"3"},"3":{"crop_coord":[144.80278027777777,168.20192111111092,829.9994066666667,476.1972130555554],"bbox":[53.9290009,622.3690033,296.9997864,729.6473084],"page":"4"},"4":{"crop_coord":[886.9277613888887,168.22158805555551,1572.1215055555554,578.622216111111],"bbox":[321.0939941,585.4960022,564.163742,729.6402283],"page":"4"},"5":{"crop_coord":[886.9277613888887,168.22510611111105,1572.103746111111,711.4722272222223],"bbox":[321.0939941,537.6699982,564.1573486,729.6389618000001],"page":"5"},"6":{"crop_coord":[144.80278027777777,168.2299805555555,830.0001272222222,449.8610941666669],"bbox":[53.9290009,631.8500061,297.0000458,729.637207],"page":"6"},"7":{"crop_coord":[886.9277613888887,168.2299805555555,1572.1251083333332,449.8610941666669],"bbox":[321.0939941,631.8500061,564.165039,729.637207],"page":"6"},"8":{"crop_coord":[886.9277613888887,168.23502444444458,1572.0714483333331,584.4972313888888],"bbox":[321.0939941,583.3809967,564.1457214,729.6353912],"page":"7"},"9":{"crop_coord":[144.80278027777777,168.22569972222237,830.0024586111111,529.5305380555557],"bbox":[53.9290009,603.1690063,297.0008851,729.6387480999999],"page":"8"},"10":{"crop_coord":[886.9277613888887,168.22290222222205,1572.0839944444442,688.8583205555553],"bbox":[321.0939941,545.8110046,564.150238,729.6397552000001],"page":"8"},"11":{"crop_coord":[144.80278027777777,168.21688333333333,829.9308269444444,510.22499083333315],"bbox":[53.9290009,610.1190033,296.9750977,729.641922],"page":"9"},"12":{"crop_coord":[886.9277613888887,168.2156541666666,1572.0558080555552,510.89722527777786],"bbox":[321.0939941,609.8769989,564.1400908999999,729.6423645],"page":"9"},"13":{"crop_coord":[144.80278027777777,617.9240416666665,829.9308269444444,963.6360930555554],"bbox":[53.9290009,446.8910065,296.9750977,567.747345],"page":"9"}}}},{"filename":"3313831.3376714","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376714.pdf","paper_id":"3313831.3376714","venue":"chi2020","keywords":["Virtual reality","Sword ﬁghting","Machine learning","Animation","Gesture recognition"],"doi":"10.1145/3313831.3376714","paragraph_containing_keyword":"more realistic and engaging interactions than simple hand-\ncrafted interaction logic, while supporting a controllable and \nunderstandable behaviour design. \nAuthor Keywords \nvirtual reality; sword ﬁghting; machine learning; animation; \ngesture recognition \nCCS Concepts \n•Human-centered \nreality; \n•Computing  methodologies  →  Machine  learning;  Ani-\nmation; \nINTRODUCTION \nAs VR takes its place as a commoditised medium accessible \nto end users in their own homes, expectations about the possi-\nbilities of the technology are raised. From a user perspective, \nVR should allow us to transform traditional screen-based pro-\nductions into embodied experiences, where one can directly \ninteract with the virtual world from “inside” [46]. However, \ndesigning the mechanics for the virtual world is not always \nstraightforward. Close-range interactions are particularly dif-\nﬁcult,  due  to  the  unpredictability  of  user  actions,  and  con-\nventional methods used in non-immersive environments, like \nscreen-based video games, extrapolate poorly to VR.","sections":[{"word_count":562,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1811,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":2390,"figure_citations":{"1":["Figure 1 shows a high-level view of Touché."]},"section_index":2,"title":"SYSTEM DESIGN"},{"word_count":176,"figure_citations":{},"section_index":3,"title":"INTERACTION DESIGN METHODOLOGY"},{"word_count":861,"figure_citations":{"3":["Figure 3 plots the prediction accuracy for gesture class and progress."],"4":["Figure 4 visualises this formula for a single pair of bones."],"5":["Figure 5 shows the distribution of the pose difference, measured in square meters."]},"section_index":4,"title":"TECHNICAL EVALUATION"},{"word_count":1536,"figure_citations":{"6":["Figure 6 shows the results of the study."],"7":["Figure 7 and Table 1."]},"section_index":5,"title":"USER EVALUATION"},{"word_count":942,"figure_citations":{},"section_index":6,"title":"DISCUSSION"},{"word_count":225,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":42,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGEMENTS"},{"word_count":2441,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Touché: Data-Driven Interactive Sword Fighting in Virtual Reality","authors":"Javier Dehesa, Andrew Vidler, Christof Lutteroth, Julian Padget","abstract":"VR games offer new freedom for players to interact naturally using motion. This makes it harder to design games that react to player motions convincingly. We present a framework for VR sword fighting experiences against a virtual character that simplifies the necessary technical work to achieve a convincing simulation. The framework facilitates VR design by abstracting from difficult details on the lower \"physical\" level of interaction, using data-driven models to automate both the identification of user actions and the synthesis of character animations. Designers are able to specify the character's behaviour on a higher \"semantic\" level using parameterised building blocks, which allow for control over the experience while minimising manual development work. We conducted a technical evaluation, a questionnaire study and an interactive user study. Our results suggest that the framework produces more realistic and engaging interactions than simple hand-crafted interaction logic, while supporting a controllable and understandable behaviour design.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Left: Our framework splits the problem of simulating interactive VR sword ﬁghting characters into a “physical” level, \nrelying on data-driven models, and a “semantic” level, where designers can conﬁgure the behaviour of the character.  Right: \nThe framework generates responsive animations against player attacks (top), avoiding nonreactive behaviour from the character \n(bottom left). A neural network parameterised by the position of the player’s sword synthesises the animation (bottom right). \n","bbox":[53.6,364.26899199999997,568.1108930999999,407.108992],"page":"1"},"2":{"caption":"Figure 2: Character behaviour diagram. Transitions marked \nwith * depend on the conﬁguration of the framework. \n","bbox":[53.57,478.428992,299.551791,499.350992],"page":"5"},"3":{"caption":"Figure 3: Gesture class and progress prediction accuracy. \n","bbox":[61.578,514.747992,291.8528189999999,524.710992],"page":"7"},"4":{"caption":"Figure  4:  The  pose  difference  measures  the  total  distance \nbetween pairs of corresponding bones (bone A and bone B), \ncomputing the area of the four triangles from the bone ends \n(A1, A2, B1, B2) to the mid point (M). \n","bbox":[320.766,554.0759919999999,567.9565915999999,596.9159920000001],"page":"7"},"5":{"caption":"Figure 5:  Normalised histogram and kernel density estima-\ntion of the distribution of pose difference between the motion \npredicted by our model and the evaluation data. \n","bbox":[53.929,506.359992,299.4887472,538.239992],"page":"8"},"6":{"caption":"Figure 6: Interest / Enjoyment, Realism, Skill and Repetitive-\nness scores for the questionnaire study. The control condition \nC was generally inferior, while differences between A and D \nreﬂect the impact of the framework conﬁguration. \n","bbox":[321.094,466.638992,566.6844633000001,509.478992],"page":"8"},"7":{"caption":"Figure 7: Interest / Enjoyment, Realism, Skill and Repetitive-\nness scores for the interactive study. \n","bbox":[53.929,488.556992,298.6618576,509.478992],"page":"9"}},"crops":{"1":{"crop_coord":[188.84723250000002,469.9196791291666,954.2437481375001,1046.8555366666667],"bbox":[69.7850037,416.9320068,341.7277493295,621.0289155135],"page":"1"},"2":{"crop_coord":[984.5027583333334,469.9488830555555,1561.4094119444444,1046.8555366666667],"bbox":[356.220993,416.9320068,560.3073883,621.0184021],"page":"1"},"3":{"crop_coord":[212.32220111111113,168.22644275555547,762.4947594444445,790.627788888889],"bbox":[78.2359924,509.173996,272.6981134,729.638480608],"page":"5"},"4":{"crop_coord":[120.06112,168.2259961111111,852.5492266666665,720.1860977777778],"bbox":[45.0220032,534.5330048,305.1177216,729.6386414],"page":"7"},"5":{"crop_coord":[1021.966646111111,168.21985027777777,1437.0749072222222,519.6138933333332],"bbox":[369.7079926,606.7389984,515.5469666,729.6408539],"page":"7"},"6":{"crop_coord":[144.80278027777777,168.22896333333318,829.9958463888889,682.6027680555555],"bbox":[53.9290009,548.0630035,296.9985047,729.6375732],"page":"8"},"7":{"crop_coord":[885.5263588045833,166.54234449530156,1162.03661743375,437.7541399772457],"bbox":[320.58948916965,636.2085096081915,416.53318227615,730.2447559816915],"page":"8"},"8":{"crop_coord":[1256.887476860139,166.54234449530156,1533.3977354893057,437.7541399772457],"bbox":[454.27949166965004,636.2085096081915,550.22318477615,730.2447559816915],"page":"8"},"9":{"crop_coord":[885.5263588045833,460.05901227307936,1162.03661743375,731.2708077550235],"bbox":[320.58948916965,530.5425092081915,416.53318227615,624.5787555816914],"page":"8"},"10":{"crop_coord":[1256.887476860139,460.05901227307936,1533.3977354893057,731.2708077550235],"bbox":[454.27949166965004,530.5425092081915,550.22318477615,624.5787555816914],"page":"8"},"11":{"crop_coord":[144.80278027777777,168.22712083333315,458.64169555555554,438.5361055555555],"bbox":[53.9290009,635.927002,163.31101040000001,729.6382365000001],"page":"9"},"12":{"crop_coord":[516.1638980555555,168.22712083333315,830.0028133333334,438.5361055555555],"bbox":[187.6190033,635.927002,297.0010128,729.6382365000001],"page":"9"},"13":{"crop_coord":[144.80278027777777,461.743788611111,458.64169555555554,732.0527733333333],"bbox":[53.9290009,530.2610016,163.31101040000001,623.9722361],"page":"9"},"14":{"crop_coord":[516.1638980555555,461.743788611111,830.0028133333334,732.0527733333333],"bbox":[187.6190033,530.2610016,297.0010128,623.9722361],"page":"9"}}}},{"filename":"3313831.3376719","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503 Design Project 1/_PaperVis/papers/chi2020/3313831.3376719.pdf","paper_id":"3313831.3376719","venue":"chi2020","keywords":["Agency","Social Touch","Virtual Reality","Human-likeness"],"paragraph_containing_keyword":"Author Keywords \nAgency; Social Touch; Virtual Reality; Human-likeness.","paragraph_after_keyword":"CCS Concepts \n•Human-centered computing → Virtual reality;","doi":"10.1145/10.1145/3313831.3376719"},"fig":{"captions":{"1":{"caption":"Figure 1.  Is the presented character a human-controlled avatar or computer-controlled agent?  If the character reaches out and physically performs a \nsocial touch on a user, it will blur the boundaries between avatar and agent. For this we used a heat-able hand prototype with ﬂexible joints, to recreate \na touch sensation that is indistinguishable from a real hand. \n","bbox":[53.929,454.24177,566.1529299999999,480.14426999999995],"page":"1"},"2":{"caption":"Figure 2. The above-mentioned features were named by the participants \nof the main and preliminary studies as features that inﬂuenced the deci-\nsion whether they perceived the presented character as an agent or an \navatar. Social cues – such as the in the study induced social touch – were \nselected as clear hints towards a human-controlled entity. \n","bbox":[53.929,468.45227,298.99056,512.28727],"page":"4"},"3":{"caption":"Figure 3. The silicone hand contains a baseplate for solidity, a wire skele-\nton, a heat-able foil element and a mount for tripods. The inner skeleton \nprovides adaptability for the hand posture.  The mount allows the hand \nto be connected to other mechanical devices such as levers. \n","bbox":[321.094,472.35452,566.1555599999999,507.22326999999996],"page":"4"},"4":{"caption":"Figure 4.  The Figure shows the ratings of the participants regarding four measures and two factors (Entity and Touch).  Higher ratings indicate for \nall measures that the character was perceived as human-controlled.  For all measures the avatar was rated signiﬁcantly higher compared to the agent \nconditions. For Perceived Agency, Co-Presence and Embarrassment we see an signiﬁcant increase based on the introduction of Touch. \n","bbox":[53.928999999999974,550.30077,566.33624,576.20327],"page":"7"}},"crops":{"1":{"crop_coord":[144.80278027777777,557.6054975000002,1572.0085483333332,858.3361222222223],"bbox":[53.9290009,484.798996,564.1230773999999,589.4620209],"page":"1"},"2":{"crop_coord":[144.80278027777777,168.2257156538889,830.0309265327778,769.0499963888889],"bbox":[53.9290009,516.9420013,297.0111335518,729.6387423646],"page":"4"},"3":{"crop_coord":[886.9277613888887,168.2278558450001,1572.155907643889,783.1194475],"bbox":[321.0939941,511.8769989,564.1761267518,729.6379718958],"page":"4"},"4":{"crop_coord":[144.80278027777777,168.22574653999973,1572.1756238099997,591.5055508333331],"bbox":[53.9290009,580.8580017,564.1832245715999,729.6387312456001],"page":"7"},"5":{"crop_coord":[153.38055083333333,1715.1705511111113,494.09712888888885,1805.1388975],"bbox":[57.0169983,143.9499969,176.0749664,172.7386016],"page":"9"}}}},{"filename":"3313831.3376762","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376762.pdf","paper_id":"3313831.3376762","venue":"chi2020","keywords":["Virtual Reality","Interactive Digital Storytelling","Interactive Performance","Roleplaying","Narrative","Drama"],"paragraph_containing_keyword":"ABSTRACT \nIn this paper we describe the design and evaluation of The \nNext  Fairy  Tale  (TNFT)  VR,  a  theatrical  interactive \nstorytelling  system  created in  virtual  reality  and  informed \nby performing arts theories. TNFT was designed to produce \nopportunities  for  interactors  to  experience  role-taking  and \ncharacter identification using design principles drawn from \nactor  training  and  theatrical  performance.  We  report  the \nresults of a pilot qualitative study of interactors using TNFT \nto  explore  the  elements  of  the  design  that  supported  or \nhindered  roleplaying  behavior.  We  identify  four  design \npatterns that supported roleplaying in the system: (1) using \nexplicit roles to set player expectations, (2) embracing the \n“mask  and  the  mirror”  effect,  (3)  attending  to  visual  and \ninteractional  details,  and  (4)  easing  the  player  gently  into \nthe  roleplaying  experience.  These  patterns  speak  to  a \nbroader  need \nthrough  explicit \nscaffolding  of  desired  player  behaviors in  digital  narrative \nexperiences. \nAuthor Keywords \nVirtual Reality; Interactive Digital Storytelling; Interactive \nPerformance; Roleplaying; Narrative; Drama \nCSS Concepts \n• Human-centered computing~Virtual reality   • Software\nand its engineering~Interactive games\nINTRODUCTION \nThe current discourse around interactive narratives and VR \nis  saturated  with  (contested)  claims  about  “empathy”  and \n“perspective taking” [15,18,39,43,58]. Theorists of VR and \nnarrative  are  invested  in  the  possibility  of  the  medium to \nsupport  some  sort  of  transformative  experience,  however \nthere  is  little  understanding  of  the  design  patterns  that \nsupport or deny roleplaying and character identification. In \nthis  paper  we  investigate  practices  of  role-playing  and \nidentity  transformation  exhibited  by  players  of  a  virtual \nPermission  to  make digital or  hard copies of  all or part  of  this work  for  personal  or \nclassroom use is granted without fee provided that copies are not made or distributed \nfor  profit  or  commercial  advantage  and  that  copies  bear  this  notice  and  the  full \ncitation  on  the first  page. Copyrights for components of  this work owned  by  others \nthan ACM must be honored. Abstracting with credit is permitted. To copy otherwise, \nor  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior  specific \npermission and/or a fee. Request permissions from Permissions@acm.org \nCHI '20, April 25–30, 2020, Honolulu, HI, USA \n© 2020 Association for Computing Machinery. \nACM ISBN 978-1-4503-6708-0/20/04...$15.00 \nhttps://doi.org/10.1145/3313831.3376762","doi":"10.1145/3313831.3376762","paragraph_after_keyword":"Janet  Murray  proposed","sections":[{"word_count":649,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1373,"figure_citations":{"1":["Figure 1 – (a) Path to Minerva (b) Calliope’s Reflection in Mirror (c ) The main area not."]},"section_index":1,"title":"LITERATURE REVIEW"},{"word_count":1408,"figure_citations":{"1":["Figure 1a).","Figure 1b)."],"2":["Figure 2 – (a) The Emotion Spells (b) Interacting with the spells (c) Activating a spell with the wand Paper 633 Page 4 CHI 2020 Paper CHI 2020, April 25–30, 2020, Honolulu, HI, USA Figure 3 - The interactional logic of TNFT's dialogue system, the emotion spells, and Minerva's responses Figure 3 illustrates the interactional logic of our dialogue system."],"3":["Figure 3), following the same color mappings as the spells.","Figure 3 we visualize this and show how a player might navigate the expressive space of TNFT."]},"section_index":2,"title":"DESIGNING A PARTICIPATORY THEATER EXPERIENCE"},{"word_count":386,"figure_citations":{},"section_index":3,"title":"METHODS"},{"word_count":2504,"figure_citations":{},"section_index":4,"title":"FINDINGS"},{"word_count":1830,"figure_citations":{},"section_index":5,"title":"DESIGN RECOMMENDATIONS"},{"word_count":413,"figure_citations":{},"section_index":6,"title":"CONCLUSION AND FUTURE WORK"},{"word_count":51,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGEMENTS"},{"word_count":1593,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Investigating Roleplaying and Identity Transformation in a Virtual Reality Narrative Experience","authors":"Saumya Gupta, Theresa Jean Tanenbaum, Meena Devii Muralikumar, Aparajita S. Marathe","abstract":"In this paper we describe the design and evaluation of The Next Fairy Tale (TNFT) VR, a theatrical interactive storytelling system created in virtual reality and informed by performing arts theories. TNFT was designed to produce opportunities for interactors to experience role-taking and character identification using design principles drawn from actor training and theatrical performance. We report the results of a pilot qualitative study of interactors using TNFT to explore the elements of the design that supported or hindered roleplaying behavior. We identify four design patterns that supported roleplaying in the system: (1) using explicit roles to set player expectations, (2) embracing the \"mask and the mirror\" effect, (3) attending to visual and interactional details, and (4) easing the player gently into the roleplaying experience. These patterns speak to a broader need to support roleplay through explicit scaffolding of desired player behaviors in digital narrative experiences.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{},"crops":{"1":{"crop_coord":[149.16666666666666,165.83333333333331,1562.1388888888887,426.16666666666663],"bbox":[55.5,640.38,560.5699999999999,730.5],"page":"3"},"2":{"crop_coord":[151.66666666666666,1689.4444444444446,1561.6666666666667,1947.6444444444444],"bbox":[56.4,92.648,560.4,182],"page":"4"},"3":{"crop_coord":[163.425,170.83333333333314,1525.286111111111,724.8055555555555],"bbox":[60.633,532.87,547.303,728.7],"page":"5"}}}},{"filename":"3313831.3376788","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376788.pdf","paper_id":"3313831.3376788","venue":"chi2020","keywords":["Ethics","Moral dilemmas","VR","Decision-making","Ethical AI"],"paragraph_containing_keyword":"ABSTRACT \nA moral dilemma is a decision-making paradox without un-\nambiguously  acceptable  or  preferable  options.  This  paper \ninvestigates if and how the virtual enactment of two renowned \nmoral dilemmas—the Trolley and the Mad Bomber—inﬂuence \ndecision-making when compared with mentally visualizing \nsuch  situations.  We  conducted  two  user  studies  with  two \ngender-balanced samples of 60 participants in total that com-\npared between paper-based and virtual-reality (VR) conditions, \nwhile simulating 5 distinct scenarios for the Trolley dilemma, \nand 4 storyline scenarios for the Mad Bomber’s dilemma. Our \nﬁndings suggest that the VR enactment of moral dilemmas \nfurther fosters utilitarian decision-making, while it ampliﬁes \nbiases such as sparing juveniles and seeking retribution. Ulti-\nmately, we theorize that the VR enactment of renowned moral \ndilemmas can yield ecologically-valid data for training future \nArtiﬁcial Intelligence (AI) systems on ethical decision-making, \nand we elicit early design principles for the training of such \nsystems. \nAuthor Keywords \nEthics; moral dilemmas; VR; decision-making; ethical AI. \nCCS Concepts \n•Human-centered  computing  →  Human  computer  inter-\naction (HCI); Virtual reality; User studies; \nINTRODUCTION \nHumanity has been continuously confronted with moral dilem-\n \n \nmas ever since the dawn of logical reasoning, dating back to \n \nthe classical era in ancient Greece (5th and 4th centuries BC). \nOver time, several schools of thought emerged including the \n \nCynics, the Cyrenaics, Aristotle’s school of ethics, the Epicure-\n \nans and the Stoics, holistically described as “Ancient Ethics”; \n \n \nPermission to make digital or hard copies of all or part of this work for personal or \n \n \n \n \nclassroom use is granted without fee provided that copies are not made or distributed \n \nfor profit or commercial advantage and that copies bear this notice and the full citation \n \n \n \n \non  the  first  page.  Copyrights  for  components  of  this  work  owned  by  others  than \n \n \nACM  must  be  honored.  Abstracting  with  credit  is  permitted.  To  copy  otherwise, \n \n \n \nor  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior  specific \n \n \n \n \n \n \npermission and/or a fee. Request permissions from permissions@acm.org.\n \n \nCHI ’20, April 25–30, 2020, Honolulu, HI, USA.\n \n \n \n© 2020 Association for Computing Machinery. \nACM ISBN 978-1-4503-6708-0/20/04...$15.00. \nhttp://dx.doi.org/10.1145/3313831.3376788","doi":"10.1145/3313831.3376788","paragraph_after_keyword":"","sections":[{"word_count":784,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1753,"figure_citations":{"1":["Figure 1)."]},"section_index":1,"title":"RELATED WORK"},{"word_count":1776,"figure_citations":{"1":["Figure 1).","Figure 1a).","Figure 1c)."]},"section_index":2,"title":"STUDY"},{"word_count":3828,"figure_citations":{"2":["Figure 2).","Figure 2).","Figure 2)."],"3":["Figure 3)."],"4":["Figure 4).","Figure 4).","Figure 4)."],"5":["Figure 5)."],"6":["Figure 6).","Figure 6)."],"7":["Figure 7).","Figure 7)."]},"section_index":3,"title":"RESULTS"},{"word_count":1172,"figure_citations":{},"section_index":4,"title":"DESIGNING FOR ETHICAL AI TRAINING"},{"word_count":213,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":1232,"figure_citations":{},"section_index":6,"title":"REFERENCES"}],"title":"Would you do it?: Enacting Moral Dilemmas in Virtual Reality for Understanding Ethical Decision-Making","authors":"Evangelos Niforatos, Adam Palma, Roman Gluszny, Athanasios Vourvopoulos, Fotis Liarokapis","abstract":"A moral dilemma is a decision-making paradox without unambiguously acceptable or preferable options. This paper investigates if and how the virtual enactment of two renowned moral dilemmas---the Trolley and the Mad Bomber---influence decision-making when compared with mentally visualizing such situations. We conducted two user studies with two gender-balanced samples of 60 participants in total that compared between paper-based and virtual-reality (VR) conditions, while simulating 5 distinct scenarios for the Trolley dilemma, and 4 storyline scenarios for the Mad Bomber's dilemma. Our findings suggest that the VR enactment of moral dilemmas further fosters utilitarian decision-making, while it amplifies biases such as sparing juveniles and seeking retribution. Ultimately, we theorize that the VR enactment of renowned moral dilemmas can yield ecologically-valid data for training future Artificial Intelligence (AI) systems on ethical decision-making, and we elicit early design principles for the training of such systems.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1.  (a) The train platform and the avatars approaching in the Trolley study,  (b) A participant in the Trolley study wearing the Oculus DK2 \nand relying on Kinect for input, (c) Interacting with the bomber avatar in the Mad Bomber study, (d) A participant wearing Oculus DK2 and using a \nmouse/keyboard for input. \n","bbox":[53.929,574.1177700000001,566.1529299999997,600.02027],"page":"2"},"2":{"caption":"Figure 2. Lever pulls (%) by scenario and condition in the Trolley study. \n","bbox":[53.929,522.01127,300.38530999999995,529.98127],"page":"6"},"3":{"caption":"Figure 3. Tactics employed (%) by condition in the Mad Bomber study. \n","bbox":[321.935,533.6332699999999,565.3148899999999,541.60327],"page":"6"},"4":{"caption":"Figure 4.  Tactics employed in all scenarios and both conditions in the \nMad Bomber study. \n","bbox":[321.094,550.48502,566.1555599999999,567.4212699999999],"page":"7"},"5":{"caption":"Figure 5. Median self-reported workload for all scenarios and both con-\nditions in the Mad Bomber study. \n","bbox":[53.929,505.58702000000005,298.32108,522.52327],"page":"8"},"6":{"caption":"Figure 6.  Lever pulls (%) by scenario for both genders and both condi-\ntions in the Trolley study. \n","bbox":[321.094,512.58702,565.4860799999999,529.52327],"page":"8"},"7":{"caption":"Figure 7. Tactics employed (%) for both genders and both conditions in \nthe Mad Bomber study. \n","bbox":[53.929,497.96702,298.99056,514.90327],"page":"9"}},"crops":{"1":{"crop_coord":[144.80277777777778,168.2309722222223,1573.4483333333333,525.3472222222224],"bbox":[53.929,604.675,564.6414,729.63685],"page":"2"},"2":{"crop_coord":[144.80277777777778,-53.52350000000021,848.9694444444443,719.9000000000001],"bbox":[53.929,534.636,303.82899999999995,809.46846],"page":"6"},"3":{"crop_coord":[886.9277777777778,-514.8122222222224,1910.4522222222222,682.0833333333333],"bbox":[321.094,548.25,685.9628,975.5324],"page":"6"},"4":{"crop_coord":[886.9277777777778,-548.0799999999999,1924.7011111111112,615.8999999999999],"bbox":[321.094,572.076,691.0924,987.5088000000001],"page":"7"},"5":{"crop_coord":[-34.18344444444444,472.2699999999998,1434.2698888888888,1876.2611111111112],"bbox":[-10.506039999999999,118.346,514.53716,620.1828],"page":"8"},"6":{"crop_coord":[722.9866111111111,713.0797222222226,1825.2292777777777,1645.7392222222225],"bbox":[262.07518,201.33387999999997,655.2825399999999,533.4912999999999],"page":"8"},"7":{"crop_coord":[144.80277777777775,-497.3317777777779,1194.066222222222,761.7833333333334],"bbox":[53.92899999999999,519.558,428.0638399999999,969.2394400000001],"page":"9"}}}},{"filename":"3313831.3376803","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376803.pdf","paper_id":"3313831.3376803","venue":"chi2020","keywords":["Attention guidance","Virtual reality","Immersion","Eye-tracking","Particle swarms","User studies"],"paragraph_containing_keyword":"Author Keywords \nAttention guidance, virtual reality, immersion, eye-tracking, \nparticle swarms, user studies","paragraph_after_keyword":"CCS Concepts \n•Human-centered computing → Virtual reality; User cen-\ntered design; User studies;\nPermission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed \nfor proﬁt or commercial advantage and that copies bear this notice and the full citation \non the ﬁrst page.  Copyrights for components of this work owned by others than the \nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or \nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission \nand/or a fee. Request permissions from permissions@acm.org. \nCHI’20, April 25–30, 2020, Honolulu, HI, USA \n© 2020 Copyright held by the owner/author(s).  Publication rights licensed to ACM. \nISBN 978-1-4503-6708-0/20/04. . . $15.00 \nDOI: https://doi.org/10.1145/3313831.3376803","doi":"10.1145/3313831.3376803","sections":[{"word_count":947,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1014,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":4150,"figure_citations":{"2":["Figure 2)."],"4":["Figure 4a).","Figure 4d).","Figure 4b).","Figure 4f).","Figure 4c)."]},"section_index":2,"title":"GENERAL APPROACH"},{"word_count":1435,"figure_citations":{},"section_index":3,"title":"SGD"},{"word_count":445,"figure_citations":{},"section_index":4,"title":"GENERAL DISCUSSION"},{"word_count":137,"figure_citations":{},"section_index":5,"title":"FUTURE WORK"},{"word_count":132,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":1586,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"HiveFive: Immersion Preserving Attention Guidance in Virtual Reality","authors":"Daniel Lange, Tim Claudius Stratmann, Uwe Gruenefeld, Susanne Boll","abstract":"Recent advances in Virtual Reality (VR) technology, such as larger fields of view, have made VR increasingly immersive. However, a larger field of view often results in a user focusing on certain directions and missing relevant content presented elsewhere on the screen. With HiveFive, we propose a technique that uses swarm motion to guide user attention in VR. The goal is to seamlessly integrate directional cues into the scene without losing immersiveness. We evaluate HiveFive in two studies. First, we compare biological motion (from a prerecorded swarm) with non-biological motion (from an algorithm), finding further evidence that humans can distinguish between these motion types and that, contrary to our hypothesis, non-biological swarm motion results in significantly faster response times. Second, we compare HiveFive to four other techniques and show that it not only results in fast response times but also has the smallest negative effect on immersion.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1:  Our proposed technique HiveFive, in which a swarm visualization is used as a diegetic cue for guiding attention in \nVirtual Reality (in two different environments - left: forest, right: city\n","bbox":[53.57,460.6820784,566.7035897999997,481.8328182],"page":"1"},"2":{"caption":"Figure 2: The visible FOV of the HTC Vive is given as 110◦ \n(red line) under optimal conditions. However, since the visible \nrange depends on many factors such as the ﬁt of the headset, \nfacial  geometry  and  the  distance  between  eyes  and  lenses, \nwe have identiﬁed an average FOV of 65◦  (blue line).  The \nlemniscate is located in the centre with a total width of 45◦ \n(white line). Best seen in color. \n","bbox":[53.573337,486.27307840000003,300.7794456,563.7370738],"page":"4"},"3":{"caption":"Figure 3:  Boxplot of times to ﬁrst ﬁxation for both swarm \nmotion types for each environment (forest, city). \n","bbox":[321.094,576.4400784,566.7047059999999,597.3616784000001],"page":"5"},"4":{"caption":"Figure 4: The apple tree used in the second study as overview (a) and the compared techniques (b-f). Best seen in color. \n","bbox":[70.643,461.3790784,549.943686,471.5708182],"page":"7"},"5":{"caption":"Figure 5: Boxplot of times to ﬁrst ﬁxation for all techniques. \n","bbox":[54.644,569.9000784,298.77751299999994,579.8626784],"page":"8"},"6":{"caption":"Figure 6: Results from the 5-point Likert-scale questionnaire. \n","bbox":[53.929,310.7280784,299.4581500999999,320.6906784],"page":"9"}},"crops":{"1":{"crop_coord":[148.43611111111113,549.3027777777778,859.9916666666667,839.925],"bbox":[55.237,491.427,307.797,592.451],"page":"1"},"2":{"crop_coord":[856.9222222222221,549.3027777777778,1568.4777777777776,839.925],"bbox":[310.292,491.427,562.852,592.451],"page":"1"},"3":{"crop_coord":[144.80277777777778,168.22788888888877,835.0827777777778,616.6305555555556],"bbox":[53.929,571.813,298.8298,729.63796],"page":"4"},"4":{"crop_coord":[886.9277777777778,168.22300000000035,1577.2317777777776,518.3750000000001],"bbox":[321.094,607.185,566.00344,729.6397199999999],"page":"5"},"5":{"crop_coord":[144.96666666666667,168.62666666666652,622.6666666666666,480.0333333333332],"bbox":[53.988,620.988,222.36,729.4944],"page":"7"},"6":{"crop_coord":[619.6055555555556,168.23249999999996,1097.3377777777778,480.43611111111125],"bbox":[224.858,620.843,393.2416,729.6363],"page":"7"},"7":{"crop_coord":[1094.2416666666666,168.23249999999996,1571.973888888889,480.43611111111125],"bbox":[395.727,620.843,564.1106,729.6363],"page":"7"},"8":{"crop_coord":[144.96666666666667,514.7158333333333,622.698888888889,826.9194444444444],"bbox":[53.988,496.109,222.3716,604.9023],"page":"7"},"9":{"crop_coord":[619.6055555555556,514.7158333333333,1097.3377777777778,826.9194444444444],"bbox":[224.858,496.109,393.2416,604.9023],"page":"7"},"10":{"crop_coord":[1094.2416666666666,514.7158333333333,1571.973888888889,826.9194444444444],"bbox":[395.727,496.109,564.1106,604.9023],"page":"7"},"11":{"crop_coord":[144.80277777777778,168.22333333333347,835.1327777777778,566.9833333333332],"bbox":[53.929,589.686,298.8478,729.6396],"page":"8"},"12":{"crop_coord":[144.80277777777778,885.6704662222222,835.0947777777777,1286.9055555555556],"bbox":[53.929,330.514,298.83412,471.35863216],"page":"9"}}}},{"filename":"3313831.3376821","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503 Design Project 1/_PaperVis/papers/chi2020/3313831.3376821.pdf","paper_id":"3313831.3376821","venue":"chi2020","keywords":["Redirected Walking","Virtual Reality","Telewalk"],"doi":"10.1145/3313831.3376821","paragraph_containing_keyword":"we found that even though motion sickness susceptible participants \nreported respective symptoms, Telewalk did result in stronger feel-\nings of presence and immersion and was seen as more natural then \nTeleportation. \nCCS CONCEPTS \n• Human-centered computing → Virtual reality. \nKEYWORDS \nRedirected Walking; Virtual Reality; Telewalk \nACM Reference Format: \nMichael Rietzler, Martin Deubzer, Thomas Dreja, and Enrico Rukzio. 2020. \nTelewalk: Towards Free and Endless Walking in Room-Scale Virtual Reality. \nIn CHI Conference on Human Factors in Computing Systems (CHI ’20), \nApril 25–30, 2020, Honolulu, HI, USA. ACM, New York, NY, USA, 9 pages. \nhttps://doi.org/10.1145/3313831.3376821","paragraph_after_keyword":"1  INTRODUCTION \nNavigating inside virtual worlds is challenging to realize, since the \nscale of the virtual world does not necessarily match the one of the \nreal world. When consuming VR content in a room-scale application, \nthe available real world space does seldom exceed 3m x 3m. The \ncurrent solution to allow navigation within such a small space is"},"fig":{"captions":{"1":{"caption":"Figure 1: The concept of Telewalk: The combination of perceivable curvature and translation gains along with a head based camera \ncontrol allows to compress any virtual space to a pre-defned real world radius (in our case 1.5m). (Left) illustration of walking paths \nand (right) plots of the virtual and real path walked in our study application. \n","bbox":[53.798,343.253202,560.4218299999997,374.132106],"page":"1"},"2":{"caption":"Figure  2:  The  visualization  of  the  optimal  path  including  the \nspot  with  the  optimal  distance  to  the  tracking  center  and  the \ntwo  lines  indicating  the  optimal  direction.  The  user’s  viewing \ndirection is visualized by a single line. \n","bbox":[53.798,484.31974999999994,296.274504,526.1551059999999],"page":"5"},"3":{"caption":"Figure  3:  Boxplots  of  presence  (SUS),  immersion  and  enjoy-\nment  (E2I)  and  simulator  sickness  (SSQ)  scores.  The  marked \ncomparisons were signifcant on the 5% level. \n","bbox":[317.953116,522.080654,560.438586,552.962106],"page":"6"},"4":{"caption":"Figure 4: Diverging Stacked Bar Charts of the single item questions as described in the Method section. \n","bbox":[109.553,505.715378,504.68461999999994,514.753106],"page":"7"}},"crops":{"1":{"crop_coord":[144.4388888888889,700.616165,1555.4683388888889,1130.925],"bbox":[53.798,386.667,558.168602,537.9781806],"page":"1"},"2":{"crop_coord":[144.4388888888889,227.4680091666667,821.7746605555557,708.6416666666668],"bbox":[53.798,538.689,294.0388778,708.3115167],"page":"5"},"3":{"crop_coord":[878.2083333333334,227.46055555555557,1555.5581111111112,634.1750000000003],"bbox":[317.955,565.497,558.20092,708.3142],"page":"6"},"4":{"crop_coord":[144.4388888888889,227.45672222222242,1555.588888888889,740.3138888888889],"bbox":[53.798,527.287,558.212,708.31558],"page":"7"}}}},{"filename":"3313831.3376847","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376847.pdf","paper_id":"3313831.3376847","venue":"chi2020","keywords":["Virtual reality sickness","Discomfort","Realism","Vestibular system","Vibrotactile feedback"],"paragraph_containing_keyword":"Author Keywords \nVirtual reality sickness; Discomfort; Realism; Vestibular \nsystem; Vibrotactile feedback.","paragraph_after_keyword":"CCS Concepts \n•Human-centered computing → Virtual reality; Haptic de-\nvices;","doi":"10.1145/3313831.3376847","sections":[{"word_count":930,"figure_citations":{"1":["Figure 1 and Figure 2, uses two small vibration motors controlled by an Arduino microcontroller and is entirely integrated into and powered by the VR headset."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":793,"figure_citations":{"3":["Figure 3 (c)).","Figure 3 (b)) vs.","Figure 3 (a))."]},"section_index":1,"title":"RELATED WORK"},{"word_count":1945,"figure_citations":{"2":["Figure 2)."],"3":["Figure 3a and Figure 3b, respectively) were chosen based on the settings of the commercial bone-conducted earphone and other previous systems [61, 62].","Figure 3c).","Figure 3a).","Figure 3b)."],"4":["Figure 4c) that were scattered in three designed VR scenes: a city in Figure 4(a)(d), a forest in Figure 4(b)(e), and a science ﬁction-themed passage in Figure 4(c)(f)."],"5":["Figure 5, and are stated directly in the following: "]},"section_index":2,"title":"USER STUDY"},{"word_count":707,"figure_citations":{"5":["Figure 5, 6 and 7, where error bars in each ﬁgure denoted standard error of the mean value), and report our ﬁndings in the following.","Figure 5)."],"6":["Figure 6).","Figure 6 and are described as follows: • 2-sided tapping (synchronized) vs."]},"section_index":3,"title":"RESULTS"},{"word_count":1114,"figure_citations":{"7":["Figure 7)."]},"section_index":4,"title":"DISCUSSION"},{"word_count":106,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":59,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGEMENT"},{"word_count":2277,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"WalkingVibe: Reducing Virtual Reality Sickness and Improving Realism while Walking in VR using Unobtrusive Head-mounted Vibrotactile Feedback","authors":"Yi-Hao Peng, Carolyn Yu, Shi-Hong Liu, Chung-Wei Wang, Paul Taele, Neng-Hao Yu, Mike Y. Chen","abstract":"Virtual Reality (VR) sickness is common with symptoms such as headaches, nausea, and disorientation, and is a major barrier to using VR. We propose WalkingVibe, which applies unobtrusive vibrotactile feedback for VR walking experiences, and also reduces VR sickness and discomfort while improving realism. Feedback is delivered through two small vibration motors behind the ears at a frequency that strikes a balance in inducing vestibular response while minimizing annoyance. We conducted a 240-person study to explore how visual, audio, and various tactile feedback designs affect the locomotion experience of users walking passively in VR while seated statically in reality. Results showed timing and location for tactile feedback have significant effects on VR sickness and realism. With WalkingVibe, 2-sided step-synchronized design significantly reduces VR sickness and discomfort while significantly improving realism. Furthermore, its unobtrusiveness and ease of integration make WalkingVibe a practical approach for improving VR experiences with new and existing VR headsets.","publication":{"venue":"CHI '20","venue_full":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","year":"2020","date":"2020/4/21"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1.  WalkingVibe prototype with 2 vibration motors behind the \nears,  which  provide  vibrotactile  stimulation  synchronized  to  footsteps \nin  VR.  Our  240-person  study  showed  that  it  signiﬁcantly  reduced  dis-\ncomfort and VR sickness, and signiﬁcantly improved the realism of the \nvirtual walking experience in VR. \n","bbox":[320.89475,343.74227,566.1555599999999,387.57727],"page":"1"},"2":{"caption":"Figure 2. WalkingVibe prototype with 2-sided vibrotactile design. \n","bbox":[331.539,578.8782699999999,555.7111899999999,586.84827],"page":"2"},"3":{"caption":"Figure 3.  The setup for each category of tactile feedback:  (a) 2-sided vibration, (b) backside vibration, (c) 2-sided tapping feedback replicated from \nPhantomLegs [37] project. \n","bbox":[53.929,204.01302,566.1529299999996,220.94927],"page":"4"},"4":{"caption":"Figure 4.  Screenshots of three scenes and paths of the virtual walking \nenvironment: (a)(d) the city, (b)(e) the forest, and (c)(f) the sci-ﬁ Passage. \n","bbox":[321.094,469.06402,567.5503099999997,486.00027],"page":"5"},"5":{"caption":"Figure  5.  The  average  and  standard  deviation  (in  parentheses)  of  dis-\ncomfort scores (from left to right) are: 1.81 (0.61), 1.53 (0.75), 1.70 (0.92), \n1.36 (0.53), 1.07 (0.62), 1.45 (0.63), 1.86 (0.74), 1.61 (0.75). \n","bbox":[53.331250000000004,491.83177,299.98680999999993,517.73427],"page":"6"},"6":{"caption":"Figure 6.  The average and standard deviation (in parentheses) of RSS \n(from left to right) are:  22.19 (18.60), 18.70 (19.17), 18.20 (15.47), 6.73 \n(15.46), 9.35 (16.75), 9.60 (14.18), 16.95 (12.16), 14.34 (20.55). \n","bbox":[320.83099,491.83177,566.1555599999999,517.73427],"page":"6"},"7":{"caption":"Figure 7. The average and standard deviation (in parentheses) of realism \nscore  (from  left  to  right)  are:  4.17  (1.86),  4.93  (2.08),  5.70  (2.31),  4.27 \n(2.03), 5.67 (2.08), 3.50 (1.69), 4.30 (2.31), 4.23 (1.98). \n","bbox":[320.83099,491.83177,566.5540599999999,517.73427],"page":"7"}},"crops":{"1":{"crop_coord":[886.9277613888887,718.1078677777778,1572.1342213888888,1115.4666816666665],"bbox":[321.0939941,392.2319946,564.1683197,531.6811676],"page":"1"},"2":{"crop_coord":[886.9277613888887,168.2295141666669,1572.1413844444444,556.402791388889],"bbox":[321.0939941,593.4949951,564.1708983999999,729.6373748999999],"page":"2"},"3":{"crop_coord":[144.80278027777777,1261.9082641666666,1572.1274822222222,1578.3222113888887],"bbox":[53.9290009,225.6040039,564.1658936,335.9130249],"page":"4"},"4":{"crop_coord":[886.9277613888887,168.23159111111102,1572.1533794444445,842.0694477777778],"bbox":[321.0939941,490.6549988,564.1752166,729.6366272],"page":"5"},"5":{"crop_coord":[144.80278027777777,168.22408888888899,830.0092402777777,753.9194233333333],"bbox":[53.9290009,522.3890076,297.0033265,729.639328],"page":"6"},"6":{"crop_coord":[886.9277613888887,168.22408888888899,1572.1342213888888,753.9194233333333],"bbox":[321.0939941,522.3890076,564.1683197,729.639328],"page":"6"},"7":{"crop_coord":[886.9277613888887,168.22408888888899,1572.1342213888888,753.9194233333333],"bbox":[321.0939941,522.3890076,564.1683197,729.639328],"page":"7"}}}},{"filename":"360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper.pdf","paper_id":"360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper","venue":"chi2019xr","keywords":["AR/VR","Physical–digital prototyping","Wizard of Oz"],"paragraph_containing_keyword":"CCS CONCEPTS\n• Human-centered computing → Interface design pro-\ntotyping; Mixed / augmented reality;\nKEYWORDS\nAR/VR; physical–digital prototyping; Wizard of Oz.","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nCHI 2019, May 4–9, 2019, Glasgow, Scotland, UK\n© 2019 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 978-1-4503-5970-2/19/05. . . $15.00\nhttps://doi.org/10.1145/3290605.3300826","doi":"10.1145/3290605.3300826","sections":[{"word_count":365,"figure_citations":{"1":["Figure 1 illustrates our method analogous to Rettig [32]."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":621,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":3689,"figure_citations":{"1":["Figure 1 the running example in this section."],"2":["Figure 2)."],"3":["Figure 3)."],"5":["Figure 5)."],"6":["Figure 6): (1) the View pane shows a simulated or live preview of the AR/VR prototype running on a smartphone (live if used in combination with the 360proto App); (2) the Collect pane shows thumbnails of the live video feed of the Camera tool, previous captures of paper mockups, and PNG images imported from the file system; the Layers pane shows thumbnails of the 2D, 360, and AR/VR specific layers described below; the Capture pane shows a larger preview of content selected in the Collect pane and provides tools for chroma keying to make pixels of a selected color with a specified tolerance level made transparent in layers."],"7":["Figure 7 (left) shows how our designer is now starting to add content and compose the butterfly scene from layers of paper mockups.","Figure 7 (right) shows a 360 preview of the scene using spheres to render the 360 layers."],"8":["Figure 8 shows our designer using the AR marker mode in the Camera tool to superimpose an image of a butterfly over a kanji marker attached to a stick made from paper."]},"section_index":2,"title":"INITIAL DESIGN JAMS"},{"word_count":1185,"figure_citations":{"10":["Figure 10)."],"11":["Figure 11)."]},"section_index":3,"title":"AR USER"},{"word_count":2327,"figure_citations":{"13":["Figure 13 shows 29 students’ ratings for three questions."],"14":["Figure 14)."],"15":["Figure 15)."]},"section_index":4,"title":"FINAL DESIGN JAMS"},{"word_count":304,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":141,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":866,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"360proto: Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper","authors":"Michael Nebeling, Katy Madier","abstract":"We explore 360 paper prototyping to rapidly create AR/VR prototypes from paper and bring them to life on AR/VR devices. Our approach is based on a set of emerging paper prototyping templates specifically for AR/VR. These templates resemble the key components of many AR/VR interfaces, including 2D representations of immersive environments, AR marker overlays and face masks, VR controller models and menus, and 2D screens and HUDs. To make prototyping with these templates effective, we developed 360proto, a suite of three novel physical--digital prototyping tools: (1) the 360proto Camera for capturing paper mockups of all components simply by taking a photo with a smartphone and seeing 360-degree panoramic previews on the phone or stereoscopic previews in Google Cardboard; (2) the 360proto Studio for organizing and editing captures, for composing AR/VR interfaces by layering the captures, and for making them interactive with Wizard of Oz via live video streaming; (3) the 360proto App for running and testing the interactive prototypes on AR/VR capable mobile devices and headsets. Through five student design jams with a total of 86 participants and our own design space explorations, we demonstrate that our approach with 360proto is useful to create relatively complex AR/VR applications.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Inspired by Rettig [32], 360-degree paper proto-\ntyping involves participants in different roles: User tests an\nAR/VR paper mockup of an animated butterfly scene; Facili-\ntator streams the butterfly cut-out arranged on a 360◦ paper\ntemplate, while “Computer” moves the butterfly along the\n360◦ grid; Observer records User’s behavior and feedback.\n","bbox":[317.62299999999993,428.00495599999994,559.7949179999999,491.791854],"page":"1"},"2":{"caption":"Figure 2: Initial Daydream, Pokémon GO paper prototypes\n","bbox":[319.633,629.039854,556.51472,638.005854],"page":"3"},"3":{"caption":"Figure 3: 360 paper prototype versions viewed in Cardboard\ntook three hours with two hours focused on paper proto-\ntyping in groups. At the end of each round, groups were\nasked to demonstrate the three required interactions with\ntheir prototypes, which we recorded from the perspective\nof the user. We concluded with a focus group discussion to\nhave participants reflect on the experience as well as exit\nquestionnaires to gather feedback in terms of two pros and\ntwo cons comparing plain and 360 paper prototyping.\n","bbox":[317.955,444.889,559.89808608,551.778854],"page":"3"},"4":{"caption":"Figure 4: 360 grid (red), range of motion (green), FOV (blue)\n","bbox":[54.189,529.157854,293.643962,538.123854],"page":"5"},"5":{"caption":"Figure 5: Camera tool creates static captures and live streams\n","bbox":[53.798,417.431854,294.033004,426.397854],"page":"5"},"6":{"caption":"Figure 6: 360proto Studio’s View pane (top left) shows a simulated or live preview of the 360 paper prototype; Collect pane\n(bottom left) shows the live feed from the connected Camera tool and contains previous captures (here, three captures of\nmountains, trees, and butterflies, a dog and a car drawn using the 360 paper template); Capture pane (bottom right) shows a\nselected capture and provides color filters for chroma keying; Layers pane (top right) shows 2D, 360, and AR/VR specific layers\n(here, only 360 layers are used to compose a basic butterfly scene from the running example).\n","bbox":[53.52000000000004,402.526854,558.200472,455.32885400000004],"page":"6"},"7":{"caption":"Figure 7: Butterfly scene composed from three 360 layers\n","bbox":[58.632,98.000854,289.20165599999996,106.966854],"page":"6"},"8":{"caption":"Figure 8: AR marker to move virtual butterfly in AR view\n","bbox":[321.789,595.198854,554.358074,604.164854],"page":"7"},"9":{"caption":"Figure 9: AR mode with the user looking at a virtual butter-\nfly from inside (left) and outside (right) the 360 video sphere\nthat is anchored at the starting physical location\n","bbox":[317.955,393.211854,559.7949179999999,424.09585400000003],"page":"7"},"10":{"caption":"Figure 10: Architecture of our suite of 360proto tools\n","bbox":[67.276,500.870854,280.55920799999996,509.836854],"page":"8"},"11":{"caption":"Figure 11: Star Trek: two connected rooms made from paper\n","bbox":[317.955,212.257854,558.1900039999999,221.22385400000002],"page":"8"},"12":{"caption":"Figure 12: Androids on paper appear in the environment\n","bbox":[59.381,481.553854,288.453334,490.519854],"page":"9"},"13":{"caption":"Figure 13: Distribution of 29 responses for Q1–Q3\n","bbox":[338.09,336.001854,538.0586979999999,344.967854],"page":"10"},"14":{"caption":"Figure 14: Racquetball game (left), WOz version (right)\n","bbox":[63.631,375.371854,284.20356599999997,384.337854],"page":"11"},"15":{"caption":"Figure 15: WOz version of Amazon Shopping’s AR view\n","bbox":[62.012,243.734854,285.8212920000001,252.70085400000002],"page":"11"}},"crops":{"1":{"crop_coord":[878.2083383333334,536.5178933333331,1555.5458408333332,803.5444386111111],"bbox":[317.9550018,504.5240021,558.1965027,597.0535584],"page":"1"},"2":{"crop_coord":[878.2083383333334,252.36799611111098,1555.5606649999997,409.2694516666669],"bbox":[317.9550018,646.4629974,558.2018393999999,699.3475214],"page":"3"},"3":{"crop_coord":[878.2083383333334,453.8142933333333,1555.53984,648.7889100000001],"bbox":[317.9550018,560.2359924,558.1943424,626.8268544],"page":"3"},"4":{"crop_coord":[211.17499444444442,252.3626625,755.0457338888888,674.8472341666667],"bbox":[77.822998,550.8549957,270.0164642,699.3494415],"page":"5"},"5":{"crop_coord":[144.43890055555556,741.7095283333335,821.8022116666666,985.1972283333333],"bbox":[53.7980042,439.1289978,294.04879619999997,523.1845698],"page":"5"},"6":{"crop_coord":[183.01112694444444,252.36798805555554,1510.075230277778,916.7055680555554],"bbox":[67.6840057,463.7859955,541.8270829,699.3475243],"page":"6"},"7":{"crop_coord":[144.43890055555556,1642.6312330555554,821.7761716666668,1863.0416786111111],"bbox":[53.7980042,123.1049957,294.0394218,198.85275610000002],"page":"6"},"8":{"crop_coord":[878.2083383333334,252.36708055555567,1548.8580405555554,515.1416694444443],"bbox":[317.9550018,608.348999,555.7888945999999,699.347851],"page":"7"},"9":{"crop_coord":[941.4833152777777,764.9374582666666,1485.3311936777777,1015.3333366666667],"bbox":[340.7339935,428.2799988,532.919229724,514.822515024],"page":"7"},"10":{"crop_coord":[174.34998416666667,382.1682797222221,784.9474491666667,777.1666547222222],"bbox":[64.5659943,514.0200043,280.7810817,652.6194193],"page":"8"},"11":{"crop_coord":[878.2083383333334,1345.9407594444444,1555.53984,1545.6583150000001],"bbox":[317.9550018,237.3630066,558.1943424,305.6613266],"page":"8"},"12":{"crop_coord":[144.43890055555556,583.3062686111112,821.7843461111111,797.6138983333333],"bbox":[53.7980042,506.6589966,294.0423646,580.2097433],"page":"9"},"13":{"crop_coord":[878.2082258536943,609.9512047511108,1612.564508669528,1235.1348346427778],"bbox":[317.95496130733,349.1514595286,578.7232231210301,570.6175662896001],"page":"10"},"14":{"crop_coord":[157.6639047222222,918.5183205555555,801.6279199999999,1125.7750108333335],"bbox":[58.5590057,388.5209961,286.7860512,459.5334046],"page":"11"},"15":{"crop_coord":[157.6639047222222,1199.82218,801.6500227777778,1491.4333258333334],"bbox":[58.5590057,256.8840027,286.7940082,358.2640152],"page":"11"}}}},{"filename":"Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation.pdf","paper_id":"Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation","venue":"chi2019xr","keywords":["Galvanic Vestibular Stimulation","Wearables","Interaction design","Haptic feedback","Virtual Reality","Cybersickness"],"doi":"10.1145/3290605.3300905","paragraph_containing_keyword":"KEYWORDS\nGalvanic Vestibular Stimulation; Wearables; Interaction de-\nsign; Haptic feedback; Virtual Reality; Cybersickness","paragraph_after_keyword":"ACM Reference Format:\nMisha Sra, Abhinandan Jain, and Pattie Maes. 2019. Adding Propri-\noceptive Feedback to Virtual Reality Experiences Using Galvanic\nVestibular Stimulation: Extended Abstract. In CHI Conference on Hu-\nman Factors in Computing Systems Proceedings (CHI 2019), May 4–9,\n2019, Glasgow, Scotland UK. ACM, New York, NY, USA, Article 4,\n14 pages. https://doi.org/10.1145/3290605.3300905","sections":[{"word_count":1250,"figure_citations":{"4":["Figure 4,6) that can help reduce the initial time and effort and enable researchers to quickly focus on the design of novel applications and interaction techniques."],"5":["Figure 5)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":958,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":380,"figure_citations":{},"section_index":2,"title":"CYBERSICKNESS VS SIMULATOR SICKNESS VS"},{"word_count":2087,"figure_citations":{"1":["Figure 1 shows four example application scenarios for GVS induced feedback."],"2":["Figure 2) and tested it with six participants (5 females, average age 22.","Figure 2)."],"3":["Figure 3 shows the second GVS device we created after receiving feedback from the pilot study with the first prototype (Figure 2)."],"4":["Figure 4).","Figure 4)."],"5":["Figure 5) as zero-mean current noise of an imperceptible magnitude.","Figure 5 shows the mastoid, which is the back part of the temporal bones situated at the sides and base of the skull."],"6":["Figure 6 shows the opened up neckband with the PCB and the connected GVS electrodes, EDA electrodes and the HR sensor.","Figure 6 shows EDA and HR sensors connected to the PCB through the two bands of black and white wires."]},"section_index":3,"title":"GVS FOR VR"},{"word_count":1141,"figure_citations":{"7":["Figure 7)."]},"section_index":4,"title":"EVALUATION"},{"word_count":1491,"figure_citations":{"8":["Figure 8 shows the boxplot of 20 participants’ scores for both trials."],"9":["Figure 9 shows the results of Flow in the two conditions."],"11":["Figure 11)."],"12":["Figure 12)."]},"section_index":5,"title":"RESULTS"},{"word_count":777,"figure_citations":{"8":["Figure 8)."]},"section_index":6,"title":"DISCUSSION"},{"word_count":100,"figure_citations":{},"section_index":7,"title":"LIMITATIONS AND FUTURE WORK"},{"word_count":136,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":2023,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation","authors":"Misha Sra, Abhinandan Jain, Pattie Maes","abstract":"We present a small and lightweight wearable device that enhances virtual reality experiences and reduces cybersickness by means of galvanic vestibular stimulation (GVS). GVS is a specific way to elicit vestibular reflexes that has been used for over a century to study the function of the vestibular system. In addition to GVS, we support physiological sensing by connecting heart rate, electrodermal activity and other sensors to our wearable device using a plug and play mechanism. An accompanying Android app communicates with the device over Bluetooth (BLE) for transmitting the GVS stimulus to the user through electrodes attached behind the ears. Our system supports multiple categories of virtual reality applications with different types of virtual motion such as driving, navigating by flying, teleporting, or riding. We present a user study in which participants (N = 20) experienced significantly lower cybersickness when using our device and rated experiences with GVS-induced haptic feedback as significantly more immersive than a no-GVS baseline.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Our GVS device induces proprioceptive feedback\nfor VR motions: (a) riding a roller coaster, (b) driving a car,\n(c) leaping forward, and (d) navigating by flying.\n","bbox":[53.52,520.3508540000001,295.12685600000003,551.2338540000001],"page":"5"},"2":{"caption":"Figure 2: The first GVS prototype built using a Light Blue\nBean Board with a custom current driver circuit board.\n","bbox":[317.955,458.795854,558.190004,478.72085400000003],"page":"5"},"3":{"caption":"Figure 3: The second and final GVS prototype in the form of\na small wearable device that sits on the user’s neck, similar\nto behind-the-head earphones. The red wire connects the cir-\ncuit to an electrode behind the user’s ear. A similar wire con-\nnects to a second electrode behind the other ear. The white\nwires connect to EDA and HR sensors.\n","bbox":[53.449000000000005,460.69785399999995,295.637918,524.457854],"page":"6"},"4":{"caption":"Figure 4: GVS prototype with the 3D printed housing, PCB,\nLiPo battery, sensor ports, and the electrode connectors.\n","bbox":[317.955,504.39085400000005,559.283856,524.3158540000001],"page":"6"},"5":{"caption":"Figure 5: We attach electrodes to the mastoid process on the\ntemporal bone behind each ear to electrically stimulate the\nvestibular system. Image courtesy: Wikipedia\n","bbox":[53.565000000000005,522.2498540000001,294.033004,553.133854],"page":"7"},"6":{"caption":"Figure 6: Our GVS device without the 3D printed housing.\nThe image shows the EDA and HR sensors plugged into the\nPCB using the onboard connection ports.\n","bbox":[317.659,490.33085400000004,559.72319,521.2148540000001],"page":"7"},"7":{"caption":"Figure 7: VR scenes for the with-GVS and without-GVS trials.\n(a) Full visibility, breezy sunny day. (b) Full visibility, over-\ncast day with occasional thunder in the distance.\n","bbox":[53.52,252.55685400000002,295.63786400000004,283.439854],"page":"8"},"8":{"caption":"Figure 8: Participants had significantly higher presence\nwhen using the GVS device than without.\n","bbox":[53.449000000000005,395.547854,294.03300399999995,415.472854],"page":"10"},"9":{"caption":"Figure 9: A Pairwise Wilcoxon test shows Flow was signifi-\ncantly higher in VR with the GVS device.\n","bbox":[317.955,507.949854,559.794918,527.874854],"page":"10"},"10":{"caption":"Figure 10: Positive Affect was significantly higher and there\nwas no significant difference in the Negative Affect between\nthe with-GVS and without-GVS conditions.\n","bbox":[317.605,80.88785399999999,558.190808,111.77185399999999],"page":"10"},"11":{"caption":"Figure 11: Perceived realism of experience is higher as indi-\ncated by responses to related questions in the SUS inventory\nfor with-GVS and without-GVS conditions.\n","bbox":[317.955,585.2318540000001,559.7957220000001,616.115854],"page":"11"},"12":{"caption":"Figure 12: Results show that 85% of the users preferred the\nVR experience with the GVS device.\n","bbox":[53.484,581.4308540000001,294.03300399999995,601.355854],"page":"12"}},"crops":{"1":{"crop_coord":[144.43890055555556,252.3726655555555,821.7455463888889,638.4277683333333],"bbox":[53.7980042,563.9660034,294.0283967,699.3458404],"page":"5"},"2":{"crop_coord":[911.5777841666668,379.79652416666664,1522.1819475,839.8555672222221],"bbox":[329.9680023,491.4519958,546.1855011,653.4732513],"page":"5"},"3":{"crop_coord":[177.80834611111112,252.3555416666668,788.4083555555557,712.8055488888889],"bbox":[65.8110046,537.1900024,282.027008,699.352005],"page":"6"},"4":{"crop_coord":[911.5777841666668,252.37414888888878,1522.1821169444445,713.1999883333333],"bbox":[329.9680023,537.0480042,546.1855621,699.3453064],"page":"6"},"5":{"crop_coord":[277.9083336111111,252.36694333333315,688.3249833333333,633.1527624999999],"bbox":[101.8470001,565.8650055,245.996994,699.3479004000001],"page":"7"},"6":{"crop_coord":[911.5777841666668,252.3746577777777,1522.1555413888889,721.8166691666668],"bbox":[329.9680023,533.9459991,546.1759949,699.3451232],"page":"7"},"7":{"crop_coord":[211.17499444444442,811.7574988888888,755.0250072222221,1382.3000080555555],"bbox":[77.822998,296.1719971,270.0090026,497.9673004],"page":"8"},"8":{"crop_coord":[144.43890055555556,564.599109166667,821.7889066666667,1015.544458888889],"bbox":[53.7980042,428.2039948,294.0440064,586.9443206999999],"page":"10"},"9":{"crop_coord":[878.2083383333334,252.37130916666655,1555.5583444444446,703.3166588888888],"bbox":[317.9550018,540.6060028,558.201004,699.3463287000001],"page":"10"},"10":{"crop_coord":[880.9333461111112,1666.0769652777778,1204.572668888889,1859.1583166666667],"bbox":[318.9360046,124.503006,431.8461608,190.41229249999998],"page":"10"},"11":{"crop_coord":[1229.1805352777778,1667.0774333333334,1552.8198580555556,1859.1583166666667],"bbox":[444.3049927,124.503006,557.2151489,190.052124],"page":"10"},"12":{"crop_coord":[880.9333461111112,254.25865166666688,1204.572838611111,458.19998000000004],"bbox":[318.9360046,628.8480072,431.8462219,698.6668854],"page":"11"},"13":{"crop_coord":[1229.1805352777778,252.35999222222216,1552.8305394444444,458.19998000000004],"bbox":[444.3049927,628.8480072,557.2189942,699.3504028],"page":"11"},"14":{"crop_coord":[244.5444402777778,252.36376444444448,721.68396,499.20276222222213],"bbox":[89.8359985,614.0870056,258.0062256,699.3490448],"page":"12"}}}},{"filename":"Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality.pdf","paper_id":"Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality","venue":"chi2019xr","keywords":["Affinity diagrams","Visual analytics","Augmented reality"],"paragraph_containing_keyword":"supports easy switching between qualitative and quantita-\ntive ‘views’ of data, without surrendering the lightweight\nbenefits of existing AD practice.\nCCS CONCEPTS\n• Human-centered computing → HCI design and eval-\nuation methods; Visual analytics; Mixed / augmented re-\nality; Visualization systems and tools.\nKEYWORDS\naffinity diagrams; visual analytics; augmented reality\nACM Reference Format:\nHariharan  Subramonyam,  Steven  M.  Drucker,  and  Eytan  Adar. \n2019. Affinity Lens: Data-Assisted Affinity Diagramming with Aug-\nmented Reality. In Proceedings of CHI Conference on Human Factors \nin Computing Systems Proceedings (CHI 2019) May 4–9, 2019, \nGlasgow, Scotland UK. ACM, New York, NY, USA, 13 pages. https://\ndoi.org/10.1145/3290605.3300628\n1 \nINTRODUCTION\nAffinity Diagrams (AD) and related approaches are the method \nof choice for many designers and UX researchers. AD sup-\nports analysis and synthesis of interview notes, brainstorm-\ning, creating user personas, and evaluating interactive proto-\ntypes [24]. Notes can be placed on walls or surfaces in a way \nthat leverages spatial cognition, offers flexibility in grouping \nand clustering, and then physically persists. Both individu-\nals and groups can participate on large shared surfaces. AD \nusers  work  to  derive  structure  from  inherently  fuzzy  and \nseemingly unstructured input. Though software tools have","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request\npermissions from permissions@acm.org.\nCHI 2019, May 4–9, 2019, Glasgow, Scotland UK\n© 2019 Association for Computing Machinery.\nACM ISBN 978-1-4503-5970-2/19/05. . . $15.00\nhttps://doi.org/10.1145/3290605.3300628","doi":"10.1145/3290605.3300628","sections":[{"word_count":674,"figure_citations":{"1":["Figure 1a).","Figure 1b), the designer can use the phone to look at distributions of sleeping schedules for each cluster (Figure 1c)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1029,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1070,"figure_citations":{},"section_index":2,"title":"A DESIGN PROBE FOR DAAD"},{"word_count":475,"figure_citations":{},"section_index":3,"title":"DESIGN GUIDELINES"},{"word_count":547,"figure_citations":{"2":["Figure 2 captures the four main regions of the mobile interface: the largest, is dedicated to the camera and visualization augmentation (a), a contextual menu occupies the right edge of the display (b) and dynamically changes depending on what is present in the camera’s field of view, a data attribute Paper 398 CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK a b d cook eatout housing emp grade exercise c Figure 2: Affinity Lens User Interface."],"3":["Figure 3) we follow a designer, Dave, as he uses DAAD to analyze the food choice dataset (this example is based on a combination of real use cases from our user studies).","Figure 3a).","Figure 3b)."],"4":["Figure 4 a)."]},"section_index":4,"title":"USER EXPERIENCE"},{"word_count":890,"figure_citations":{"3":["Figure 3 d) so he can continue working without pointing at the physical notes (D2, D3).","Figure 3 e) and alerts him that all but one student in the on-campus sub-cluster are first years (D4)."],"4":["Figure 4 b).","Figure 4 f).","Figure 4) to support common tasks."]},"section_index":5,"title":"PRINT"},{"word_count":2,"figure_citations":{},"section_index":6,"title":"STILL IMAGE"},{"word_count":1140,"figure_citations":{"1":["Figure 1a)."],"4":["Figure 4i).","Figure 4c) displays those data values that are the same and those that are different (a weak representation of affinity).","Figure 4d) will generate an overlay of common words (sized by frequency) on top of the notes.","Figure 4g).","Figure 4h)."]},"section_index":7,"title":"LIVE"},{"word_count":979,"figure_citations":{"5":["Figure 5, Affinity Lens is comprised of five main components: (1) Scene Analyzer, (2) Lens Controller, (3) Dynamic View Configurator, (4) lenses, and (5) the Data Access and Analytics Module."]},"section_index":8,"title":"SYSTEM ARCHITECTURE"},{"word_count":1834,"figure_citations":{},"section_index":9,"title":"EVALUATION"},{"word_count":462,"figure_citations":{},"section_index":10,"title":"DISCUSSION AND FUTURE WORK"},{"word_count":130,"figure_citations":{},"section_index":11,"title":"CONCLUSION"},{"word_count":475,"figure_citations":{},"section_index":12,"title":"REFERENCES"},{"word_count":1068,"figure_citations":{},"section_index":13,"title":"ACKNOWLEDGMENTS"}],"title":"Affinity Lens: Data-Assisted Affinity Diagramming with Augmented Reality","authors":"Hariharan Subramonyam, Steven M. Drucker, Eytan Adar","abstract":"Despite the availability of software to support Affinity Diagramming (AD), practitioners still largely favor physical sticky-notes. Physical notes are easy to set-up, can be moved around in space and offer flexibility when clustering un-structured data. However, when working with mixed data sources such as surveys, designers often trade off the physicality of notes for analytical power. We propose AffinityLens, a mobile-based augmented reality (AR) application for Data-Assisted Affinity Diagramming (DAAD). Our application provides just-in-time quantitative insights overlaid on physical notes. Affinity Lens uses several different types of AR overlays (called lenses) to help users find specific notes, cluster information, and summarize insights from clusters. Through a formative study of AD users, we developed design principles for data-assisted AD and an initial collection of lenses. Based on our prototype, we find that Affinity Lens supports easy switching between qualitative and quantitative 'views' of data, without surrendering the lightweight benefits of existing AD practice.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Affinity Lens used to split a larger affinity cluster based on income level. (a) The user applies a heatmap lens to an\nexisting cluster which shows two sub-groups. (b) The designer regroups the notes. (c) A histogram lens compares sleeping\nschedules for the two sub-clusters found in (a).\n","bbox":[53.798,428.230854,558.1803300000003,459.11485400000004],"page":"1"},"2":{"caption":"Figure 2: Affinity Lens User Interface. (a) main camera view,\n(b) contextual lens selector, (c) lens configuration options, (d)\nlens modes\n","bbox":[317.67699999999996,552.570854,559.283856,583.454854],"page":"5"},"3":{"caption":"Figure 3: Affinity Lens workflow. Data is acquired (a) and automatically tagged for a Marker (b) for printing. Various forms of\nDAAD (c, d, e) can be documented (f) along with associated insights (g).\n","bbox":[53.798,499.98285400000003,558.1803299999998,519.907854],"page":"6"},"4":{"caption":"Figure 4: A sampling of Affinity Lens AR Lenses\n","bbox":[208.8,416.107854,403.19184600000006,425.073854],"page":"7"},"5":{"caption":"Figure 5: System Architecture. (1) Scene analyzer extracts notes from camera feed, (2) lens controller determines set of lenses\napplicable to notes in view, (3) dynamic view configurator updates the interface with available lenses, (4) lens queries for data\nfrom the (5) Data access and analytics module, and renders the augmented visualization.\n","bbox":[53.798,523.7888540000001,558.18033,554.672854],"page":"9"}},"crops":{"1":{"crop_coord":[144.43890055555556,589.3152549486113,1555.5387790130555,894.316686111111],"bbox":[53.7980042,471.845993,558.1939604447,578.0465082185],"page":"1"},"2":{"crop_coord":[896.924981111111,252.36789550222198,1536.854481462222,548.9277649999999],"bbox":[324.6929932,596.1860046,551.4676133263999,699.3475576192001],"page":"5"},"3":{"crop_coord":[195,252.36637307027792,1505.0216103694445,725.4444461111111],"bbox":[72,532.6399994,540.007779733,699.3481056946999],"page":"6"},"4":{"crop_coord":[164.9999913888889,252.366770525,1535.029234836111,988.8750202777778],"bbox":[61.1999969,437.8049927,550.810524541,699.347962611],"page":"7"},"5":{"crop_coord":[245,252.36644967999996,1454.9823882511112,628.8750033333332],"bbox":[90,567.4049988,521.9936597704,699.3480781152],"page":"9"}}}},{"filename":"Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions.pdf","paper_id":"Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions","venue":"VR_2017","keywords":["Presence","Ambient Temperature","Virtual Wind","Virtual Reality","Multimodal Interaction"],"paragraph_containing_keyword":"Author Keywords\nPresence; Ambient Temperature; Virtual Wind; Virtual\nReality; Multimodal Interaction","paragraph_after_keyword":"INTRODUCTION\nSimulating environmental conditions by utilizing multisen-\nsory stimuli is essential to providing sensory-rich experiences\nand enhancing immersion in Virtual Reality (VR). However,\ndespite ongoing research and development, the majority of\nexisting VR systems utilize interfaces that do not provide users\nwith multisensory feedback that accurately mirrors their ex-\nperiences of environmental conditions in the real-world. This\nlack of additional environmental feedback restricts potential","doi":"10.1145/3025453.3025723","sections":[{"word_count":675,"figure_citations":{"1":["Figure 1, the system is equipped with 1) an Ambient Temperature Simulation Module that utilizes Peltier elements to provide heating and cooling sensations (worn on the neck) and 2) a Wind Simulation Module that utilizes multidirectional fans focused towards the user’s face."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1326,"figure_citations":{},"section_index":1,"title":"BACKGROUND AND RELATED WORK"},{"word_count":2078,"figure_citations":{"2":["Figure 2, were chosen for the study due to their proximity to the thermoregulatory centre of the central nervous system [15]."],"3":["Figure 3 (A)).","Figure 3 (B))."],"4":["Figure 4 (A): sensation scores).","Figure 4 (B): sensation scores).","Figure 4 (A): comfort scores).","Figure 4 (B): comfort scores)."]},"section_index":2,"title":"STIMULI DESIGN CONSIDERATIONS"},{"word_count":383,"figure_citations":{},"section_index":3,"title":"SYSTEM IMPLEMENTATION"},{"word_count":2972,"figure_citations":{"7":["Figure 7 (A)) and a snowy mountain environment (Figure 7 (B)) for 30 seconds each, presented in a randomized order."],"10":["Figure 10)."]},"section_index":4,"title":"EVALUATION"},{"word_count":761,"figure_citations":{},"section_index":5,"title":"DISCUSSION AND FUTURE WORK"},{"word_count":146,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":34,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGMENTS"},{"word_count":1106,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Ambiotherm: Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions","authors":"Nimesha Ranasinghe, Pravar Jain, Shienny Karwita, David Tolley, Ellen Yi-Luen Do","abstract":"In this paper, we present and evaluate Ambiotherm, a wearable accessory for Head Mounted Displays (HMD) that provides thermal and wind stimuli to simulate real-world environmental conditions, such as ambient temperatures and wind conditions, to enhance the sense of presence in Virtual Reality (VR). Ambiotherm consists of a Ambient Temperature Module that is attached to the user's neck, a Wind Simulation Module focused towards the user's face, and a Control Module utilizing Bluetooth communication. We demonstrate Ambiotherm with two VR environments, a hot desert, and a snowy mountain, to showcase the different types of simulated environmental conditions. We conduct several studies to 1) address design factors of the system and 2) evaluate Ambiotherm's effect on factors related to a user's sense of presence. Our findings show that the addition of wind and thermal stimuli significantly improves sensory and realism factors, contributing towards an enhanced sense of presence when compared to traditional VR experiences.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. A participant is using the Ambiotherm system with a Sam-\nsung™ Gear VR HMD. The control module is attached behind the neck.\n","bbox":[321.094,415.20027000000005,565.55781,432.13727],"page":"1"},"2":{"caption":"Figure 2. Different locations selected for delivering thermal stimuli: (A)\nBehind The Ear, (B) On The Throat, and (C) Behind The Neck.\n","bbox":[53.929,600.47227,297.5320500000002,617.40827],"page":"4"},"3":{"caption":"Figure 3. Normalized average rating scores: (A) heating sensation at\nthree locations and (B) cooling sensation at three locations. (Error bars\nrepresent 95% CI, n = 15).\n","bbox":[321.094,566.34827,564.1630600000001,592.25127],"page":"4"},"4":{"caption":"Figure 4. Normalized average sensation and comfort scores: (A) four\nheating stimuli and (B) four cooling stimuli. (Error bars represent 95%\nCI, n = 15).\n","bbox":[321.094,562.5232699999999,564.9600600000001,588.4262699999999],"page":"5"},"5":{"caption":"Figure 5. Operational procedure of Ambiotherm: (1) VR environment\n(Android Application) sends commands to (2) control module that ac-\ntives (3a) thermal module and (3b) wind module.\n","bbox":[53.666000000000004,510.76827,298.3210900000002,536.6712699999999],"page":"6"},"6":{"caption":"Figure 6. Ambiotherm: (A) Main components, (B) Control Module, (C)\nWind Simulation Module, and (D) Ambient Temperature Module.\n","bbox":[320.69599999999997,457.47227000000004,564.6970500000001,474.40927],"page":"6"},"7":{"caption":"Figure 7. User’s ﬁrst-person view following the virtual guide in (A) the\nhot desert and (B) the snowy mountain virtual environments.\n","bbox":[53.929,608.5912699999999,296.9980600000001,625.5272699999999],"page":"7"},"8":{"caption":"Figure 8. Normalized average Sensory and Realism Factors scores for\nfour stimuli conﬁgurations (Error bars represent 95% CI, n = 20).\n","bbox":[321.094,592.23527,564.3384000000002,609.17227],"page":"7"},"9":{"caption":"Figure 9. Normalized average engagement scores for four stimuli conﬁg-\nurations (Error bars represent 95% CI, n = 20).\n","bbox":[53.929,635.37727,298.32108000000017,652.31427],"page":"8"},"10":{"caption":"Figure 10. Normalized average thermal, wind, and overall Involvement\nscores for four stimuli conﬁgurations (Error bars represent 95% CI, n =\n20).\n","bbox":[321.094,532.41827,564.1630600000002,558.32127],"page":"8"},"11":{"caption":"Figure 11. Normalized average consistency scores of VR Environment\ncompared to Real-World and consistency scores for sensory information\nfor four stimuli conﬁgurations (Error bars represent 95% CI, n = 20).\n","bbox":[53.929,575.79027,296.99806000000024,601.69327],"page":"9"},"12":{"caption":"Figure 12. Normalized average scores assessing the quickness to adjust\nto the VR Environment with four stimuli conﬁgurations (Error bars rep-\nresent 95% CI, n = 20).\n","bbox":[321.094,602.14927,565.4860800000001,628.05227],"page":"9"}},"crops":{"1":{"crop_coord":[886.9277613888887,554.8271263888888,1572.107009722222,991.6888683333333],"bbox":[321.0939941,436.7920074,564.1585235,590.4622345],"page":"1"},"2":{"crop_coord":[155.26946166666667,168.22243583333318,819.5412444444445,477.0472125],"bbox":[57.6970062,622.0630035,293.234848,729.6399231],"page":"4"},"3":{"crop_coord":[886.9277613888887,168.2234952777779,1572.1410452777777,546.927761388889],"bbox":[321.0939941,596.9060059,564.1707762999999,729.6395417],"page":"4"},"4":{"crop_coord":[886.9277613888887,168.22442777777752,1572.1399008333333,557.5527952777777],"bbox":[321.0939941,593.0809937,564.1703643,729.6392060000001],"page":"5"},"5":{"crop_coord":[144.80278027777777,168.23413444444475,829.9388802777778,701.3194530555556],"bbox":[53.9290009,541.3249969,296.9779969,729.6357115999999],"page":"6"},"6":{"crop_coord":[886.9277613888887,168.2289208333334,1572.125066111111,874.2694347222223],"bbox":[321.0939941,479.0630035,564.1650238,729.6375885],"page":"6"},"7":{"crop_coord":[144.80278027777777,168.22811555555552,830.0223797222222,454.4944255555557],"bbox":[53.9290009,630.1820068,297.0080567,729.6378784],"page":"7"},"8":{"crop_coord":[886.9277613888887,168.2301499999999,1572.109764722222,499.9277666666665],"bbox":[321.0939941,613.826004,564.1595153,729.637146],"page":"7"},"9":{"crop_coord":[144.80278027777777,168.22485194444445,830.0066547222222,380.0861274999998],"bbox":[53.9290009,656.9689941,297.00239569999997,729.6390533],"page":"8"},"10":{"crop_coord":[886.9277613888887,168.22374999999997,1572.1331616666666,641.1777836111111],"bbox":[321.0939941,562.9759979,564.1679382,729.63945],"page":"8"},"11":{"crop_coord":[144.80278027777777,168.2240041666668,830.0142416666666,520.6999800000001],"bbox":[53.9290009,606.3480072,297.005127,729.6393585],"page":"9"},"12":{"crop_coord":[886.9277613888887,168.22315638888887,1572.136679722222,447.4805536111111],"bbox":[321.0939941,632.7070007,564.1692046999999,729.6396637],"page":"9"}}}},{"filename":"Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories.pdf","paper_id":"Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories","venue":"chi2019xr","keywords":["Virtual Reality","Locomotion","Teleportation","Orientation Indication","Virtual Environments","Point & Teleport"],"doi":"10.1145/3290605.3300377","paragraph_containing_keyword":"KEYWORDS \nVirtual Reality, Locomotion, Teleportation, Orientation Indi-\ncation, Virtual Environments, Point & Teleport \nACM Reference Format: \nMarkus Funk, Florian Müller, Marco Fendrich, Megan Shene, Moritz \nKolvenbach, Niclas Dobbertin, Sebastian Günther, Max Mühlhäuser. \n2019. Assessing the Accuracy of Point & Teleport Locomotion with \nOrientation Indication for Virtual Reality using Curved Trajectories. \nIn CHI Conference on Human Factors in Computing Systems Proceed-\nings (CHI 2019), May 4–9, 2019, Glasgow, Scotland UK. ACM, New \nYork, NY, USA, 12 pages. https://doi.org/10.1145/3290605.3300377","sections":[{"word_count":410,"figure_citations":{"1":["Figure 1)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":2227,"figure_citations":{"2":["Figure 2)."],"3":["Figure 3)."],"4":["Figure 4)."],"5":["Figure 5)."],"6":["Figure 6)."]},"section_index":1,"title":"RELATED WORK"},{"word_count":2822,"figure_citations":{"8":["Figure 8)."]},"section_index":2,"title":"EVALUATION"},{"word_count":959,"figure_citations":{},"section_index":3,"title":"DISCUSSION"},{"word_count":190,"figure_citations":{},"section_index":4,"title":"LIMITATIONS"},{"word_count":257,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":18,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":1672,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories","authors":"Markus Funk, Florian Müller, Marco Fendrich, Megan Shene, Moritz Kolvenbach, Niclas Dobbertin, Sebastian Günther, Max Mühlhäuser","abstract":"Room-scale Virtual Reality (VR) systems have arrived in users' homes where tracked environments are set up in limited physical spaces. As most Virtual Environments (VEs) are larger than the tracked physical space, locomotion techniques are used to navigate in VEs. Currently, in recent VR games, point &amp; teleport is the most popular locomotion technique. However, it only allows users to select the position of the teleportation and not the orientation that the user is facing after the teleport. This results in users having to manually correct their orientation after teleporting and possibly getting entangled by the cable of the headset. In this paper, we introduce and evaluate three different point &amp; teleport techniques that enable users to specify the target orientation while teleporting. The results show that, although the three teleportation techniques with orientation indication increase the average teleportation time, they lead to a decreased need for correcting the orientation after teleportation.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: A user is teleporting herself in a Virtual Environment using the Curved Teleport. It allows her to teleport around an \nobstacle and graphically choose the orientation, which she wants to face after teleportation only by using the curved trajectory \nvisualization with orientation indication, and without having to turn her body in the physical world. \n","bbox":[53.555907200000036,364.42088,560.6865248,395.4266912],"page":"1"},"2":{"caption":"Figure 2: A user is using the Linear Teleport to move onto a \ntarget. The orientation that the user is facing is the forward \nvector of the straight teleport line. \n","bbox":[53.56487360000003,592.6728800000001,296.28532160000003,623.6786912],"page":"3"},"3":{"caption":"Figure 3: The Parabola Teleport uses a parabola shaped visu-\nalization to indicate the target position of the teleport. After \nthe teleport, users face the forward vector of the teleport. \n","bbox":[317.9549999999999,592.6728800000001,560.6306159999999,623.6786912],"page":"3"},"4":{"caption":"Figure 4: The AngleSelect Teleport uses a parabola visualiza-\ntion to show the user the target position. Further it uses an \norientation indicator that lets the user select the orientation \nthat the user is facing after the teleport. \n","bbox":[53.798,581.7159392000001,296.294288,623.6786912],"page":"4"},"5":{"caption":"Figure 5: The Curved Teleport visualizes the trajectory of the \nteleportation in a curved line. The orientation that the user \nis facing at the target location can be infuenced by adjusting \nthe steepness of the curve. \n","bbox":[317.955,581.7159392000001,560.6395823999999,623.6786912],"page":"4"},"6":{"caption":"Figure  6:  The  visualization  of  the  HPCurved  Teleport  frst \nuses a parabola that behaves like a state-of-the-art Parabola \nTeleport, but at the high point of the parabola turns into a \nCurved Teleport that lets the users chose the target orienta-\ntion. \n","bbox":[53.79799999999997,570.7589984000001,296.28532160000003,623.6786912],"page":"5"},"7":{"caption":"Figure 7: We used round targets in our accuracy study. The \ndirection of the target is indicated by the green target area \nand supported by the yellow target area. The rest of the tar-\nget is colored in red indicating the wrong direction. In this \nscreenshot of our accuracy study, the participant is using the \nParabola Teleport. (Picture of participant added for clarity). \n","bbox":[317.95499999999976,557.5741584000001,560.4423215999999,621.4776912],"page":"5"},"8":{"caption":"Figure 8: The VE that we created as the environment for con-\nducting the study. The castle in the middle of the environ-\nment is the tutorial area, where participants can practice the \nteleportation method before starting the study. Once partic-\nipants leave the tutorial castle, it disappears and becomes a \ngreen lawn area. \n","bbox":[53.79799999999986,494.5650576000001,296.2853215999999,558.4416912],"page":"6"},"9":{"caption":"Figure 9: The average angle αcor r ect ed  that was corrected by \nthe participants using each teleportation method. All error \nbars  indicate  the  standard  error.  All  conditions  are  signif-\nicantly  diferent,  except  the  two  baseline  conditions  (indi-\ncated with n.s.). \n","bbox":[53.798,516.4880992000001,296.54077120000005,569.4346912],"page":"7"},"10":{"caption":"Figure  10:  The  average  time  that  participants  took  to  tele-\nport tt el epor t  using each teleportation method. All error bars \nindicate the standard error. All teleportation methods were \nsignifcantly diferent, except the ones indicated with n.s. \n","bbox":[317.9549999999999,527.3174512,560.4428224,569.4346912],"page":"7"},"11":{"caption":"Figure 11: The average time that participants took to correct \ntheir position tcor r ect ed  using natural walking for each tele-\nportation method. All error bars indicate the standard error. \nThe asterisk (*) indicates a statistically signifcant diference \nbetween the teleportation methods. \n","bbox":[53.49799680000001,516.5129392000001,297.81446400000004,569.4346912],"page":"8"},"12":{"caption":"Figure 12: The NASA-TLX scores of the VR locomotion tech-\nniques that were assessed in the user study. All error bars \nindicate the standard error. The asterisk (*) indicates a sig-\nnifcant diference between the locomotion techniques. \n","bbox":[317.9549999999998,527.4719392000001,560.4423215999999,569.4346912],"page":"8"}},"crops":{"1":{"crop_coord":[144.43890055555556,693.1536863888891,1555.554911388889,1071.5777841666668],"bbox":[53.7980042,408.0319977,558.1997681,540.6646728999999],"page":"1"},"2":{"crop_coord":[144.43890055555556,252.3694441666666,821.7722491666666,437.54445388888894],"bbox":[53.7980042,636.2839966,294.0380097,699.3470001000001],"page":"3"},"3":{"crop_coord":[878.2083383333334,252.3694441666666,1555.5416869444446,437.54445388888894],"bbox":[317.9550018,636.2839966,558.1950073,699.3470001000001],"page":"3"},"4":{"crop_coord":[144.43890055555556,252.3694441666666,821.7722491666666,437.54445388888894],"bbox":[53.7980042,636.2839966,294.0380097,699.3470001000001],"page":"4"},"5":{"crop_coord":[878.2083383333334,252.3694441666666,1555.5416869444446,437.54445388888894],"bbox":[317.9550018,636.2839966,558.1950073,699.3470001000001],"page":"4"},"6":{"crop_coord":[144.43890055555556,252.3694441666666,821.7722491666666,437.54445388888894],"bbox":[53.7980042,636.2839966,294.0380097,699.3470001000001],"page":"5"},"7":{"crop_coord":[878.2083383333334,252.36762166666665,1555.5580477777778,443.65555666666666],"bbox":[317.9550018,634.0839996,558.2008972,699.3476562],"page":"5"},"8":{"crop_coord":[144.43890055555556,252.37050361111108,821.800096388889,618.7583413888889],"bbox":[53.7980042,571.0469971,294.0480347,699.3466187],"page":"6"},"9":{"crop_coord":[144.43890055555556,252.36808777777793,821.7882286111111,588.2222408333333],"bbox":[53.7980042,582.0399933,294.04376229999997,699.3474884],"page":"7"},"10":{"crop_coord":[878.2083383333334,252.36808777777793,1555.557666388889,588.2222408333333],"bbox":[317.9550018,582.0399933,558.2007599,699.3474884],"page":"7"},"11":{"crop_coord":[144.43890055555556,252.36808777777793,821.7882286111111,588.2222408333333],"bbox":[53.7980042,582.0399933,294.04376229999997,699.3474884],"page":"8"},"12":{"crop_coord":[878.2083383333334,252.36808777777793,1555.557666388889,588.2222408333333],"bbox":[317.9550018,582.0399933,558.2007599,699.3474884],"page":"8"}}}},{"filename":"Augmented Reality Views for Occluded Interaction","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Augmented Reality Views for Occluded Interaction.pdf","paper_id":"Augmented Reality Views for Occluded Interaction","venue":"chi2019xr","keywords":["Augmented reality","Manipulation task","Finger-camera"],"paragraph_containing_keyword":"CCS CONCEPTS\n• Human-centered computing → Mixed / augmented\nreality; Empirical studies in HCI .\nKEYWORDS\nAugmented reality, manipulation task, finger-camera\nACM Reference Format:\nKlemen Lilija, Henning Pohl, Sebastian Boring, and Kasper Hornbæk.\n2019. Augmented Reality Views for Occluded Interaction. In ACM\nConference on Human Factors in Computing Systems Proceedings (CHI\n2019), May 4–9, 2019, Glasgow, Scotland, UK. ACM, New York, NY,\nUSA, 12 pages. https://doi.org/10.1145/3290605.3300676","doi":"10.1145/3290605.3300676","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nCHI 2019, May 4–9, 2019, Glasgow, Scotland, UK\n© 2019 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 978-1-4503-5970-2/19/05. . . $15.00\nhttps://doi.org/10.1145/3290605.3300676","sections":[{"word_count":429,"figure_citations":{"1":["Figure 1)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":983,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":426,"figure_citations":{},"section_index":2,"title":"TYPES OF OCCLUDED INTERACTION"},{"word_count":774,"figure_citations":{"1":["Figure 1)."]},"section_index":3,"title":"VISUALIZING OCCLUDED OBJECTS"},{"word_count":1733,"figure_citations":{"2":["Figure 2) were as follows: Pressing a Light Switch: Required either pressing (changing the state of) the left button, right button or both of the buttons on a two-button light switch.","Figure 2)."],"3":["Figure 3a).","Figure 3b).","Figure 3c), to the left of the participants.","Figure 3a)."],"4":["Figure 4)."]},"section_index":4,"title":"EVALUATING OCCLUDED INTERACTION VIEWS"},{"word_count":2314,"figure_citations":{"5":["Figure 5, the manipulation delay differed between the views.","Figure 5 also shows how this delay differed depending on the task."],"6":["Figure 6).","Figure 6)."],"7":["Figure 7 shows how the view influenced the two kinds of manipulation Paper 446 Dynamic camera Cloned 3D See-through −0.","Figure 7 also shows the interactions between task and error."],"8":["Figure 8).","Figure 8)."]},"section_index":5,"title":"RESULTS"},{"word_count":869,"figure_citations":{},"section_index":6,"title":"DISCUSSION"},{"word_count":107,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":25,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGMENTS"},{"word_count":1343,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Augmented Reality Views for Occluded Interaction","authors":"Klemen Lilija, Henning Pohl, Sebastian Boring, Kasper Hornbæk","abstract":"We rely on our sight when manipulating objects. When objects are occluded, manipulation becomes difficult. Such occluded objects can be shown via augmented reality to re-enable visual guidance. However, it is unclear how to do so to best support object manipulation. We compare four views of occluded objects and their effect on performance and satisfaction across a set of everyday manipulation tasks of varying complexity. The best performing views were a see-through view and a displaced 3D view. The former enabled participants to observe the manipulated object through the occluder, while the latter showed the 3D view of the manipulated object offset from the object's real location. The worst performing view showed remote imagery from a simulated hand-mounted camera. Our results suggest that alignment of virtual objects with their real-world location is less important than an appropriate point-of-view and view stability.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: In some situations, we need to manipulate objects out of our sight. We investigate how different views of occluded\nobjects support users during manipulation tasks. An example of such a task is plugging in an HDMI cable. While the port is\nnormally out of sight, see-through view (middle) and displaced 3D view (right) enable visual feedback during interactions.\n","bbox":[53.798,451.113,558.180401728,481.997],"page":"1"},"2":{"caption":"Figure 2: During the study participants performed five different tasks (left to right): pressing one of two light switches, rotating\na dial, dragging a slider, plugging an HDMI stick into one of four ports, and placing a key fob onto one of three hooks.\n","bbox":[53.797999999999945,569.4479560000001,558.200172,591.47],"page":"5"},"3":{"caption":"Figure 3: Four of the views used in the experiment: A) Static\ncamera, b) Dynamic camera, c) Cloned 3D and d) See-through\nview. No visualization view is not shown, as it does not render\nanything in user’s field of view.\n","bbox":[317.722,82.95899999999999,558.3784059999999,124.80199999999999],"page":"5"},"4":{"caption":"Figure 4: During the study, participants were seated in front\nof a 1 m3 frame. They reached in from the right side and\ninteracted with objects placed on a wall, that was tilted at a\n30 ° angle. Movement inside the frame was tracked with an\nOptitrack setup and participants received visual feedback in\na HoloLens headset.\n","bbox":[53.79799999999999,401.70199999999994,294.039973034,465.462],"page":"6"},"5":{"caption":"Figure 5: Delay till start of manipulation per view (top) and\nper view and object (bottom). Error bars show bootstrapped\n95 % confidence intervals.\n","bbox":[317.955,373.759,558.190039864,404.64300000000003],"page":"7"},"6":{"caption":"Figure 6: Duration of manipulation per view (top) and per\nview and object (bottom). Error bars show bootstrapped 95 %\nconfidence intervals.\n","bbox":[53.565000000000005,394.88,294.32007683000006,425.764],"page":"8"},"7":{"caption":"Figure 7: Relative error and absolute errors per view (top\ntwo) and per view and object (bottom two). Error bars show\nbootstrapped 95 % confidence intervals.\n","bbox":[317.955,251.40800000000002,558.5396242040001,282.29200000000003],"page":"8"},"8":{"caption":"Figure 8: At the end of the study, participants provided an\noverall rating of each view, indicating on a 7-point Likert\nscale how well the view supported them in the tasks. Shown\nhere are the number of ratings for each level of the scale,\nstacked horizontally to highlight overall trends.\n","bbox":[53.798,535.0340000000002,295.126856,587.835],"page":"9"}},"crops":{"1":{"crop_coord":[144.4388888888889,529.3278577777777,1555.5748888888888,847.5777777777778],"bbox":[53.798,488.672,558.20696,599.6419712000001],"page":"1"},"2":{"crop_coord":[144.4388888888889,252.36786666666654,1555.5914222222223,537.9499999999999],"bbox":[53.798,600.138,558.212912,699.347568],"page":"5"},"3":{"crop_coord":[878.2083333333334,1217.6392644444445,1555.5636666666664,1839.786111111111],"bbox":[317.955,131.477,558.20292,351.8498648],"page":"5"},"4":{"crop_coord":[144.4388888888889,252.37077333333332,821.8048888888889,893.5083333333334],"bbox":[53.798,472.137,294.04976,699.3465216],"page":"6"},"5":{"crop_coord":[878.2083333333334,252.3682666666666,1555.5568653333332,521.4166666666666],"bbox":[317.955,606.09,558.20047152,699.347424],"page":"7"},"6":{"crop_coord":[878.2083333333334,563.1373555555556,1555.5568653333332,1051.3805555555557],"bbox":[317.955,415.303,558.20047152,587.470552],"page":"7"},"7":{"crop_coord":[144.4388888888889,252.3682666666666,821.787420888889,521.4166666666666],"bbox":[53.798,606.09,294.04347152,699.347424],"page":"8"},"8":{"crop_coord":[144.4388888888889,529.9280444444445,821.787420888889,998.2444444444444],"bbox":[53.798,434.432,294.04347152,599.425904],"page":"8"},"9":{"crop_coord":[878.2083333333334,252.367088888889,1555.5568653333332,780.4638888888888],"bbox":[317.955,512.833,558.20047152,699.347848],"page":"8"},"10":{"crop_coord":[878.2083333333334,788.9765555555554,1555.5568653333332,1396.7805555555556],"bbox":[317.955,290.959,558.20047152,506.16844000000003],"page":"8"},"11":{"crop_coord":[144.4388888888889,252.36862222222203,821.787420888889,302.2222222222221],"bbox":[53.798,685,294.04347152,699.347296],"page":"9"},"12":{"crop_coord":[144.4388888888889,298.92562222222193,821.787420888889,548.0472222222221],"bbox":[53.798,596.503,294.04347152,682.5867760000001],"page":"9"}}}},{"filename":"Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences.pdf","paper_id":"Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences","venue":"chi2019xr","keywords":["Virtual Reality","Nonfiction","Immersive media","Interaction"],"doi":"10.1145/3290605.3300736","paragraph_containing_keyword":"CCS CONCEPTS\n• Human-centered computing → Virtual reality; • Ap-\nplied computing → Media arts; • Software and its en-\ngineering → Virtual worlds software.\nKEYWORDS\nVirtual Reality, nonfiction, immersive media, interaction.","paragraph_after_keyword":"ACM Reference Format:\nChris Bevan, David Phillip Green, Harry Farmer, Mandy Rose,\nKirsten Cater, Danaë Stanton Fraser, and Helen Brown. 2019. Behind\nthe Curtain of the “Ultimate Empathy Machine\": On the Composi-\ntion of Virtual Reality Nonfiction Experiences. In CHI Conference\non Human Factors in Computing Systems Proceedings (CHI 2019),\nMay 4–9, 2019, Glasgow, Scotland UK. ACM, New York, NY, USA,\n12 pages. https://doi.org/10.1145/3290605.3300736","sections":[{"word_count":1651,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":464,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":741,"figure_citations":{},"section_index":2,"title":"THE CHARACTERISTICS OF VRNF"},{"word_count":849,"figure_citations":{},"section_index":3,"title":"METHOD"},{"word_count":2541,"figure_citations":{},"section_index":4,"title":"RESULTS"},{"word_count":1329,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":178,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":251,"figure_citations":{},"section_index":7,"title":"LIMITATIONS AND FURTHER WORK"},{"word_count":37,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGEMENTS"},{"word_count":1107,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Behind the Curtain of the \"Ultimate Empathy Machine\": On the Composition of Virtual Reality Nonfiction Experiences","authors":"Chris Bevan, David Philip Green, Harry Farmer, Mandy Rose, Kirsten Cater, Danaë Stanton Fraser, Helen Brown","abstract":"Virtual Reality nonfiction (VRNF) is an emerging form of immersive media experience created for consumption using panoramic \"Virtual Reality\" headsets. VRNF promises nonfiction content producers the potential to create new ways for audiences to experience \"the real\"; allowing viewers to transition from passive spectators to active participants. Our current project is exploring VRNF through a series of ethnographic and experimental studies. In order to document the content available, we embarked on an analysis of VR documentaries produced to date. In this paper, we present an analysis of a representative sample of 150 VRNF titles released between 2012-2018. We identify and quantify 64 characteristics of the medium over this period, discuss how producers are exploiting the affordances of VR, and shed light on new audience roles. Our findings provide insight into the current state of the art in VRNF and provide a digital resource for other researchers in this area.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Hunger In Los Angeles (2012). Viewer on right is us-\ning prototypical VR hardware that would later become the\nOculus Rift.\n","bbox":[53.79799999999999,511.71195600000004,295.645712,542.621854],"page":"2"},"2":{"caption":"Figure 2: Characteristics of VRNF titles (n=150) by frequency of occurrence.\n","bbox":[153.245,411.250854,458.743518,420.216854],"page":"7"},"3":{"caption":"Figure 3: Diegetic visual annotations. 6x9: A Virtual Experi-\nence of Solitary Confinement, 2016.\n","bbox":[53.798,522.6549560000001,295.000712,542.606854],"page":"8"},"4":{"caption":"Figure 4: Branching narrative gaze selection. Decisions:\nParty’s Over (2018)\n","bbox":[53.798,522.665956,294.73389000000003,542.617854],"page":"9"}},"crops":{"1":{"crop_coord":[178.1305611111111,252.37016472222228,788.1108941666668,662.3499891666668],"bbox":[65.927002,555.3540039,281.9199219,699.3467406999999],"page":"2"},"2":{"crop_coord":[185.01668305555555,252.36668916666656,1514.9685669444445,1002.3666722222222],"bbox":[68.4060059,432.947998,543.5886841,699.3479919],"page":"7"},"3":{"crop_coord":[178.11390333333333,252.36643472222238,788.1139033333332,662.3944516666668],"bbox":[65.9210052,555.3379974,281.92100519999997,699.3480834999999],"page":"8"},"4":{"crop_coord":[178.12776361111113,252.3554144444446,788.127763611111,662.3610941666668],"bbox":[65.9259949,555.3500061,281.9259949,699.3520507999999],"page":"9"}}}},{"filename":"Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality.pdf","paper_id":"Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality","venue":"chi2019xr","keywords":["Quadcopter","Drone","UAV","Encountered-Type","Human-Drone Interaction","Robotic Graphics","Haptics","Virtual Reality"],"doi":"10.1145/3290605.3300589","paragraph_containing_keyword":"KEYWORDS\nQuadcopter, Drone, UAV, Encountered-Type, Human-Drone\nInteraction, Robotic Graphics, Haptics, Virtual Reality.\nACM Reference Format:\nParastoo Abtahi, Benoit Landry, Jackie (Junrui) Yang, Marco Pavone,\nSean Follmer, James A. Landay. 2019. Beyond The Force:, Using\nQuadcopters to Appropriate Objects and, the Environment for Hap-\ntics in Virtual Reality. In CHI Conference on Human Factors in Com-\nputing Systems Proceedings (CHI 2019), May 4–9, 2019, Glasgow,\nScotland Uk. ACM, New York, NY, USA, 13 pages. https://doi.org/\n10.1145/3290605.3300589","sections":[{"word_count":730,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":3992,"figure_citations":{"2":["Figure 2a) and 1.","Figure 2b).","Figure 2c)."],"3":["Figure 3a).","Figure 3b).","Figure 3c).","Figure 3b."],"5":["Figure 5b."]},"section_index":1,"title":"RELATED WORK"},{"word_count":529,"figure_citations":{},"section_index":2,"title":"TECHNICAL EVALUATION"},{"word_count":2598,"figure_citations":{"10":["Figure 10, consisted of a safe-to-touch quadcopter, Vicon motion capture cameras, two HTC Vive lighthouses, and the HTC Vive Head-Mounted Display (HMD)."],"11":["Figure 11)."],"13":["Figure 13, and placed it in their shopping basket."],"14":["Figure 14a.","Figure 14b."]},"section_index":3,"title":"USER EVALUATION"},{"word_count":89,"figure_citations":{},"section_index":4,"title":"CONCLUSION"},{"word_count":39,"figure_citations":{},"section_index":5,"title":"ACKNOWLEDGMENTS"},{"word_count":1689,"figure_citations":{},"section_index":6,"title":"REFERENCES"}],"title":"Beyond The Force: Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality","authors":"Parastoo Abtahi, Benoit Landry, Jackie (Junrui) Yang, Marco Pavone, Sean Follmer, James A. Landay","abstract":"Quadcopters have been used as hovering encountered-type haptic devices in virtual reality. We suggest that quadcopters can facilitate rich haptic interactions beyond force feedback by appropriating physical objects and the environment. We present HoverHaptics, an autonomous safe-to-touch quadcopter and its integration with a virtual shopping experience. HoverHaptics highlights three affordances of quadcopters that enable these rich haptic interactions: (1) dynamic positioning of passive haptics, (2) texture mapping, and (3) animating passive props. We identify inherent challenges of hovering encountered-type haptic devices, such as their limited speed, inadequate control accuracy, and safety concerns. We then detail our approach for tackling these challenges, including the use of display techniques, visuo-haptic illusions, and collision avoidance. We conclude by describing a preliminary study (n = 9) to better understand the subjective user experience when interacting with a quadcopter in virtual reality using these techniques.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Haptic interactions using a quadcopter. Left: user touching fabrics attached to the quad for texture rendering. Middle:\nuser picking up a physical hanger attached to the quad. Right: user picking up the turned off quad as a passive haptic device.\n","bbox":[53.798,358.068854,559.3280138639999,377.993854],"page":"1"},"2":{"caption":"Figure 2: Affordances of quadcopters as haptic devices in VR,\nfocused on force feedback: (a) weight simulation (b) surface\nstiffness simulation (c) lateral force feedback.\n","bbox":[53.798,405.158854,295.12685600000003,436.04285400000003],"page":"4"},"3":{"caption":"Figure 3: We present three techniques for appropriating ob-\njects and the environment: (a) dynamic positioning of pas-\nsive haptics (b) texture mapping (c) animating passive props.\n","bbox":[317.955,578.224854,559.794918,609.107854],"page":"4"},"4":{"caption":"Figure 4: Based on the prediction algorithm, the closest ren-\nderable patch is shown to the user.\n","bbox":[53.798,132.132854,295.637918,152.05785400000002],"page":"5"},"5":{"caption":"Figure 5: Visuo-haptic illusions are used to compensate for\nlack of control accuracy. (a) Quad is accurately positioned\nand no illusion is needed. (b) To correct for the position off-\nset, as the virtual hand touches the virtual target, body warp-\ning is used to retarget the hand towards the quad.\n","bbox":[317.955,265.222854,559.794918,318.024854],"page":"5"},"6":{"caption":"Figure 6: The emergency scene: the box in the center rep-\nresents the current position of the quadcopter and warning\nsigns pointing towards the quad are placed around the user.\n","bbox":[317.955,156.689854,559.794918,187.573854],"page":"6"},"7":{"caption":"Figure 7: Upon the quad’s arrival the patch is highlighted in\ngreen, indicating that the user can touch the virtual object.\n","bbox":[53.798,618.567854,294.033004,638.492854],"page":"7"},"8":{"caption":"Figure 8: Safe-to-touch quadcopter with the protective cage.\n","bbox":[53.798,156.846854,294.03300399999995,165.81285400000002],"page":"7"},"9":{"caption":"Figure 9: Virtual boutique scene.\n","bbox":[372.382,559.170854,503.76976399999995,568.136854],"page":"8"},"10":{"caption":"Figure 10: Experimental Setup.\n","bbox":[375.812,192.916854,500.34077399999995,201.882854],"page":"8"},"11":{"caption":"Figure 11: Texture rendering: user feeling the material of a\nscarf and a shirt.\n","bbox":[53.798,178.057854,294.03300399999995,197.982854],"page":"9"},"12":{"caption":"Figure 12: Animating passive props: user picking up a\nhanger and dropping it in the shopping basket.\n","bbox":[317.955,406.927854,558.190004,426.85285400000004],"page":"9"},"13":{"caption":"Figure 13: Dynamic positioning of passive haptics: quad\nlanding on a table in the room to render a shoebox, and user\npicking up the shoebox.\n","bbox":[317.955,68.79285399999999,558.3782900000001,99.67685399999999],"page":"9"},"14":{"caption":"Figure 14: Affordances of multiple quads for haptics in VR:\n(a) complex geometry and (b) continuous surface rendering.\n","bbox":[317.67699999999996,165.489854,559.337652,185.41485400000002],"page":"11"}},"crops":{"1":{"crop_coord":[144.4388888888889,628.7471666666665,1555.5819444444444,1119.65275],"bbox":[53.798,390.72501,558.2094999999999,563.8510200000001],"page":"1"},"2":{"crop_coord":[144.34098972222222,1711.3068558333332,432.74641222222226,1819.3814933333333],"bbox":[53.7627563,138.8226624,153.9887084,174.12953190000002],"page":"1"},"3":{"crop_coord":[144.43890055555556,779.9369472222222,821.7733936111111,958.4055497222222],"bbox":[53.7980042,448.7740021,294.0384217,509.422699],"page":"4"},"4":{"crop_coord":[878.2083383333334,252.3721569444445,1555.542831388889,477.6666769444443],"bbox":[317.9550018,621.8399963,558.1954193,699.3460235],"page":"4"},"5":{"crop_coord":[144.43890055555556,1474.8687827777776,821.7724186111111,1747.2527736111113],"bbox":[53.7980042,164.7890015,294.0380707,259.2472382],"page":"5"},"6":{"crop_coord":[878.2083383333334,900.8832380555555,1555.5359225,1286.2333425],"bbox":[317.9550018,330.7559967,558.1929321,465.8820343],"page":"5"},"7":{"crop_coord":[878.2083383333334,1417.6784599999999,1555.5305394444445,1648.5972425],"bbox":[317.9550018,200.3049927,558.1909942,279.83575440000004],"page":"6"},"8":{"crop_coord":[144.43890055555556,252.372495833333,821.7440627777777,396.0444472222222],"bbox":[53.7980042,651.223999,294.0278626,699.3459015000001],"page":"7"},"9":{"crop_coord":[144.43890055555556,1507.4520194444444,821.7917463888889,1709.0416716666666],"bbox":[53.7980042,178.5449982,294.0450287,247.517273],"page":"7"},"10":{"crop_coord":[878.2083383333334,252.37096999999972,1555.5283355555555,591.4749908333332],"bbox":[317.9550018,580.8690033,558.1902008,699.3464508000001],"page":"8"},"11":{"crop_coord":[878.2083383333334,1378.6289299999999,1555.5611841666666,1608.8500044444443],"bbox":[317.9550018,214.6139984,558.2020262999999,293.8935852],"page":"8"},"12":{"crop_coord":[144.43890055555556,1129.3513066666667,821.7724186111111,1619.6833208333333],"bbox":[53.7980042,210.7140045,294.0380707,383.6335296],"page":"9"},"13":{"crop_coord":[878.2083383333334,494.13033388888897,1555.541856388889,983.9333344444444],"bbox":[317.9550018,439.5839996,558.1950683,612.3130798],"page":"9"},"14":{"crop_coord":[878.2083383333334,1401.1010572222222,1555.541856388889,1892.7555422222222],"bbox":[317.9550018,112.4080048,558.1950683,285.8036194],"page":"9"},"15":{"crop_coord":[878.2083383333334,1427.7275594444445,1555.542831388889,1654.591657777778],"bbox":[317.9550018,198.1470032,558.1954193,276.2180786],"page":"11"}}}},{"filename":"Can Mobile Augmented Reality Stimulate a Honeypot","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Can Mobile Augmented Reality Stimulate a Honeypot.pdf","paper_id":"Can Mobile Augmented Reality Stimulate a Honeypot","venue":"chi2019xr","keywords":["Audience","Augmented reality","Honeypot efect","Public space"],"paragraph_containing_keyword":"ABSTRACT  \nIn  HCI,  the  honeypot  efect  describes  a  form  of  audience  en-\ngagement  in  which  a  person’s  interaction  with  a  technology  \nstimulates  passers-by  to  observe,  approach  and  engage  in  an  \ninteraction  themselves.  In  this  paper  we  explore  the  potential  \nfor  honeypot  efects  to  arise  in  the  use  of  mobile  augmented  \nreality  (AR)  applications  in  urban  spaces.  We  present  an  ob-\nservational  study  of  Santa’s  Lil  Helper,  a  mobile  AR  game  that  \ncreated  a  Christmas-themed  treasure  hunt  in  a  metropolitan  \narea.  Our  study  supports  a  consideration  of  three  factors  that  \nmay  impede  the  honeypot  efect:  the  presence  of  people  in  \nrelation  to  the  game  and  its  interactive  components;  the  visi-\nbility  of  gameplay  in  urban  space;  and  the  extent  to  which  the  \ngame  permits  a  shared  experience.  We  consider  how  these  \nfactors  can  inform  the  design  of  future  AR  experiences  that  \nare  capable  of  stimulating  honeypot  efects  in  public  space.  \nCCS CONCEPTS \n \n• Human-centered computing → HCI theory, concepts \n \n \n \nand models. \n \n \nKEYWORDS \n \nAudience; Augmented reality; Honeypot efect; Public space. \n \n \n \n \nACM Reference Format: \n \nRyan M. Kelly, Hasan Shahid Ferdous, Niels Wouters, and Frank \n \n \n \nVetere. 2019. Can Mobile Augmented Reality Stimulate a Honeypot \n \n \nEfect? Observations from Santa’s Lil Helper. In CHI Conference on \n \n \n \n \nHuman Factors in Computing Systems Proceedings (CHI 2019), May \n \n \n \n4–9, 2019, Glasgow, Scotland, UK. ACM, New York, NY, USA, 13 pages. \n \n \n \n \nhttps://doi.org/10.1145/3290605.3300515 \n \nPermission to make digital or hard copies of all or part of this work for \n \n \n \n \npersonal or classroom use is granted without fee provided that copies are not \n \n \n \nmade or distributed for proft or commercial advantage and that copies bear \n \n \nthis notice and the full citation on the frst page. Copyrights for components \n \n \nof this work owned by others than the author(s) must be honored. Abstracting \n \n \n \nwith credit is permitted. To copy otherwise, or republish, to post on servers or \n \n \n \nto redistribute to lists, requires prior specifc permission and/or a fee. Request \n \n \n \n \npermissions from permissions@acm.org. \n \n \nCHI 2019, May 4–9, 2019, Glasgow, Scotland, UK \n \n \n \n \n© 2019 Copyright held by the owner/author(s). Publication rights licensed to \n \n \n \n \n \n \nACM. \n \nACM ISBN 978-1-4503-5970-2/19/05. . . $15.00 \n \n \nhttps://doi.org/10.1145/3290605.3300515","doi":"10.1145/3290605.3300515","paragraph_after_keyword":"","sections":[{"word_count":3224,"figure_citations":{"1":["Figure 1a).","Figure 1b).","Figure 1c).","Figure 1d).","Figure 1a), which is a bustling shopping area and tourist destination, and Federation Square (location F in Figure 1a), which is a major venue for arts, culture and public events.","Figure 1b).","Figure 1c).","Figure 1c)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":793,"figure_citations":{},"section_index":1,"title":"METHODS"},{"word_count":3147,"figure_citations":{"1":["Figure 1b).","Figure 1d).","Figure 1c), which required users to point their device at a marker positioned above a large red throne."],"2":["Figure 2d)."]},"section_index":2,"title":"FINDINGS"},{"word_count":1352,"figure_citations":{},"section_index":3,"title":"DISCUSSION AND IMPLICATIONS"},{"word_count":134,"figure_citations":{},"section_index":4,"title":"CONCLUSION"},{"word_count":22,"figure_citations":{},"section_index":5,"title":"ACKNOWLEDGMENTS"},{"word_count":2432,"figure_citations":{},"section_index":6,"title":"REFERENCES"}],"title":"Can Mobile Augmented Reality Stimulate a Honeypot Effect?: Observations from Santa's Lil Helper","authors":"Ryan M. Kelly, Hasan Shahid Ferdous, Niels Wouters, Frank Vetere","abstract":"In HCI, the honeypot effect describes a form of audience engagement in which a person's interaction with a technology stimulates passers-by to observe, approach and engage in an interaction themselves. In this paper we explore the potential for honeypot effects to arise in the use of mobile augmented reality (AR) applications in urban spaces. We present an observational study of Santa's Lil Helper, a mobile AR game that created a Christmas-themed treasure hunt in a metropolitan area. Our study supports a consideration of three factors that may impede the honeypot effect: the presence of people in relation to the game and its interactive components; the visibility of gameplay in urban space; and the extent to which the game permits a shared experience. We consider how these factors can inform the design of future AR experiences that are capable of stimulating honeypot effects in public space.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: (a) The map of locations used by Santa’s Lil Helper; \n \n(b) A column location marker; (c) A signpost location marker; \n \n \n(d) A family interacting with the application. \n \n \n","bbox":[317.6769999999999,242.75888,561.1326927999999,273.7646912],"page":"4"},"2":{"caption":"Figure 2: (a) AR content in SLH; (b) Users interacting in city \n \nspace; (c) A marker at Federation Square; (d) Markers ‘blend-\n \ning in’ and being blocked within urban space. \n \n","bbox":[317.95517600000005,231.79988000000003,560.6935568,262.80569120000007],"page":"5"}},"crops":{"1":{"crop_coord":[878.2083333333334,252.36733333333342,1217.475,726.6333333333332],"bbox":[317.955,532.212,436.491,699.34776],"page":"4"},"2":{"crop_coord":[1216.297222222222,252.36733333333342,1555.5638888888889,726.6333333333332],"bbox":[439.667,532.212,558.203,699.34776],"page":"4"},"3":{"crop_coord":[878.2083333333334,827.3312222222222,1217.475,1301.597222222222],"bbox":[317.955,325.225,436.491,492.36076],"page":"4"},"4":{"crop_coord":[1216.297222222222,828.1543888888889,1555.5638888888889,1301.597222222222],"bbox":[439.667,325.225,558.203,492.06442000000004],"page":"4"},"5":{"crop_coord":[878.2083333333334,252.36733333333342,1217.475,726.6333333333332],"bbox":[317.955,532.212,436.491,699.34776],"page":"5"},"6":{"crop_coord":[1216.297222222222,252.36733333333342,1555.5638888888889,726.6333333333332],"bbox":[439.667,532.212,558.203,699.34776],"page":"5"},"7":{"crop_coord":[879.8527777777778,857.7728888888888,1219.1194444444445,1332.038888888889],"bbox":[318.547,314.266,437.083,481.40176],"page":"5"},"8":{"crop_coord":[1214.6527777777778,857.7728888888888,1553.9194444444445,1332.038888888889],"bbox":[439.075,314.266,557.611,481.40176],"page":"5"}}}},{"filename":"CarVR- Enabling In-Car Virtual Reality Entertainment","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/CarVR- Enabling In-Car Virtual Reality Entertainment.pdf","paper_id":"CarVR- Enabling In-Car Virtual Reality Entertainment","venue":"VR_2017","keywords":["Force-feedback","Motion platform","Immersion","Virtual reality","Automotive","Entertainment","Gaming"],"paragraph_containing_keyword":"Author Keywords\nforce-feedback; motion platform; immersion; virtual reality;\nautomotive; entertainment; gaming","paragraph_after_keyword":"INTRODUCTION\nMobile virtual reality (VR) is currently becoming a consumer\nproduct. Major companies such as Google (Cardboard), Sam-\nsung (GearVR) and Zeiss (VR One) are releasing high-quality\nand low-cost mobile VR head-mounted displays (HMDs). Due\nto their low price and easy accessibility, they are more likely to\npenetrate the consumer market. One of the major application\nscenarios for current consumer VR HMDs is entertainment,","doi":"10.1145/3025453.3025665","sections":[{"word_count":558,"figure_citations":{"1":["Figure 1)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1609,"figure_citations":{"2":["Figure 2)."]},"section_index":1,"title":"RELATED WORK"},{"word_count":1410,"figure_citations":{"3":["Figure 3)."],"4":["Figure 4)."],"5":["Figure 5)."],"6":["Figure 6)."]},"section_index":2,"title":"DESIGN SPACE"},{"word_count":851,"figure_citations":{"7":["Figure 7)."],"8":["Figure 8)."]},"section_index":3,"title":"IMPLEMENTATION"},{"word_count":3349,"figure_citations":{"9":["Figure 9)."],"10":["Figure 10)."],"11":["Figure 11 shows the results of the final comparison regarding general discomfort between both conditions."]},"section_index":4,"title":"STUDY"},{"word_count":193,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":39,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENT"},{"word_count":1102,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"CarVR: Enabling In-Car Virtual Reality Entertainment","authors":"Philipp Hock, Sebastian Benedikter, Jan Gugenheimer, Enrico Rukzio","abstract":"Mobile virtual reality (VR) head-mounted displays (HMDs) allow users to experience highly immersive entertainment whilst being in a mobile scenario. Long commute times make casual gaming in public transports and cars a common occupation. However, VR HMDs can currently not be used in moving vehicles since the car's rotation affects the HMD's sensors and simulator sickness occurs when the visual and vestibular system are stimulated with incongruent information. We present CarVR, a solution to enable VR in moving vehicles by subtracting the car's rotation and mapping vehicular movements with the visual information. This allows the user to actually feel correct kinesthetic forces during the VR experience. In a user study (n = 21), we compared CarVR inside a moving vehicle with the baseline of using VR without vehicle movements. We show that the perceived kinesthetic forces caused by CarVR increase enjoyment and immersion significantly while simulator sickness is reduced compared to a stationary VR experience. Finally, we explore the design space of in-car VR entertainment applications using real kinesthetic forces and derive design considerations for practitioners.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. A player is sitting on the front passenger seat playing the game\nwhile the car is moving. Kinesthetic forces caused by the car match the\nmovements in VR.\n","bbox":[320.807,382.82727,564.1630600000002,408.73026999999996],"page":"1"},"2":{"caption":"Figure 2. Schematic view of the position of devices and people involved\nin the apparatus. The driver acts as normal driver, following a route to a\ndestination, the co-driver is playing the game on the font-passenger seat\nwith a Samsung GearVR attached. An x-IMU measures the vehicle’s\ninertia. An OBD-II reader attached to the car’s diagnostic port is used\nto measure the car’s velocity.\n","bbox":[53.642,81.65427000000001,296.99806000000024,134.45627],"page":"3"},"3":{"caption":"Figure 3. Route generation based on predeﬁned route. (left) The trajec-\ntory is extracted from the topology, (center) A depth map based on the\ntrajectory is generated. (right) The terrain according to the depth map\nis generated.\n","bbox":[53.929,618.72227,298.3210800000001,653.59127],"page":"4"},"4":{"caption":"Figure 4. The warp effect can be used to visualize acceleration.\n","bbox":[69.084,175.69327,281.8431500000002,183.66327],"page":"4"},"5":{"caption":"Figure 5. Two approaches of visualizing roll rotation. Left: the cockpit\nand the camera rotates along the roll axis. Right: only the cockpit ro-\ntates along the roll axis, the camera’s roll rotation stays in line with the\nhorizon. This concept also applies for the pitch axis.\n","bbox":[321.094,609.91027,565.4860800000001,644.77927],"page":"4"},"6":{"caption":"Figure 6. Effects of vehicular rotation while braking, accelerating and\nturning. The weight transfer due to inertia in a car forces the car to\nrotate forward when braking, backward when accelerating on the pitch\naxis, and towards the outside of a curve when turning on the roll axis.\nThe rotation in a helicopter behaves inversely.\n","bbox":[320.831,265.25727,565.5578100000001,309.09227],"page":"4"},"7":{"caption":"Figure 7. The scene the player is ﬂying through. The map is a valley with\ndifferent highlights (a train, sheep, houses and a castle). The ﬁve small\nimages show the view from the ego perspective but without the cockpit.\n","bbox":[321.094,472.51727,564.1630600000002,498.42026999999996],"page":"5"},"8":{"caption":"Figure 8. The view from inside the cockpit. A balloon is shot through\naiming via gaze and shooting via button press on a game controller.\n","bbox":[321.094,87.47227000000001,564.1630600000002,104.40827],"page":"5"},"9":{"caption":"Figure 9. The track driven during the study consisted of 6 curves (3 left-\nhand and 3 right-hand 90-degree curves) and three 360-degree turn (2\nleft-hand and 1 right-hand). It started in the parking lot (a). After three\nright-hand curves, the ﬁrst 360-degree turn (b) was reached. Followed\nby another right-hand curve, the second 360-degree turn (c) was reached.\nThen after a left curve, the track went back to the ﬁrst 360-degree turn\n(b). After three further left-hand curves, the track ended in the parking\nlot from the beginning s(a).\n","bbox":[53.666000000000004,352.64127,298.39281000000005,423.37527],"page":"7"},"10":{"caption":"Figure 10. E2I total score and subscale score for the two conditions park-\ning and driving. It can be seen that the rating for driving in all three\nscores (total, presence and enjoyment) is higher than for the parking\ncondition. The effect is signiﬁcant. Error bars represent one standard\ndeviation of uncertainty\n","bbox":[321.09399999999994,278.21127,565.48308,323.322752],"page":"7"},"11":{"caption":"Figure 11. Directly compared simulator sickness between the parking\ncondition and the driving condition.\n","bbox":[53.929,621.8602699999999,296.9980600000001,638.7962699999999],"page":"8"}},"crops":{"1":{"crop_coord":[886.9277613888887,574.203542222222,1591.4246791666665,1056.7083486111112],"bbox":[321.0939941,413.3849945,571.1128845,583.4867248],"page":"1"},"2":{"crop_coord":[144.80278027777777,1288.0977700951305,849.3329912586806,1818.5833316666665],"bbox":[53.9290009,139.1100006,303.959876853125,326.484802765753],"page":"3"},"3":{"crop_coord":[144.80278027777777,170.99585767956364,849.29170407245,376.5388827777778],"bbox":[53.9290009,658.2460022,303.945013466082,728.6414912353571],"page":"4"},"4":{"crop_coord":[198.94168416666668,1339.1385905555555,775.8532544444445,1676.3583205555556],"bbox":[73.4190063,190.3110046,277.5071716,308.1101074],"page":"4"},"5":{"crop_coord":[886.9277613888887,170.99514035186658,1577.2215470308886,401.0166508333336],"bbox":[321.0939941,649.4340057,565.9997569311199,728.641749473328],"page":"4"},"6":{"crop_coord":[886.9277613888887,1007.1061706163673,1577.229158975599,1333.4805722222222],"bbox":[321.0939941,313.746994,566.0024972312156,427.6417785781078],"page":"4"},"7":{"crop_coord":[941.0666655555556,293.1819155555558,1517.994681388889,807.5694530555556],"bbox":[340.5839996,503.0749969,544.6780853,684.6545103999999],"page":"5"},"8":{"crop_coord":[1011.9277613888887,1646.125547777778,1447.1101038888887,1902.0472125],"bbox":[366.0939941,109.0630035,519.1596374,197.5948028],"page":"5"},"9":{"crop_coord":[269.79998277777776,688.3625343665941,704.9760581501838,1016.0277811111112],"bbox":[98.9279938,428.0299988,251.9913809340662,542.3894876280261],"page":"7"},"10":{"crop_coord":[886.9277613888887,1110.0225728322566,1577.2275323683511,1297.4972111111113],"bbox":[321.0939941,326.701004,566.0019116526064,390.59187378038763],"page":"7"},"11":{"crop_coord":[340.67222583333336,168.2285481247098,634.1371409733823,417.6361],"bbox":[124.4420013,643.451004,226.4893707504176,729.6377226751044],"page":"8"}}}},{"filename":"Crossing-Based Selection with Virtual Reality Head-Mounted Displays","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Crossing-Based Selection with Virtual Reality Head-Mounted Displays.pdf","paper_id":"Crossing-Based Selection with Virtual Reality Head-Mounted Displays","venue":"chi2019xr","keywords":["Crossing","Pointing","Virtual reality head-mounted displays","Ray-casting selection","Fitts’ law"],"paragraph_containing_keyword":"KEYWORDS\nCrossing, pointing, virtual reality head-mounted displays,\nray-casting selection, Fitts’ law","paragraph_after_keyword":"∗Corresponding author","doi":"10.1145/3290605.3300848","sections":[{"word_count":966,"figure_citations":{"1":["Figure 1) through ray-casting pointing is a difficult task [20, 23, 24], due to the fact that the interaction takes place in free space without physical support for the hand.","Figure 1), corresponding to pointing at spheres in 3D space and circular targets on 2D plane respectively."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1386,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":3634,"figure_citations":{"2":["Figure 2).","Figure 2(a)) that was made up by the positions of the two targets and the virtual camera with the camera’s position as the vertex.","Figure 2(a)).","Figure 2(a)) were within the apparatus’ FOV (horizontal FOV: around 80◦ [27]) while targets with long distance were not (target angle: 90.","Figure 2(a)), indicating participants had to first locate a target using head motion before making a selection.","Figure 2(b)) to confirm the selection.","Figure 2(b)) of the controller only when crossing a goal.","Figure 2(b))."],"3":["Figure 3).","Figure 3(b-e)), so that participants could move the ray cursor to “pass” through a disc from its one side to the other.","Figure 3(a)).","Figure 3(d)).","Figure 3(e))."],"4":["Figure 4(a))."],"5":["Figure 5(a), C/DC follows a similar regression line to A/DP, suggesting a possibility of substituting the pointing task with the crossing task."],"6":["Figure 6(a)).","Figure 6(b))."]},"section_index":2,"title":"3D SPACE"},{"word_count":2040,"figure_citations":{"2":["Figure 2)."],"5":["Figure 5(b), C/DC was uniformly faster than A/DP."],"7":["Figure 7(a)), we adopted a standard 2D pointing design with circular targets [16] given pointing tasks in most 2D interfaces are typically twodimensional ([32]).","Figure 7(b-e)), task design was the same as their counterpart in Experiment One, except that we used goal lines which were displayed on constrained planes."],"8":["Figure 8(a)).","Figure 8(b)."],"9":["Figure 9(a)).","Figure 9(b))."]},"section_index":3,"title":"2D PLANE"},{"word_count":1192,"figure_citations":{"5":["Figure 5), which further verifies their similar performances in target selection.","Figure 5(a&b)) shows an upward curvature of time away for the regression line for low values of ID."]},"section_index":4,"title":"GENERAL DISCUSSION"},{"word_count":488,"figure_citations":{},"section_index":5,"title":"IMPLICATIONS FOR DESIGN"},{"word_count":155,"figure_citations":{},"section_index":6,"title":"FUTURE WORK"},{"word_count":150,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":63,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGMENTS"},{"word_count":1742,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Crossing-Based Selection with Virtual Reality Head-Mounted Displays","authors":"Huawei Tu, Susu Huang, Jiabin Yuan, Xiangshi Ren, Feng Tian","abstract":"This paper presents the first investigation into using the goal-crossing paradigm for object selection with virtual reality (VR) head-mounted displays. Two experiments were carried out to evaluate ray-casting crossing tasks with target discs in 3D space and goal lines on 2D plane respectively in comparison to ray-casting pointing tasks. Five factors, i.e. task difficulty, the direction of movement constraint (collinear vs. orthogonal), the nature of the task (discrete vs. continuous), field of view of VR devices and target depth, were considered in both experiments. Our findings are: (1) crossing generally had shorter or no longer time, and higher or similar accuracy than pointing, indicating crossing can complement or substitute pointing; (2) crossing tasks can be well modelled with Fitts' Law; (3) crossing performance depended on target depth; (4) crossing target discs in 3D space differed from crossing goal lines on 2D plane in many aspects such as time and error performance, the effects of target depth and the parameters of Fitts' models. Based on these findings, we formulate a number of design recommendations for crossing-based interaction in VR.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Crossing a (a) target disc in 3D space; (b) goal line\non 2D plane.\n","bbox":[53.798,607.8528540000001,294.033004,627.777854],"page":"2"},"2":{"caption":"Figure 2: (a) Target positions in the virtual environment\nfrom an overhead perspective. Z-axis is the default orien-\ntation of the virtual camera in VR. Dotted lines represent\ntarget depths (768, 2304 and 3456 mm). Circles with iden-\ntical numbers indicate target pairs in an experiment trial.\nEach target pair corresponds to a target angle, either 20.4◦\nor 90.2◦. The shaded area indicates the apparatus’s default\nFOV (≈ 80◦) (b) Participant wearing the Oculus Rift using\na touch controller. Inset: close-up of the Oculus Touch con-\ntroller. “Button A” is highlighted in a dotted red circle.\n","bbox":[317.9549999999999,469.4698540000003,559.794918,577.0658540000001],"page":"4"},"3":{"caption":"Figure 3: The five tested task types in 3D space.\n","bbox":[210.876,610.538854,401.11658800000004,619.504854],"page":"6"},"4":{"caption":"Figure 4: Mean selection time for the five task types in (a)\nthree target depths and (b) seven IDs. Error bars represent\n0.95 confidence interval.\n","bbox":[53.798,374.535854,294.59786199999996,405.419854],"page":"7"},"5":{"caption":"Figure 5: Fitts’ law models for the pointing and four crossing\ntasks in (a) Experiment One and (b) Two.\n","bbox":[317.955,80.887854,558.190004,100.812854],"page":"7"},"6":{"caption":"Figure 6: Error rates for the five task types in (a) three target\ndepths and (b) seven IDs.\n","bbox":[53.798,572.163854,294.033004,592.088854],"page":"8"},"7":{"caption":"Figure 7: The five tested task types on 2D planes.\n","bbox":[207.63,613.031854,404.361972,621.997854],"page":"9"},"8":{"caption":"Figure 8: Mean selection time for the five task types in (a)\nthree target depths and (b) seven IDs.\n","bbox":[317.955,285.520854,558.754862,305.445854],"page":"9"},"9":{"caption":"Figure 9: Error rates for the five task types in (a) three target\ndepths and (b) seven IDs.\n","bbox":[53.798,80.887854,294.033004,100.812854],"page":"10"}},"crops":{"1":{"crop_coord":[144.43819165555553,-4.522122723333095,953.7993292305556,425.8096791666666],"bbox":[53.797748995999996,640.5085155,341.567758523,791.8279641803999],"page":"2"},"2":{"crop_coord":[878.2083383333334,89.54764833999967,1555.5615039500003,566.6737213555552],"bbox":[317.9550018,589.7974603120001,558.2021414220001,757.9628465976001],"page":"4"},"3":{"crop_coord":[144.4386897666667,-706.5174976222221,1740.4679023666665,549.6483122777778],"bbox":[53.79792831600001,595.92660758,624.768444852,1044.546299144],"page":"5"},"4":{"crop_coord":[144.43890055555556,252.36749444444445,1555.511678333333,448.78611249999994],"bbox":[53.7980042,632.2369995,558.1842042,699.347702],"page":"6"},"5":{"crop_coord":[144.43861853333334,-791.2719024333335,1712.4312878666663,383.7544652000002],"bbox":[53.797902672,655.6483925279999,614.6752636319999,1075.0578848760001],"page":"7"},"6":{"crop_coord":[144.4385701,-262.3496950333333,1721.4636464999996,664.265267166667],"bbox":[53.797885236000006,554.6645038199999,617.9269127399999,884.645890212],"page":"7"},"7":{"crop_coord":[144.43890055555556,489.07059794777757,821.7953863305555,1043.4679672777777],"bbox":[53.7980042,418.15153178,294.046339079,614.1345847388001],"page":"7"},"8":{"crop_coord":[878.2083383333334,1390.9395396255554,1555.5429694083334,1889.6011059555556],"bbox":[317.9550018,113.543601856,558.195468987,289.4617657348],"page":"7"},"9":{"crop_coord":[144.43890055555556,-28.72274277111086,821.7836945388889,524.9427323999997],"bbox":[53.7980042,604.8206163360001,294.042130034,800.5401873976],"page":"8"},"10":{"crop_coord":[144.43890055555556,252.37029194444455,1555.5558863888891,441.8638780555558],"bbox":[53.7980042,634.7290039,558.2001191,699.3466949],"page":"9"},"11":{"crop_coord":[144.43861853333334,-463.6975352222217,1712.4312878666663,660.1071556111114],"bbox":[53.797902672,556.1614239799999,614.6752636319999,957.1311126799999],"page":"9"},"12":{"crop_coord":[144.43842730000003,108.5142214111111,1721.6385555666666,940.8074214777777],"bbox":[53.79783382800001,455.109328268,617.9898800039999,751.134880292],"page":"9"},"13":{"crop_coord":[878.2083383333334,767.5105947066669,1555.5531323166665,1321.1760698777778],"bbox":[317.9550018,318.176614844,558.199127634,513.8961859056],"page":"9"},"14":{"crop_coord":[144.43890055555556,1335.9355824844445,821.7836945388889,1889.6010576555555],"bbox":[53.7980042,113.54361924400001,294.042130034,309.2631903056],"page":"10"}}}},{"filename":"Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials.pdf","paper_id":"Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials","venue":"chi2019xr","keywords":["Force feedback","EEG","Elecrical muscle stimulation","Virtual reality","ERP","Prediction error"],"doi":"10.1145/3290605.3300657","paragraph_containing_keyword":"response in unrealistic VR interaction, we also presented the\nfeedback prematurely in 25% of the trials.\nWe found that the early negativity component of the ERP\n(so called prediction error) was more pronounced in the\nmismatch trials, indicating we successfully detected haptic\nconflicts using our technique. Our results are a first step\ntowards using ERPs to automatically detect visuo-haptic\nmismatches in VR, such as those that can cause a loss of the\nuser’s immersion.\nCCS CONCEPTS\n• Human-centered computing → Haptic devices; Vir-\ntual reality.\nKEYWORDS\nforce feedback; EEG; elecrical muscle stimulation; virtual\nreality; ERP; prediction error\nACM Reference Format:\nLukas Gehrke, Sezen Akman, Pedro Lopes, Albert Chen, Avinash\nKumar Singh, Hsiang-Ting Chen, Chin Teng Lin, and Klaus Gra-\nmann. 2019. Detecting Visuo-Haptic Mismatches in Virtual Reality\nusing the Prediction Error Negativity of Event-Related Brain Poten-\ntials. In CHI Conference on Human Factors in Computing Systems Pro-\nceedings (CHI 2019), May 4–9, 2019, Glasgow, Scotland Uk. ACM, New\nYork, NY, USA, 11 pages. https://doi.org/10.1145/3290605.3300657\n1 INTRO\nA key challenge in virtual reality is to create a user experi-\nence that mimics the natural experience as closely as possible.\nThis challenge has propelled advancements in display soft-\nware and hardware (VR headsets and rendering), interaction","sections":[{"word_count":343,"figure_citations":{},"section_index":0,"title":"INTRO"},{"word_count":672,"figure_citations":{},"section_index":1,"title":"IMMERSION"},{"word_count":782,"figure_citations":{},"section_index":2,"title":"RELATED WORK"},{"word_count":1852,"figure_citations":{"2":["Figure 2, comprised: (1) a VR headset and a wrist-mounted wearable VIVE tracker, (2) a 64-channel EEG system, (3) one vibrotactile actuator worn on the fingertip, and (4) a medically-compliant EMS device connected via two electrodes worn on the forearm."],"3":["Figure 3, was as follows: (1) participants moved their hands from the resting position to the ready position, to indicate they were ready to start the next trial; (2) participants waited for a new target to appear (the time of a new target spawning was randomized between 1-2 s); (3) then, the target (a cube) would appear in one of three possible positions (center, left, right), all equidistant from the participant’s ready position; (4) then, participants acquired the target by moving and touching the target with their index finger."],"4":["Figure 4), we filtered the EEG data with a 0."]},"section_index":3,"title":"USER STUDY"},{"word_count":1232,"figure_citations":{"4":["Figure 4(A).","Figure 4(B), we observed a negative deflection around 170ms after a participant had selected the object (i."],"6":["Figure 6(B), we observed no significant differences for the peak latencies over the three conditions."]},"section_index":4,"title":"RESULTS"},{"word_count":323,"figure_citations":{},"section_index":5,"title":"CONCLUSIONS"},{"word_count":19,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":1919,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials","authors":"Lukas Gehrke, Sezen Akman, Pedro Lopes, Albert Chen, Avinash Kumar Singh, Hsiang-Ting Chen, Chin-Teng Lin, Klaus Gramann","abstract":"Designing immersion is the key challenge in virtual reality; this challenge has driven advancements in displays, rendering and recently, haptics. To increase our sense of physical immersion, for instance, vibrotactile gloves render the sense of touching, while electrical muscle stimulation (EMS) renders forces. Unfortunately, the established metric to assess the effectiveness of haptic devices relies on the user's subjective interpretation of unspecific, yet standardized, questions.Here, we explore a new approach to detect a conflict in visuo-haptic integration (e.g., inadequate haptic feedback based on poorly configured collision detection) using electroencephalography (EEG). We propose analyzing event-related potentials (ERPs) during interaction with virtual objects. In our study, participants touched virtual objects in three conditions and received either no haptic feedback, vibration, or vibration and EMS feedback. To provoke a brain response in unrealistic VR interaction, we also presented the feedback prematurely in 25% of the trials.We found that the early negativity component of the ERP (so called prediction error) was more pronounced in the mismatch trials, indicating we successfully detected haptic conflicts using our technique. Our results are a first step towards using ERPs to automatically detect visuo-haptic mismatches in VR, such as those that can cause a loss of the user's immersion.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: We propose using the prediction error negativity\nof the brain’s event related potential (ERP) to detect visuo-\nhaptic conflicts arising from unrealistic VR feedback. In our\nstudy, participants selected objects in VR. To provoke their\nbrains to process an unrealistic interaction, we sometimes\nprovided the haptic feedback prematurely. When subtract-\ning these ERPs to the ERPs from realistic interactions, we\nfound that the negative amplitude of the error prediction\nincreased, hinting at a loss in immersion.\n","bbox":[317.955,406.36699999999996,559.794918,503.004],"page":"2"},"2":{"caption":"Figure 2: Our experimental setup (image with consent from\nparticipant).\n","bbox":[317.955,483.185,558.1900039999999,503.11],"page":"4"},"3":{"caption":"Figure 3: Interaction flow depicting one trial in our 3D object selection task.\n","bbox":[152.586,582.192,459.40252000000004,591.158],"page":"5"},"4":{"caption":"Figure 4: ERP amplitude (in µV ) and standard error of the\nmean at the forehead electrode FCz across [A] all match tri-\nals and [B] mismatch trials in a -300ms to 700ms window\ncentered at the object selection time, for each of the three\nfeedback conditions (Visual, Vibro and EMS).\n","bbox":[53.798,276.592,295.63791799999996,329.394],"page":"7"},"5":{"caption":"Figure 5: Amplitude and standard error of the mean of the\nresulting ERPs obtained by subtracting the mean amplitude\nof all match trials from the mean amplitude of all mismatch\ntrials for each participant (in µV ) at the forehead electrode\nFCz; for all three feedback conditions (Visual, Vibro and\nEMS)\n","bbox":[317.955,371.561,558.199796,435.322],"page":"7"},"6":{"caption":"Figure 6: Negative peak amplitudes (in µV ) and latencies (in\nms) 100 to 300ms post object selection event in difference\nERPs, see figure 5. Dots represent individual participants.\nUncorrected p-values of pairwise comparisons were com-\nputed with non-parametric rank-sum tests.\n","bbox":[53.79799999999997,415.47999999999996,295.637918,468.281],"page":"8"}},"crops":{"1":{"crop_coord":[878.2083383333334,252.36999500000002,1555.5659313888889,778.152796388889],"bbox":[317.9550018,513.6649933,558.2037353,699.3468018],"page":"2"},"2":{"crop_coord":[878.2083383333334,252.36707944778476,1555.5695200331654,777.8610991666667],"bbox":[317.9550018,513.7700043,558.2050272119395,699.3478513987975],"page":"4"},"3":{"crop_coord":[144.43890055555556,252.3664347222221,1555.5911086111112,541.5861255555553],"bbox":[53.7980042,598.8289948,558.2127991,699.3480835],"page":"5"},"4":{"crop_coord":[144.43890055555556,252.35833888888897,821.7944591666667,1254.8694525],"bbox":[53.7980042,342.0469971,294.0460053,699.350998],"page":"7"},"5":{"crop_coord":[878.2083383333334,252.36766388888907,1555.5608027777776,960.6249830555556],"bbox":[317.9550018,447.9750061,558.2018889999999,699.347641],"page":"7"},"6":{"crop_coord":[144.43890055555556,252.36750638888884,821.8140705555555,869.0722063888887],"bbox":[53.7980042,480.9340057,294.0530654,699.3476977],"page":"8"}}}},{"filename":"Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay.pdf","paper_id":"Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay","venue":"VR_2017","keywords":["Virtual Reality","Empathic Computing","Collaborative Gameplay","Physiological Sensors","Emotions","User Study"],"paragraph_containing_keyword":"Author Keywords\nVirtual Reality; Empathic Computing; Collaborative\nGameplay; Physiological Sensors; Emotions, User Study.","paragraph_after_keyword":"INTRODUCTION\nThe recent development of low cost consumer head-mounted\ndisplays (HMDs) such as the Oculus Rift [17] and HTC VIVE\n[11], have led to new and diverse ranges of immersive Virtual\nReality (VR) experiences. One of the interesting applications\nof VR is for generating empathy. Referring to VR, Thomas\nwrites “Perhaps the most profound long term applications,","doi":"10.1145/3025453.3026028","sections":[{"word_count":773,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1072,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1122,"figure_citations":{"1":["Figure 1), ensured that that the sensors remained in the correct position on the hand.","Figure 1)."],"2":["Figure 2(c))."]},"section_index":2,"title":"SYSTEM"},{"word_count":1475,"figure_citations":{"2":["Figure 2 shows our independent variables."],"3":["Figure 3)."]},"section_index":3,"title":"USER STUDY"},{"word_count":1315,"figure_citations":{"4":["Figure 4)."],"5":["Figure 5)."],"6":["Figure 6)."],"7":["Figure 7, demonstrated an interesting pattern difference between the two games."]},"section_index":4,"title":"RESULTS"},{"word_count":864,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":257,"figure_citations":{},"section_index":6,"title":"LIMITATIONS"},{"word_count":858,"figure_citations":{},"section_index":7,"title":"CONCLUSION AND FUTURE WORK"},{"word_count":68,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGMENTS"},{"word_count":1064,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Effects of Sharing Physiological States of Players in a Collaborative Virtual Reality Gameplay","authors":"Arindam Dey, Thammathip Piumsomboon, Youngho Lee, Mark Billinghurst","abstract":"Interfaces for collaborative tasks, such as multiplayer games can enable more effective and enjoyable collaboration. However, in these systems, the emotional states of the users are often not communicated properly due to their remoteness from one another. In this paper, we investigate the effects of showing emotional states of one collaborator to the other during an immersive Virtual Reality (VR) gameplay experience. We created two collaborative immersive VR games that display the real-time heart-rate of one player to the other. The two different games elicited different emotions, one joyous and the other scary. We tested the effects of visualizing heart-rate feedback in comparison with conditions where such a feedback was absent. The games had significant main effects on the overall emotional experience.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Participants were asked to stand during the gameplay sessions.\nThey were wearing HTC Vive HMD and two different heart-rate sensors\n- Empatica E4 and Biometric Glove with Arduino sensors. They were\nalso wearing a Logitech noise canceling headphone to hear the sounds of\nthe game.\n","bbox":[319.771,437.84327,565.5578100000001,481.67827],"page":"3"},"2":{"caption":"Figure 2. In this user study we used two different games and two different conditions. In our control condition, a heart-rate was not shown to the\nparticipants (a) and (c). In the experimental condition, we showed a heart-rate feedback to the participant that showed the real-time heart-rate of the\nplayer (b) and (d). (a - b) show the calm butterﬂy game and (c - d) show the scary zombie game.\n","bbox":[53.929,571.5222699999999,564.1604300000014,597.42527],"page":"4"},"3":{"caption":"Figure 3. The user study setup. Both player and the observer were standing during the gameplay sessions. They were physically located in a 15’ × 15’\ncalibrated area and could speak to each other to collaborate.\n","bbox":[53.929,485.01027,565.48801,502.06681999999995],"page":"5"},"4":{"caption":"Figure 4. Subjective feedback on three question. 4(a) how much did participants understood the emotional state of the player? 4(b) how much were\nthey attentive to the gameplay? 4(c) how much did enjoy the collaboration? Lower values indicate better outcome and whiskers represent ± 1 SE.\n","bbox":[53.928999999999974,508.5642699999999,564.1604300000012,525.5002699999999],"page":"7"},"5":{"caption":"Figure 5. Analysis of PANAS data showed an evidence of non-\nsigniﬁcantly more positive affect in the presence of heart-rate feedback.\nZombie game caused higher positive and negative affect than the Butter-\nﬂy game.\n","bbox":[53.929,436.66227,298.39281,471.53126999999995],"page":"8"},"6":{"caption":"Figure 6. Analysis of viewing angle shows that the butterﬂy game had\nsigniﬁcantly less difference in viewing angle between the observer and\nthe player than the zombie game. Whiskers represent ± 1 standard er-\nror.\n","bbox":[321.094,468.12127,565.48973,502.99026999999995],"page":"8"},"7":{"caption":"Figure 7. Observers looked differently in the butterﬂy game than in the zombie game. Their gaze direction was more aligned to the player in the\nbutterﬂy game but in the zombie game the direction was less aligned. 0° shows the normalized direction of player’s gaze.\n","bbox":[53.929,496.96327,564.1604300000005,513.90027],"page":"9"}},"crops":{"1":{"crop_coord":[886.9277613888887,168.22743722222225,1572.1260833333333,854.0750205555555],"bbox":[321.0939941,486.3329926,564.16539,729.6381226],"page":"3"},"2":{"crop_coord":[144.80278027777777,168.22972611111106,1572.1061200000001,532.5555505555557],"bbox":[53.9290009,602.0800018,564.1582032,729.6372986],"page":"4"},"3":{"crop_coord":[144.80278027777777,168.22120666666672,1572.1317630555554,797.7722083333333],"bbox":[53.9290009,506.602005,564.1674347,729.6403656],"page":"5"},"4":{"crop_coord":[151.72170702978335,-364.67326479249977,665.9079281202833,690.4966050959445],"bbox":[56.419814530722,545.22122216546,237.92685412330198,921.4823753252999],"page":"7"},"5":{"crop_coord":[633.276776866575,-436.83672195127804,1148.739645459075,690.5011838823053],"bbox":[229.779639671967,545.2195738023701,411.74627236526703,947.4612199024601],"page":"7"},"6":{"crop_coord":[1114.8331993644888,-435.6683399880279,1628.773712743683,690.5036901389445],"bbox":[403.139951771216,545.21867154998,584.5585365877259,947.04060239569],"page":"7"},"7":{"crop_coord":[144.80309915573054,-518.719701381944,882.517326313175,882.2678391553612],"bbox":[53.929115696063,476.18357790407,315.906237472743,976.9390924974998],"page":"8"},"8":{"crop_coord":[886.9276516231,-420.08877737500035,1614.3140914498779,794.8760625862221],"bbox":[321.093954584316,507.64461746896,579.353072921956,941.4319598550001],"page":"8"},"9":{"crop_coord":[144.80278027777777,168.2297683333332,1572.1205733333334,764.5722197222223],"bbox":[53.9290009,518.5540009,564.1634064,729.6372834],"page":"9"}}}},{"filename":"Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects ","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects .pdf","paper_id":"Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects ","venue":"VR_2017","keywords":["Organic User Interfaces","DisplayObjects","Shaped Displays","Vibrotactile Feedback"],"paragraph_containing_keyword":"ABSTRACT \nIn  this  paper,  we  report  on  a  study  investigating  a  novel \nhaptic  illusion  for  altering  the  perception  of  3D  shapes \nusing  a  non-planar  screen  and  vibrotactile  friction.  In  our \nstudy,  we  presented  an  image  of  a  rectangular  prism  on  a \ncylindrical  and  a  flat  display.  Participants  were  asked  to \nmove  their  index  finger  horizontally  along  the  surface  of \nthe  displays  towards  the  edge  of  the  rectangular  prism. \nParticipants  were  asked  whether  they  were  experiencing  a \nflat,  cylindrical  or  rectangular  shape.  In  one  condition,  a \nvibrotactile  stimulus  simulated  increasing  friction  towards \nthe  visible  edge  of  the  rectangular  prism,  with  a  sudden \ndrop-off when this edge was crossed by the finger. Results \nsuggest that presenting an image of a rectangular prism, and \napplying  vibrotactile  friction,  particularly  on  a  cylindrical \ndisplay,  significantly  increased  participant  ratings  stating \nthat they were experiencing a physical rectangular shape. \nAuthor Keywords \nOrganic User Interfaces; DisplayObjects; Shaped Displays; \nVibrotactile Feedback.  \nACM Classification Keywords \nH.5.m. Information interfaces and presentation (e.g., HCI): \nMiscellaneous. \nINTRODUCTION \nAccording  to  Ernst  et  al.  [9],  when  assessing  the  physical \nproperties  of  objects,  users  combine  visual  and  haptic \ninformation in an integral way. This allows users to not just \nassess  visual  affordances  of  the  object,  but  also  physical \nproperties \nshape,  weight,  weight  distribution, \ntemperature,  texture  and  other  material  properties.  Such \nhaptic properties are sensed through receptors in the skin of \nthe  fingertips  as  well  as  by  mechanoreceptors  in  muscle, \ntendons  and  joints  that  provide  kinesthetic  feedback  [23]. \nWhile  there  has  been  a  substantial  amount  of  work \nperformed  on  the  simulation  of  kinesthetic  properties  in \nVirtual Reality systems [31,12,18] and vibrotactile [13] and","paragraph_after_keyword":"such","doi":"10.1145/3025453.3025488","sections":[{"word_count":583,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":890,"figure_citations":{"3":["Figure 3a).","Figure 3b)."]},"section_index":1,"title":"RELATED WORK"},{"word_count":673,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2).","Figure 2), with one of its corners facing the participant."]},"section_index":2,"title":"EXPERIMENTAL STUDY"},{"word_count":217,"figure_citations":{"4":["Figure 4a).","Figure 4b).","Figure 4b).","Figure 4b)."]},"section_index":3,"title":"DISCUSSION"},{"word_count":301,"figure_citations":{"4":["Figure 4c)."]},"section_index":4,"title":"RESULTS"},{"word_count":165,"figure_citations":{},"section_index":5,"title":"ACKNOWLEDGEMENTS"},{"word_count":319,"figure_citations":{},"section_index":6,"title":"REFERENCES"},{"word_count":1046,"figure_citations":{},"section_index":7,"title":"CONCLUSIONS"}],"title":"Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects","authors":"Juan Pablo Carrascal, Roel Vertegaal","abstract":"In this paper, we report on a study investigating a novel haptic illusion for altering the perception of 3D shapes using a non-planar screen and vibrotactile friction. In our study, we presented an image of a rectangular prism on a cylindrical and a flat display. Participants were asked to move their index finger horizontally along the surface of the displays towards the edge of the rectangular prism. Participants were asked whether they were experiencing a flat, cylindrical or rectangular shape. In one condition, a vibrotactile stimulus simulated increasing friction towards the visible edge of the rectangular prism, with a sudden drop-off when this edge was crossed by the finger. Results suggest that presenting an image of a rectangular prism, and applying vibrotactile friction, particularly on a cylindrical display, significantly increased participant ratings stating that they were experiencing a physical rectangular shape.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Cylindrical DisplayObject with rectangular prism \nimage. \n","bbox":[321.8344,360.96336,552.792544,380.16192],"page":"1"},"2":{"caption":"Figure 2. Apparatus used in the study: Cylindrical display \n(left), Flat display (right), and Leap Motion (bottom). \n","bbox":[63.4368,536.1636,288.90444000000014,555.60192],"page":"2"},"3":{"caption":"Figure 3. Exploratory procedure used during the experiment \nfor the cylindrical display: a. Vibrotactile feedback is applied \nwhen the participant’s finger is dragged within the boundaries \nof the rectangular prism. Feedback amplitude increases \nlinearly as the finger is dragged towards the corner from \neither side. b. A strong pulse is produced when the finger \ncrosses the corner line, and feedback is stopped. \n","bbox":[55.07345999999998,520.0905599999996,297.3536039999997,590.8819199999999],"page":"3"},"4":{"caption":"Figure 4. Boxplots for scores of participants’ responses (7-point Likert scale, N = 19). \n","bbox":[142.4573,555.60192,470.2713800000001,564.48192],"page":"4"}},"crops":{"1":{"crop_coord":[875.6666480555556,467.6666344444443,1546.3333044444446,1128.3333163888888],"bbox":[317.0399933,387.6000061,554.8799896,621.8400116],"page":"1"},"2":{"crop_coord":[145,1670.9999594444444,820.9999847222222,2040.3332944444444],"bbox":[54,59.280014,293.7599945,188.6400146],"page":"1"},"3":{"crop_coord":[146.3333213888889,170.33331972222226,859.666646111111,649.6666716666667],"bbox":[54.4799957,559.9199982,307.6799926,728.8800049],"page":"2"},"4":{"crop_coord":[197.77777777777777,-1152.2222222222222,460.55555555555554,541.1111111111112],"bbox":[73,599,164,1205],"page":"4"},"5":{"crop_coord":[186.66666666666666,-1118.8888888888887,457.77777777777777,557.7777777777778],"bbox":[69,593,163,1193],"page":"4"},"6":{"crop_coord":[292.22222222222223,-1152.2222222222222,671.6666666666666,541.1111111111112],"bbox":[107,599,240,1205],"page":"4"},"7":{"crop_coord":[292.22222222222223,-1116.1111111111109,668.8888888888889,560.5555555555554],"bbox":[107,592,239,1192],"page":"4"},"8":{"crop_coord":[164.44444444444446,-1474.4444444444448,446.6666666666667,513.3333333333334],"bbox":[61,609,159,1321],"page":"4"},"9":{"crop_coord":[272.77777777777777,-1474.4444444444448,663.3333333333334,513.3333333333334],"bbox":[100,609,237,1321],"page":"4"},"10":{"crop_coord":[381.1111111111111,-1474.4444444444448,880,513.3333333333334],"bbox":[139,609,315,1321],"page":"4"},"11":{"crop_coord":[489.44444444444446,-1474.4444444444448,1096.6666666666667,513.3333333333334],"bbox":[178,609,393,1321],"page":"4"},"12":{"crop_coord":[636.6666666666666,-1474.4444444444448,1388.3333333333333,513.3333333333334],"bbox":[231,609,498,1321],"page":"4"},"13":{"crop_coord":[745,-1474.4444444444448,1605,513.3333333333334],"bbox":[270,609,576,1321],"page":"4"},"14":{"crop_coord":[853.3333333333334,-1474.4444444444448,1824.4444444444443,513.3333333333334],"bbox":[309,609,655,1321],"page":"4"},"15":{"crop_coord":[961.6666666666666,-1474.4444444444448,2041.111111111111,513.3333333333334],"bbox":[348,609,733,1321],"page":"4"},"16":{"crop_coord":[1108.888888888889,-1474.4444444444448,2335.5555555555557,513.3333333333334],"bbox":[401,609,839,1321],"page":"4"},"17":{"crop_coord":[1217.2222222222222,-1474.4444444444448,2552.222222222222,513.3333333333334],"bbox":[440,609,917,1321],"page":"4"},"18":{"crop_coord":[1325.5555555555557,-1474.4444444444448,2768.8888888888887,513.3333333333334],"bbox":[479,609,995,1321],"page":"4"},"19":{"crop_coord":[1433.888888888889,-1474.4444444444448,2985.5555555555557,513.3333333333334],"bbox":[518,609,1073,1321],"page":"4"},"20":{"crop_coord":[406.1111111111111,-1152.2222222222222,877.2222222222222,541.1111111111112],"bbox":[148,599,314,1205],"page":"4"},"21":{"crop_coord":[395,-1118.8888888888887,874.4444444444445,557.7777777777778],"bbox":[144,593,313,1193],"page":"4"},"22":{"crop_coord":[500.55555555555554,-1152.2222222222222,1085.5555555555557,541.1111111111112],"bbox":[182,599,389,1205],"page":"4"},"23":{"crop_coord":[500.55555555555554,-1116.1111111111109,1085.5555555555557,560.5555555555554],"bbox":[182,592,389,1192],"page":"4"},"24":{"crop_coord":[658.8888888888889,-1152.2222222222222,1385.5555555555557,541.1111111111112],"bbox":[239,599,497,1205],"page":"4"},"25":{"crop_coord":[650.5555555555555,-1118.8888888888887,1385.5555555555557,557.7777777777778],"bbox":[236,593,497,1193],"page":"4"},"26":{"crop_coord":[756.1111111111111,-1152.2222222222222,1596.6666666666667,541.1111111111112],"bbox":[274,599,573,1205],"page":"4"},"27":{"crop_coord":[753.3333333333334,-1118.8888888888887,1591.111111111111,557.7777777777778],"bbox":[273,593,571,1193],"page":"4"},"28":{"crop_coord":[870,-1152.2222222222222,1807.7777777777778,541.1111111111112],"bbox":[315,599,649,1205],"page":"4"},"29":{"crop_coord":[858.8888888888889,-1118.8888888888887,1802.2222222222222,557.7777777777778],"bbox":[311,593,647,1193],"page":"4"},"30":{"crop_coord":[964.4444444444445,-1152.2222222222222,2016.111111111111,541.1111111111112],"bbox":[349,599,724,1205],"page":"4"},"31":{"crop_coord":[964.4444444444445,-1118.8888888888887,2013.3333333333333,557.7777777777778],"bbox":[349,593,723,1193],"page":"4"},"32":{"crop_coord":[1139.4444444444443,-1152.2222222222222,2346.6666666666665,541.1111111111112],"bbox":[412,599,843,1205],"page":"4"},"33":{"crop_coord":[1131.111111111111,-1118.8888888888887,2346.6666666666665,557.7777777777778],"bbox":[409,593,843,1193],"page":"4"},"34":{"crop_coord":[1236.6666666666667,-1152.2222222222222,2557.777777777778,541.1111111111112],"bbox":[447,599,919,1205],"page":"4"},"35":{"crop_coord":[1233.888888888889,-1118.8888888888887,2552.222222222222,557.7777777777778],"bbox":[446,593,917,1193],"page":"4"},"36":{"crop_coord":[1350.5555555555557,-1152.2222222222222,2766.1111111111113,541.1111111111112],"bbox":[488,599,994,1205],"page":"4"},"37":{"crop_coord":[1339.4444444444443,-1118.8888888888887,2763.3333333333335,557.7777777777778],"bbox":[484,593,993,1193],"page":"4"},"38":{"crop_coord":[1445,-1152.2222222222222,2977.222222222222,541.1111111111112],"bbox":[522,599,1070,1205],"page":"4"},"39":{"crop_coord":[1445,-1118.8888888888887,2974.4444444444443,557.7777777777778],"bbox":[522,593,1069,1193],"page":"4"}}}},{"filename":"Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures.pdf","paper_id":"Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures","venue":"chi2019xr","keywords":["Texture perception","Passive haptic feedback","3D printing"],"paragraph_containing_keyword":"ABSTRACT \nExperiencing materials in virtual reality (VR) is enhanced \nby  combining  visual  and  haptic  feedback.  While  VR  eas-\nily allows changes to visual appearances, modifying haptic \nimpressions remains challenging. Existing passive haptic \ntechniques require access to a large set of tangible proxies. \nTo reduce the number of physical representations, we look \ntowards fabrication to create more versatile counterparts. In \na user study, 3D-printed hairs with length varying in steps \nof 2.5 mm were used to infuence the feeling of roughness \nand hardness. By overlaying fabricated hair with visual tex-\ntures, the resolution of the user’s haptic perception increased. \nAs changing haptic sensations are able to elicit perceptual \nswitches, our approach can extend a limited set of textures \nto a much broader set of material impressions. Our results \ngive insights into the efectiveness of 3D-printed hair for \nenhancing texture perception in VR. \nCCS CONCEPTS \n• Human-centered computing → User studies; Haptic \ndevices; Virtual reality. \nKEYWORDS \ntexture perception; passive haptic feedback; 3D printing \nACM Reference Format: \nDonald Degraen, André Zenner, and Antonio Krüger. 2019. En-\nhancing Texture Perception in Virtual Reality Using 3D-Printed \nHair Structures. In CHI Conference on Human Factors in Computing \nSystems Proceedings (CHI 2019), May 4–9, 2019, Glasgow, Scotland \nPermission to make digital or hard copies of all or part of this work for \npersonal  or  classroom  use  is  granted  without  fee  provided  that  copies \nare not made or distributed for proft or commercial advantage and that \ncopies bear this notice and the full citation on the frst page. Copyrights \nfor components of this work owned by others than the author(s) must \nbe honored. Abstracting with credit is permitted. To copy otherwise, or \nrepublish, to post on servers or to redistribute to lists, requires prior specifc \npermission and/or a fee. Request permissions from permissions@acm.org. \nCHI 2019, May 4–9, 2019, Glasgow, Scotland UK \n© 2019 Copyright held by the owner/author(s). Publication rights licensed \nto ACM. \nACM ISBN 978-1-4503-5970-2/19/05. . . $15.00 \nhttps://doi.org/10.1145/3290605.3300479","doi":"10.1145/3290605.3300479","paragraph_after_keyword":"Figure  1:  Augmenting  texture  perception  by  overlaying  a \nphysical structure with diferent virtual textures.","sections":[{"word_count":419,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1201,"figure_citations":{"1":["Figure 1 illustrates superimposing virtual texture images on top of physical textures."]},"section_index":1,"title":"RELATED WORK"},{"word_count":883,"figure_citations":{"2":["Figure 2), the customer is able to Page 3 CHI 2019 Paper mm 2."],"3":["Figure 3) was printed using an Autodesk Ember1 with an X-Y resolution of 50 µm and a layer thickness of 25 µm.","Figure 3, were purchased from various sources and imported into the Unity environment."]},"section_index":2,"title":"PERCEPTION"},{"word_count":1319,"figure_citations":{"4":["Figure 4, allowed participants to precisely hit a required hair structure without touching a diferent surface.","Figure 4, while answering 6 questions, i."]},"section_index":3,"title":"STUDY"},{"word_count":2390,"figure_citations":{"5":["Figure 5a and 5c) and the visual ratings of virtual textures without haptic information (see Figure 5b and 5d).","Figure 5a and 5c indicate signifcant diferences."]},"section_index":4,"title":"RESULTS"},{"word_count":678,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":156,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":37,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGMENTS"},{"word_count":1225,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures","authors":"Donald Degraen, André Zenner, Antonio Krüger","abstract":"Experiencing materials in virtual reality (VR) is enhanced by combining visual and haptic feedback. While VR easily allows changes to visual appearances, modifying haptic impressions remains challenging. Existing passive haptic techniques require access to a large set of tangible proxies. To reduce the number of physical representations, we look towards fabrication to create more versatile counterparts. In a user study, 3D-printed hairs with length varying in steps of 2.5 mm were used to influence the feeling of roughness and hardness. By overlaying fabricated hair with visual textures, the resolution of the user's haptic perception increased. As changing haptic sensations are able to elicit perceptual switches, our approach can extend a limited set of textures to a much broader set of material impressions. Our results give insights into the effectiveness of 3D-printed hair for enhancing texture perception in VR.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure  1:  Augmenting  texture  perception  by  overlaying  a \nphysical structure with diferent virtual textures. \n","bbox":[317.9549999999999,321.51382079999996,560.4423215999999,341.56269119999996],"page":"1"},"2":{"caption":"Figure 2: A sample book to explore diferent fabrics. \n","bbox":[332.648,569.0977616,545.7524288000001,578.1896912],"page":"3"},"3":{"caption":"Figure 3: Overview of the fve physical surfaces and fve virtual textures used in our study. Note that the material used for \nglass is highly transparent and would refect the environment. \n","bbox":[53.798,540.0048208000001,560.6327263999999,560.0536912],"page":"4"},"4":{"caption":"Figure 4: Experiment Setup. Lef:  A user touching a physi-\ncal sample on the proxy plate with a Vive controller in the \ncenter and a Leap Motion positioned above the user’s hand. \nRight Top: First person view of a user touching a virtual plas-\ntic material. Right Botom: A patch of 3D-printed hair. \n","bbox":[317.955,443.5060992000001,561.975576,496.4526912],"page":"5"},"5":{"caption":"Figure  5:  Boxplots  depicting  the  baseline  assessments.  Haptic  ratings  of  the  physical  samples  without  visual  information \nfor roughness (a) and hardness (c). Visual ratings of the virtual textures without haptic information for roughness (b) and \nhardness (d). Brackets connect groups with statistically signifcant diferences (p < .05). \n","bbox":[53.79299359999999,448.61362560000003,560.4394255999999,479.6553024],"page":"7"},"6":{"caption":"Figure 6: Stacked perception graphs per virtual texture indicating for each physical sample the percentage of samples identifed \nper material category. Unassigned space depicts the percentage of times no meaningful material or object was assigned. \n","bbox":[53.797999999999945,456.5248208,560.444432,476.5736912],"page":"9"}},"crops":{"1":{"crop_coord":[878.2083333333334,602.9330555555556,1555.5469444444443,1221.2],"bbox":[317.955,354.168,558.1968999999999,573.1441],"page":"1"},"2":{"crop_coord":[928.2583333333333,252.3723819444445,1505.4728541666668,563.9027777777779],"bbox":[335.973,590.795,540.1702275,699.3459425],"page":"3"},"3":{"crop_coord":[179.46944444444446,252.37020709538893,1520.5310715349444,614.2805555555556],"bbox":[66.409,572.659,545.59118575258,699.34672544566],"page":"4"},"4":{"crop_coord":[878.2083333333334,252.36063868999997,1555.5650882675557,790.9499999999999],"bbox":[317.955,509.058,558.20343177632,699.3501700716],"page":"5"},"5":{"crop_coord":[144.4388888888889,252.36888888888876,504.71888888888884,787.7888888888889],"bbox":[53.798,510.196,179.8988,699.3472],"page":"7"},"6":{"crop_coord":[499.3333333333333,252.36888888888876,859.6133333333333,787.7888888888889],"bbox":[181.56,510.196,307.6608,699.3472],"page":"7"},"7":{"crop_coord":[854.2277777777778,252.36888888888876,1214.5077777777778,787.7888888888889],"bbox":[309.322,510.196,435.4228,699.3472],"page":"7"},"8":{"crop_coord":[1209.1194444444445,252.36888888888876,1569.3994444444445,787.7888888888889],"bbox":[437.083,510.196,563.1838,699.3472],"page":"7"},"9":{"crop_coord":[144.4388888888889,252.36944444444447,1555.5588888888888,846.1694444444445],"bbox":[53.798,489.179,558.2012,699.347],"page":"9"}}}},{"filename":"Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation.pdf","paper_id":"Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation","venue":"chi2019xr","keywords":["Electroencephalography","Embodiment","Illusory Touch","Transcranial Direct Current Stimulation","Virtual Reality"],"paragraph_containing_keyword":"CCS CONCEPTS\n• Human-centered computing → Virtual reality; User\nstudies; Empirical studies in HCI ;\nKEYWORDS\nElectroencephalography, Embodiment, Illusory Touch, Tran-\nscranial Direct Current Stimulation, Virtual Reality","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nCHI 2019, May 4–9, 2019, Glasgow, Scotland Uk\n© 2019 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 978-1-4503-5970-2/19/05. . . $15.00\nhttps://doi.org/10.1145/3290605.3300477","doi":"10.1145/3290605.3300477","sections":[{"word_count":1915,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":603,"figure_citations":{"1":["Figure 1)."]},"section_index":1,"title":"STIMULATION"},{"word_count":1570,"figure_citations":{"1":["Figure 1)1 ."],"2":["Figure 2) consisted of just resting the hands on the desk, paying attention to the collision of the balls with the hands."],"3":["Figure 3).","Figure 3), participants were instructed to place their hands on the physical desk (which coincided with the VR desk) on their little fingers to comfortably hit the balls towards the middle of the desk, into the hole."]},"section_index":2,"title":"METHODOLOGY"},{"word_count":750,"figure_citations":{},"section_index":3,"title":"EVALUATION"},{"word_count":872,"figure_citations":{"4":["Figure 4), but this effect was not statistically significant.","Figure 4), as the passive phase hand-ball collisions elicited distinct P300, contrary to the active phase."]},"section_index":4,"title":"RESULTS"},{"word_count":1096,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":499,"figure_citations":{},"section_index":6,"title":"CONCLUSION AND FUTURE WORK"},{"word_count":1959,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation","authors":"Filip Škola, Fotis Liarokapis","abstract":"Virtual reality (VR) can be immersive to such a degree that users sometimes report feeling tactile sensations based on visualization of the touch, without any actual physical contact. This effect is not only interesting for studies of human perception, but can also be leveraged to improve the quality of VR by evoking tactile sensations without usage of specialized equipment. The aim of this paper is to study brain processing of the illusory touch and its enhancement for purposes of exploitation in VR scene design. To amplify the illusory touch, transcranial direct current stimulation (tDCS) was used. Participants attended two sessions with blinded stimulation and interacted with a virtual ball using tracked hands in VR. The effects were studied using electroencephalography (EEG), that allowed us to examine stimulation-induced changes in processing of the illusory touch in the brain, as well as to identify its neural correlates. Results confirm enhanced processing of the illusory touch after the stimulation, and some of these changes were correlated to subjective rating of its magnitude.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: EEG and tDCS channel set-up. All the channels ex-\ncept for AF3 and AF4 were used for the EEG recording (white\nand semi-white background). For right hand stimulation, C3\nwas the anode and AF4 the return electrode (yellow). For left\nhand stimulation, C4 was the anode and AF3 the return elec-\ntrode (green).\n","bbox":[317.60499999999996,384.455854,559.794909034,448.21585400000004],"page":"4"},"2":{"caption":"Figure 2: Experimental setting and the experimental VR scene.\n","bbox":[179.139,499.243854,432.849893034,508.209854],"page":"5"},"3":{"caption":"Figure 3: Active phase of the experiment.\n","bbox":[222.832,499.383854,389.1602659999999,508.349854],"page":"6"},"4":{"caption":"Figure 4: Grand-averaged ERPs for the dominant hand at electrode location Pz, passive condition. Left plot – non-stimulated,\nright plot – stimulated. To generate these plots, data were cleaned with higher high-pass filter setting (cut-off 2.5 Hz) than for\nthe analysis purposes. Enhanced N140 and P100 are visible at these plots, as well as stronger P300 in the stimulated condition.\n","bbox":[53.798,557.2268540000001,559.2741551019998,588.109854],"page":"8"}},"crops":{"1":{"crop_coord":[878.2083333333334,252.35961111111146,1555.5447777777777,924.588888888889],"bbox":[317.955,460.948,558.19612,699.3505399999999],"page":"4"},"2":{"crop_coord":[144.4388888888889,266.5861111111111,840.9722222222222,705.6694444444445],"bbox":[53.798,539.759,300.95,694.229],"page":"5"},"3":{"crop_coord":[859.0027777777777,266.22111111111116,1555.461111111111,705.6694444444445],"bbox":[311.041,539.759,558.1659999999999,694.3604],"page":"5"},"4":{"crop_coord":[144.4388888888889,266.19722222222225,840.9722222222222,705.2805555555557],"bbox":[53.798,539.899,300.95,694.369],"page":"6"},"5":{"crop_coord":[859.0027777777777,266.46233333333345,1555.5525,705.2805555555557],"bbox":[311.041,539.899,558.1989,694.27356],"page":"6"},"6":{"crop_coord":[144.4388888888889,252.36599999999987,1555.563111111111,535.9944444444445],"bbox":[53.798,600.842,558.20272,699.34824],"page":"8"}}}},{"filename":"Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality.pdf","paper_id":"Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality","venue":"chi2019xr","keywords":["Interaction techniques","Controlled experiments"],"paragraph_containing_keyword":"ABSTRACT\nWe present an empirical comparison of eleven bare hand,\nmid-air mode-switching techniques suitable for virtual real-\nity in two experiments. The first evaluates seven techniques\nspanning dominant and non-dominant hand actions. Tech-\nniques represent common classes of actions selected by a\nmethodical examination of 56 examples of prior art. The\nstandard “subtraction method” protocol is adapted for 3D\ninterfaces, with two baseline selection methods, bare hand\npinch and device controller button. A second experiment\nwith four techniques explores more subtle dominant-hand\ntechniques and the effect of using a dominant hand device\nfor selection. Results provide guidance to practitioners when\nchoosing bare hand, mid-air mode-switching techniques,\nand for researchers when designing new mode-switching\nmethods in VR.\nKEYWORDS\ninteraction techniques, controlled experiments\nACM Reference Format:\nHemant Bhaskar Surale, Fabrice Matulic, and Daniel Vogel. 2019.\nExperimental Analysis of Bare Hand Mid-air Mode-Switching Tech-\nniques in Virtual Reality. In CHI Conference on Human Factors in\nComputing Systems Proceedings (CHI 2019), May 4–9, 2019, Glas-\ngow, Scotland UK. ACM, New York, NY, USA, 14 pages. https:\n//doi.org/10.1145/3290605.3300426\n1 INTRODUCTION\nRaskin defines a mode as a distinct setting within an inter-\nface where the same user input produces results different\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request\npermissions from permissions@acm.org.\nCHI 2019, May 4–9, 2019, Glasgow, Scotland UK\n© 2019 Association for Computing Machinery.\nACM ISBN 978-1-4503-5970-2/19/05.\nhttps://doi.org/10.1145/3290605.3300426","doi":"10.1145/3290605.3300426","paragraph_after_keyword":"from those it would produce in other settings [68]. Mode-\nswitching is simply the transition from one mode to another.\nModes are common in all interfaces, including interfaces\nfor Virtual Reality (VR) and Augmented Reality (AR). For\nexample, a hand gesture-based 3D modelling application\nmay have different modes for object creation, selection, and\ntransformation. Depending on the mode, the movement of\nthe hand is interpreted differently. Completing a task typi-\ncally requires frequent mode-switching, so understanding\nthe performance of different methods is important.\nIn VR, bare hand mid-air input is an alternative to device\ncontrollers. Techniques have been proposed for VR, AR, and\nrelated contexts using hands only (e.g. [61, 64, 93]) and hands\ncombined with body postures (e.g. [12, 100]). Evaluations\nhave focused on tasks like pointing (e.g. [99]), object manip-\nulation (e.g. [69]), selection (e.g. [59]), and annotation [16],\nbut no extensive comparisons of mode-switching techniques\nhave been performed yet. Mode-switching techniques for\nmice [18], styli [48, 88], and touch [83] have been evaluated,\nbut generalizing those results to 3D environments like VR is\nnot straightforward.\nWe provide missing empirical evidence for the perfor-\nmance of bare hand mid-air mode-switching techniques in\nVR. Our focus is absolute, single-point input, suitable for the\nkind of direct object manipulations common in VR such as\npointing at, grabbing and moving 3D elements in the virtual\nenvironment. To select techniques to evaluate, we examined\nbare hand mid-air interaction in different settings, then used\nthree criteria to identify six classes of techniques suitable for\nmode-switching in VR. In two related experiments, we com-\npare common input actions selected from each class using an\nadapted “subtraction method” protocol [18], used previously\nfor 2D input.\nThe first experiment compares seven techniques, with\na dominant-hand pinch as the fundamental manipulation\ntrigger. The mode-switching techniques include three dom-\ninant hand postures: a fist, an open palm and pointing the\nindex finger; and four non-dominant hand postures: a fist,\nan open palm, bringing the hand into the field-of-view and\ntouching the head. As a comparison baseline, the button of a\ndevice controller held in the non-dominant hand was also","sections":[{"word_count":661,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":4670,"figure_citations":{"2":["Figure 2)."],"3":["Figure 3)."]},"section_index":1,"title":"BACKGROUND AND RELATED WORK"},{"word_count":1809,"figure_citations":{"5":["Figure 5)."],"8":["Figure 8)."],"9":["Figure 9)."]},"section_index":2,"title":"BASELINE"},{"word_count":60,"figure_citations":{},"section_index":3,"title":"PINCH"},{"word_count":17,"figure_citations":{},"section_index":4,"title":"CONTROLLER"},{"word_count":724,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":146,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":31,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGEMENTS"},{"word_count":3322,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality","authors":"Hemant Bhaskar Surale, Fabrice Matulic, Daniel Vogel","abstract":"We present an empirical comparison of eleven bare hand, mid-air mode-switching techniques suitable for virtual reality in two experiments. The first evaluates seven techniques spanning dominant and non-dominant hand actions. Techniques represent common classes of actions selected by a methodical examination of 56 examples of prior art. The standard \"subtraction method\" protocol is adapted for 3D interfaces, with two baseline selection methods, bare hand pinch and device controller button. A second experiment with four techniques explores more subtle dominant-hand techniques and the effect of using a dominant hand device for selection. Results provide guidance to practitioners when choosing bare hand, mid-air mode-switching techniques, and for researchers when designing new mode-switching methods in VR.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Selected Mode-switching techniques: (a) non dominant fist (nd-fist); (b) non dominant palm (nd-palm); (c) hand in\nfield of view (nd-fov); (d) touch head (nd-head); (e) dominant fist (d-fist); (f) dominant palm (d-palm); (g) point (d-point) .\n","bbox":[53.798,592.64217,558.1803300000001,612.5671699999999],"page":"4"},"2":{"caption":"Figure 2: Line drawing task: (a) baseline; (b) compound.\n","bbox":[326.402,533.4861699999999,549.7450600000001,542.4521699999999],"page":"5"},"3":{"caption":"Figure 3: Mode-switch time by techniqe (error bars in all\ngraphs are 95% CI).\n","bbox":[317.9549999999999,528.95817,558.198462,548.88317],"page":"7"},"4":{"caption":"Figure 4: Overall error rate by techniqe.\n","bbox":[353.037,285.430956,523.11834,296.45017],"page":"7"},"5":{"caption":"Figure 5: Proportion of specific error rates by techniqe.\nNote multiple error types can occur in a cycle.\n","bbox":[53.798,553.93217,295.57634,573.85717],"page":"8"},"6":{"caption":"Figure 6: Mode switching techniques evaluated in experi-\nment 2. (a) orientated pinch (d-orient); (b) middle finger\npinch (d-middle)\n","bbox":[53.798,90.86416999999999,295.6379180000001,121.74817],"page":"9"},"7":{"caption":"Figure 7: Mode-switch times by techniqe.\n","bbox":[349.177,223.18817,526.97834,232.15417],"page":"9"},"8":{"caption":"Figure 8: Overall error rate by techniqe. Baseline tech-\nniques have start and end error rates.\n","bbox":[53.798,450.024956,295.645598,472.00317],"page":"10"},"9":{"caption":"Figure 9: Proportion of specific error rates by techniqe.\n","bbox":[56.884,354.10017,290.95734,363.06617],"page":"10"}},"crops":{"1":{"crop_coord":[149.94030424910557,287.7200416180557,1550.8452277166057,529.1002373333331],"bbox":[55.778509529678004,603.32391456,556.504281977978,686.6207850175],"page":"4"},"2":{"crop_coord":[870.4554836791389,255.08175049847222,1578.0711444424724,732.6211804876386],"bbox":[315.16397412449,530.0563750244501,566.30561199929,698.37056982055],"page":"5"},"3":{"crop_coord":[877.8773073611111,252.36780777777773,1555.5667783333333,675.8254920833335],"bbox":[317.83583065,550.5028228499999,558.2040402,699.3475892],"page":"7"},"4":{"crop_coord":[896.5997570880555,966.8726640482499,1568.5266408958332,1368.2360171438054],"bbox":[324.5759125517,301.23503382823003,562.8695907225,442.12584094263],"page":"7"},"5":{"crop_coord":[144.197038125,225.86181887722202,821.7944259919444,606.3641040972223],"bbox":[53.710933725000004,575.508922525,294.0459933571,708.8897452042],"page":"8"},"6":{"crop_coord":[147.32776747861112,1692.7150616666665,865.0780054916665,1933.1904749583332],"bbox":[54.8379962923,97.851429015,309.62808197699997,180.8225778],"page":"9"},"7":{"crop_coord":[877.7870156250001,1254.1359814640223,1555.5835083333334,1568.3899620598554],"bbox":[317.803325625,229.17961365845198,558.210063,338.71104667295197],"page":"9"},"8":{"crop_coord":[149.58395079355557,610.0199901024779,830.5118098435555,885.3288593491445],"bbox":[55.65022228568,475.081610634308,297.18425154368,570.592803563108],"page":"10"},"9":{"crop_coord":[144.1970063888889,850.5548357588889,821.7949115122224,1191.8946922222221],"bbox":[53.7109223,364.7179108,294.04616814440004,484.0002591268],"page":"10"}}}},{"filename":"Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements.pdf","paper_id":"Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements","venue":"chi2019xr","keywords":["Virtual reality","Interaction fidelity","Games","Whole body interaction","Virtual objects","Player experience"],"doi":"10.1145/3290605.3300644","paragraph_containing_keyword":"KEYWORDS\nvirtual reality; interaction fidelity; games; whole body inter-\naction; virtual objects; player experience.","paragraph_after_keyword":"ACM Reference Format:\nKatja Rogers, Jana Funke, Julian Frommel, Sven Stamm, and Michael\nWeber. 2019. Exploring Interaction Fidelity in Virtual Reality: Object\nManipulation and Whole-Body Movements. In CHI Conference on\nHuman Factors in Computing Systems Proceedings (CHI 2019), May 4ś\n9, 2019, Glasgow, Scotland UK. ACM, New York, NY, USA, 14 pages.\nhttps://doi.org/10.1145/3290605.3300644","sections":[{"word_count":496,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":784,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":2299,"figure_citations":{},"section_index":2,"title":"RESEARCH QUESTIONS"},{"word_count":4673,"figure_citations":{},"section_index":3,"title":"INTUI"},{"word_count":527,"figure_citations":{},"section_index":4,"title":"OVERALL IMPLICATIONS"},{"word_count":226,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":1670,"figure_citations":{},"section_index":6,"title":"REFERENCES"}],"title":"Exploring Interaction Fidelity in Virtual Reality: Object Manipulation and Whole-Body Movements","authors":"Katja Rogers, Jana Funke, Julian Frommel, Sven Stamm, Michael Weber","abstract":"High degrees of interaction fidelity (IF) in virtual reality (VR) are said to improve user experience and immersion, but there is also evidence of low IF providing comparable experiences. VR games are now increasingly prevalent, yet we still do not fully understand the trade-off between realism and abstraction in this context. We conducted a lab study comparing high and low IF for object manipulation tasks in a VR game. In a second study, we investigated players' experiences of IF for whole-body movements in a VR game that allowed players to crawl underneath virtual boulders and \"dangle'' along monkey bars. Our findings show that high IF is preferred for object manipulation, but for whole-body movements, moderate IF can suffice, as there is a trade-off with usability and social factors. We provide guidelines for the development of VR games based on our results.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: In two user studies, we explored player experiences with varying degrees of interaction fidelity in VR. One study\nexplored object manipulation (a & b), while the other focused on whole-body movements (c).\n","bbox":[53.798,366.07009199999993,558.4313779999999,385.99409199999997],"page":"1"},"2":{"caption":"Figure 2: Task T1: In the HF condition, users had to physi-\ncally use each buckle a⃝ and then push open the chest’s lid\nb⃝, while the low-fidelity version offered clicks and a widget\nc⃝ to allow users to interact with the chest and its contents.\n","bbox":[53.79699999999998,585.1910919999999,295.64086,629.150068],"page":"4"},"3":{"caption":"Figure 3: Potion brewing (T2) in HF involved physically\npouring potion vial contents into the cauldron a⃝. In the\nLF variant, players dragged potion vials from their inven-\ntory to the activated cauldron widget b⃝.\n","bbox":[317.955,591.1190920000001,559.7981080000001,635.078068],"page":"4"},"4":{"caption":"Figure 4: The higher fidelity condition resulted in signifi-\ncantly higher values for all IEQ factors except challenge.\n","bbox":[317.955,592.919092,559.794918,612.8440919999999],"page":"5"},"5":{"caption":"Figure 5: The player (b) and their view (a) while crawling,\ncompared to the player (d) and their view (c) while dangling\nalong two bars.\n","bbox":[317.955,544.7800920000001,559.283856,575.664092],"page":"7"}},"crops":{"1":{"crop_coord":[144.44800466111113,-289.02373842777774,1705.067586686111,1097.185205197222],"bbox":[53.801281678,398.813326129,612.024331207,894.2485458340001],"page":"1"},"2":{"crop_coord":[144.43890055555556,252.37024944444448,821.6555616666667,427.6999919444443],"bbox":[53.7980042,639.8280029,293.9960022,699.3467102],"page":"4"},"3":{"crop_coord":[911.5777841666668,252.37334361111118,1522.163891111111,411.2333425000002],"bbox":[329.9680023,645.7559967,546.1790008,699.3455963],"page":"4"},"4":{"crop_coord":[877.9094097444444,-1514.8290635472224,2438.9179043805557,445.7815831111109],"bbox":[317.847387508,633.31863008,876.210445577,1335.538462877],"page":"5"},"5":{"crop_coord":[935.2712419361112,-1459.459019513889,1927.5450458722223,391.8588495805555],"bbox":[338.497647097,652.730814151,692.116216514,1315.605247025],"page":"5"},"6":{"crop_coord":[981.1573741527778,-1465.248261525,2019.3089764638887,391.8588495805555],"bbox":[355.016654695,652.730814151,725.1512315269999,1317.689374149],"page":"5"},"7":{"crop_coord":[1077.5243632916668,-1509.1037143222222,2172.2460828361113,354.04543199166676],"bbox":[389.708770785,666.3436444829999,780.208589821,1333.477337156],"page":"5"},"8":{"crop_coord":[1077.5243632916668,-1498.8141978111114,2172.2460828361113,359.19019024722195],"bbox":[389.708770785,664.4915315110001,780.208589821,1329.773111212],"page":"5"},"9":{"crop_coord":[1077.5243632916668,-1503.9589560666668,2172.2460828361113,356.61503317222224],"bbox":[389.708770785,665.418588058,780.208589821,1331.625224184],"page":"5"},"10":{"crop_coord":[1057.6203714444443,-1571.4325161472223,2172.243304888889,329.53004775555576],"bbox":[382.54333371999996,675.169182808,780.20758976,1355.915705813],"page":"5"},"11":{"crop_coord":[1103.503725713889,-1552.800824127778,2264.0100134277773,348.1617397750002],"bbox":[399.061341257,668.461773681,813.2436048339999,1349.208296686],"page":"5"},"12":{"crop_coord":[1199.8790486944445,-1487.505174669444,2416.9471198,364.8460907916667],"bbox":[433.75645753000003,662.455407315,868.3009631279999,1325.701862881],"page":"5"},"13":{"crop_coord":[1179.969500952778,-1562.6958721333335,2416.9415639055555,335.18317035277784],"bbox":[426.589020343,673.134058673,868.2989630059999,1352.7705139680002],"page":"5"},"14":{"crop_coord":[1245.7568470694443,-1348.7022637638884,2508.7110503916665,434.2475462444445],"bbox":[450.27246494499997,637.470883352,901.3359781409999,1275.7328149549999],"page":"5"},"15":{"crop_coord":[1225.8528552222224,-1533.774663602778,2508.708272444444,354.84825873888883],"bbox":[443.10702788000003,666.054626854,901.3349780799999,1342.358878897],"page":"5"},"16":{"crop_coord":[1322.2281782027776,-1401.144351425,2661.6537126583335,408.02650241388875],"bbox":[477.802144153,646.910459131,956.395336557,1294.611966513],"page":"5"},"17":{"crop_coord":[1322.2281782027776,-1431.982343538889,2661.6537126583335,392.6088953305554],"bbox":[477.802144153,652.460797681,956.395336557,1305.713643674],"page":"5"},"18":{"crop_coord":[1302.3241863555554,-1539.9444843833332,2661.6537126583335,351.37860265833314],"bbox":[470.636707088,667.303703043,956.395336557,1344.580014378],"page":"5"},"19":{"crop_coord":[1368.1115324722223,-1362.5808880861114,2753.4148653027773,427.30823408333333],"bbox":[494.32015169,639.96903573,989.4293515089998,1280.729119711],"page":"5"},"20":{"crop_coord":[1348.207540625,-1506.2090933166671,2753.4120873555557,370.6520004861111],"bbox":[487.154714625,660.365279825,989.4283514480001,1332.4352735940001],"page":"5"},"21":{"crop_coord":[1424.673315863889,-1523.696271080556,2906.349193727778,362.11814661944425],"bbox":[514.682393711,663.4374672170001,1044.4857097420002,1338.7306575890002],"page":"5"},"22":{"crop_coord":[1470.5566701333335,-1498.0863756388883,2998.115902266667,372.02986230833335],"bbox":[531.200401248,659.869249569,1077.5217248160002,1329.51109523],"page":"5"},"23":{"crop_coord":[1215.3327279385924,-1679.558425216699,2454.9521331279007,270.9348722193739],"bbox":[439.31978205789324,696.2634460010254,881.9827679260442,1394.8410330780116],"page":"5"},"24":{"crop_coord":[1263.8051273845213,-1679.558425216699,2551.8969320197584,270.9348722193739],"bbox":[456.76984585842763,696.2634460010254,916.8828955271131,1394.8410330780116],"page":"5"},"25":{"crop_coord":[978.373871036111,-1382.4487666194443,2438.89845875,570.2947335055555],"bbox":[354.014593573,588.493895938,876.20344515,1287.881555983],"page":"7"}}}},{"filename":"Exploring Virtual Agents for Augmented Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Exploring Virtual Agents for Augmented Reality.pdf","paper_id":"Exploring Virtual Agents for Augmented Reality","venue":"chi2019xr","keywords":["Augmented reality","Embodied conversational agents"],"paragraph_containing_keyword":"KEYWORDS \nAugmented reality; embodied conversational agents","paragraph_after_keyword":"ACM Reference format: \nIsaac Wang, Jesse Smith, and Jaime Ruiz. 2019. Exploring Virtual Agents \nfor  Augmented  Reality.  In  2019  CHI  Conference  on  Human  Factors  in \nComputing  Systems  Proceedings  (CHI  2019),  May  4–9,  2019,  Glasgow, \nScotland,  UK.  ACM,  New  York,  NY,  USA.  Paper  281,  10  pages. \nhttps://doi.org/10.1145/3290605.3300511","doi":"10.1145/3290605.3300511","sections":[{"word_count":650,"figure_citations":{"1":["Figure 1)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":856,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1180,"figure_citations":{"2":["Figure 2).","Figure 2), and that he has a slightly higher-pitched voice than Jake to account for his smaller size."]},"section_index":2,"title":"AUGMENTED REALITY AGENTS"},{"word_count":672,"figure_citations":{"4":["Figure 4)."]},"section_index":3,"title":"USER STUDY"},{"word_count":1679,"figure_citations":{"5":["Figure 5).","Figure 5b) as well as the length of each utterance (Figure 5c).","Figure 5d), as well as the duration of each gaze (Figure 5e)."],"7":["Figure 7)."]},"section_index":4,"title":"RESULTS"},{"word_count":1511,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":432,"figure_citations":{},"section_index":6,"title":"LIMITATIONS AND FUTURE WORK"},{"word_count":139,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":67,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGMENTS"},{"word_count":1649,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Exploring Virtual Agents for Augmented Reality","authors":"Isaac Wang, Jesse Smith, Jaime Ruiz","abstract":"Prior work has shown that embodiment can benefit virtual agents, such as increasing rapport and conveying non-verbal information. However, it is unclear if users prefer an embodied to a speech-only agent for augmented reality (AR) headsets that are designed to assist users in completing real-world tasks. We conducted a study to examine users' perceptions and behaviors when interacting with virtual agents in AR. We asked 24 adults to wear the Microsoft HoloLens and find objects in a hidden object game while interacting with an agent that would offer assistance. We presented participants with four different agents: voice-only, non-human, full-size embodied, and a miniature embodied agent. Overall, users preferred the miniature embodied agent due to the novelty of his size and reduced uncanniness as opposed to the larger agent. From our results, we draw conclusions about how agent representation matters and derive guidelines on designing agents for AR headsets.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Example of a user wearing an augmented reality \nheadset and interacting with a virtual agent projected onto \nthe real world through the headset. \n","bbox":[317.57,455.296,558.9590000000001,484.816],"page":"1"},"2":{"caption":"Figure 2. The three agents with visual representations, as \nviewed in context from the HoloLens. The fourth agent, \nAva (not pictured), was a voice-only agent. \n","bbox":[321.77,582.0459999999999,555.652,611.5659999999999],"page":"3"},"3":{"caption":"Figure 3. An excerpt from one of the hidden object puzzles. \nIn this image, participants would need to find the rabbit \n(highlighted with a red circle). \n","bbox":[317.93,73.05,559.597,102.57000000000001],"page":"4"},"4":{"caption":"Figure 4. The experiment setup. Participants wore a \nMicrosoft HoloLens and interacted with a hidden object \ngame running on an HP Sprout PC. \n","bbox":[324.29,573.526,553.207,602.9259999999999],"page":"5"},"5":{"caption":"Figure 5. Boxplots comparing (a) number of utterances per trial, (b) words per utterance, \n(c) number of gaze events per trial, and (d) gaze duration across agents. \n","bbox":[126.86,555.1659999999999,487.63,574.366],"page":"6"},"6":{"caption":"Figure 6. Graph of the average subjective ratings from the \nquestionnaire. Each agent was rated against the eight \nscales.  Error bars represent the standard deviation. \n","bbox":[55.32,526.9359999999999,292.644,556.366],"page":"7"},"7":{"caption":"Figure 7. Participant ratings for most-liked and most-\ndisliked agent. Net score (likes – dislikes) is also shown. \n","bbox":[323.93,568.126,552.31,587.3259999999999],"page":"7"}},"crops":{"1":{"crop_coord":[920,510.99998472222205,1508.000005,820.9999847222223],"bbox":[333,498.2400055,541.0800018,606.2400055],"page":"1"},"2":{"crop_coord":[920.3333197222222,195.66330126575113,1510.6709206405778,468.9999813888888],"bbox":[333.1199951,624.9600067,542.0415314306081,719.7612115443296],"page":"3"},"3":{"crop_coord":[928.6666530555556,1597.6682724978223,1496.241302091722,1881.6666836111112],"bbox":[336.1199951,116.3999939,536.84686875302,215.039421900784],"page":"4"},"4":{"crop_coord":[918.6666786111111,195.65990012457678,1512.0069563172174,492.999979722222],"bbox":[332.5200043,616.3200073,542.5225042741982,719.7624359551523],"page":"5"},"5":{"crop_coord":[155.6666563888889,224.85601168871688,1543.0002975178015,571.6666666666666],"bbox":[57.8399963,588,553.6801071064085,709.2518357920619],"page":"6"},"6":{"crop_coord":[144.33331805555554,195.65072390532666,815.3373278184739,621.9999863888888],"bbox":[53.7599945,569.8800049,291.72143801465063,719.7657393940824],"page":"7"},"7":{"crop_coord":[888.6666700000001,195.6666394444444,1542.6666597222222,536.6666497222222],"bbox":[321.7200012,600.6000061,553.5599975,719.7600098],"page":"7"}}}},{"filename":"Extending the Body for Interaction with Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/Extending the Body for Interaction with Reality.pdf","paper_id":"Extending the Body for Interaction with Reality","venue":"VR_2017","keywords":["Ownership","Augmented Reality","Ubiquitous Computing","Virtual Hand Illusion"],"paragraph_containing_keyword":"Author Keywords\nOwnership; Augmented Reality; Ubiquitous Computing;\nVirtual Hand Illusion","paragraph_after_keyword":"ACM Classiﬁcation Keywords\nH.5.m. Information Interfaces and Presentation (e.g. HCI):\nMiscellaneous","doi":"10.1145/3025453.3025689","sections":[{"word_count":1019,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":565,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":944,"figure_citations":{},"section_index":2,"title":"THE PSYCHOLOGICAL CONCEPT OF OWNERSHIP"},{"word_count":1400,"figure_citations":{"2":["Figure 2)."],"3":["Figure 3, A).","Figure 3, B), which is then filled with background-colored pixels (Figure 3, C)."],"5":["Figure 5, where the hand is cut off at the edge of the table, and thus appears to be reaching underneath it, or seems to hover above the table projecting a shadow onto it."]},"section_index":3,"title":"SUPPORTING OWNERSHIP IN INTERACTION"},{"word_count":759,"figure_citations":{"6":["Figure 6, top left).","Figure 6, top right).","Figure 6, bottom left).","Figure 6, bottom right)."]},"section_index":4,"title":"DESIGN PROCESS"},{"word_count":747,"figure_citations":{"6":["Figure 6) in counterbalanced order."],"7":["Figure 7)."]},"section_index":5,"title":"EVALUATION"},{"word_count":1346,"figure_citations":{"8":["Figure 8, left and center).","Figure 8, right)."],"9":["Figure 9, right).","Figure 9)."],"10":["Figure 10, left).","Figure 10, center)."]},"section_index":6,"title":"RESULTS"},{"word_count":962,"figure_citations":{"11":["Figure 11)."]},"section_index":7,"title":"DISCUSSION"},{"word_count":86,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":1769,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Extending the Body for Interaction with Reality","authors":"Tiare Feuchtner, Jörg Müller","abstract":"In this paper, we explore how users can control remote devices with a virtual long arm, while preserving the perception that the artificial arm is actually part of their own body. Instead of using pointing, speech, or a remote control, the users' arm is extended in augmented reality, allowing access to devices that are out of reach. Thus, we allow users to directly manipulate real-world objects from a distance using their bare hands. A core difficulty we focus on is how to maintain ownership for the unnaturally long virtual arm, which is the strong feeling that one's limbs are actually part of the own body. Fortunately, what the human brain experiences as being part of the own body is very malleable and we find that during interaction the user's virtual arm can be stretched to more than twice its real length, without breaking the user's sense of ownership for the virtual limb.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: We present an ownership-preserving direct manipulation technique in augmented reality, which allows interaction with\nremote devices in a ubicomp environment with the help of a long virtual arm. While the user’s real hand is close to the body the\nvirtual arm is of normal length (A) and by simply reaching out the user can make it extend to access remote devices in the room.\nFor instance we allow adjusting the height of a table (B), opening and closing a curtain (C) and adjusting the angle of a tilting\nsurface (D).\n","bbox":[53.929,453.18999199999996,564.1840449999999,506.988992],"page":"1"},"2":{"caption":"Figure 2: Optical tracking of the user and the actuated objects\nin the room is achieved with retro-reﬂective markers. This\npicture shows the view of one of the OptiTrack cameras.\n","bbox":[53.929,78.804992,297.006274,110.685992],"page":"4"},"3":{"caption":"Figure 3: (A) Hand in green sleeve, (B) binary mask after chroma keying, (C) Memory Inpainting.\n","bbox":[113.183,593.4559919999999,504.9150369999999,603.6481409999999],"page":"5"},"4":{"caption":"Figure 4: The arm-stretch function was implemented based\non the Go-Go Interaction Technique. The graph shows the\nfunction adjusted to a participant’s arm length, with the expo-\nnent p = 4 and a maximum virtual arm length of Rv = 5.\n","bbox":[321.094,179.687992,564.1712740000002,222.527992],"page":"4"},"5":{"caption":"Figure 5: To provide a better sense of interaction with a 3D\nenvironment, we implemented depth cues such as shadows\n(A) and occlusion (B) of the virtual hand.\n","bbox":[321.094,78.804992,564.171274,110.685992],"page":"5"},"6":{"caption":"Figure 6: In the user study we compared 3 different types of\nhand representations in 4 conditions: (C1) arm, (C2) hand,\n(C3) abstract-hand, and (C4) arm w/o inpainting (the real\narm was simultaneously visible).\n","bbox":[53.929,475.130992,297.0062740000001,517.970992],"page":"6"},"7":{"caption":"Figure 7: To establish a baseline participants were asked to\nrepeatedly tap 3 virtual cubes with their virtual hand, while\ntheir virtual arm was of equal length to their real arm.\n","bbox":[321.094,78.804992,564.171274,110.685992],"page":"6"},"8":{"caption":"Figure 8: Left: When interacting in the baseline and with the extended arm in C1 participants felt like the virtual hand was part\nof their body. Center: In the baseline and the arm condition participants felt like the virtual hand was their own hand. Right:\nWhen interacting with the long arm, some participants felt that their real arm was becoming longer.\n","bbox":[53.928999999999974,558.8479920000001,564.169579,590.958141],"page":"8"},"9":{"caption":"Figure 9: Left: Across all conditions and in the baseline participants maintained strong expectations of the virtual hand following\nthe movement of their real hand. Center: Participants indicated that they felt strongly in control of the devices in the environment\nacross all conditions. Right: When interacting with the devices the participants adapted the movement of their real hand based\non that of the virtual hand. E.g., they stopped when they saw the virtual hand stop at the table’s edge, and adjusted their hand\nmovement to the speed of the table movement.\n","bbox":[53.929,354.04699199999993,564.1840449999996,407.914733],"page":"8"},"10":{"caption":"Figure 10: Left: Seeing both the real arm and the virtual arm elicited the feeling of owning more than one right hand. Center:\nDuring interaction participants concentrated more on the virtual hand than on their real hand, even if it was simultaneously visible\n(as in C4, arm w/o inpainting). Right: After all trials were concluded, we asked each participant to stretch both arms out in front\nof him and rate the following two statements: P1) I feel like my right arm is longer than my left. P2) I feel like my right arm can\nreach further than my left.\n","bbox":[53.929,537.1601410000002,564.1840449999997,590.798733],"page":"9"},"11":{"caption":"Figure 11: As a step beyond command languages, direct manipulation interfaces allow users to directly manipulate a representa-\ntion of data. In the ﬁrst generation, with Graphical User Interfaces (GUIs), the screen provided the border between the physical\nand digital worlds. Tangible User Interfaces (TUIs) and Radical Atoms provide a physical representation of the digital world,\nthereby moving this border into the physical world. Body extension goes beyond these paradigms by moving the border between\nthe physical and the digital into the user’s own body.\n","bbox":[53.929,551.2199920000002,564.184045,605.017992],"page":"10"}},"crops":{"1":{"crop_coord":[144.80278027777777,394.32235946388914,1572.6855078027775,769.4111208333333],"bbox":[53.9290009,516.8119965,564.3667828089999,648.2439505929999],"page":"1"},"2":{"crop_coord":[144.80278027777777,1300.3298366944446,830.0043685777779,1870.2527702777777],"bbox":[53.9290009,120.5090027,297.001572688,322.08125879],"page":"4"},"3":{"crop_coord":[920.6906106674445,1044.7166987540554,1579.7196689674445,1559.5809011368333],"bbox":[333.24861984028,232.35087559074,566.89908082828,414.10198844854],"page":"4"},"4":{"crop_coord":[144.80278027777777,168.2276053722223,1572.0366484444442,501.5499877777777],"bbox":[53.9290009,613.2420044,564.1331934399999,729.638062066],"page":"5"},"5":{"crop_coord":[886.9277613888887,1535.8070974333334,1572.1222962555555,1870.2527702777777],"bbox":[321.0939941,120.5090027,564.164026652,237.309444924],"page":"5"},"6":{"crop_coord":[144.80278027777777,168.22802907777753,829.9579441777777,738.9083352777777],"bbox":[53.9290009,527.7929993,296.98485990399996,729.6379095320001],"page":"6"},"7":{"crop_coord":[886.9277613888887,1514.567265775,1572.0927928944445,1870.2527702777777],"bbox":[321.0939941,120.5090027,564.153405442,244.955784321],"page":"6"},"8":{"crop_coord":[144.80278027777777,168.22741037777786,608.3587324777778,536.7999944444445],"bbox":[53.9290009,600.552002,217.209143692,729.638132264],"page":"8"},"9":{"crop_coord":[605.2750141666667,168.22741037777786,1068.8309663666666,536.7999944444445],"bbox":[219.6990051,600.552002,382.979147892,729.638132264],"page":"8"},"10":{"crop_coord":[1065.7472058333333,168.22741037777786,1529.3031580333334,536.7999944444445],"bbox":[385.4689941,600.552002,548.749136892,729.638132264],"page":"8"},"11":{"crop_coord":[166.21387055555556,676.238528988889,629.7698227555555,1044.8111130555556],"bbox":[61.6369934,417.6679993,224.917136192,546.754129564],"page":"8"},"12":{"crop_coord":[626.6861047222222,698.1006965888888,1090.242241188889,1044.8111130555556],"bbox":[227.4069977,417.6679993,390.687206828,538.883749228],"page":"8"},"13":{"crop_coord":[1087.158338888889,698.1006965888888,1550.7144753555556,1044.8111130555556],"bbox":[393.177002,417.6679993,556.457211128,538.883749228],"page":"8"},"14":{"crop_coord":[144.80278027777777,168.22741037777786,608.3587324777778,536.7999944444445],"bbox":[53.9290009,600.552002,217.209143692,729.638132264],"page":"9"},"15":{"crop_coord":[605.2750141666667,189.2278443777778,1068.8204766333333,536.7999944444445],"bbox":[219.6990051,600.552002,382.97537158800003,722.077976024],"page":"9"},"16":{"crop_coord":[1065.7472058333333,168.22741037777786,1529.3031580333334,536.7999944444445],"bbox":[385.4689941,600.552002,548.749136892,729.638132264],"page":"9"},"17":{"crop_coord":[215.67222583333333,168.22818166666673,1501.1927947222223,497.108323888889],"bbox":[79.4420013,614.8410034,538.6294061,729.6378546],"page":"10"}}}},{"filename":"FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display.pdf","paper_id":"FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display","venue":"chi2019xr","keywords":["Fish tank virtual reality","Spherical display","3D perception"],"paragraph_containing_keyword":"CCS CONCEPTS\n• Computing methodologies → Perception; Virtual reality;\nKEYWORDS\nFish tank virtual reality, spherical display, 3D perception\nACM Reference Format:\nDylan Fafard, Ian Stavness, Martin Dechant, Regan Mandryk, Qian\nZhou, and Sidney Fels. 2019. FTVR in VR: Evaluating 3D Perfor-\nmance With a Simulated Volumetric Fish-Tank Virtual Reality Dis-\nplay. In CHI Conference on Human Factors in Computing Systems Pro-\nceedings (CHI 2019), May 4–9, 2019, Glasgow, Scotland UK. ACM, New\nYork, NY, USA, 12 pages. https://doi.org/10.1145/3290605.3300763","doi":"10.1145/3290605.3300763","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request\npermissions from permissions@acm.org.\nCHI 2019, May 4–9, 2019, Glasgow, Scotland UK\n© 2019 Association for Computing Machinery.\nACM ISBN 978-1-4503-5970-2/19/05. . . $15.00\nhttps://doi.org/10.1145/3290605.3300763","sections":[{"word_count":661,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1339,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":314,"figure_citations":{"1":["Figure 1) and set the output to a VR headset."]},"section_index":2,"title":"SIMULATED SPHERICAL FTVR SYSTEM"},{"word_count":3044,"figure_citations":{"2":["Figure 2)."],"5":["Figure 5), therefore we accept H1-3 (NonStereo is aligned to mid-point of the eyes)."],"6":["Figure 6)."]},"section_index":3,"title":"EXPERIMENTS"},{"word_count":1454,"figure_citations":{},"section_index":4,"title":"DISCUSSION"},{"word_count":411,"figure_citations":{},"section_index":5,"title":"LIMITATIONS AND FUTURE WORK"},{"word_count":109,"figure_citations":{},"section_index":6,"title":"CONCLUSIONS"},{"word_count":21,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGMENTS"},{"word_count":1717,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"FTVR in VR: Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display","authors":"Dylan Fafard, Ian Stavness, Martin Dechant, Regan Mandryk, Qian Zhou, Sidney Fels","abstract":"Spherical fish tank virtual reality (FTVR) displays attempt to create a virtual \"crystal ball\" experience using head-tracked rendering. Almost all of these systems have omitted stereo cues, making them easy to build, but it is not clear how much this omission degrades the 3D experience. In this study, we evaluate performance and subjective effects of stereo on 3D perception and interaction tasks with a spherical FTVR display. To control for calibration error and tracking latency, we perform the evaluation on a simulated spherical display in VR. The results of our study provide a clear recommendation for the design and use of spherical FTVR displays: while omitting stereo may not be readily apparent for users, their performance will be significantly degraded (20% - 91% increase in median task time). Therefore, including stereo viewing in spherical displays is critical for use in FTVR.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: A spherical FTVR display (right) is simulated in VR\n(left) to evaluate the importance of stereo cues.\n","bbox":[317.67699999999996,373.148854,558.190004,393.073854],"page":"1"},"2":{"caption":"Figure 2: An example, with exaggerated stereo disparity, of\nwhat the left and right eyes would see in the Stereo (top), Non-\nStereo (middle), and Monocular (bottom) viewing conditions.\nNote that non-stereo rendering creates a perspective mis-\nmatch between the background 3D world and the display.\n","bbox":[317.60499999999996,406.181854,559.794918,458.98285400000003],"page":"4"},"3":{"caption":"Figure 3: Pattern alignment task: the pattern starts distorted\n(left) and then the participant moves their head left, right,\nup, or down to align their viewpoint so that the pattern ap-\npears to have straight lines and circular rings (right).\n","bbox":[317.67699999999996,560.6878540000001,559.7949179999999,602.530854],"page":"5"},"4":{"caption":"Figure 4: Mean Time and Error vs. Viewing Condition for the\nPattern Alignment task. Error bars represent the standard\nerror of the mean, highlighted bars indicate significant best\nresults, and dashed lines indicate a significant difference.\n","bbox":[317.9549999999999,373.000854,558.1974019999999,414.843854],"page":"5"},"5":{"caption":"Figure 5: The geometric mean (red) of measurements (blue/orange) is shown relative to the ground truth (green). Calibrations\nwere perturbed by 5 cm (black circles) at the start of each trial. Plots are scaled to 6.3 cm pupillary distance.\n","bbox":[53.449000000000005,538.570854,558.1803300000001,558.495854],"page":"6"},"6":{"caption":"Figure 6: Subjective preference task: the participant was\nforced to move left and right to induce a minimum amount\nof head motion before selecting their preference between a\npair of viewing conditions.\n","bbox":[53.798,284.049854,294.03300400000006,325.892854],"page":"6"},"7":{"caption":"Figure 7: Point cloud visualizations on our simulated display\nin the Distance (left), Selection (middle), and Manipulation\n(right) tasks in Experiment 3.\n","bbox":[317.67699999999996,565.6408540000001,558.441052,596.524854],"page":"7"},"8":{"caption":"Figure 8: Mean Time, Error and Head Speed vs. Viewing Con-\ndition grouped by Task. Error bars represent the standard\nerror of the mean, highlighted bars indicate significant best\nresults, and dashed lines indicate a significant difference.\n","bbox":[53.798,434.853854,294.997918,476.69685400000003],"page":"8"}},"crops":{"1":{"crop_coord":[878.2083383333334,633.98752,1555.5523255555554,1077.7611116666667],"bbox":[317.9550018,405.8059998,558.1988372,561.9644928],"page":"1"},"2":{"crop_coord":[961.6277566666665,252.36707055555559,1472.1397738888888,894.6805658333334],"bbox":[347.9859924,471.7149963,528.1703186,699.3478546],"page":"4"},"3":{"crop_coord":[878.2083383333334,252.36775122222218,1555.5544983333334,495.93890722222204],"bbox":[317.9550018,615.2619934,558.1996194000001,699.34760956],"page":"5"},"4":{"crop_coord":[878.2083383333334,681.8870172125002,1555.554977141493,1017.2916752777778],"bbox":[317.9550018,427.5749969,558.1997917709375,544.7206738035],"page":"5"},"5":{"crop_coord":[284.55001833333336,252.36346777777766,1415.47003,618.2555644444444],"bbox":[104.2380066,571.2279968,507.7692108,699.3491516],"page":"6"},"6":{"crop_coord":[177.80834611111112,743.8590255000001,788.4186661111111,1264.3777975],"bbox":[65.8110046,338.6239929,282.0307198,522.41075082],"page":"6"},"7":{"crop_coord":[878.2083383333334,252.367421388889,1555.5544983333334,512.6222313888891],"bbox":[317.9550018,609.2559967,558.1996194000001,699.3477283],"page":"7"},"8":{"crop_coord":[144.43890055555556,252.36665248821402,821.7855480989583,845.477795],"bbox":[53.7980042,489.4279938,294.04279731562497,699.3480051042429],"page":"8"}}}},{"filename":"Handsfree Omnidirectional VR Navigation using Head Tilt","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/Handsfree Omnidirectional VR Navigation using Head Tilt.pdf","paper_id":"Handsfree Omnidirectional VR Navigation using Head Tilt","venue":"VR_2017","keywords":["Virtual reality","Locomotion","Mobile VR","Walking-in-place","Head-tilt","Simulator-sickness","Games","Inertial sensing"],"paragraph_containing_keyword":"ABSTRACT\nNavigating mobile virtual reality (VR) is a challenge due to\nlimited input options and/or a requirement for handsfree inter-\naction. Walking-in-place (WIP) is considered to offer a higher\npresence than controller input but only allows unidirectional\nnavigation in the direction of the user’s gaze–which impedes\nnavigation efﬁciency. Leaning input enables omnidirectional\nnavigation but currently relies on bulky controllers, which\naren’t feasible in mobile VR contexts. This note evaluates\nthe use of head-tilt –implemented using inertial sensing– to\nallow for handsfree omnidirectional VR navigation on mobile\nVR platforms. A user study with 24 subjects compared three\ninput methods using an obstacle avoidance navigation task: (1)\nhead-tilt alone (TILT); (2) a hybrid method (WIP-TILT) that uses\nhead tilting for direction and WIP to control speed; and (3)\ntraditional controller input. TILT was signiﬁcantly faster than\nWIP-TILT and joystick input, while WIP-TILT and TILT offered\nthe highest presence. There was no difference in cybersickness\nbetween input methods.\nACM Classiﬁcation Keywords\nI.3.7 Graphics: 3D Graphics and Realism–Virtual Reality;\nAuthor Keywords\nVirtual reality; locomotion; mobile VR; walking-in-place;\nhead-tilt; simulator-sickness; games; inertial sensing.\nINTRODUCTION\nVirtual reality (VR) has recently enjoyed signiﬁcant commer-\ncial success, but virtual navigation has remained a challenge\n[8, 23]. Low-cost VR smartphone adapters, like Google Card-\nboard [4] have the potential to bring VR to the masses, but\ntheir current input options are limited [32]. Positional tracking\ninput generally delivers the most immersive experiences with a\nlow possibility of inducing cybersickness [33, 28]. Positional\ntracking generally isn’t available on mobile VR platforms, as\nit is computationally intensive and requires a depth camera\nto reliably track movement, which aren’t available on smart-\nphones. Another constraint for Cardboard is the lack of a\nhead-strap; which forces users to hold the adapter with both\nhands and limits the rotation speed of the head to the torso to\nminimize cybersickness [4]. Though useful– this constraint\nprevents using a controller for navigation.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI 2017, May 06 - 11, 2017, Denver, CO, USA\n© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nISBN 978-1-4503-4655-9/17/05. . . $15.00\nDOI: http://dx.doi.org/10.1145/3025453.3025521","doi":"10.1145/3025453.3025521","paragraph_after_keyword":"Figure 1. Head tilt is used to indicate the direction of travel\nWalking-in-place (WIP) closely mimics walking, e.g., users\nprovide step-like motions while remaining stationary [29].\nWIP closely approximates real walking input in terms of per-\nformance [25] and presence [27]. Compared to a controller,\nWIP is handsfree; offers higher presence [33]; improves spa-\ntial orientation [19]; and is less likely to induce cybersickness\n[16], because of the generation of proprioceptive feedback.\nHowever, a controller allows for 360◦ omnidirectional nav-\nigation, where WIP only navigates users in the direction of\ntheir gaze. This impedes navigation efﬁciency, for example,\nif a user wants to back up a little bit, it requires them to turn\naround, move forward, turn around again and then move to\nwhere they want to be. Because prior studies [25, 13, 27,\n37] have only evaluated navigation tasks that include forward\nmotion, they ﬁnd a similar performance for WIP as controller\ninput. However, these results are misleading, as VR naviga-\ntion also contains lateral movements [15] and controller input\noutperforms WIP as it allows omnidirectional navigation.\nAs a result of bipedalism, humans lean their body in the di-\nrection they walk; to align with the gravitational vertical [14].\nLeaning interfaces exploit this characteristic and are widely\nused, for example, in popular hoverboards. Leaning interfaces\nhave been explored for virtual navigation [36, 22]. Like a con-\ntroller they offer omnidirectional navigation with a signiﬁcant\ndifference that leaning interfaces are handsfree. Controller in-\nput is faster but leaning interfaces offer a higher presence [36]\nbecause they generate vestibular feedback. Current leaning\ninterfaces are difﬁcult to enable on mobile VR platforms, as\nthey rely on bulky sensors [36, 22, 12].\nThis note explores augmenting gaze-based navigation with\nhead-tilt input to enable handsfree omnidirectional VR nav-\nigation. Because head-tilt is similar to whole body leaning,\nwe anticipate that similar to prior results [36] it could offer a\nhigher presence than controller input. To improve WIP, we\nevaluate a hybrid method (WIP-TILT) that uses head tilt to in-\ndicate a direction of travel and WIP to control locomotion\nspeed. WIP-TILT is novel in that it offers both proprioceptive\nand vestibular feedback and thus approximates real walking\ninput much more closely than current WIP implementations;\nwhich could improve presence and reduce cybersickness.","sections":[{"word_count":642,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1075,"figure_citations":{},"section_index":1,"title":"BACKGROUND"},{"word_count":883,"figure_citations":{"3":["Figure 3)."],"4":["Figure 4 shows the corridor and a visualization of colliding into an obstacle."]},"section_index":2,"title":"EVALUATION"},{"word_count":553,"figure_citations":{},"section_index":3,"title":"TILT"},{"word_count":611,"figure_citations":{},"section_index":4,"title":"DISCUSSION AND LIMITATIONS"},{"word_count":1137,"figure_citations":{},"section_index":5,"title":"REFERENCES"}],"title":"Handsfree Omnidirectional VR Navigation using Head Tilt","authors":"Sam Tregillus, Majed Al Zayer, Eelke Folmer","abstract":"Navigating mobile virtual reality (VR) is a challenge due to limited input options and/or a requirement for handsfree interaction. Walking-in-place (WIP) is considered to offer a higher presence than controller input but only allows unidirectional navigation in the direction of the user's gaze--which impedes navigation efficiency. Leaning input enables omnidirectional navigation but currently relies on bulky controllers, which aren't feasible in mobile VR contexts. This note evaluates the use of head-tilt - implemented using inertial sensing - to allow for handsfree omnidirectional VR navigation on mobile VR platforms. A user study with 24 subjects compared three input methods using an obstacle avoidance navigation task: (1) head-tilt alone (TILT) (2) a hybrid method (WIP-TILT) that uses head tilting for direction and WIP to control speed; and (3) traditional controller input. TILT was significantly faster than WIP-TILT and joystick input, while WIP-TILT and TILT offered the highest presence. There was no difference in cybersickness between input methods.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Head tilt is used to indicate the direction of travel\nWalking-in-place (WIP) closely mimics walking, e.g., users\nprovide step-like motions while remaining stationary [29].\nWIP closely approximates real walking input in terms of per-\nformance [25] and presence [27]. Compared to a controller,\nWIP is handsfree; offers higher presence [33]; improves spa-\ntial orientation [19]; and is less likely to induce cybersickness\n[16], because of the generation of proprioceptive feedback.\nHowever, a controller allows for 360◦ omnidirectional nav-\nigation, where WIP only navigates users in the direction of\ntheir gaze. This impedes navigation efﬁciency, for example,\nif a user wants to back up a little bit, it requires them to turn\naround, move forward, turn around again and then move to\nwhere they want to be. Because prior studies [25, 13, 27,\n37] have only evaluated navigation tasks that include forward\nmotion, they ﬁnd a similar performance for WIP as controller\ninput. However, these results are misleading, as VR naviga-\ntion also contains lateral movements [15] and controller input\noutperforms WIP as it allows omnidirectional navigation.\n","bbox":[320.626,328.567992,565.91318566,537.7102699999999],"page":"1"},"2":{"caption":"Figure 2. head tilt de-\nﬁned using vectors.\n","bbox":[482.526,479.28227,565.48573,496.21826999999996],"page":"2"},"3":{"caption":"Figure 3. Reticle shows: 1) no 2) right\n3) right-back 4) back motion.\n","bbox":[169.441,212.74027,297.00085000000007,229.67627000000002],"page":"3"},"4":{"caption":"Figure 4. Left: navigation task showing corridor with the obstacles the\nparticipant users must navigate. Right: when colliding with an obstacle\nit turns red and becomes sticky; requiring users to navigate laterally.\n","bbox":[321.094,562.36727,564.1630600000002,588.26927],"page":"3"},"5":{"caption":"Figure 5. Avg Likert scores and standard deviation for each method\n","bbox":[327.682,612.1142699999999,557.57834,620.08427],"page":"4"}},"crops":{"1":{"crop_coord":[886.9277613888887,411.5051777777775,1572.0912422222223,692.894439722222],"bbox":[321.0939941,544.3580017,564.1528472,642.0581360000001],"page":"1"},"2":{"crop_coord":[1301.7533280555556,489.55727472222225,1572.1274819444445,813.6861080555555],"bbox":[470.4311981,500.8730011,564.1658935,613.9593811],"page":"2"},"3":{"crop_coord":[439.1294352777778,1459.1525691666668,829.9784258333334,1554.0805730555555],"bbox":[159.8865967,234.3309937,296.9922333,264.9050751],"page":"3"},"4":{"crop_coord":[886.9277613888887,168.2289208333334,1572.0566558333333,557.9889000000002],"bbox":[321.0939941,592.923996,564.1403961,729.6375885],"page":"3"},"5":{"crop_coord":[886.9277613888887,148.22278361111114,1572.1327802777776,464.07779277777774],"bbox":[321.0939941,626.7319946,564.1678009,736.8397979],"page":"4"}}}},{"filename":"HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality ","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality .pdf","paper_id":"HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality ","venue":"VR_2017","keywords":["Guidance","Navigation","Haptic feedback","Vibrotactile","Virtual reality","Augmented reality","Spatial interaction","3D output"],"paragraph_containing_keyword":"ABSTRACT \nCurrent  virtual  and  augmented  reality  head-mounted  dis-\nplays usually include no or only a single vibration motor for \nhaptic feedback and do not use it for guidance. We present \nHapticHead,  a  system  utilizing  multiple  vibrotactile  actua-\ntors distributed in three concentric ellipses around the head \nfor intuitive haptic guidance through moving tactile cues. We \nconducted  three  experiments,  which  indicate  that  Hap-\nticHead vibrotactile feedback is both faster (2.6 s vs. 6.9 s) \nand more precise (96.4 % vs. 54.2 % success rate) than spa-\ntial audio (generic head-related transfer function) for finding \nvisible virtual objects in 3D space around the user. The base-\nline  of  visual  feedback  is  –  as  expected  –  more  precise \n(99.7 %  success  rate)  and  faster  (1.3 s)  in  comparison,  but \nthere are many applications in which visual feedback is not \ndesirable or available due to lighting conditions, visual over-\nload, or visual impairments. Mean final precision with Hap-\nticHead  feedback  on  invisible  targets  is  2.3°  compared  to \n0.8° with visual feedback. We successfully navigated blind-\nfolded users to real household items at different heights using \nHapticHead vibrotactile feedback independently of a head-\nmounted display. \nAuthor Keywords \nGuidance; navigation; haptic feedback; vibrotactile; virtual \nreality; augmented reality; spatial interaction; 3D output \nACM Classification Keywords \nH.5.2.  Information  interfaces  and  presentation:  User  Inter-\nfaces – haptic I/O, input devices and strategies \nINTRODUCTION \nNavigation and 3D guidance systems use a large variety of \ndifferent  technologies  to  stimulate  the  visual,  auditory,  or \nhaptic channels. The visual channel is usually the channel of \nchoice as it typically has a higher bandwidth than the other \nchannels [29]. However, sometimes the visual channel is not \nthe  desired  primary  channel  to  be  used  for  some  kinds  of \nPermission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. Copyrights for com-\nponents of this work owned by others than ACM must be honored. Abstract-\ning  with  credit  is  permitted.  To  copy  otherwise,  or  republish,  to  post  on\nservers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from Permissions@acm.org. \nCHI 2017, May 06-11, 2017, Denver, CO, USA \n© 2017 ACM. ISBN 978-1-4503-4655-9/17/05…$15.00 \nDOI: http://dx.doi.org/10.1145/3025453.3025684","doi":"10.1145/3025453.3025684","paragraph_after_keyword":"Figure 1. Placement of actuators in HapticHead. Note the three \nconcentric ellipses around the user’s head and no actuators \nclose to the ear openings. The red ellipse contains 8 equidistant \nactuators, the green and blue ellipses each contain 6 actuators.\nfeedback or in special situations such as when driving a car \n[6].  The  visual  channel  might  be  overtaxed  and  important \nfeedback can be overlooked or lighting conditions may pre-\nvent the user from seeing the feedback at all. Another reason \nto use the tactile instead of the visual or auditory feedback \nchannels are faster initial reaction times, as shown in several \nstudies such as [25].  \nTo  relieve  the  visual  channel,  we  propose  HapticHead,  a \nhigh-resolution,  omnidirectional  vibrotactile  display  worn \non the head that presents 3D directional and distance infor-\nmation through moving tactile cues and patterns. It consists \nof a grid of vibrotactile actuators arranged in three concentric \nellipses around the head for uniform coverage, optimized for \nhead  shape  and  user  comfort  (Figure  1).  The  head  is  well \nsuited for guidance applications and tactile feedback, as it is \nsensitive to mechanical stimuli [9,18] and provides a large \nspherical  surface.  This  allows  displaying  precise  3D  infor-\nmation and allows the user to intuitively turn the head in the \ndirection of a stimulus. We left important parts of the face \nuncovered  and  did  not  place  actuators  too  close  (less  than \n4 cm) to the ear openings because noise through bone con-\nduction increases dramatically in their proximity.  \nHapticHead  may  be  combined  with  virtual  reality  (VR)  or \naugmented reality (AR) head-mounted displays (HMDs) to \nincrease  the  sense of  immersion.  The visual  content of  the \nHMD  is  then  synchronized  with  vibrotactile  feedback  of","sections":[{"word_count":943,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":192,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":340,"figure_citations":{"2":["Figure 2) consists of a bathing cap with 17 vibration motors (Parallax, 12 mm coin type, 3.","Figure 2, left).","Figure 2)."]},"section_index":2,"title":"INITIAL PROTOTYPE"},{"word_count":3016,"figure_citations":{"2":["Figure 2, right)."],"3":["Figure 3)."],"4":["Figure 4 shows targets in front and back of the user for the auditory condition."],"5":["Figure 5 show the measured dependent variables with merged data from all participants and all trials, not just successful ones."],"6":["Figure 6 shows, compared to auditory feedback, targets off the horizontal plane worked much better with vibrotactile feedback."],"7":["Figure 7 shows the selection time by angular distance between the starting orientation of the user and the orientation of each target."],"8":["Figure 8 shows the selection time by yaw (horizontal heading) distance between the starting orientation and each target."],"9":["Figure 9 shows the development of completion time over all trials."],"10":["Figure 10)."],"11":["Figure 11 shows the success rates over time."],"12":["Figure 12 and Figure 13)."]},"section_index":3,"title":"EXPERIMENTS"},{"word_count":2929,"figure_citations":{"3":["Figure 3) were used, but here with tiny white 1-pixel targets instead of the large ones (4 repetitions × 20 targets per user)."],"15":["Figure 15, participants agreed that the HapticHead vibrotactile feedback was helpful for finding virtual targets and most of the participants could intuitively map the feedback to the targets.","Figure 15, participants found the vibrotactile feedback helpful for finding real targets around them and could intuitively map vibrotactile signals to targets, thus research question 2 can be answered positively."],"16":["Figure 16)."]},"section_index":4,"title":"REFINEMENT OF CONCEPT AND PROTOTYPE"},{"word_count":184,"figure_citations":{},"section_index":5,"title":"CONCLUSION"},{"word_count":63,"figure_citations":{},"section_index":6,"title":"LIMITATIONS"},{"word_count":874,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"HapticHead: A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality","authors":"Oliver Beren Kaul, Michael Rohs","abstract":"Current virtual and augmented reality head-mounted displays usually include no or only a single vibration motor for haptic feedback and do not use it for guidance. We present HapticHead, a system utilizing multiple vibrotactile actuators distributed in three concentric ellipses around the head for intuitive haptic guidance through moving tactile cues. We conducted three experiments, which indicate that HapticHead vibrotactile feedback is both faster (2.6 s vs. 6.9 s) and more precise (96.4% vs. 54.2% success rate) than spatial audio (generic head-related transfer function) for finding visible virtual objects in 3D space around the user. The baseline of visual feedback is as expected more precise (99.7% success rate) and faster (1.3 s) in comparison, but there are many applications in which visual feedback is not desirable or available due to lighting conditions, visual overload, or visual impairments. Mean final precision with HapticHead feedback on invisible targets is 2.3° compared to 0.8° with visual feedback. We successfully navigated blindfolded users to real household items at different heights using HapticHead vibrotactile feedback independently of a head-mounted display.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Placement of actuators in HapticHead. Note the three \nconcentric ellipses around the user’s head and no actuators \nclose to the ear openings. The red ellipse contains 8 equidistant \nactuators, the green and blue ellipses each contain 6 actuators.\n","bbox":[317.4,398.493,561.9504696000001,438.51599999999996],"page":"1"},"2":{"caption":"Figure 2. First HapticHead prototype (left) and Unity scene \nview with visible targets from the outside (right). The user’s \ncamera is in the center. \nWe built a simple VR environment in Unity 5.3 that spawns \n20  small  (r  =  1 m)  equidistant  spheres  on  the  surface  of  a \nlarger (r = 5 m) invisible sphere with the viewer at its center \n(see Figure 2). The spheres were distributed with pack.3.20 \ncoordinates [27]. As the user rotates the head the location of \nthe spheres stays fixed with respect to the environment. The \ntarget spheres do not coincide with the actuator positions. \n","bbox":[316.796277,492.8682399999998,560.5746524000002,604.956],"page":"3"},"3":{"caption":"Figure 3. Attention funnels with a tiny red crosshair in the \nview’s center. Visual feedback from the user’s perspective. \n","bbox":[324.3578037,313.419,551.12585,332.796],"page":"3"},"5":{"caption":"Figure 5. Boxplots of completion times for all conditions  \nwith merged data from all participants. \n","bbox":[330.06,367.179,549.21940446,386.556],"page":"4"},"7":{"caption":"Figure 7. Selection time by angular distance between the start \nbox and the target center. Visual (R2 = 0.89) and vibration (R2 \n= 0.59) conditions show a linear relationship. \n","bbox":[316.8,137.4961,560.2276071000002,167.196],"page":"5"},"8":{"caption":"Figure  8.  Selection  time  by yaw (horizontal  heading)  distance \nbetween the start box and the target center. Visual (R2 = 0.86) \nand  vibrotactile  (R2 =  0.72)  conditions  show  a  linear  relation-\nship. The auditory condition is slow with targets in front of and \nbehind the user. \n","bbox":[53.997,516.4560000000001,297.5121,566.856],"page":"6"},"9":{"caption":"Figure 9. Average learning effect – time. Merged data, all par-\nticipants. Curves: Gaussian weighted moving average \n(width=3, blue=visual, green=vibrotactile, red=auditory). \nand below the plane and roughly has a parabola shape. The \nvisual and vibrotactile conditions do not exhibit this effect. \n","bbox":[53.99831999999998,224.50950000000003,297.782916,279.5161],"page":"6"},"10":{"caption":"Figure 10. Qualitative results of Experiment 1. Diverging stacked bar chart: scales in percent, and absolute values on bars.  \n","bbox":[71.76,72.3361,545.7198,81.3361],"page":"6"},"11":{"caption":"Figure 11. Average learning effect - success rate. Merged data, \nall participants. Curves: Gaussian weighted moving average \n(width=3, blue=visual, green=vibrotactile, red=auditory). \nFigure 11 shows the success rates over time. In the auditory \ncondition, participants had a steep learning curve, which flat-\ntens after trial 40 but still shows a high variance compared to \nthe other conditions. Despite the fact that participants learned \nto be a lot faster after trial 65 in the auditory condition, the \nsuccess rate did not drop but even increased a bit. In the vi-\nbrotactile condition, participants needed less than 15 trials to \naccommodate  themselves  with  this  new  form  of  feedback. \nAfter the first few trials, the success rate curve for vibrotac-\ntile feedback flattens and stays close to 97% without much \nvariance or measurable fatigue effects. \n","bbox":[316.26,404.22821999999996,560.5393799999998,563.796],"page":"6"},"12":{"caption":"Figure 12. Second, refined HapticHead prototype, side and \nfront view. Notice actuators located on the outside, the flexible \nchinstrap, and five instead of three actuators on each, the fore-\nhead and chinstrap. Positions of the 10 forehead and chin ac-\ntuators forming a “ring” around the face are marked in red. \n","bbox":[53.828261399999974,67.95600000000002,295.71440286,118.356],"page":"7"},"13":{"caption":"Figure 13. Side and front view of modeled actuator positions. \nDoes not fit perfectly due to arbitrary size and asymmetries of \nthe Styrofoam head. The refined guidance algorithm uses trian-\ngles between actuators, including a virtual point zero (in green) \nbetween the eyes. \n","bbox":[320.46093510000003,66.17578,566.0013202800001,116.7961],"page":"7"},"14":{"caption":"Figure 14. Intensity cal-\nculation visualization. \n","bbox":[464.52,264.099,556.0503612599999,283.476],"page":"7"},"15":{"caption":"Figure 15. Qualitative results of Experiments 2 and 3. Diverging stacked bar chart: scales in percent, and absolute values on bars. \n","bbox":[56.94,618.0360000000001,555.68408235,627.0360000000001],"page":"9"},"16":{"caption":"Figure 16. 10 items for Experiment 3. From left to right (height): Three boo\n(1.7 m, 6 cm diameter), a screwdriver (0.8 m), a remote control (1.4 m) and \n","bbox":[324.12,67.95609999999999,613.85145,87.3361],"page":"9"}},"crops":{"1":{"crop_coord":[870.3333197222222,526.3333297222224,1238.3333333333333,975.6666733333334],"bbox":[315.1199951,442.5599976,444,600.7200012999999],"page":"1"},"2":{"crop_coord":[1225.0000086111113,518.9999983333332,1578.9999983333337,970.3333452777779],"bbox":[442.8000031,444.4799957,566.6399994000001,603.3600006],"page":"1"},"3":{"crop_coord":[875.0000086111113,181.00001888888872,1162.0000119444444,435.0000086111112],"bbox":[316.8000031,637.1999969,416.52000430000004,725.0399932],"page":"3"},"4":{"crop_coord":[875.0000086111113,424.99996611111135,1162.0000119444444,515.833316388889],"bbox":[316.8000031,608.1000061,416.52000430000004,637.2000122],"page":"3"},"5":{"crop_coord":[1208.999981388889,180.1666766666666,1552.499974722222,508.00000499999993],"bbox":[437.0399933,610.9199982,557.0999909,725.3399964],"page":"3"},"6":{"crop_coord":[989.6666716666667,1052.333331666667,1440.3333366666666,1276.3333130555557],"bbox":[358.0800018,334.3200073,516.7200012,411.3600006],"page":"3"},"7":{"crop_coord":[939.6666716666667,1500.3333283333334,1509.666688611111,1985.6666480555557],"bbox":[340.0800018,78.9600067,541.6800079,250.0800018],"page":"4"},"8":{"crop_coord":[203.66665305555554,1499.666646388889,774.3333094444444,1986.33333],"bbox":[75.1199951,78.7200012,276.9599914,250.3200073],"page":"4"},"9":{"crop_coord":[210.99998472222225,164.33335194444444,781.0000016666668,649.6666716666667],"bbox":[77.7599945,559.9199982,279.36000060000003,731.0399933],"page":"5"},"10":{"crop_coord":[937.6666683333333,164.9999913888888,1506.3333213888889,649.6666716666667],"bbox":[339.3600006,559.9199982,540.4799957,730.8000031],"page":"5"},"11":{"crop_coord":[941.6666752777779,1472.3333233333333,1500.333353888889,1868.9999813888887],"bbox":[340.8000031,120.9600067,538.3200074,260.1600036],"page":"7"},"12":{"crop_coord":[188.33335027777778,1436.3333469444444,801.0000355555555,1870.3333452777777],"bbox":[69.6000061,120.4799957,286.5600128,273.1199951],"page":"7"},"13":{"crop_coord":[144.33331805555554,1745.6666819444445,1555,1950.333353611111],"bbox":[53.7599945,91.6799927,558,161.7599945],"page":"9"}}}},{"filename":"I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR.pdf","paper_id":"I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR","venue":"VR_2017","keywords":["In-motion","In-car","Automobile","Autonomous Car","Passenger","Virtual Reality","Mixed Reality","Motion Sickness","HMD"],"paragraph_containing_keyword":"ABSTRACT\nThis paper explores the use of VR Head Mounted Displays\n(HMDs) in-car and in-motion for the ﬁrst time. Immersive\nHMDs are becoming everyday consumer items and, as they\noffer new possibilities for entertainment and productivity, peo-\nple will want to use them during travel in, for example, au-\ntonomous cars. However, their use is confounded by motion\nsickness caused in-part by the restricted visual perception\nof motion conﬂicting with physically perceived vehicle mo-\ntion (accelerations/rotations detected by the vestibular system).\nWhilst VR HMDs restrict visual perception of motion, they\ncould also render it virtually, potentially alleviating sensory\nconﬂict. To study this problem, we conducted the ﬁrst on-road\nand in motion study to systematically investigate the effects\nof various visual presentations of the real-world motion of\na car on the sickness and immersion of VR HMD wearing\npassengers. We established new baselines for VR in-car mo-\ntion sickness, and found that there is no one best presentation\nwith respect to balancing sickness and immersion. Instead,\nuser preferences suggest different solutions are required for\ndifferently susceptible users to provide usable VR in-car. This\nwork provides formative insights for VR designers and an en-\ntry point for further research into enabling use of VR HMDs,\nand the rich experiences they offer, when travelling.\nACM Classiﬁcation Keywords\nH.5.m. Information Interfaces and Presentation (e.g. HCI):\nMiscellaneous\nAuthor Keywords\nIn-motion; In-car; Automobile; Autonomous Car; Passenger;\nVirtual Reality; Mixed Reality; Motion Sickness; HMD;\nINTRODUCTION\nFor many travellers, a long journey is not to be relished. Jour-\nneys can last for signiﬁcant durations, for example car journeys\nin UK last on average 22 minutes [17], with commutes lasting\n55 minutes [58]; in the USA, drivers spend 56 minutes a day on\naverage in-transit [76]. These journeys can be repetitive (e.g.\nthe commute to work), with travellers frequently noting that\nsuch trips are wasted time [24, 80]. Whilst collocated social\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI 2017, May 06 - 11, 2017, Denver, CO, USA\n© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nISBN 978-1-4503-4655-9/17/05. . . $15.00\nDOI: http://dx.doi.org/10.1145/3025453.3026046","doi":"10.1145/3025453.3026046","paragraph_after_keyword":"interaction can offer some respite [33], journeys are often con-\nducted without the physical presence of friends or family, with\nthe car providing solitary personal space [24]. Entertainment\nand productivity options are limited to displays signiﬁcantly\nsmaller than those in the home or ofﬁce (e.g. phones, tablets,\nlaptops, dashboards and rear-seat systems [81]). In the speciﬁc\ncase of car journeys, these issues will gain increasing preva-\nlence given the arrival of fully autonomous cars, which would\nfree drivers from the driving task, and consequently increase\nthe occurrence of passenger experiences.\nWhile autonomous cars will allow for radical redesign of the\ncar interior (e.g. seating locations and internal display conﬁgu-\nrations [20]), passengers will still perceive themselves as being\nin a constrained space, with the physical limitations of the in-\nterior dictating what is possible to be rendered and displayed.\nMoreover, the passenger’s visual perception of motion may\nbe compromised by use of these displays, through changes in\ngaze angle (e.g. looking down/away from windows) and occlu-\nsion (presenting content over windows, or occluding windows\nto enhance immersion [43]). This has implications for motion\nsickness, which in-part arises from the sensory mismatch of\nvisually and physically perceived motion [59, 85].\nMany people become travel sick when watching TV, reading\nor working in vehicles, meaning that they cannot use the time\nproductively. These problems will grow in number with the\narrival of autonomous cars [20, 19, 71]; the act of driving\nstops many people from feeling sick due to the anticipatory\ncues of being the driver [75] and without these cues people\nwho did not get sick will now do so. Consideration needs to\nbe given to how entertainment and productivity can be sup-\nported whilst minimizing motion sickness. Virtual Reality\n(VR) and Augmented Reality (AR) Head Mounted Displays\n(HMDs) have the potential to signiﬁcantly expand the display\nspace, enabling immersive entertainment and workspaces that\ngo beyond the physical limitations of the car interior. Problem-\natically, VR HMDs also occlude visual perception of reality\n[44, 6] and thus the car’s motion, and are likely to lead to\nsensory mismatch and, consequently, motion sickness. How-\never, assuming the orientation and velocity of the vehicle can\nbe tracked at low latency, HMDs have the potential to por-\ntray the vehicle motion virtually. Accordingly, for both VR\nHMDs, and passengers more generally, the problem of occlud-\ning the visual perception of motion, and the resultant sensory\nmismatch this causes, can be solved (as demonstrated in con-\nsumer VR rollercoasters which run over a known and precisely\ncontrolled route [77]). VR and AR HMDs are capable of con-\nveying the motion of the vehicle at all times, from all viewing\nangles. Consequently, the problem is then: how should these","sections":[{"word_count":794,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":4510,"figure_citations":{"1":["Figure 1) paired with a Samsung S7 smartphone (VR framework version 11, service 1 To see how conditions operated in-motion, view the attached video."]},"section_index":1,"title":"RELATED RESEARCH"},{"word_count":2422,"figure_citations":{"3":["Figure 3, with sickness being in-part minimized by preference."],"4":["Figure 4), we see how preferences were influenced by perceived sickness."]},"section_index":2,"title":"RESULTS"},{"word_count":551,"figure_citations":{},"section_index":3,"title":"DISCUSSION"},{"word_count":624,"figure_citations":{},"section_index":4,"title":"RECOMMENDATIONS FOR FURTHER RESEARCH"},{"word_count":313,"figure_citations":{},"section_index":5,"title":"CONCLUSIONS"},{"word_count":2298,"figure_citations":{},"section_index":6,"title":"REFERENCES"}],"title":"I Am The Passenger: How Visual Motion Cues Can Influence Sickness For In-Car VR","authors":"Mark McGill, Alexander Ng, Stephen Brewster","abstract":"This paper explores the use of VR Head Mounted Displays (HMDs) in-car and in-motion for the first time. Immersive HMDs are becoming everyday consumer items and, as they offer new possibilities for entertainment and productivity, people will want to use them during travel in, for example, autonomous cars. However, their use is confounded by motion sickness caused in-part by the restricted visual perception of motion conflicting with physically perceived vehicle motion (accelerations/rotations detected by the vestibular system). Whilst VR HMDs restrict visual perception of motion, they could also render it virtually, potentially alleviating sensory conflict. To study this problem, we conducted the first on-road and in motion study to systematically investigate the effects of various visual presentations of the real-world motion of a car on the sickness and immersion of VR HMD wearing passengers. We established new baselines for VR in-car motion sickness, and found that there is no one best presentation with respect to balancing sickness and immersion. Instead, user preferences suggest different solutions are required for differently susceptible users to provide usable VR in-car. This work provides formative insights for VR designers and an entry point for further research into enabling use of VR HMDs, and the rich experiences they offer, when travelling.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Left: Gear VR HMD used in study. Right: Peripheral blending\nof Condition 6, combining motion landscape and 360° video.\n","bbox":[321.094,601.90427,564.1630600000002,618.84127],"page":"5"},"2":{"caption":"Figure 2. 0.86km test route velocity proﬁle, as captured throughout the\nstudy across participants using GPS and OBD2 velocity.\n","bbox":[321.094,569.59327,564.1630600000001,586.52927],"page":"6"},"3":{"caption":"Figure 3. Stacked density plot (geom_density in R, using ..count.. and\n“stack”) of motion sickness susceptibility against preferred condition\n(higher is more susceptible), with labels indicating susceptibility per-\ncentiles for the general population from [41]. 50th%ile is considered\n“slightly susceptible”, and 75th%ile “moderately susceptible”.\n","bbox":[319.899,538.3502699999999,565.4860900000001,582.1862699999999],"page":"7"},"4":{"caption":"Figure 4. Plot of smoothed conditional mean real-time sickness ratings against condition and preference across participants (in black), with individual\nparticipants plotted (in colour) and if they stopped the condition prematurely (coloured circles show time stopped).\n","bbox":[53.929,483.03827,564.1604300000013,499.97526999999997],"page":"8"},"5":{"caption":"Figure 5. Interaction plot of SSQ total sickness score against condition\nand preference (higher is worse). Boxplots indicate the ﬁrst and third\nquartiles (25th and 75th percentiles). Coloured lines indicate means.\nGrey background line indicates the sickness score of the 2F64C ﬂight\nsimulator, suggested as indicating a problematic level of sickness [37].\n","bbox":[53.929,179.61426999999998,298.39281000000017,223.44927],"page":"8"}},"crops":{"1":{"crop_coord":[886.9277613888887,154.39742194444452,1572.068438888889,481.3694338888888],"bbox":[321.0939941,620.5070038,564.144638,734.6169281],"page":"5"},"2":{"crop_coord":[886.9277613888887,154.38915472222243,1572.126061388889,587.727771388889],"bbox":[321.0939941,582.2180023,564.1653821,734.6199042999999],"page":"6"},"3":{"crop_coord":[886.9277613888887,146.08862250000004,1572.1323030555557,599.7944641666668],"bbox":[321.0939941,577.8739929,564.1676291,737.6080959],"page":"7"},"4":{"crop_coord":[144.80278027777777,159.92538766972217,1572.7680595483334,828.1583488888888],"bbox":[53.9290009,495.6629944,564.3965014374,732.6268604389],"page":"8"},"5":{"crop_coord":[144.80278027777777,1076.4366831569444,829.9997405986111,1571.3777669444446],"bbox":[53.9290009,228.1040039,296.9999066155,402.6827940635],"page":"8"}}}},{"filename":"Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony.pdf","paper_id":"Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony","venue":"chi2019xr","keywords":["Virtual Reality (VR)","Embodied Avatars","Implict Gender Bias","Implicit Association Test (IAT)"],"paragraph_containing_keyword":"KEYWORDS\nVirtual Reality (VR), Embodied Avatars, Implict Gender Bias,\nImplicit Association Test (IAT)","paragraph_after_keyword":"∗Corresponding author byuksel@usfca.edu","doi":"10.1145/3290605.3300787","sections":[{"word_count":492,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1701,"figure_citations":{"1":["Figure 1 in [2], it appears as though the trackers are worn only on the wrists of participants.","Figure 1)."]},"section_index":1,"title":"RELATED WORK"},{"word_count":1574,"figure_citations":{"2":["Figure 2)."]},"section_index":2,"title":"SYNCHRONY AND AVATAR TRACKING"},{"word_count":788,"figure_citations":{"4":["Figure 4) to get acquainted with their VR bodies.","Figure 4)."]},"section_index":3,"title":"METHODOLOGY"},{"word_count":953,"figure_citations":{"7":["Figure 7 compares the ratings (from Table 2) of MyBody (Q1) compared to TwoBodies (Q2)."],"8":["Figure 8)."]},"section_index":4,"title":"RESULTS"},{"word_count":1393,"figure_citations":{"5":["Figure 5) and that participants embodied in female avatars actually had a mean increase in postIAT scores compared to a decrease in those embodied in male avatars (Figure 6)."],"6":["Figure 6) demonstrates the strength of the findings."],"7":["Figure 7 shows that the levels of body ownership are similar in both conditions.","Figure 7 shows that even though participants embodied in female avatars did not feel that the avatar resembled them in appearance, they still felt strongly that the movements of the virtual avatar were caused by their own movements."]},"section_index":5,"title":"DISCUSSION"},{"word_count":193,"figure_citations":{},"section_index":6,"title":"FUTURE WORK"},{"word_count":168,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":59,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGMENTS"},{"word_count":1544,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony","authors":"Sarah Lopez, Yi Yang, Kevin Beltran, Soo Jung Kim, Jennifer Cruz Hernandez, Chelsy Simran, Bingkun Yang, Beste F. Yuksel","abstract":"Previous research has shown that when White people embody a black avatar in virtual reality (VR) with full body visuomotor synchrony, this can reduce their implicit racial bias. In this paper, we put men in female and male avatars in VR with full visuomotor synchrony using wearable trackers and investigated implicit gender bias and embodiment. We found that participants embodied in female avatars displayed significantly higher levels of implicit gender bias than those embodied in male avatars. The implicit gender bias actually increased after exposure to female embodiment in contrast to male embodiment. Results also showed that participants felt embodied in their avatars regardless of gender matching, demonstrating that wearable trackers can be used for a realistic sense of avatar embodiment in VR. We discuss the future implications of these findings for both VR scenarios and embodiment technologies.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Left: Participant wearing all nine HTC Vive track-\ners and headset (highlighted by red cirles). Right: HTC Vive\ntrackers, headset, and accompanying devices.\n","bbox":[53.798,403.697,295.637918,434.581],"page":"4"},"2":{"caption":"Figure 2: Male and female avatars used for embodiment.\n","bbox":[324.282,532.963,551.865978,541.929],"page":"4"},"3":{"caption":"Figure 3: VR and avatar setup: Participant’s physical body\n(left) and tracked virtual body in female avatar (right). Par-\nticipant follows movements of the virtual Tai Chi Teacher.\n","bbox":[53.52,532.7590000000001,295.637864,563.643],"page":"6"},"4":{"caption":"Figure 4: Top: Participant’s point of view when looking\ndown at virtual body in female (left) and male (right) avatar.\nBottom: VR setting with virtual mirrors to the front and left\nof participant. Participant is following movements of vir-\ntual Tai Chi teacher.\n","bbox":[317.955,516.7850000000002,559.7949179999999,569.587],"page":"6"},"5":{"caption":"Figure 5: Bar chart (means and standard errors) of ∆IAT by\nembodiment (male or female). For those embodied in a male\navatar (red) the mean ± SE is -0.08 ± 0.010 and for those em-\nbodied in a female avatar (green) 0.30 ± 0.115.\n","bbox":[53.79799999999997,486.197168,295.64560599999993,529.403],"page":"7"},"6":{"caption":"Figure 6: For male-embodying participants (red), preIAT\nmean ± SE is 0.255 ± 0.085 and postIAT mean ± SE is 0.179\n± 0.115. For female-embodying participants (green), preIAT\nmean ± SE is 0.174 ± 0.138 and postIAT mean ± SE is 0.476 ±\n0.117.\n","bbox":[317.9549999999999,478.66499999999996,558.491142,531.467],"page":"7"},"7":{"caption":"Figure 7: Box plot of body ownership questions for top:\nMyBody and TwoBodies and bottom: MyMovements and\nMyFeatures. The thick black horizontal lines are the medi-\nans, the boxes are the interquartile ranges, and the whiskers\nextend to ±1.5 x IQR, or the range. Individual points are out-\nliers.\n","bbox":[53.798,311.393,295.641824,375.154],"page":"8"},"8":{"caption":"Figure 8: Bar graph of participants’ subjective ratings on\nphysical attraction of male and female virtual avatars\n","bbox":[53.798,119.47899999999998,294.033004,139.404],"page":"8"}},"crops":{"1":{"crop_coord":[197.894465,634.9451786111111,768.1752861111112,968.2194519444445],"bbox":[73.0420074,445.2409973,274.743103,561.6197357],"page":"4"},"2":{"crop_coord":[931.6639030555557,347.856191111111,1502.095574166667,670.0305430555555],"bbox":[337.1990051,552.5890045,538.9544067,664.9717712],"page":"4"},"3":{"crop_coord":[246.93057583333334,252.36473916666677,719.2835405555556,609.7111086111112],"bbox":[90.6950073,574.3040009,257.1420746,699.3486939],"page":"6"},"4":{"crop_coord":[931.6639030555557,252.36724000000027,1502.0216961111112,593.2027944444445],"bbox":[337.1990051,580.246994,538.9278106,699.3477935999999],"page":"6"},"5":{"crop_coord":[144.43890055555556,252.36664666666678,854.9979233333332,704.8249902777776],"bbox":[53.7980042,540.0630035,305.9992524,699.3480072],"page":"7"},"6":{"crop_coord":[878.2083383333334,252.36690111111125,1658.8436041666666,699.0916697222223],"bbox":[317.9550018,542.1269989,595.3836974999999,699.3479156],"page":"7"},"7":{"crop_coord":[162.86668555555556,252.3763952777776,803.347261111111,1133.2944572222223],"bbox":[60.4320068,385.8139954,287.405014,699.3444977],"page":"8"},"8":{"crop_coord":[144.43890055555556,1361.2917241666664,854.9832155555555,1788.1555683333333],"bbox":[53.7980042,150.0639954,305.9939576,300.1349793],"page":"8"}}}},{"filename":"Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit ","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit .pdf","paper_id":"Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit ","venue":"VR_2017","keywords":["Electrical circuits","Multiple representations","Augmented reality","Agent-based modeling","Design","Interactive surfaces","Museum learning"],"paragraph_containing_keyword":"ABSTRACT \nUnderstanding electrical circuits can be difficult for novices \nof  all  ages.  In  this  paper,  we  describe  a  science  museum \nexhibit that enables visitors to make circuits on an interactive \ntabletop  and  observe  a  simulation  of  electrons  flowing \nthrough \nto  use  multiple \nrepresentations to help convey basic concepts of current and \nresistance.  To  study  visitor  interaction  and  learning,  we \ntested  the  design  at  a  popular  science  museum  with  60 \nparent-child  dyads  in  three  conditions:  a  control  condition \nwith no electron simulation; a condition with the simulation \ndisplayed  alongside  the  circuit  on  the  same  screen;  and  an \naugmented  reality  condition, with  the  simulation  displayed \non  a  tablet  that  acts  as  a  lens  to  see  into  the  circuit.  Our \nfindings show that children did significantly better on a post-\ntest \nin  both  experimental  conditions,  with  children \nperforming best in the AR condition. However, analysis of \nsession videos shows unexpected parent-child collaboration \nin the AR condition. \nAuthor Keywords \nElectrical  circuits;  multiple  representations;  augmented \nreality;  agent-based  modeling;  design;  interactive  surfaces; \nmuseum learning.  \nACM Classification Keywords \nH.5.2 Information Interfaces and Presentation: User \nInterfaces - Interaction styles \nINTRODUCTION \nUnderstanding the flow of current in electrical circuits can \nbe  challenging  for  learners  of  all  ages  [17,  28,  36,  39]. \nResearch in Learning Sciences has documented a variety of \nmental  models  that  novices  rely  on  as  they  struggle  with \nconcepts  like  resistance,  current,  and  voltage  drop.  One \nstream of studies has shown that novices have an insufficient \nunderstanding  of  what  happens  at  the  level  of  atoms  and \nelectrons in a circuit [9, 30, 34]. For example, learners might \nPermission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear  this  notice  and  the  full  citation  on  the  first  page.  Copyrights  for\ncomponents  of  this  work  owned  by  others  than  ACM  must  be  honored.\nAbstracting  with  credit  is  permitted.  To  copy  otherwise,  or  republish,  to\npost on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from Permissions@acm.org. \nCHI 2017, May 06-11, 2017, Denver, CO, USA  \n© 2017 ACM. ISBN 978-1-4503-4655-9/17/05…$15.00  \nDOI: http://dx.doi.org/10.1145/3025453.3025479","doi":"10.1145/3025453.3025479","paragraph_after_keyword":"think of current as something like water in a pipe that flows \nout  of  the  battery  and  encounters  each  component  in  turn  \n[30]. Or, they might think of current as a substance that gets \nconsumed by things like lightbulbs and resistors. And, while \nthese  models  have  some  value  for  understanding  electrical \nphenomena, they differ from the scientific understanding in \nways that makes it difficult to predict things like the relative \nbrightness of lightbulbs in a series circuit (e.g. Figure 5).  \nOne promising strategy to help learners understand circuits \nis  to  provide  dynamic  visual  representations  of  electrical \nconcepts [15, 34]. For example, Frederiksen et al. explored \ndifferent  ways  of  visualizing  the  concept  of  voltage  for \nlearners by relating it to the distribution of charged particles \nin a circuit [15]. In another example, Sengupta and Wilesnky \n[34] created an agent-based representation of current based \non Drude’s model. In this model, a cloud of free electrons \nhas  a  net  movement  through  a  circuit  when  a  potential \ndifference  is  applied.  Simple  kinetic  interactions  between \nfree  electrons  and  ions  in  conductive  materials  result  in \nemergent properties that approximate Ohm’s law (see [34]). \nResearch  has  shown  that  this  kind  of  electron-level \nrepresentation  along  with  structured  curriculum  can  help \nstudents  develop  more  sophisticated  understandings  of \nsimple circuits [35]. \nIn this paper, we present the design and evaluation of Spark, \nan augmented circuit exhibit for science museums (Figures \n1-3). Spark combines a virtual circuit building environment \nwith  a  simulation  of  current  flow  based  on  Sengupta  and \nWilensky’s instantiation of Drude’s model. Visitors drag and \nconnect  circuit  components  (wires,  batteries,  resistors,  and","sections":[{"word_count":832,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2a), the simulation is displayed alongside the circuit on the same display."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":557,"figure_citations":{"4":["Figure 4)."]},"section_index":1,"title":"BACKGROUND"},{"word_count":245,"figure_citations":{"3":["Figure 3): a DC circuit simulator, an agent-based model of current flow, a virtual multimeter, and a brief textual description of basic concepts on electricity."],"4":["Figure 4)."]},"section_index":2,"title":"DESIGN OF SPARK"},{"word_count":1710,"figure_citations":{"3":["Figure 3)."],"5":["Figure 5).","Figure 5)."],"6":["Figure 6)."],"7":["Figure 7 illustrates two snapshots of the synchronized video recordings."]},"section_index":3,"title":"METHODS"},{"word_count":2903,"figure_citations":{"5":["Figure 5), families are shown two circuits; the first circuit has one lightbulb, and the second circuit has one resistor and one bulb after the resistor (assuming a counter-clockwise direction of current flow)."],"8":["Figure 8 shows the differences in usage of incorrect and correct conceptions for current path in each condition."],"9":["Figure 9)."],"10":["Figure 10 shows the number of incorrect and correct predictions in each condition."],"11":["Figure 11 illustrates the distribution of parental roles in each group."]},"section_index":4,"title":"RESULTS"},{"word_count":451,"figure_citations":{"12":["Figure 12), similar to the digital circuit simulator."]},"section_index":5,"title":"DISCUSSION"},{"word_count":145,"figure_citations":{},"section_index":6,"title":"CONCLUSION AND FUTURE WORK"},{"word_count":66,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGMENTS"},{"word_count":1413,"figure_citations":{"12":["Figure 12)."]},"section_index":8,"title":"REFERENCES"}],"title":"Looking Inside the Wires: Understanding Museum Visitor Learning with an Augmented Circuit Exhibit","authors":"Elham Beheshti, David Kim, Gabrielle Ecanow, Michael S. Horn","abstract":"Understanding electrical circuits can be difficult for novices of all ages. In this paper, we describe a science museum exhibit that enables visitors to make circuits on an interactive tabletop and observe a simulation of electrons flowing through the circuit. Our goal is to use multiple representations to help convey basic concepts of current and resistance. To study visitor interaction and learning, we tested the design at a popular science museum with 60 parent-child dyads in three conditions: a control condition with no electron simulation; a condition with the simulation displayed alongside the circuit on the same screen; and an augmented reality condition, with the simulation displayed on a tablet that acts as a lens to see into the circuit. Our findings show that children did significantly better on a post-test in both experimental conditions, with children performing best in the AR condition. However, analysis of session videos shows unexpected parent-child collaboration in the AR condition.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Spark interactive tabletop exhibit. \n","bbox":[354.84,60.635999999999996,523.8236199300001,69.636],"page":"1"},"2":{"caption":"Figure 2. Linking two representations of a circuit (a) side-by-\nside on one display and (b) with AR on a second display. \n","bbox":[321.3,470.373,555.83094294,489.69599999999997],"page":"2"},"3":{"caption":"Figure 3. Snapshot of Spark system in the single-display \ncondition. \n","bbox":[332.34,564.3330000000001,547.91971881,583.6560000000001],"page":"3"},"4":{"caption":"Figure 4. Electron model display in the AR condition. The \nblue dots are moving electrons and the red dots represent ions \nin conductive materials. Resistors have higher ion densities \nthan wires. \n","bbox":[319.3120349999999,345.762,559.4393246399995,385.839],"page":"3"},"5":{"caption":"Figure 5. Final task (task 3) in the series of tasks.  \n","bbox":[79.74,538.1761,271.73421656999994,547.1761],"page":"4"},"6":{"caption":"Figure 6. Post-interview questions and two examples of \nchildren’s drawings to show the flow of current. \n","bbox":[67.92008849999996,316.962,281.1615584399998,336.339],"page":"4"},"7":{"caption":"Figure 7. Snapshots of parent-child dyads using the exhibit in \nthe single-display condition (left) and the AR condition (right)\n","bbox":[319.7988297,567.3330000000001,558.4439408400003,586.6560000000001],"page":"5"},"8":{"caption":"Figure 8. Children’s mental models of current flow path in \ncontrol condition (C1); single-display condition (C2); and AR \ncondition (C3). \n","bbox":[56.457,550.9560000000001,293.922,580.6560000000001],"page":"7"},"9":{"caption":"Figure 9. Children’s mental models of mechanism of current \nflow with cyclic model and concurrent model grouped as \n”progressed” conceptions. \n","bbox":[322.02,551.19,556.3763999999998,580.836],"page":"7"},"10":{"caption":"Figure 10. Parent-child dyad predictions for task 3. \n","bbox":[339.72,568.836,538.6811999999998,577.836],"page":"8"},"11":{"caption":"Figure 11. Different types of parental roles in task 2 in the \nsingle-display condition (C2) and the AR condition (C3). \n","bbox":[326.82,414.99899999999997,551.6174999999998,434.376],"page":"9"},"12":{"caption":"Figure 12. A demonstration of Tangible Spark. \n","bbox":[349.26,535.5360000000001,530.0097000000001,544.5360000000001],"page":"10"}},"crops":{"1":{"crop_coord":[881.0000186111112,1509.6667055555554,1553.6666786111111,1991.0000186111113],"bbox":[318.9600067,77.0399933,557.5200043,246.719986],"page":"1"},"2":{"crop_coord":[881.6666580555554,175.66664777777785,1555,824.9999913888887],"bbox":[319.1999969,496.8000031,558,726.9600068],"page":"2"},"3":{"crop_coord":[883.6666616666666,174.3333263888889,1555.3333197222223,561.4999897222223],"bbox":[319.9199982,591.6600037,558.1199951,727.4400025],"page":"3"},"4":{"crop_coord":[897.0000033333334,635.6666649999999,1535.666690277778,1113.6666530555556],"bbox":[324.7200012,392.8800049,551.0400085,561.3600006],"page":"3"},"5":{"crop_coord":[153.00001361111114,175.00000833333328,810.3333283333334,665.0000169444444],"bbox":[56.8800049,554.3999939,289.9199982,727.199997],"page":"4"},"6":{"crop_coord":[221.66668361111113,764.9999916666666,740.3333366666667,1250.9999933333331],"bbox":[81.6000061,343.4400024,264.7200012,514.8000030000001],"page":"4"},"7":{"crop_coord":[881.6666580555554,173.66668694444456,1555,555],"bbox":[319.1999969,594,558,727.6799927],"page":"5"},"8":{"crop_coord":[149.49998638888886,165.9999933333332,817.3333399999999,570.6666649999999],"bbox":[55.6199951,588.3600006,292.44000239999997,730.4400024],"page":"7"},"9":{"crop_coord":[879.3333349999999,165.50001361111114,1554.5000202777778,570.1666852777778],"bbox":[318.3600006,588.5399933,557.8200073,730.6199951],"page":"7"},"10":{"crop_coord":[879.3333349999999,165.49997111111134,1554.5000202777778,579.1666580555554],"bbox":[318.3600006,585.3000031,557.8200073,730.6200104],"page":"8"},"11":{"crop_coord":[879.3333349999999,597.5000255555556,1554.5000202777778,993.3333419444444],"bbox":[318.3600006,436.1999969,557.8200073,575.0999908],"page":"9"},"12":{"crop_coord":[882.5,165.9999933333332,1554.1666580555554,627.6666769444442],"bbox":[319.5,567.8399963,557.6999969,730.4400024],"page":"10"},"13":{"crop_coord":[882.5,617.6666769444447,1554.1666580555554,672.3333486111112],"bbox":[319.5,551.7599945,557.6999969,567.8399962999999],"page":"10"}}}},{"filename":"MagicFace- Stepping into Character through an Augmented Reality Mirror","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/MagicFace- Stepping into Character through an Augmented Reality Mirror.pdf","paper_id":"MagicFace- Stepping into Character through an Augmented Reality Mirror","venue":"VR_2017","keywords":["Augmented reality","Opera characters","Interface design","Inthe-wild study"],"paragraph_containing_keyword":"ABSTRACT\nAugmented Reality (AR) is coming of age and appearing in \nvarious  smartphone  apps.  One  emerging  AR  type  uses  the \nfront-facing  camera  and  overlays  a  user’s  face  with  digital \nfeatures that transform the physical appearance, making the \nuser look like someone else, such as a popstar or a historical \ncharacter. However, little is known about how people react \nto  such  stepping  into  character  and  how  convincing  they \nperceive it to be. We developed an app with two Egyptian \nlooks,  MagicFace,  which  was  situated  both  in  an  opera \nhouse  and  a  museum.  In  the  first  setting,  people  were \ninvited to use the app, while in the second setting they came \nacross  it  on  their  own  when  visiting  the  exhibition.  Our \nfindings  show  marked  differences  in  how  people  approach \nand  experience  the  MagicFace  in  these  different  contexts. \nWe  discuss  how  realistic  and  compelling  this  kind  of  AR \ntechnology is, as well as its implications for educational and \ncultural settings. \nAuthor Keywords \nAugmented  reality;  Opera  characters;  Interface  design;  In-\nthe-wild study.  \nACM Classification Keywords \nH.5.m. Information interfaces and presentation (e.g., HCI): \nMiscellaneous. \nINTRODUCTION \nAugmented Reality (AR) has much potential for overlaying \na variety of digital content onto the real world. So far, it has \nbeen  mainly  used  for  educational,  commercial  and  gaming \nlocation-based \nexample,  providing \napplications, \ninformation  about  buildings  and  landmarks,  and  signage \nabout places of interest. The recent craze, Pokémon Go, has \ndemonstrated  its  popularity  as  a  gaming  platform,  where \nplayers  hunt  virtual  Pokémon  creatures  in  real  life.  Most \nAR apps use the back-facing camera in smartphones/tablets","paragraph_after_keyword":"for","doi":"10.1145/3025453.3025722","sections":[{"word_count":817,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":917,"figure_citations":{"3":["Figure 3)."]},"section_index":1,"title":"BACKGROUND"},{"word_count":108,"figure_citations":{},"section_index":2,"title":"RESEARCH AIMS"},{"word_count":4934,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2)."]},"section_index":3,"title":"MAGICFACE APP AND INSTALLATION"},{"word_count":1229,"figure_citations":{"5":["Figure 5), turning their heads in different directions and pouting to watch themselves with the virtual looks from various angles."]},"section_index":4,"title":"DISCUSSION"},{"word_count":346,"figure_citations":{},"section_index":5,"title":"CONCLUSIONS"},{"word_count":44,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":701,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"MagicFace: Stepping into Character through an Augmented Reality Mirror","authors":"Ana Javornik, Yvonne Rogers, Delia Gander, Ana Moutinho","abstract":"Augmented Reality (AR) is coming of age and appearing in various smartphone apps. One emerging AR type uses the front-facing camera and overlays a user's face with digital features that transform the physical appearance, making the user look like someone else, such as a popstar or a historical character. However, little is known about how people react to such stepping into character and how convincing they perceive it to be. We developed an app with two Egyptian looks, MagicFace, which was situated both in an opera house and a museum. In the first setting, people were invited to use the app, while in the second setting they came across it on their own when visiting the exhibition. Our findings show marked differences in how people approach and experience the MagicFace in these different contexts. We discuss how realistic and compelling this kind of AR technology is, as well as its implications for educational and cultural settings.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Four landing pages: (i) screen saver that is automatically replaced with virtual make-up once a user appears in front of \nthe camera (first from the left); (ii) screen saver with buttons that a user clicks on to see the virtual make-up (second from the left); \n(iii) screen saver is automatically replaced with the virtual mirror with the buttons for switching between the two looks (third from \nthe left); (iv) screen saver with the choice of two names that the user taps on to see the virtual looks (fourth from the left) \n","bbox":[54.96,85.258,559.056288,125.77192000000001],"page":"3"},"2":{"caption":"Figure 2: Info page that dropped down if an info button in the \nupper right corner of the virtual mirror was tapped \n","bbox":[55.5806,80.88336,295.7473040000002,100.08192000000001],"page":"4"},"3":{"caption":"Figure 3. Different groups of visitors trying on virtual make-up in the opera dressing room: opera singer (first from the left); \npupil (second from the left); make-up artist (third from the left), opera singer (fourth from the left). \n","bbox":[73.32,86.30192,553.9642080000002,105.74192000000001],"page":"5"},"5":{"caption":"Figure 5: A museum visitor doing an Egyptian walk while \ninteracting with the MagicFace \n","bbox":[63.7105,266.88336000000004,287.71294,286.08192],"page":"9"}},"crops":{"1":{"crop_coord":[145,1671.6387516666666,820.9999847222222,2040.9720866666667],"bbox":[54,59.0500488,293.7599945,188.4100494],"page":"1"},"2":{"crop_coord":[145,1841.6387430555556,1555,1975.6387413888888],"bbox":[54,82.5700531,558,127.21005249999999],"page":"3"},"3":{"crop_coord":[145,1380.1111091666664,1555,1850.6666733333334],"bbox":[54,127.5599976,558,293.3600007],"page":"3"},"4":{"crop_coord":[145,1001.9986130555554,822.7777608333333,1903.7261030555555],"bbox":[54,108.4586029,294.3999939,429.4804993],"page":"4"},"5":{"crop_coord":[875.0002627777777,911.9960869444446,1552.9169208333335,1367.2738733333333],"bbox":[316.8000946,301.5814056,557.2500915,461.88140869999995],"page":"4"},"6":{"crop_coord":[78.58832472222221,1374.3421852777778,1652.5183275,2488.3616469444446],"bbox":[30.0917969,-102.0101929,593.1065979,295.4368133],"page":"5"},"7":{"crop_coord":[170,1889.2777847222223,1565.333336666667,1974.6111044444444],"bbox":[63,82.9400024,561.7200012000001,110.0599975],"page":"5"},"8":{"crop_coord":[145,1011.0155402777777,817.2222052777778,1389.8933324999998],"bbox":[54,293.4384003,292.3999939,426.2344055],"page":"9"}}}},{"filename":"Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality.pdf","paper_id":"Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality","venue":"chi2019xr","keywords":["Social","Virtual reality","Photo sharing","Questionnaire","Immersion","Presence"],"doi":"10.1145/3290605.3300897","paragraph_containing_keyword":"CCS CONCEPTS\n• Human-centered computing → Virtual reality.\nKEYWORDS\nSocial, virtual reality, photo sharing, questionnaire, immer-\nsion, presence\nACM Reference Format:\nJie Li, Yiping Kong, Thomas Röggla, Francesca De Simone, Swamy\nAnanthanarayan, Huib de Ridder, Abdallah El Ali, and Pablo Cesar.\n2019. Measuring and Understanding Photo Sharing Experiences\nin Social Virtual Reality. In CHI Conference on Human Factors in\nComputing Systems Proceedings (CHI 2019), May 4–9, 2019, Glasgow,\nScotland UK. ACM, New York, NY, USA, 14 pages. https://doi.org/\n10.1145/3290605.3300897\n1 INTRODUCTION\nEver since the introduction of film cameras in the late 19th\ncentury, photographs have been inherently social artifacts,\nwhich are captured and shared in social contexts [11]. Fami-\nlies and friends typically gathered in the living room, pass-\ning around printed photos or browsing albums [36] or they\nshared photos with each other via post [38]. Collocated (co-\npresent) or remote photo sharing is a way of strengthening","sections":[{"word_count":863,"figure_citations":{"1":["Figure 1), use a 2D real-time user video as a virtual view of that user, or in the near future, use a highly detailed photo-realistic point cloud video [44]."],"2":["Figure 2) 1 https://www."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":2994,"figure_citations":{"2":["Figure 2) was to identify typical experiences of photo sharing activities, and to develop a questionnaire for measuring socialVR photo sharing experiences later used in Part 2."],"3":["Figure 3 illustrates the five phases with an example of a timeline, where actions were coded with blue dots and interpreted with a phrase.","Figure 3) identified in the context mapping sessions were introduced."]},"section_index":1,"title":"RELATED WORK"},{"word_count":1455,"figure_citations":{"2":["Figure 2), we present a within-subject controlled user study compare photo sharing experiences in three conditions: face-to-face (F2F), Facebook Spaces (FBS), Skype (SKP)."],"5":["Figure 5)."]},"section_index":2,"title":"STUDY"},{"word_count":2201,"figure_citations":{"6":["Figure 6a.","Figure 6b.","Figure 6c.","Figure 6d), which shows FBS was perceived to be more exciting and cheerful than F2F and SKP.","Figure 6d."]},"section_index":3,"title":"RESULTS"},{"word_count":1188,"figure_citations":{},"section_index":4,"title":"DISCUSSION"},{"word_count":161,"figure_citations":{},"section_index":5,"title":"FUTURE WORK AND CONCLUSION"},{"word_count":36,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":2086,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality","authors":"Jie Li, Yiping Kong, Thomas Röggla, Francesca De Simone, Swamy Ananthanarayan, Huib de Ridder, Abdallah El Ali, Pablo Cesar","abstract":"Millions of photos are shared online daily, but the richness of interaction compared with face-to-face (F2F) sharing is still missing. While this may change with social Virtual Reality (socialVR), we still lack tools to measure such immersive and interactive experiences. In this paper, we investigate photo sharing experiences in immersive environments, focusing on socialVR. Running context mapping (N=10), an expert creative session (N=6), and an online experience clustering questionnaire (N=20), we develop and statistically evaluate a questionnaire to measure photo sharing experiences. We then ran a controlled, within-subject study (N=26 pairs) to compare photo sharing under F2F, Skype, and Facebook Spaces. Using interviews, audio analysis, and our questionnaire, we found that socialVR can closely approximate F2F sharing. We contribute empirical findings on the immersiveness differences between digital communication media, and propose a socialVR questionnaire that can in the future generalize beyond photo sharing.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Photo sharing experience in Facebook Spaces© with\nremote partners (a) and (b) looking at the same photo.\n","bbox":[317.9556,365.59242400000005,558.2033464000001,386.50824320000004],"page":"1"},"2":{"caption":"Figure 2: Methodological approach for constructing our So-\ncialVR questionnaire.\n","bbox":[317.9556,533.606724,559.8063072,553.5300648],"page":"2"},"3":{"caption":"Figure 3: An example of experience timeline and five phases\nof actions during face-to-face photo sharing.\n","bbox":[317.9556,595.8419240000001,558.2013215999999,615.7652648000001],"page":"4"},"4":{"caption":"Figure 4: Correspondence analysis, showing three distinct\nexperience components, along with the 20 (numerically dis-\nplayed) interactions.\n","bbox":[53.7981,455.78268320000006,295.6488072,486.6629648],"page":"6"},"5":{"caption":"Figure 5: Illustrations of the three experimental conditions:\n(a) Face-to-face (b) Skype (c) FB Spaces.\n","bbox":[317.6776416,492.363624,559.3490208,512.2869648],"page":"7"},"6":{"caption":"Figure 6: (a)-(c) Sum of scores boxplots for across photo sharing conditions for face-to-face (F2F), FB Spaces (FBS), Skype (SKP).\n(d) Self-reported emotion ratings for each condition. ** = p<.001\n","bbox":[53.51972320000003,527.7272800000001,559.7357679999996,547.6506208000001],"page":"10"}},"crops":{"1":{"crop_coord":[892.8166791666666,920.5666944444446,1083.5166847222222,1014.9333444444444],"bbox":[323.2140045,428.423996,388.2660065,458.79598999999996],"page":"1"},"2":{"crop_coord":[1072.9833475,920.5666944444446,1263.6833530555555,1014.9333444444444],"bbox":[388.0740051,428.423996,453.12600710000004,458.79598999999996],"page":"1"},"3":{"crop_coord":[1253.3166758333332,920.5666944444446,1443.8500213888888,1014.9333444444444],"bbox":[452.9940033,428.423996,517.9860077,458.79598999999996],"page":"1"},"4":{"crop_coord":[1433.4833441666665,920.5666519444445,1534.0166641666665,1098.7666575],"bbox":[517.8540039,398.2440033,550.4459991,458.7960053],"page":"1"},"5":{"crop_coord":[892.8166791666666,1004.5666672222222,1083.5166847222222,1098.7666575],"bbox":[323.2140045,398.2440033,388.2660065,428.5559998],"page":"1"},"6":{"crop_coord":[1072.9833475,1004.5666672222222,1263.6833530555555,1098.7666575],"bbox":[388.0740051,398.2440033,453.12600710000004,428.5559998],"page":"1"},"7":{"crop_coord":[1253.3166758333332,1004.5666672222222,1443.8500213888888,1098.7666575],"bbox":[452.9940033,398.2440033,517.9860077,428.5559998],"page":"1"},"8":{"crop_coord":[893.1499988888888,252.23334416666688,1533.8500044444443,458.93333444444465],"bbox":[323.3339996,628.5839996,550.3860016,699.3959960999999],"page":"4"},"9":{"crop_coord":[963.1499905555556,252.23334416666688,1463.6833105555556,746.4333344444445],"bbox":[348.5339966,525.0839996,525.1259918000001,699.3959960999999],"page":"7"}}}},{"filename":"Personalising the TV Experience using Augmented","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Personalising the TV Experience using Augmented.pdf","paper_id":"Personalising the TV Experience using Augmented","venue":"chi2019xr","keywords":["Accessibility","Augmented Reality","BSL","SSE","Companion Screen","Connected Experiences","DGS","HbbTV 2.0","HoloLens","Interaction Techniques","Personalisation","Second Screen","Sign Language","Synchronisation","Television"],"doi":"10.1145/3290605.3300762","paragraph_containing_keyword":"KEYWORDS\nAccessibility, Augmented Reality, BSL, SSE, Companion Screen,\nConnected Experiences, DGS, HbbTV 2.0, HoloLens, Interac-\ntion Techniques, Personalisation, Second Screen, Sign Lan-\nguage, Synchronisation, Television\nACM Reference Format:\nVinoba Vinayagamoorthy, Maxine Glancy, Christoph Ziegler, and Ri-\nchard Schäffer. 2019. Personalising the TV Experience using Aug-\nmented Reality: An Exploratory Study on Delivering Synchro-\nnised Sign Language Interpretation. In CHI Conference on Human\nFactors in Computing Systems Proceedings (CHI 2019), May 4–9,\n2019, Glasgow, Scotland Uk. ACM, New York, NY, USA, 12 pages.\nhttps://doi.org/10.1145/3290605.3300762","paragraph_after_keyword":"1 INTRODUCTION\nResearchers in the media sector have worked on solutions\nto enable communication between personal devices and a\nconnected television (TV). The aim is to extend the viewers’\n“screen” beyond the TV as part of a personalised companion\nexperience in order to aid content discovery, to provide inter-\nactive services, or enable the collection of analytics [10, 44].\nAugmented Reality (AR) technologies offer the potential to\ncreate new display surfaces in the living room for placing\ncontent such as an interactive information graphics for elec-\ntion coverage on the coffee table, a three-dimensional model\nof a dinosaur skeleton on a nearby wall while watching an\narchaeological documentary, or a presenter on the sofa next\nto the viewer explaining a Shakespearean drama. As part\nof an exploratory study, we used an optical head mounted\ndisplay (HMD), in conjunction with a TV, to explore a way\nto deliver synchronised sign language interpretations.\nThe distribution of signed content forms part of the ac-\ncess services offered by broadcasters. In the UK, British Sign\nLanguage (BSL) [12, 15] is the main language used in signed\ncontent while in Germany, Deutsche Gebärdensprache (DGS)\n[17–19] is used. Traditionally, signed content is delivered as\na picture-in-picture video (in-vision) with a signer placed on\nthe bottom right corner of the TV. Although, functionally","sections":[{"word_count":305,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":877,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1230,"figure_citations":{"1":["Figure 1).","Figure 1).","Figure 1): 1."]},"section_index":2,"title":"RESEARCH OBJECTIVES"},{"word_count":1597,"figure_citations":{},"section_index":3,"title":"METHOD"},{"word_count":947,"figure_citations":{"2":["Figure 2 show descriptive statistics on participant responses."],"3":["Figure 3a.","Figure 3b illustrates participant responses to questions on the acceptance of the TV+HoloLens system.","Figure 3c) illustrates the distribution of responses of our participants."]},"section_index":4,"title":"QUANTITATIVE RESULTS"},{"word_count":1748,"figure_citations":{},"section_index":5,"title":"QUALITATIVE RESULTS"},{"word_count":903,"figure_citations":{"3":["Figure 3b and Figure 3c)."]},"section_index":6,"title":"DISCUSSION"},{"word_count":31,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGMENTS"},{"word_count":1479,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Personalising the TV Experience using Augmented Reality: An Exploratory Study on Delivering Synchronised Sign Language Interpretation","authors":"Vinoba Vinayagamoorthy, Maxine Glancy, Christoph Ziegler, Richard Schäffer","abstract":"Augmented Reality (AR) technology has the potential to extend the screen area beyond the rigid frames of televisions. The additional display area can be used to augment televisions (TVs) with extra information tailored to individuals, for instance, the provision of access services like sign language interpretations. We invited 23 (11 in the UK, 12 in Germany) users of signed content to evaluate three methods of watching a sign language interpreted programme - one traditional in-vision method with signed programme content on TV and two AR-enabled methods in which an AR sign language interpreter (a 'half-body' version and a 'full-body' version) is projected just outside the frame of the TV presenting the programme. In the UK, participants were split 3-ways in their preferences while in Germany, half the participants preferred the traditional method followed closely by the 'half-body' version. We discuss our participants reasoning behind their preferences and implications for future research.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Conditions in our study (Left column: The UK,\nRight column: Germany; Top images: traditional control in-\nvision condition (T), Middle images: ‘half-body’ AR condi-\ntions (H), Bottom images: ‘full-body’ AR conditions (F)\n","bbox":[317.722,412.656854,559.795034,454.499854],"page":"3"},"2":{"caption":"Figure 2: Descriptive statistics on responses to post condition questions [39]: max: maximum, 3rd: third quartile, 2nd: median,\n1st: first quartile, min: minimum. Solid boxes indicate where Friedman test indicates an influence of method (T: traditional,\nH: half-body, F: full-body). Dashed boxes indicate where Mann-Whitney U test indicate a difference across countries.\n","bbox":[53.565000000000005,541.0618540000002,559.274298,571.945854],"page":"7"},"3":{"caption":"Figure 3: a) Participant preference of the methods; b) Responses on questions about the acceptance of the TV+HoloLens system;\nc) Responses to the question on whether the HoloLens changed the television experience.\n","bbox":[53.798,574.225854,558.870712,594.150854],"page":"8"}},"crops":{"1":{"crop_coord":[878.2083383333334,252.37711583333328,1555.5494858333334,907.1361033333333],"bbox":[317.9550018,467.2310028,558.1978149,699.3442383],"page":"3"},"2":{"crop_coord":[144.43890055555556,252.36707027777777,1555.6086138888888,580.8972166666665],"bbox":[53.7980042,584.677002,558.219101,699.3478547],"page":"7"},"3":{"crop_coord":[144.43890055555556,252.36668916666707,1555.5846661111113,519.2139011111113],"bbox":[53.7980042,606.8829956,558.2104798,699.3479918999999],"page":"8"}}}},{"filename":"Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation ","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation .pdf","paper_id":"Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation ","venue":"VR_2017","keywords":["Muscle interfaces","Virtual reality","EMS","Force feedback"],"paragraph_containing_keyword":"ABSTRACT \nWe  explore  how  to  add  haptics  to  walls  and  other  heavy \nobjects in virtual reality. When a user tries to push such an \nobject,  our  system  actuates  the  user’s  shoulder,  arm,  and \nwrist  muscles  by  means  of  electrical  muscle  stimulation, \ncreating a counter force that pulls the user's arm backwards. \nOur device accomplishes this in a wearable form factor. \nIn our first user study, participants wearing a head-mounted \ndisplay  interacted  with  objects  provided  with  different \ntypes  of  EMS  effects.  The  repulsion  design  (visualized  as \nan electrical field) and the soft design (visualized as a mag-\nnetic  field)  received  high  scores  on  “prevented  me  from \npassing through” as well as “realistic.” \nIn a second study, we demonstrate the effectiveness of our \napproach  by  letting  participants  explore  a  virtual  world  in \nwhich  all  objects  provide  haptic  EMS  effects,  including \nwalls, gates, sliders, boxes, and projectiles. \nAuthor Keywords \nMuscle interfaces; virtual reality; EMS; force feedback. \nACM Classification Keywords  \nH5.2 [Information interfaces and presentation]: User Inter-\nfaces. - Graphical user interfaces. \nINTRODUCTION \nRecent virtual reality systems allow users to walk freely in \nthe virtual world (aka real walking [36]). As the next step \ntowards  realism  and  immersion,  many  researchers  argue \nthat  these  systems  should  also  support  the  haptic  sense  in \norder to convey the physicality of the virtual world [3,4].  \nThere has been a good amount of progress towards simulat-\ning the haptic qualities of lightweight objects, such as con-\ntact  with  surfaces  [17]  or  textures  [8].  Solutions  generally \nrevolve around simulating the tactile qualities of the object, \ni.e., how the object affects the receptors in the user’s skin.\nThese  include  inflatable  pads  at  the  user’s  fingertips [20], \nvibro-tactile gloves [5], and glove exoskeletons [17].","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies \nbear this notice and the full citation on the first page. Copyrights for \ncomponents of this work owned by others than the author(s) must be \nhonored. Abstracting with credit is permitted. To copy otherwise, or \nrepublish, to post on servers or to redistribute to lists, requires prior specif-\nic permission and/or a fee. Request permissions from Permis-\nsions@acm.org.  \nCHI 2017, May 06 - 11, 2017, Denver, CO, USA  \nCopyright is held by the owner/author(s). Publication rights licensed to \nACM.  \nACM 978-1-4503-4655-9/17/05...$15.00 \nDOI: http://dx.doi.org/10.1145/3025453.3025600","doi":"10.1145/3025453.3025600","sections":[{"word_count":387,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":555,"figure_citations":{"1":["Figure 1a shows an example.","Figure 1b illustrates how our system implements the physicality of the cube, i."],"2":["Figure 2 illustrates the idea in more detail."],"3":["Figure 3, our system stimulates up to four different muscle groups."],"4":["Figure 4 illustrates the naïve approach to rendering objects using EMS: (a) From the moment the user’s fingertips reach the virtual wall, we actuate the user’s hand just strongly enough to prevent it from passing through."],"22":["Figure 22 in the Implementation section), which we control via USB from within our VR simulators."]},"section_index":1,"title":"ELECTRICAL MUSCLE STIMULATION HAPTICS FOR VR"},{"word_count":2065,"figure_citations":{"1":["Figure 1, which is designed to suggest an increasingly solid inside under a soft, permeable surface."],"5":["Figure 5 shows the same concept wrapped in visuals suggesting a magnetic field, suggesting a magnetic force that carefully pushes the user’s hand backwards."],"6":["Figure 6 shows what we call electro visuals.","Figure 6)."],"7":["Figure 7 shows the five “walls” arranged in a pentagon with the participant inside."],"8":["Figure 8, eight participants picked the repulsion wall as their favorite."],"9":["Figure 9 shows how participants rated the five conditions with respect to the question “what I feel matches what I see."],"10":["Figure 10 shows participants’ assessment of “this wall was able to prevent me from passing through”."],"11":["Figure 11, measured using our optical tracking system Optitrack 17w)."]},"section_index":2,"title":"DESIGN"},{"word_count":215,"figure_citations":{},"section_index":3,"title":"BENEFITS AND CONTRIBUTION"},{"word_count":1969,"figure_citations":{"12":["Figure 12, this room is designed as a jail cell with an “electrified” gate and walls."],"15":["Figure 15, users operate a pair of traditional sliders in order to align the pipeline elements that establish a hydraulic link.","Figure 15)."],"16":["Figure 16, their hand is pushed backwards by the water’s viscosity."],"18":["Figure 18), they feel a resistance when their hands come together as to grasp the cube.","Figure 18, we see how users pick up the cube and throw it over a glass wall, down a chute, which activates a second button."],"19":["Figure 19 shows a third cube that rests on a slide, which leads up to the last of the three buttons."],"20":["Figure 20, the EMS condition received substantially higher ratings."],"21":["Figure 21)."]},"section_index":4,"title":"EXAMPLE WIDGETS"},{"word_count":363,"figure_citations":{},"section_index":5,"title":"RELATED WORK"},{"word_count":696,"figure_citations":{"23":["Figure 23 depicts the exact electrode placement we used in our prototypes to actuate the user’s arm and hand."]},"section_index":6,"title":"IMPLEMENTATION"},{"word_count":161,"figure_citations":{},"section_index":7,"title":"CONCLUSIONS"},{"word_count":1256,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation","authors":"Pedro Lopes, Sijing You, Lung-Pan Cheng, Sebastian Marwecki, Patrick Baudisch","abstract":"We explore how to add haptics to walls and other heavy objects in virtual reality. When a user tries to push such an object, our system actuates the user's shoulder, arm, and wrist muscles by means of electrical muscle stimulation, creating a counter force that pulls the user's arm backwards. Our device accomplishes this in a wearable form factor.In our first user study, participants wearing a head-mounted display interacted with objects provided with different types of EMS effects. The repulsion design (visualized as an electrical field) and the soft design (visualized as a magnetic field) received high scores on \"prevented me from passing through\" as well as \"realistic\".In a second study, we demonstrate the effectiveness of our approach by letting participants explore a virtual world in which all objects provide haptic EMS effects, including walls, gates, sliders, boxes, and projectiles.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: (a) As this user lifts a virtual cube, our system \nlets the user feel the weight and resistance of the cube. \n(b) Our system implements this by actuating the user’s \nopposing muscles using electrical muscle stimulation. \n","bbox":[319.25598923378425,295.12439004757016,558.78790115609,339.52438855027646],"page":"1"},"2":{"caption":"Figure 2: (a) When a user picks up a physical cube, its \nweight causes tension in the user’s biceps. (b) Our sys-\ntem creates this tension by instead actuating the oppos-\ning muscles, here the user’s triceps and shoulders. \n","bbox":[56.35899809941492,398.0843865754677,291.60141016637476,442.484385078174],"page":"2"},"3":{"caption":"Figure 3: We use up to 8 electrode pairs, actuating \n(a) wrist, (b) biceps, (c) triceps, and (d) shoulders. \n","bbox":[330.36698885909,307.60438962670924,547.7422415285811,328.96438890638956],"page":"2"},"4":{"caption":"Figure 4: Implementing rigid walls requires stimulating \nmuscles with strong impulses over long periods. This \ndraws undesired attention to the electrical stimulation. \n","bbox":[54.967998146323474,551.2043814118276,295.1374300471303,584.0843803030209],"page":"3"},"5":{"caption":"Figure 5: The magnetic visuals allow the user’s hand to \npenetrate the surfaces of objects. \n","bbox":[56.76899808558861,212.084392847914,293.3666301068466,233.44439212759434],"page":"3"},"6":{"caption":"Figure 6: The electro visuals.  \n","bbox":[376.6099872996449,306.88438965098976,504.05998300167016,316.7243893191571],"page":"3"},"7":{"caption":"Figure 7: Study participant in the virtual world of the \nstudy, here facing the vibro only condition. \n","bbox":[59.269698001257986,335.2043886959591,290.98575018713666,356.5643879756394],"page":"4"},"11":{"caption":"Figure 11: The repulsion wall stopped participants’ \nhands on average 3.6 cm (error bars denote std. dev.).   \n","bbox":[59.282698000819565,227.92439231374428,295.84399002330304,249.28439159342463],"page":"5"},"12":{"caption":"Figure 12: The jail cell features “electrified” walls and a \ngate. Touching any of these repel the user’s hand. \n","bbox":[54.293698169062786,198.1643933173357,295.8725900223386,219.52439259701606],"page":"6"},"13":{"caption":"Figure 13: Pushing this soft button opens the gate. \n","bbox":[331.0406888363707,594.644379946908,547.1286815492718,604.4843796150753],"page":"6"},"14":{"caption":"Figure 14: Projectiles based on the repulsion effect. \n","bbox":[329.26598889621863,310.00438954577436,548.9039814894037,319.8443892139417],"page":"6"},"15":{"caption":"Figure 15: The user is dragging the knob of a slider \nmechanism. The two buttons on the left form a rocker.  \n","bbox":[57.61699805699164,589.6043801168711,295.01329005131663,610.9643793965514],"page":"7"},"16":{"caption":"Figure 16: Playing with the water in these fish tanks \npushes the user’s hand backwards. \n","bbox":[62.87969787951858,340.0043885340895,287.2978903115018,361.36438781376984],"page":"7"},"17":{"caption":"Figure 17: Two cubes that users can push onto the \nbutton on the right, or pick up and carry around. \n","bbox":[66.76169774860665,91.6043969108405,283.47833044030835,112.72439619861433],"page":"7"},"18":{"caption":"Figure 18: The user has picked up a cube and is about \nto throw it over the glass barrier. \n","bbox":[322.6956891177879,515.6843826096627,555.4593212683392,537.0443818893432],"page":"7"},"20":{"caption":"Figure 20: Participants rated their experience in the \nEMS condition higher than in baseline. \n","bbox":[62.915697878304535,285.5243903713094,287.2594303127986,306.8843896509897],"page":"8"},"21":{"caption":"Figure 21: Participants preferred the experience in the \nEMS condition. \n","bbox":[57.365698065466255,76.7243974126361,292.81675012539,98.08439669231643],"page":"8"},"22":{"caption":"Figure 22: The muscle stimulator we used. \n","bbox":[347.4449882831712,194.80439343064447,530.7249821024509,204.6443930988118],"page":"9"},"23":{"caption":"Figure 23: Electrode placement for arm and shoulder. \n","bbox":[58.59799802390957,385.6043869963286,291.57599016723213,395.444386664496],"page":"10"}},"crops":{"1":{"crop_coord":[931.0110907494064,874.5861602520688,1607.583299035059,1329.1055690598282],"bbox":[336.9639926697863,315.3219951384619,576.9299876526213,475.34898230925523],"page":"1"},"2":{"crop_coord":[881.1277374671275,858.7500385020091,1588.336032717873,1333.7111376768219],"bbox":[319.00598548816595,313.66399043634414,570.0009717784343,481.0499861392767],"page":"1"},"3":{"crop_coord":[881.1277374671275,637.2111634827065,1554.9249231256142,981.1889146050919],"bbox":[319.00598548816595,440.5719907421669,557.9729723252211,560.8039811462256],"page":"1"},"4":{"crop_coord":[1045.024970560744,658.9056004015275,1436.8499591588088,1108.4166933595436],"bbox":[378.00898940186784,394.7699903905643,515.4659852971712,552.9939838554501],"page":"1"},"5":{"crop_coord":[144.66667497140867,1690.6388862931215,820.6666362965703,2059.9722086776783],"bbox":[53.880002989707116,52.21000487603585,293.63998906676534,181.57000093447624],"page":"1"},"6":{"crop_coord":[1160.2499319102158,967.586145099557,1643.999923730334,1293.6889036361256],"bbox":[419.4899754876777,328.07199469099476,590.0399725429203,441.86898776415944],"page":"2"},"7":{"crop_coord":[1176.838862939952,685.5445205543173,1655.9832861430762,1008.5750491685139],"bbox":[425.46199065838266,430.712982299335,594.3539830115075,543.4039726004457],"page":"2"},"8":{"crop_coord":[828.436088917064,927.0250639134123,1369.3360505893677,1291.2611456074749],"bbox":[300.036992010143,328.9459875813091,491.1609782121724,456.4709769911716],"page":"2"},"9":{"crop_coord":[778.6638704969316,683.6222523031063,1290.0999592326145,1028.1972578280102],"bbox":[282.11899337889537,423.64898718191637,462.63598532374124,544.0959891708817],"page":"2"},"10":{"crop_coord":[769.8416204280586,353.73618770281877,1321.2388232819055,724.966728532864],"bbox":[278.9429833541011,532.811977728169,473.84597638148597,662.8549724269852],"page":"2"},"11":{"crop_coord":[1160.2499743016908,397.7000635301095,1651.3749624691454,728.7250529155564],"bbox":[419.48999074860865,531.4589809503997,592.6949864888924,647.0279771291606],"page":"2"},"12":{"crop_coord":[150.22223033961515,256.3278540735685,825.4999847239517,567.7167323681894],"bbox":[55.880002922261454,589.4219763474518,295.3799945006226,697.9219725335154],"page":"3"},"13":{"crop_coord":[150.22223033961515,1211.8555711256736,825.4999847239517,1541.3000043530758],"bbox":[55.880002922261454,238.9319984328927,295.3799945006226,353.93199439475745],"page":"3"},"14":{"crop_coord":[883.6943928568897,968.2333720502154,1558.9721472412261,1310.1778048144129],"bbox":[319.9299814284803,322.13599026681135,559.4299730068415,441.6359860619225],"page":"3"},"15":{"crop_coord":[150.22223033961515,843.7806159229298,824.11109589579,1199.6139371076436],"bbox":[55.880002922261454,361.9389826412483,294.87999452248437,486.43897826774526],"page":"4"},"16":{"crop_coord":[150.22223033961515,1234.344482282245,826.8888735798932,1580.455581586487],"bbox":[55.880002922261454,224.83599062886466,295.87999448876155,345.83598637839185],"page":"6"},"17":{"crop_coord":[883.6943928568897,164.6667431146282,1557.5832584130644,510.7778424188704],"bbox":[319.9299814284803,609.9199767292066,558.9299730287032,730.9199724787338],"page":"6"},"18":{"crop_coord":[883.6943928568897,954.0778363374285,1557.5832584130644,1301.5778244698324],"bbox":[319.9299814284803,325.23198319086026,558.9299730287032,446.7319789185257],"page":"6"},"19":{"crop_coord":[150.22223033961515,164.66674314240737,825.4999847239517,492.7222875138683],"bbox":[55.880002922261454,616.4199764950074,295.3799945006226,730.9199724687334],"page":"7"},"20":{"crop_coord":[150.22223033961515,904.0805931360662,826.8888735798932,1186.3028057892718],"bbox":[55.880002922261454,366.7309899158621,295.87999448876155,464.7309864710162],"page":"7"},"21":{"crop_coord":[150.22223033961515,1531.8555937975202,825.4999847239517,1876.5778042458212],"bbox":[55.880002922261454,118.23199047150436,295.3799945006226,238.73198623289267],"page":"7"},"22":{"crop_coord":[880.0666158815478,322.98896587558016,1554.8054807289614,708.8306020391867],"bbox":[318.62398171735725,538.6209832658927,557.929973062426,673.9239722847911],"page":"7"},"23":{"crop_coord":[883.6943928568897,1173.508370589122,1557.5832584130644,1516.841692209261],"bbox":[319.9299814284803,247.73699080466596,558.9299730287032,367.736986587916],"page":"7"},"24":{"crop_coord":[883.6943928568897,1322.6889259923792,1560.041576389092,1621.438907361314],"bbox":[319.9299814284803,210.08199334992696,559.8149675000731,314.0319866427435],"page":"9"},"25":{"crop_coord":[283.3222266663784,624.5639166310397,1047.444404396636,1137.3194667100663],"bbox":[103.79600159989623,384.36499198437616,375.2799855827889,565.3569900128257],"page":"10"}}}},{"filename":"Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives.pdf","paper_id":"Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives","venue":"chi2019xr","keywords":["Embodiment","Tool Embodiment","Embodied Interaction","Virtual Reality","Ready-to-hand","Unready-to-hand","Tools"],"paragraph_containing_keyword":"ABSTRACT\nVirtual reality (VR) strives to replicate the sensation of the\nphysical environment by mimicking people’s perceptions and\nexperience of being elsewhere. These experiences are often\nmediated by the objects and tools we interact with in the\nvirtual world (e.g., a controller). Evidence from psychology\nposits that when using the tool proficiently, it becomes em-\nbodied (i.e., an extension of one’s body). There is little work,\nhowever, on how to measure this phenomenon in VR, and\non how different types of tools and controllers can affect the\nexperience of interaction. In this work, we leverage cognitive\npsychology and philosophy literature to construct the Locus-\nof-Attention Index (LAI), a measure of tool embodiment. We\ndesigned and conducted a study that measures readiness-to-\nhand and unreadiness-to-hand for three VR interaction tech-\nniques: hands, a physical tool, and a VR controller. The study\nshows that LAI can measure differences in embodiment with\nworking and broken tools and that using the hand directly\nresults in more embodiment than using controllers.\nCCS CONCEPTS\n• Human-centered computing → HCI design and evalu-\nation methods; Interaction devices; Empirical studies in HCI .\nKEYWORDS\nEmbodiment, Tool Embodiment, Embodied Interaction, Vir-\ntual Reality, Ready-to-hand, Unready-to-hand, Tools.\nACM Reference Format:\nAyman Alzayat, Mark Hancock, and Miguel A. Nacenta. 2019. Quan-\ntitative Measurement of Tool Embodiment for Virtual Reality Input\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nCHI 2019, May 4–9, 2019, Glasgow, Scotland, UK\n© 2019 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 978-1-4503-5970-2/19/05. . . $15.00\nhttps://doi.org/10.1145/3290605.3300673","doi":"10.1145/3290605.3300673","paragraph_after_keyword":"Alternatives. In CHI Conference on Human Factors in Computing Sys-\ntems Proceedings (CHI 2019), May 4–9, 2019, Glasgow, Scotland, UK.\nACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3290605.\n3300673\n1 INTRODUCTION\nResearch in virtual reality has a long tradition striving to re-\nproduce the sensations, perceptions and experience of being\nelsewhere (this is often referred to as presence [39, 43], immer-\nsion [19] or spatial-temporal deformation [1]). An important\npart of this presence is interaction; people in an immersive\nVR environment need to be able to interact realistically with\nthe objects in the environment in order not to break the illu-\nsion of immersion in an alternate reality [37]. Presumably, in\norder to produce high-fidelity interaction in immersive VR\nenvironments, we also need an understanding of the issues\nof human physical interaction in the real world.\nFrom research in psychology and philosophy, we know\nthat interaction with objects of the environment is mediated\nby the physicality of our body [8, 9, 11, 29]. I.e., interaction in\nthis context is not only a matter of how an external object is\nperceived by the person’s sensory mechanisms but, perhaps\nmore importantly, involves how the human body itself affects\nthe physical object and how the overall interplay between\nboth the object and the body is processed by the brain.\nTo make things more complex, interaction with both phys-\nical and virtual objects is often mediated by tools. Tools are\nobjects themselves that serve as intermediaries between our\nbodies and other objects that we intend to modify (e.g., we\ncan use a stick in our hand to topple a vase that is out of\nour reach). Tools are particularly interesting in the context\nof human-computer interaction for two main reasons: first,\nVR interaction is often implemented through the use of in-\nput devices (e.g., the Oculus Touch VR Controller), which are\nthemselves presented in VR as tools that mediate interaction\nwith virtual objects. Second, from evidence in psychology\nand theories from philosophy, we know that tools become,\nin some ways, extensions of our own body when sufficient\nproficiency with the tool has been achieved. We know, for\nexample, that experts often refer to their tools as “almost part\nof their own bodies”, and that psychologists have found ev-\nidence that tool use can produce changes in body perception","sections":[{"word_count":543,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":4243,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2) wearing the Oculus Rift and holding one of three controllers (see Conditions)."],"3":["Figure 3) to successively find and grab letter cubes from the bookshelf and place them (by releasing with the tool) in the placeholders marked with the corresponding letter.","Figure 3)."],"4":["Figure 4c shows the physical apparatus for this condition.","Figure 4b)."],"5":["Figure 5a).","Figure 5a."],"6":["Figure 6)."],"7":["Figure 7)."]},"section_index":1,"title":"RELATED WORK"},{"word_count":1242,"figure_citations":{"9":["Figure 9 shows the distribution of the indices split by interaction technique, state and phase."],"10":["Figure 10 shows the distributions of the GEQ scores by technique, state and phase."]},"section_index":2,"title":"RESULTS"},{"word_count":738,"figure_citations":{},"section_index":3,"title":"GENERAL DISCUSSION"},{"word_count":83,"figure_citations":{},"section_index":4,"title":"CONCLUSION"},{"word_count":42,"figure_citations":{},"section_index":5,"title":"ACKNOWLEDGMENTS"},{"word_count":1224,"figure_citations":{},"section_index":6,"title":"REFERENCES"}],"title":"Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives","authors":"Ayman Alzayat, Mark Hancock, Miguel A. Nacenta","abstract":"Virtual reality (VR) strives to replicate the sensation of the physical environment by mimicking people's perceptions and experience of being elsewhere. These experiences are of-ten mediated by the objects and tools we interact with in the virtual world (e.g., a controller). Evidence from psychology posits that when using the tool proficiently, it becomes em-bodied (i.e., an extension of one's body). There is little work,however, on how to measure this phenomenon in VR, andon how different types of tools and controllers can affect the experience of interaction. In this work, we leverage cognitive psychology and philosophy literature to construct the Locus-of-Attention Index (LAI), a measure of tool embodiment. We designed and conducted a study that measures readiness-to-hand and unreadiness-to-hand for three VR interaction techniques: hands, a physical tool, and a VR controller. The study shows that LAI can measure differences in embodiment with working and broken tools and that using the hand directly results in more embodiment than using controllers.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Our operationalization of tool embodiment relies\non the idea that, when embodied with a tool, a person’s\nattention will be on the task, rather than on the tool.\n","bbox":[53.798,146.629854,294.03300400000006,177.513854],"page":"3"},"2":{"caption":"Figure 2: A photograph (a) and diagram (b) of the physical study setup.\n","bbox":[165.896,479.224854,446.0932700000001,488.190854],"page":"4"},"3":{"caption":"Figure 3: shows the virtual tool used in our study\n","bbox":[76.717,351.324854,271.118616,360.290854],"page":"4"},"4":{"caption":"Figure 4: The three controllers used in our study.\n","bbox":[208.997,455.834854,402.994342,464.800854],"page":"5"},"5":{"caption":"Figure 5: Virtual study Scene Setup. a. Virtual study scene setup with a virtual tool, bookshelf, task plates, and information\ndisplay. b. The virtual study scene when the pause button is pressed. Numbers on visual dots support the secondary task.\n","bbox":[53.797999999999995,516.4548540000001,558.181134,536.379854],"page":"6"},"6":{"caption":"Figure 6: From left to right, example of breakage (cube falls).\n","bbox":[53.798,322.690854,294.033808,331.656854],"page":"6"},"7":{"caption":"Figure 7: Our LAI measure corresponds to our theory above\nand measures across the entirety of a trial whether the\nattention is on the tool (negative) or on the task (positive).\n","bbox":[53.798,387.471854,294.033004,418.355854],"page":"7"},"8":{"caption":"Figure 8: SLAI measures a shift in attention from either the\ntask to the tool (e.g., when a tool breaks) or vice versa.\n","bbox":[317.955,583.605854,558.1900039999998,603.530854],"page":"7"},"9":{"caption":"Figure 9: Results for the locus of attention index (LAI). There\nwere main effects of interaction technique, state of the\ntool (working/broken) and phase. Note only [0,1] range of\npossible [-1,1] range is represented.\n","bbox":[53.449000000000005,511.5448540000002,294.03367799999995,553.3878540000001],"page":"8"},"10":{"caption":"Figure 10: Results for engagement (GEQ). There was a signif-\nicant main effect of state (working/broken) and a marginally\nsignificant effect of interaction technique.\n","bbox":[53.798,522.5038540000002,295.63791799999996,553.3878540000001],"page":"9"}},"crops":{"1":{"crop_coord":[144.43890055555556,1537.8978664444444,821.7793820472222,1676.5416802777777],"bbox":[53.7980042,190.2449951,294.040577537,236.55676808],"page":"3"},"2":{"crop_coord":[195.00279749999999,252.35281623472218,701.9714193041666,813.5500080555555],"bbox":[72.0010071,500.9219971,250.90971094949998,699.3529861555],"page":"4"},"3":{"crop_coord":[697.5055355555555,252.35858005222235,1504.9985682322224,813.5500080555555],"bbox":[252.9019928,500.9219971,539.9994845636,699.3509111812],"page":"4"},"4":{"crop_coord":[144.43890055555556,908.7108849466669,821.79557714,1168.8277688888888],"bbox":[53.7980042,373.0220032,294.0464077704,463.06408141919997],"page":"4"},"5":{"crop_coord":[145.90556666666666,257.0198527902778,618.2876632833332,878.5222286111111],"bbox":[54.326004,477.5319977,220.783558782,697.6728529955],"page":"5"},"6":{"crop_coord":[613.813875,253.1734786111112,1086.1823676111112,878.5222286111111],"bbox":[222.772995,477.5319977,389.22565234,699.0575477],"page":"5"},"7":{"crop_coord":[1081.7222255555557,252.36755273333324,1554.1098642066668,878.5222286111111],"bbox":[391.2200012,477.5319977,557.6795511144,699.347681016],"page":"5"},"8":{"crop_coord":[158.02498722222222,252.3714546755557,1113.7636465911112,679.6916877777778],"bbox":[58.6889954,549.1109924,399.1549127728,699.3462763168],"page":"6"},"9":{"crop_coord":[1109.2972311111112,252.3731479052779,1541.9776008377778,679.6916877777778],"bbox":[401.1470032,549.1109924,553.3119363016,699.3456667541],"page":"6"},"10":{"crop_coord":[226.3805475,805.2978819161109,739.8506877408333,1248.3638677777776],"bbox":[83.2969971,344.3890076,264.5462475867,500.2927625102],"page":"6"},"11":{"crop_coord":[144.43890055555556,858.7969319344446,821.774989816111,1007.5360955555556],"bbox":[53.7980042,431.0870056,294.03899633379996,481.0331045036],"page":"7"},"12":{"crop_coord":[878.2083383333334,252.36765839444433,1555.564150358333,493.1611294444442],"bbox":[317.9550018,616.2619934,558.203094129,699.3476429780001],"page":"7"},"13":{"crop_coord":[176.52224222222222,252.36492030555542,789.7087822722222,632.4472130555554],"bbox":[65.3480072,566.1190033,282.495161618,699.34862869],"page":"8"},"14":{"crop_coord":[148.86110944444445,252.35174603555538,817.3688114222223,632.4472130555554],"bbox":[55.3899994,566.1190033,292.452772112,699.3533714272],"page":"9"}}}},{"filename":"Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality.pdf","paper_id":"Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality","venue":"VR_2017","keywords":["Augmented reality","Virtual reality","Retargeting","Video tutorial"],"paragraph_containing_keyword":"ABSTRACT\nA video tutorial effectively conveys complex motions, but\nmay be hard to follow precisely because of its restriction to\na predetermined viewpoint. Augmented reality (AR) tutori-\nals have been demonstrated to be more effective. We bring\nthe advantages of both together by interactively retargeting\nconventional, two-dimensional videos into three-dimensional\nAR tutorials. Unlike previous work, we do not simply overlay\nvideo, but synthesize 3D-registered motion from the video.\nSince the information in the resulting AR tutorial is registered\nto 3D objects, the user can freely change the viewpoint with-\nout degrading the experience. This approach applies to many\nstyles of video tutorials. In this work, we concentrate on a\nclass of tutorials which alter the surface of an object.\nACM Classiﬁcation Keywords\nH.5.1 Information Interfaces and Presentation: Multimedia In-\nformation Systems - Artiﬁcial, augmented and virtual realities\nAuthor Keywords\nAugmented reality;virtual reality;retargeting;video tutorial","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\nand/or a fee. Request permissions from Permissions@acm.org.\nCHI 2017, May 06 – 11, 2017, Denver, CO, USA\nCopyright is held by the owner/author(s). Publication rights licensed to ACM.\nACM 978-1-4503-4655-9/17/05...$15.00.\nDOI: http://dx.doi.org/10.1145/3025453.3025688","doi":"10.1145/3025453.3025688","sections":[{"word_count":923,"figure_citations":{"1":["Figure 1 demonstrates drawing thick dots."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":732,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":508,"figure_citations":{"2":["Figure 2), or the user interactively models the 3D object of interest.","Figure 2 (left) the system recognizes the face and automatically registers a deformable face mesh.","Figure 2 (middle), we retarget the trajectory of the make-up brush by registering the face mesh along with its previously generated 2D texture to the user’s face.","Figure 2 (right), we abstract the motion by using one arrow for each segment of the trajectory.","Figure 2 (right), resulting in a presentation which uses arrows along the outline of the trajectory (see Figure 7)."]},"section_index":2,"title":"OVERVIEW"},{"word_count":1223,"figure_citations":{"2":["Figure 2, a deformable model can be tracked by determining an update to the deformation parameters in each frame."],"3":["Figure 3).","Figure 3).","Figure 3(b))."]},"section_index":3,"title":"EXTRACTION"},{"word_count":608,"figure_citations":{"2":["Figure 2 were originally applied to a jewelry box, but later retargeted to a teapot."],"4":["Figure 4(c)."]},"section_index":4,"title":"EDITING"},{"word_count":458,"figure_citations":{"2":["Figure 2, middle)."],"6":["Figure 6(b)), followed by an additional segmentation of the paths into smaller segments.","Figure 6(c) uses a threshold of 90◦ ).","Figure 6(c))."]},"section_index":5,"title":"VISUALIZATION"},{"word_count":1539,"figure_citations":{"4":["Figure 4 illustrates the painting tutorial."],"5":["Figure 5 illustrates the video tutorial.","Figure 5(a)).","Figure 5(b)."],"6":["Figure 6(d)).","Figure 6(d))."]},"section_index":6,"title":"EVALUATING THE AUTHORING"},{"word_count":1534,"figure_citations":{"7":["Figure 7(c)).","Figure 7(a)).","Figure 7(a)).","Figure 7).","Figure 7).","Figure 7(b)).","Figure 7(b))."],"8":["Figure 8 shows the boxplots of the measurements."]},"section_index":7,"title":"EVALUATING EFFICIENCY OF AR KANJI TUTORIAL"},{"word_count":75,"figure_citations":{},"section_index":8,"title":"SUS"},{"word_count":796,"figure_citations":{},"section_index":9,"title":"DISCUSSION AND FUTURE WORK"},{"word_count":36,"figure_citations":{},"section_index":10,"title":"ACKNOWLEDGMENTS"},{"word_count":1419,"figure_citations":{},"section_index":11,"title":"REFERENCES"}],"title":"Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality","authors":"Peter Mohr, David Mandl, Markus Tatzgern, Eduardo Veas, Dieter Schmalstieg, Denis Kalkofen","abstract":"A video tutorial effectively conveys complex motions, but may be hard to follow precisely because of its restriction to a predetermined viewpoint. Augmented reality (AR) tutorials have been demonstrated to be more effective. We bring the advantages of both together by interactively retargeting conventional, two-dimensional videos into three-dimensional AR tutorials. Unlike previous work, we do not simply overlay video, but synthesize 3D-registered motion from the video. Since the information in the resulting AR tutorial is registered to 3D objects, the user can freely change the viewpoint without degrading the experience. This approach applies to many styles of video tutorials. In this work, we concentrate on a class of tutorials which alter the surface of an object.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Retargeting a ’henna decoration’ video tutorial to a teapot decoration scenario in the user’s workspace. (left) The user extracts relevant\nmotion from the video, (middle) scales it and aligns the result to a 3D scan of a teapot in the current workspace. With our editing tools, the user can\nquickly alter the original tutorial to meet their requirements. In this example, the original video tutorial shows a decoration consisting of dots, which\nrequires a special henna pen. The user chooses to connect the dots into lines which can be drawn with a ceramic pen on the teapot. The user also scales\ndown the entire ornament to better ﬁt the desired aesthetics. (right) Using augmented reality, the user validates the result directly on the real teapot.\n","bbox":[53.929,405.28227000000004,564.1604300000012,449.11827],"page":"1"},"2":{"caption":"Figure 2. System overview. (left) We extract object and user motions by tracking known model features in the 2D video. Here, tracked features are\nused to record the path of the brush and to align a face model in each frame. (middle) After validating and possibly editing the extracted motion, we\nretarget the motion data to real world 3D objects. This requires registering the same 3D model as used in the extraction stage, in this case, a face model,\nto the live camera image. By tracking the model in 3D, we are able to relate video data to the real world. In this example, we present the recorded path\nof the brush directly on the user’s face. (right) Since we retarget the extracted motion data in 3D, we can choose an arbitrary point of view. To provide\neffective visual instructions, we generate dynamic glyphs (here: timed arrows) and we indicate the position of the brush over time using a red circle.\n","bbox":[53.929,511.89627,565.1566800000007,564.69827],"page":"3"},"3":{"caption":"Figure 3. Extracting motion with surface contact. (a) We extract the 2D trajectory by tracking the tip of the tool in unwrapped texture space. (b) We\nconvert the 2D trajectory to 3D by back-projecting the video data to a corresponding 3D model, in this case, a face.\n","bbox":[53.929,552.93927,564.1604300000013,569.87527],"page":"4"},"4":{"caption":"Figure 4. Segmentation and Layering. (a) We interactively segment the input data by selecting starting and ending frames. (b) This results in a set of\nactions (red), which we can use to derive image layers.\n","bbox":[53.929,629.3022699999999,564.160430000001,646.2392699999999],"page":"5"},"5":{"caption":"Figure 5. Experiment setup for a retargeted make-up tutorial. (a) Input video tutorial. (b) We showed the resulting AR tutorial using an AR mirror,\nwhich consisted of a camera and an USB display. (c) Participants could use the AR mirror and the video which we placed next to the mirror.\n","bbox":[53.642,604.47227,565.156680000001,621.40827],"page":"7"},"6":{"caption":"Figure 6. Path generation and ﬁrst revision of AR visualization. (a) We generate path illustrations from motion capture data. (b) The extracted path\ndata is analyzed and simpliﬁed. In particular, we remove zig-zag overdraw along the trajectory by clustering and detect turning points (marked in\ngreen). (c) We generate arrows in-between turning points, the start point and end point. (d) At runtime, we use the arrows to provide a preview of the\nmotions. To minimize occlusion, the arrow is replaced by the border of the tool’s trajectory. The red dot shows the extracted tool position over time. (e)\nThe combination of visualization techniques provide an overview ﬁrst, before the user can follow the exact motion.\n","bbox":[53.666000000000004,547.1292699999999,564.694420000001,590.9652699999999],"page":"8"},"7":{"caption":"Figure 7. Retargeted Kanji tutorial and ﬁnal revision of AR visualization. (a) The AR visualization is presented using an Optical See-Through HMD\n(Microsoft Hololens) and a handheld clicker that the user is holding in one hand. (b) The video tutorial is shown on a tablet mounted right above the\ndrawing area. This reduced the inﬂuence of head motion. (c) Our ﬁnal glyph design encodes the direction of the stroke on its border using arrow heads.\nThe system presents one glyph at a time next to a full preview of the ﬁnal drawing. This picture shows the six instructions presented to the user in AR.\n","bbox":[53.666000000000004,578.88527,565.555180000001,613.75527],"page":"9"},"8":{"caption":"Figure 8. Kanji study results. Stars indicate signiﬁcant differences.\n","bbox":[62.183,550.47227,288.7463200000001,558.44227],"page":"10"}},"crops":{"1":{"crop_coord":[144.80278027777777,516.5157827777779,1572.052798888889,940.3527916666667],"bbox":[53.9290009,455.272995,564.1390076,604.2543182],"page":"1"},"2":{"crop_coord":[144.80278027777777,168.22544527777794,1572.1340097222223,623.4666527777779],"bbox":[53.9290009,569.352005,564.1682435,729.6388397],"page":"3"},"3":{"crop_coord":[144.80278027777777,168.23934749999987,1572.052798888889,609.0833366666665],"bbox":[53.9290009,574.5299988,564.1390076,729.6338349],"page":"4"},"4":{"crop_coord":[144.80278027777777,168.23218444444447,1572.052798888889,396.96111888888913],"bbox":[53.9290009,650.8939972,564.1390076,729.6364136],"page":"5"},"5":{"crop_coord":[144.80278027777777,168.23434638888892,1572.052798888889,465.9361013888888],"bbox":[53.9290009,626.0630035,564.1390076,729.6356353],"page":"7"},"6":{"crop_coord":[144.80278027777777,168.23566027777758,1572.052798888889,550.5000136111111],"bbox":[53.9290009,595.6199951,564.1390076,729.6351623],"page":"8"},"7":{"crop_coord":[144.80278027777777,168.2367197222223,1572.052798888889,487.19723166666665],"bbox":[53.9290009,618.4089966,564.1390076,729.6347809],"page":"9"},"8":{"crop_coord":[262.96387555555555,301.57344416666643,711.8378022222223,635.3055658333333],"bbox":[96.4669952,565.0899963,254.46160880000002,681.6335601000001],"page":"10"}}}},{"filename":"ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users.pdf","paper_id":"ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users","venue":"VR_2017","keywords":["Co-located virtual reality","ShareVR","Asymmetric virtual reality","Multi-user virtual reality","Consumer virtual reality"],"doi":"10.1145/3025453.3025683","paragraph_containing_keyword":"Author Keywords\nCo-located virtual reality; shareVR; asymmetric virtual\nreality; multi-user virtual reality; consumer virtual reality","paragraph_after_keyword":"INTRODUCTION\nVirtual Reality (VR) head-mounted displays (HMD) are cur-\nrently getting released as consumer devices (e.g. Oculus Rift,\nHTC Vive, and PlayStation VR) and are becoming part of the\nhome entertainment environment. The technical progress al-\nlows for creating highly immersive virtual environments (IVEs)\nwhere users can even physically walk around and interact using\ntheir hands (roomscale VR) [13]. Having this physical explo-\nration leads to a higher spatial understanding and therefore fur-\nther increases immersion and enjoyment for the HMD user [4].\nDespite VR aiming to become an essential part of the future\nliving room entertainment, most current VR systems focus\nmainly on the HMD user. However, Alladi Venkatesh describes\nthe living room as a highly social environment where people\nexperience content together and interact through technology\n[61]. Since the level of engagement may vary between\nmembers of the household (e.g. some want to watch, some\nwant to have some form of interaction and some want to be\nfully part of the experience), a VR system has to cover a wide\nbandwidth of engagement [63]. Solely observing participants","sections":[{"word_count":829,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":339,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1111,"figure_citations":{},"section_index":2,"title":"ENVISIONED SCENARIO"},{"word_count":786,"figure_citations":{"2":["Figure 2)."]},"section_index":3,"title":"ONLINE SURVEY"},{"word_count":980,"figure_citations":{},"section_index":4,"title":"SHAREVR CONCEPT AND IMPLEMENTATION"},{"word_count":1745,"figure_citations":{"4":["Figure 4)."]},"section_index":5,"title":"IMPLEMENTED EXPERIENCES"},{"word_count":1434,"figure_citations":{"7":["Figure 7 summarizes the collected data of the GEQ and SUS Experiences with Virtual Reality CHI 2017, May 6–11, 2017, Denver, CO, USA Figure 7."],"8":["Figure 8 shows an overview of the final comparison (enjoyment, presence and social interaction)."]},"section_index":6,"title":"USER STUDY"},{"word_count":1338,"figure_citations":{},"section_index":7,"title":"DESIGN SPACE AND GUIDELINES"},{"word_count":269,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":22,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGEMENTS"},{"word_count":2065,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users","authors":"Jan Gugenheimer, Evgeny Stemasov, Julian Frommel, Enrico Rukzio","abstract":"Virtual reality (VR) head-mounted displays (HMD) allow for a highly immersive experience and are currently becoming part of the living room entertainment. Current VR systems focus mainly on increasing the immersion and enjoyment for the user wearing the HMD (HMD user), resulting in all the bystanders (Non-HMD users) being excluded from the experience. We propose ShareVR, a proof-of-concept prototype using floor projection and mobile displays in combination with positional tracking to visualize the virtual world for the Non-HMD user, enabling them to interact with the HMD user and become part of the VR experience. We designed and implemented ShareVR based on the insights of an initial online survey (n=48) with early adopters of VR HMDs. We ran a user study (n=16) comparing ShareVRto a baseline condition showing how the interaction using ShareVR led to an increase of enjoyment, presence and social interaction. In a last step we implemented several experiences for ShareVR, exploring its design space and giving insights for designers of co-located asymmetric VR experiences.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. ShareVR enables co-located asymmetric interaction between users wearing an HMD and users without an HMD. ShareVR uses a tracked\ndisplay (a, e) as a window into the virtual world and a ﬂoor projection to visualize the virtual environment to all Non-HMD users. It enables collaborative\nexperiences such as exploring a dungeon together (b), drawing (h), sports (c) or solving puzzles (e, f) as well as competitive experiences such as “Statues” (d)\nor a swordﬁght (g). ShareVR facilitates a shared physical and virtual space, increasing the presence and enjoyment for both HMD and Non-HMD users.\n","bbox":[53.929,407.1702700000001,564.6944200000006,441.50127],"page":"1"},"2":{"caption":"Figure 2. An excerpt from our online survey on the questions: (a) “When demonstrating my VR headset to friends and family I tend to:”, (b) “Assuming\nthat you own and actively use only one headset, please rate the following statements:”, (c) “A technology which would allow me to actively inﬂuence the\nvirtual environment of the immersed user should...” (Note: the statements are shortened and rephrased to ﬁt into one ﬁgure.)\n","bbox":[53.730000000000004,583.3732699999999,564.1604300000012,608.91727],"page":"4"},"3":{"caption":"Figure 3. Left: Display mounted on the controller of the Non-HMD user.\nRight: Physical setup of ShareVR, replicating a living-room layout\n","bbox":[321.094,581.2602699999999,565.5595099999999,598.0172699999999],"page":"5"},"4":{"caption":"Figure 4. Two users (a: handheld view, b: HMD view) ﬁghting monsters\nin the caves of BeMyLight. Note that the HMD user (b) only can see where\nthe Non-HMD user (a) shines light on.\n","bbox":[53.92899999999999,579.8382699999999,296.9989,605.38227],"page":"6"},"5":{"caption":"Figure 5. Two users playing SneakyBoxes and their individual views:\n(a) handheld (b) inside the HMD. Note that the HMD user (b) can not\ndistinguish between a regular box and the Non-HMD box.\n","bbox":[320.831,579.8382699999999,565.4860800000001,605.38227],"page":"6"},"6":{"caption":"Figure 6. An overview of the individual applications with overlaid visualizations from the Sandbox: (a) throwing a ball to the HMD user in the soccer\napplication, (b) instructing the HMD user in the puzzle application, (c) drawing a palm tree together and (d) having a lightsaber duel.\n","bbox":[53.929,607.97327,564.3357700000013,624.73027],"page":"7"},"7":{"caption":"Figure 7. Averages (with standard deviation) of the positive experiences\nsubscale (GEQ), behavioural involvement (GEQ) and presence (SUS).\n","bbox":[53.929,612.9072699999999,296.9980600000001,629.66427],"page":"8"},"8":{"caption":"Figure 8. Averages (+/- sd) of the ﬁnal questions on enjoyment (“I enjoyed\nusing {System}”), presence (“I felt being in the game using {System}”) and\nsocial interaction (“I felt engagement with the other using {System}”).\n","bbox":[321.094,616.9342699999999,564.16922,642.47827],"page":"8"}},"crops":{"1":{"crop_coord":[144.80278027777777,557.4149747222224,1572.6161025,966.1749861111109],"bbox":[53.9290009,445.977005,564.3417969,589.5306091],"page":"1"},"2":{"crop_coord":[144.80278027777777,168.24100055555567,1572.66332,501.1333380555557],"bbox":[53.9290009,613.3919983,564.3587952,729.6332398],"page":"4"},"3":{"crop_coord":[886.9277613888887,168.2246399999998,1572.1060349999998,531.4110988888887],"bbox":[321.0939941,602.4920044,564.1581726,729.6391296],"page":"5"},"4":{"crop_coord":[144.80278027777777,168.23549055555534,829.9807572222222,510.94999527777753],"bbox":[53.9290009,609.8580017,296.9930726,729.6352234000001],"page":"6"},"5":{"crop_coord":[886.9277613888887,168.23549055555534,1572.1057383333334,510.94999527777753],"bbox":[321.0939941,609.8580017,564.1580658,729.6352234000001],"page":"6"},"6":{"crop_coord":[144.80278027777777,168.24303527777764,1572.4097275,457.20832833333344],"bbox":[53.9290009,629.2050018,564.2675019,729.6325073],"page":"7"},"7":{"crop_coord":[144.80278027777777,168.23031944444452,829.9941933333334,443.5000016666668],"bbox":[53.9290009,634.1399994,296.9979096,729.637085],"page":"8"},"8":{"crop_coord":[886.9277613888887,168.23311694444433,1572.0804341666667,407.90832527777786],"bbox":[321.0939941,646.9530029,564.1489563,729.6360779],"page":"8"}}}},{"filename":"TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction.pdf","paper_id":"TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction","venue":"chi2019xr","keywords":["Haptics","VR object manipulation","Haptic texture","Haptic compliance"],"paragraph_containing_keyword":"KEYWORDS\nHaptics; VR object manipulation; Haptic texture; Haptic com-\npliance\nACM Reference Format:\nJaeyeon Lee1,2, Mike Sinclair2, Mar Gonzalez-Franco2, Eyal Ofek2,\nand Christian Holz2. 2019. TORC: A Virtual Reality Controller for\nIn-Hand High-Dexterity Finger Interaction. In CHI Conference on\nHuman Factors in Computing Systems Proceedings (CHI 2019), May 4–\n9, 2019, Glasgow, Scotland UK. ACM, New York, NY, USA, 13 pages.\nhttps://doi.org/10.1145/3290605.3300301","doi":"10.1145/3290605.3300301","paragraph_after_keyword":"Permission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nCHI 2019, May 4–9, 2019, Glasgow, Scotland UK\n© 2019 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 978-1-4503-5970-2/19/05. . . $15.00\nhttps://doi.org/10.1145/3290605.3300301","sections":[{"word_count":706,"figure_citations":{"1":["Figure 1), a novel hand-held haptic controller for VR that has a rigid shape and no moving parts, making it a suitable candidate for reliable mass manufacturing."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1103,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1995,"figure_citations":{"2":["Figure 2, we created a controller form factor that allows the user to move the thumb freely in a plane on a pad, parallel to the rests for the two fingers.","Figure 2a, we designed the device to be held with thumb and two fingers of the right hand with the remaining fingers gripping the handle.","Figure 2b), we mounted force sensors (Honey-well FSS1500) and voice coil actuators (VCA, Dayton Audio DAEX9-4SM)."],"3":["Figure 3 depicts the schematic of the desk-fixed prototype."],"5":["Figure 5)."],"6":["Figure 6 shows the distribution of scores for each combination of locations of VCAs."],"7":["Figure 7), which has a considerably lighter form factor."],"8":["Figure 8)."]},"section_index":2,"title":"TORC CONTROLLER"},{"word_count":1069,"figure_citations":{"9":["Figure 9 (left)).","Figure 9 (middle & right))."],"10":["Figure 10)."],"11":["Figure 11)."],"12":["Figure 12)."]},"section_index":3,"title":"TORC INTERACTION SCENARIOS"},{"word_count":1228,"figure_citations":{"13":["Figure 13): Locatinд (Figure 13a–c) and Rotatinд (Figure 13d–f).","Figure 13a).","Figure 13b).","Figure 13c), she presses the trigger in the left hand.","Figure 13d).","Figure 13e shows the angular difference of the key and the key hole.","Figure 13f).","Figure 13a–f)."],"14":["Figure 14), we found that people preferred TORC over VIVE (Q26)."],"15":["Figure 15 (middle) shows the distance error from VIVE and TORC in the Locatinд task.","Figure 15 (right) shows the angular error (calculated as intrinsic geodesic distance between q0 and q1.","Figure 15 (left))."]},"section_index":4,"title":"USER STUDY"},{"word_count":1008,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":115,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":11,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGMENTS"},{"word_count":2047,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"TORC: A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction","authors":"Jaeyeon Lee, Mike Sinclair, Mar Gonzalez-Franco, Eyal Ofek, Christian Holz","abstract":"Recent hand-held controllers have explored a variety of haptic feedback sensations for users in virtual reality by producing both kinesthetic and cutaneous feedback from virtual objects. These controllers are grounded to the user's hand and can only manipulate objects through arm and wrist motions, not using the dexterity of their fingers as they would in real life. In this paper, we present TORC, a rigid haptic controller that renders virtual object characteristics and behaviors such as texture and compliance. Users hold and squeeze TORC using their thumb and two fingers and interact with virtual objects by sliding their thumb on TORC's trackpad. During the interaction, vibrotactile motors produce sensations to each finger that represent the haptic feel of squeezing, shearing or turning an object. Our evaluation showed that using TORC, participants could manipulate virtual objects more precisely (e.g., position and rotate objects in 3D) than when using a conventional VR controller.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: TORC interaction – real vs VR animation rendering.\n","bbox":[181.412,437.352854,430.577193796,446.318854],"page":"1"},"2":{"caption":"Figure 2: (a) Desk-fixed prototype used in Experiments 1 and 2 (b) diagram of end view, left side view, and right side view.\n","bbox":[60.931,560.228854,551.0484329659995,569.194854],"page":"4"},"3":{"caption":"Figure 3: Schematic diagram of desk-fixed prototype.\n","bbox":[66.926,442.411854,280.908520136,451.377854],"page":"4"},"4":{"caption":"Figure 4: Measured acceleration on index finger rest with vi-\nbration from the VCA under the index finger rest. (a) with-\nout fingers touching (b) with fingers touching the finger\nrests.\n","bbox":[317.955,385.942854,559.7949359319999,427.78585400000003],"page":"4"},"5":{"caption":"Figure 5: Experiment setup. The participant compared two\nrendered sensations A and B from our device (right hand)\nto the sensation of an analog object, a\n5.08 cm silicone\nball made of Eco-flex 00-30 (durometer 27.4 (Shore hardness,\nscale: OO)) (left hand).\n","bbox":[317.955,506.41585400000014,559.2838649660001,559.217854],"page":"5"},"6":{"caption":"Figure 6: Boxplot representing distribution of the psy-\nchophysical preference score between different combina-\ntions of the location of VCAs.\n","bbox":[317.955,302.24985399999997,559.794953864,333.133854],"page":"5"},"7":{"caption":"Figure 7: Final TORC controller. (a) photos (b) diagram of end view and right side view.\n","bbox":[130.622,558.849854,481.3629988299998,567.8158540000001],"page":"6"},"8":{"caption":"Figure 8: Schematic diagram of TORC controller.\n","bbox":[339.565,435.210854,536.58392883,444.176854],"page":"6"},"9":{"caption":"Figure 9: Fingers are holding the object, with joint position\nestimated by inverse kinematics (left). Applying a little pres-\nsure outward by opening the index & middle fingers releases\nthe grabbed object (middle & right).\n","bbox":[317.955,459.59385399999996,559.794998694,501.436854],"page":"7"},"10":{"caption":"Figure 10: Context-based release. When holding the hand\nup, the thumb may be lifted without releasing the held ob-\nject (left). In contrast, doing the same while the hand points\ndown will release and drop the object (right).\n","bbox":[317.955,211.675854,559.794980762,253.518854],"page":"7"},"11":{"caption":"Figure 11: Applying force on the controller allows users to\nsqueeze virtual objects. A simulated haptic sensation ren-\ndered on TORC completes the visual simulation.\n","bbox":[53.798,578.8028540000001,295.63784627200005,609.6858540000001],"page":"8"},"12":{"caption":"Figure 12: TORC allows precise manipulation of a held ob-\njects rotation by sensing finger motion (left). Rotation is\naround the center of rotation between the fingers and it is\ncontrolled by the thumb motion. The index and middle fin-\ngers rotate in the opposite direction (middle & right).\n","bbox":[53.798,113.56685399999998,295.63786420400004,166.368854],"page":"8"},"13":{"caption":"Figure 13: Two-level docking task. Participants had to match\nthe key they were holding with the controller to a target\nkey hole. The first docking task (a, b, c) only aimed at posi-\ntional docking (Locatinд), therefore the key hole and the key\ndid not display any superficial pattern. In the second dock-\ning task (d, e, f), the key hole displayed a particular pattern\nthat participants were asked to match. In essence that meant\nthey needed to rotate their hand and key (Rotatinд).\n","bbox":[53.798,484.5789900000002,295.637980762,570.293854],"page":"9"},"14":{"caption":"Figure 14: Questionnaire Responses. (top) Boxplots of the\nscores, only showing questions with significant difference\nbetween conditions. (bottom) Histogram of preference be-\ntween two methods (response to Q26).\n","bbox":[53.798,434.171854,295.6379359320001,476.01385400000004],"page":"10"},"15":{"caption":"Figure 15: Task Performance. Boxplots of the task comple-\ntion time, distance error and angular error during the differ-\nent parts of the experiment (Locatinд task and Rotatinд task),\nfor the two controllers: TORC and VIVE.\n","bbox":[53.798,184.324854,295.637989728,226.167854],"page":"10"}},"crops":{"1":{"crop_coord":[144.4388888888889,595.8055555555553,1526.661111111111,929.8611111111111],"bbox":[53.798,459.05,547.798,575.71],"page":"1"},"2":{"crop_coord":[140.2722338888889,-1802.1976616666664,848.572233888889,225.24400500000002],"bbox":[52.2980042,712.7121582,303.6860042,1438.9911582],"page":"2"},"3":{"crop_coord":[1439.7258844444443,-1802.1976616666664,3001.68144,225.24400500000002],"bbox":[520.1013184,712.7121582,1078.8053183999998,1438.9911582],"page":"2"},"4":{"crop_coord":[139.52501944444444,-1782.9863877777782,1701.4805749999998,251.45250111111096],"bbox":[52.029007,703.2770996,610.7330069999999,1432.0750996000002],"page":"3"},"5":{"crop_coord":[211.13333333333335,252.37222222222232,1488.8555555555556,588.5388888888889],"bbox":[77.808,581.926,534.188,699.346],"page":"4"},"6":{"crop_coord":[162.74722222222223,683.6694444444447,803.5666666666666,915.8083333333334],"bbox":[60.389,464.109,287.484,544.079],"page":"4"},"7":{"crop_coord":[878.2083333333334,683.7027777777778,1599.652777777778,981.3416666666667],"bbox":[317.955,440.517,574.075,544.067],"page":"4"},"8":{"crop_coord":[140.2722338888889,-1802.1976616666664,848.572233888889,225.24400500000002],"bbox":[52.2980042,712.7121582,303.6860042,1438.9911582],"page":"4"},"9":{"crop_coord":[1439.7258844444443,-1802.1976616666664,3001.68144,225.24400500000002],"bbox":[520.1013184,712.7121582,1078.8053183999998,1438.9911582],"page":"4"},"10":{"crop_coord":[894,252.37277777777803,1539.76,616.2527777777779],"bbox":[323.64,571.949,552.5136,699.3457999999999],"page":"5"},"11":{"crop_coord":[944.5250000000001,835.6527777777778,1489.247222222222,1244.263888888889],"bbox":[341.829,345.865,534.329,489.365],"page":"5"},"12":{"crop_coord":[139.52501944444444,-1782.9863877777782,1701.4805749999998,251.45250111111096],"bbox":[52.029007,703.2770996,610.7330069999999,1432.0750996000002],"page":"5"},"13":{"crop_coord":[272.0805555555555,252.36944444444447,1427.913888888889,592.3694444444443],"bbox":[99.749,580.547,512.249,699.347],"page":"6"},"14":{"crop_coord":[911.7333333333335,687.5444444444446,1522,935.8111111111111],"bbox":[330.024,456.908,546.12,542.684],"page":"6"},"15":{"crop_coord":[140.2722338888889,-1802.1976616666664,848.572233888889,225.24400500000002],"bbox":[52.2980042,712.7121582,303.6860042,1438.9911582],"page":"6"},"16":{"crop_coord":[1439.7258844444443,-1802.1976616666664,3001.68144,225.24400500000002],"bbox":[520.1013184,712.7121582,1078.8053183999998,1438.9911582],"page":"6"},"17":{"crop_coord":[878.2083333333334,555.6444444444445,1557.6527777777776,776.7555555555555],"bbox":[317.955,514.168,558.9549999999999,590.168],"page":"7"},"18":{"crop_coord":[945.4361111111111,1227.416666666667,1488.3249999999998,1465.4166666666667],"bbox":[342.157,266.25,533.997,348.33],"page":"7"},"19":{"crop_coord":[139.52501944444444,-1782.9863877777782,1701.4805749999998,251.45250111111096],"bbox":[52.029007,703.2770996,610.7330069999999,1432.0750996000002],"page":"7"},"20":{"crop_coord":[311.7361111111111,252.36777777777758,654.4961111111111,476.0611111111111],"bbox":[114.025,622.418,233.8186,699.3476],"page":"8"},"21":{"crop_coord":[144.4388888888889,1499.5,842.5500000000001,1707.5000000000002],"bbox":[53.798,179.1,301.51800000000003,250.38],"page":"8"},"22":{"crop_coord":[140.2722338888889,-1802.1976616666664,1702.2277894444444,225.24400500000002],"bbox":[52.2980042,712.7121582,611.0020042,1438.9911582],"page":"8"},"23":{"crop_coord":[154.06388888888887,252.36611111111114,812.1705555555554,585.4861111111112],"bbox":[57.263,583.025,290.5814,699.3482],"page":"9"},"24":{"crop_coord":[139.52501944444444,-1782.9863877777782,1701.4805749999998,251.45250111111096],"bbox":[52.029007,703.2770996,610.7330069999999,1432.0750996000002],"page":"9"},"25":{"crop_coord":[170.61388888888888,252.37222222222232,795.6138888888888,847.3722222222223],"bbox":[63.221,488.746,284.621,699.346],"page":"10"},"26":{"crop_coord":[144.4388888888889,1039.7250000000001,855.2722222222222,1541.3916666666669],"bbox":[53.798,238.899,306.098,415.899],"page":"10"},"27":{"crop_coord":[140.2722338888889,-1802.1976616666664,1702.2277894444444,225.24400500000002],"bbox":[52.2980042,712.7121582,611.0020042,1438.9911582],"page":"10"},"28":{"crop_coord":[139.52501944444444,-1782.9863877777782,1701.4805749999998,251.45250111111096],"bbox":[52.029007,703.2770996,610.7330069999999,1432.0750996000002],"page":"11"},"29":{"crop_coord":[140.2722338888889,-1802.1976616666664,1702.2277894444444,225.24400500000002],"bbox":[52.2980042,712.7121582,611.0020042,1438.9911582],"page":"12"},"30":{"crop_coord":[140.96667833333333,-1782.9891655555555,1702.2277894444444,250.7580566666665],"bbox":[52.5480042,703.5270996,611.0020042,1432.0760996],"page":"13"},"31":{"crop_coord":[139.52501944444444,-1816.197207222222,314.55001944444444,218.2444594444445],"bbox":[52.029007,715.2319946,111.438007,1444.0309946],"page":"13"}}}},{"filename":"TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality.pdf","paper_id":"TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality","venue":"chi2019xr","keywords":["Interaction techniques","Virtual reality","Touch interaction"],"paragraph_containing_keyword":"KEYWORDS\ninteraction techniques, virtual reality, touch interaction\nACM Reference Format:\nHemant Bhaskar Surale, Aakar Gupta, Mark Hancock, and Daniel\nVogel. 2019. TabletInVR: Exploring the Design Space for Using a\nMulti-Touch Tablet in Virtual Reality. In CHI Conference on Human\nFactors in Computing Systems Proceedings (CHI 2019), May 4–9, 2019,\nGlasgow, Scotland UK. ACM, New York, NY, USA, 13 pages. https:\n//doi.org/10.1145/3290605.3300243\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request\npermissions from permissions@acm.org.\nCHI 2019, May 4–9, 2019, Glasgow, Scotland UK\n© 2019 Association for Computing Machinery.\nACM ISBN 978-1-4503-5970-2/19/05.\nhttps://doi.org/10.1145/3290605.3300243","doi":"10.1145/3290605.3300243","paragraph_after_keyword":"1 INTRODUCTION\nWhile virtual reality (VR) has been around in various forms\nsince at least the 1960s (e.g., [54]), advances in display tech-\nnology have sparked a new interest from both researchers\nand the public. There are clear advantages to virtual reality,\nlike the ability to look and move around in an immersive 3D\nenvironment. Yet, VR interaction is challenging due to lim-\nited tactile feedback, poor input precision when drawing [2],\nand lack of a consistent interaction vocabulary. Past research\nhas introduced methods for haptic feedback [5, 16, 37, 52, 60],\ntechniques to increase precision [43], and more standardized\ncontrol schemes [49]. In our work, we leverage the familiarity\nand ubiquity of multi-touch tablets as a means of interacting\nwith 3D content in a VR world.\nWe introduce a “TabletInVR” design space combining a\n3D-tracked tablet with mid-air barehand gestures, which we\ndemonstrate in an example interaction vocabulary for 3D\nmodelling.\nExploring VR interaction in the context of 3D modelling is\nparticularly compelling because the task should be a good fit\nfor VR, but in practice, supporting the many required opera-\ntions is challenging (e.g. object creation, selection, transfor-\nmation; world navigation; copy, paste, undo, etc.). Although\npast research has considered the use of 2D surfaces in VR, this\nhas focused on 3D-tracked props without real multi-touch in-\nput [34, 35, 44], or using multi-touch tablets for transforming\n3D objects without exploiting 3D tablet tracking [15, 46].\nOur work combines the affordances of a 3D-tracked tablet\nwith the input capabilities of its multi-touch surface. We ad-\nvocate that the tablet’s precise touch input capability, physi-\ncal shape, metaphorical associations, and natural compatibil-\nity with barehand, mid-air input can be effectively used in\nVR. Interactions involving precise mutli-touch input could\nbegin on the tablet followed by coarse hand gestures in VR,\nor tablet input could be used to transform objects or navi-\ngate the world in a familiar mutli-touch way. This suggests\ninteresting aspects when combining these two modalities.\nInteractions can leverage physical qualities like the 2D tablet\ninput providing a continuous tactile sensation and a mid-air","sections":[{"word_count":501,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":723,"figure_citations":{},"section_index":1,"title":"BACKGROUND AND RELATED WORK"},{"word_count":1046,"figure_citations":{},"section_index":2,"title":"FORMATIVE STUDY"},{"word_count":1273,"figure_citations":{},"section_index":3,"title":"DESIGN SPACE"},{"word_count":2384,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2)."],"3":["Figure 3, described in TapSense [25] and in the modeswitching study by Surale et al.","Figure 3 (b-c))."],"4":["Figure 4 (a)).","Figure 4(b-c)) follow selection, and can be performed simultaneously."],"5":["Figure 5) (O1, O3, O5, O6, D9).","Figure 5)."],"6":["Figure 6 (a)).","Figure 6 (b)), and while navigating, the scene quickly fades to black, except for the tablet and viewport.","Figure 6 (c)) and query into the mic (O4)."]},"section_index":4,"title":"EXAMPLE INTERACTION VOCABULARY"},{"word_count":1555,"figure_citations":{"7":["Figure 7 (a).","Figure 7 (b-g)).","Figure 7 (h-k))."]},"section_index":5,"title":"USER EVALUATION"},{"word_count":154,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":31,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGEMENTS"},{"word_count":2070,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"TabletInVR: Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality","authors":"Hemant Bhaskar Surale, Aakar Gupta, Mark Hancock, Daniel Vogel","abstract":"Complex virtual reality (VR) tasks, like 3D solid modelling, are challenging with standard input controllers. We propose exploiting the affordances and input capabilities when using a 3D-tracked multi-touch tablet in an immersive VR environment. Observations gained during semi-structured interviews with general users, and those experienced with 3D software, are used to define a set of design dimensions and guidelines. These are used to develop a vocabulary of interaction techniques to demonstrate how a tablet's precise touch input capability, physical shape, metaphorical associations, and natural compatibility with barehand mid-air input can be used in VR. For example, transforming objects with touch input, \"cutting\" objects by using the tablet as a physical \"knife\", navigating in 3D by using the tablet as a viewport, and triggering commands by interleaving bare-hand input around the tablet. Key aspects of the vocabulary are evaluated with users, with results validating the approach.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Creation. (a) Flip the tablet, select the object for\ncreation, (b) Tap on tablet viewport to create.\n","bbox":[317.955,86.762854,558.37829,106.687854],"page":"6"},"2":{"caption":"Figure 2: Object selection. (a) Tap on tablet viewport, (b)\nPierce tablet corner in the object and pinch, (c) Tap on the\nface of the object.\n","bbox":[53.798,577.8998540000001,294.59786199999996,608.783854],"page":"7"},"3":{"caption":"Figure 3: (a) Knuckle for multiple object selection, (b) Pinch\nto deselect, (c) Tap on viewport to deselect.\n","bbox":[53.798,81.102854,294.03300400000006,101.02785399999999],"page":"7"},"4":{"caption":"Figure 4: (a) Swipe-in to delete the object, (b) Two finger scale,\n(c) Two finger drag to translate.\n","bbox":[317.67699999999996,262.710854,559.283856,282.635854],"page":"7"},"5":{"caption":"Figure 5: Select axis of transformation using the orientation\nof the tablet. (a) Facing up to select the x-axis, (c) Portrait\nvertical to select the y-axis, and (c) Landscape left to select\nthe z-axis.\n","bbox":[53.565,201.61899,294.04484,243.496854],"page":"8"},"6":{"caption":"Figure 6: (a) Slicing an object, (b) Five-finger touch to navi-\ngate, (c) Speak to tablet and ask for help.\n","bbox":[53.798,85.006854,295.637918,104.931854],"page":"8"},"7":{"caption":"Figure 7: Sample results from ‘replication’ and ‘freeform exploration’ task. (a) Target Model, (b-g) Participant’s replication\n(P1-P6), (h-k) Participant’s creations in ‘freeform exploration’ (P1-P4)\n","bbox":[53.52,477.199956,558.1803299999999,497.151854],"page":"9"}},"crops":{"1":{"crop_coord":[878.2083383333334,1620.3378975,1555.582800833333,1898.1861030555556],"bbox":[317.9550018,110.4530029,558.2098083,206.8783569],"page":"6"},"2":{"crop_coord":[144.43890055555556,252.3660108333333,821.7991638888888,503.47501111111114],"bbox":[53.7980042,612.548996,294.04769899999997,699.3482361],"page":"7"},"3":{"crop_coord":[144.43890055555556,1665.079718611111,821.8039958333334,1913.9083352777777],"bbox":[53.7980042,104.7929993,294.0494385,190.7713013],"page":"7"},"4":{"crop_coord":[878.2083383333334,1172.286783888889,1555.5640241666665,1409.441663888889],"bbox":[317.9550018,286.401001,558.2030487,368.1767578],"page":"7"},"5":{"crop_coord":[144.43890055555556,1257.9277547222223,821.7946286111112,1518.1611294444444],"bbox":[53.7980042,247.2619934,294.0460663,337.3460083],"page":"8"},"6":{"crop_coord":[144.43890055555556,1663.1318919444443,821.809421388889,1903.0638716666665],"bbox":[53.7980042,108.6970062,294.0513917,191.4725189],"page":"8"},"7":{"crop_coord":[144.43890055555556,252.36582386870828,1555.5772761366222,813.5638683333334],"bbox":[53.7980042,500.9170074,558.207819409184,699.348303407265],"page":"9"}}}},{"filename":"Teaching Language and Culture with a Virtual Reality Game","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/Teaching Language and Culture with a Virtual Reality Game.pdf","paper_id":"Teaching Language and Culture with a Virtual Reality Game","venue":"VR_2017","keywords":["Language learning","Video games","Virtual reality"],"paragraph_containing_keyword":"Author Keywords\nlanguage learning, video games, virtual reality","paragraph_after_keyword":"ACM Classiﬁcation Keywords\nH.5.0. Information Interfaces and Presentation: General","doi":"10.1145/3025453.3025857","sections":[{"word_count":554,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":942,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":576,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2)."]},"section_index":2,"title":"CRYSTALLIZE IN VIRTUAL REALITY"},{"word_count":1164,"figure_citations":{"3":["Figure 3), and (2) conversation dialogue, in which the player engages a single NPC in conversation."],"4":["Figure 4) and, in some cases, asked to construct a sentence given several vocabulary words (Figure 5)."]},"section_index":3,"title":"TEACHING PLAYERS TO BOW"},{"word_count":571,"figure_citations":{},"section_index":4,"title":"FORMATIVE USER STUDY"},{"word_count":1399,"figure_citations":{},"section_index":5,"title":"RESULTS"},{"word_count":274,"figure_citations":{},"section_index":6,"title":"CONCLUSIONS"},{"word_count":1018,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"Teaching Language and Culture with a Virtual Reality Game","authors":"Alan Cheng, Lei Yang, Erik Andersen","abstract":"Many people want to learn a language but find it difficult to stay engaged. Ideally, we would have language learning tools that can make language learning more enjoyable by simulating immersion in a foreign language environment. Therefore, we adapted Crystallize, a 3D video game for learning Japanese, so that it can be played in virtual reality with the Oculus Rift. Specifically, we explored whether we could leverage virtual reality technology to teach embodied cultural interaction, such as bowing in Japanese greetings. To evaluate the impact of our virtual reality game designs, we conducted a formative user study with 68 participants. We present results showing that the virtual reality design trained players how and when to bow, and that it increased participants' sense of involvement in Japanese culture. Our results suggest that virtual reality technology provides an opportunity to leverage culturally-relevant physical interaction, which can enhance the design of language learning technology and virtual reality games.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Overhead view of the Crystallize virtual reality scenario that we set up to test the ability of virtual reality to communicate cultural and\nlanguage information. Players navigated through a Japanese teahouse and solved language learning questions by interacting with non-player-controlled\ncharacters.\n","bbox":[53.929,467.88427,564.1604300000012,493.78727],"page":"3"},"2":{"caption":"Figure 2. Using Unity, we made Crystallize work with the Oculus Rift.\n","bbox":[57.556,265.06227,293.37246999999996,273.03227],"page":"3"},"3":{"caption":"Figure 3. An example of an eavesdropping dialogue. The player can\nlisten in on NPC dialogues and collect new words to add to the inventory.\n","bbox":[321.094,285.53526999999997,565.5578100000001,302.47126999999995],"page":"3"},"4":{"caption":"Figure 4. An example of a multiple-choice conversation dialogue in VR.\nThe player must select an appropriate response based on what the NPC\nsaid.\n","bbox":[53.666000000000004,553.60727,298.3928100000002,579.50927],"page":"4"},"5":{"caption":"Figure 5. An example of a sentence-construction conversation dialogue\nin VR. The player must arrange vocabulary words in the correct order\nin order to respond to the NPC.\n","bbox":[53.929,363.21827,297.1734,389.12127],"page":"4"},"7":{"caption":"Figure 7. Using the Oculus Rift head tracking, we trained players to bow when greeting other characters in Japanese. A few seconds after the NPC\nbows, the player is given a prompt to bow. The third panel shows the player’s ﬁeld of view mid-bow.\n","bbox":[53.929,417.90327,564.1604300000009,434.84027],"page":"5"},"8":{"caption":"Figure 8. Analysis of survey questions. Response options used a Likert scale, spanning from 1 (None/not at all) to 5 (Very much/a great deal).\n","bbox":[70.536,595.1582699999999,547.5564400000011,603.1282699999999],"page":"7"}},"crops":{"1":{"crop_coord":[144.80278027777777,168.23387999999983,1572.0827655555554,820.4388852777778],"bbox":[53.9290009,498.4420013,564.1497956,729.6358032],"page":"3"},"2":{"crop_coord":[163.50556277777778,940.771111388889,811.2848411111112,1428.1111313888887],"bbox":[60.6620026,279.6799927,290.2625428,451.5223999],"page":"3"},"3":{"crop_coord":[905.630543888889,940.7686530555555,1553.407321388889,1351.8722025000002],"bbox":[327.8269958,307.1260071,557.4266357,451.5232849],"page":"3"},"4":{"crop_coord":[163.50556277777778,168.22459750000002,811.29815,582.3222180555555],"bbox":[60.6620026,584.1640015,290.267334,729.6391449],"page":"4"},"5":{"crop_coord":[163.50556277777778,696.1949749999999,811.29815,1111.177775],"bbox":[60.6620026,393.776001,290.267334,539.5698090000001],"page":"4"},"6":{"crop_coord":[144.80278027777777,573.7547216666667,1572.0402952777777,984.1805691666667],"bbox":[53.9290009,439.4949951,564.1345063,583.6483002],"page":"5"}}}},{"filename":"The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality.pdf","paper_id":"The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality","venue":"VR_2017","keywords":["Virtual Reality","Narrative","Theatre","Performance","Cinematic VR","Interpersonal Space","Head-Mounted Display","Workﬂow","360-degree video"],"paragraph_containing_keyword":"ABSTRACT\nUPDATED—January 23, 2017. 360-degree ﬁlming and head-\nmounted displays (HMDs) give recorded media a new sense of\nspace. Theatre practitioners’ expertise in manipulating spatial\ninteractions has much to contribute to immersive recorded\ncontent. Four theatre directors led teams of three actors to\nstage the same scene for both immersive theatre and for 360-\ndegree ﬁlming. Each team was recorded performing the scene\nat least six times, three in each condition, to extract actors’\ncoordinates. This study establishes how to quantify theatre\npractitioners’ use of spatial interactions and examines the\nspatial adaptations made when transferring these relationships\nto 360-degree ﬁlming.\nStaging for a 360-degree camera compared to staging for\nan audience member had shorter distances from the camera\nand between performers, along with fewer instances of the\ncamera being in the middle of the action. Across all groups,\ninterpersonal distance between characters and between the\naudience/camera dropped at the end of the scene when the\ncharacters come together as a team, suggesting that elements\nof Proxemics may be applicable to narrative performance.\nACM Classiﬁcation Keywords\nJ.5 Computer Applications: Arts and Humanities, Performing\narts; H.5.1 Information Systems: Multimedia Information\nSystems\nAuthor Keywords\nVirtual Reality; Narrative; Theatre; Performance; Cinematic\nVR; Interpersonal Space; Head-Mounted Display; Workﬂow;\n360-degree video\nINTRODUCTION\nThe ﬁrst head-mounted display (HMD), nicknamed ‘Sword\nof Damocles’ because of the bulky and alarming apparatus\nprecariously perched above the viewer, was developed in 1968","paragraph_after_keyword":"Permission  to  make  digital  or  hard  copies  of  part  or  all  of  this  work  for  personal  or \nclassroom use is granted without fee provided that copies are not made or distributed \nfor proﬁt or commercial advantage and that copies bear this notice and the full citation \non the first page. Copyrights for third-party components of this work must be honored. \nFor all other uses, contact the Owner/Author. \nCopyright is held by the owner/author(s). \nCHI 2017, May 06-11, 2017, Denver, CO, USA\nACM 978-1-4503-4655-9/17/05.\nhttp://dx.doi.org/10.1145/3025453.3025581","doi":"10.1145/3025453.3025581","sections":[{"word_count":678,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1604,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":2080,"figure_citations":{},"section_index":2,"title":"METHODOLOGY"},{"word_count":2806,"figure_citations":{},"section_index":3,"title":"RESULTS"},{"word_count":655,"figure_citations":{},"section_index":4,"title":"LIMITATIONS"},{"word_count":310,"figure_citations":{},"section_index":5,"title":"FUTURE DIRECTIONS"},{"word_count":283,"figure_citations":{},"section_index":6,"title":"DISCUSSION"},{"word_count":450,"figure_citations":{},"section_index":7,"title":"CONCLUSION"},{"word_count":203,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGMENTS"},{"word_count":399,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"The Geometry of Storytelling: Theatrical Use of Space for 360-degree Videos and Virtual Reality","authors":"Vanessa C. Pope, Robert Dawes, Florian Schweiger, Alia Sheikh","abstract":"360-degree filming and head-mounted displays (HMDs) give recorded media a new sense of space. Theatre practitioners' expertise in manipulating spatial interactions has much to contribute to immersive recorded content. Four theatre directors led teams of three actors to stage the same scene for both immersive theatre and for 360-degree filming. Each team was recorded performing the scene at least six times, three in each condition, to extract actors' coordinates. This study establishes how to quantify theatre practitioners' use of spatial interactions and examines the spatial adaptations made when transferring these relationships to 360-degree filming.Staging for a 360-degree camera compared to staging for an audience member had shorter distances from the camera and between performers, along with fewer instances of the camera being in the middle of the action. Across all groups, interpersonal distance between characters and between the audience/camera dropped at the end of the scene when the characters come together as a team, suggesting that elements of Proxemics may be applicable to narrative performance.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Diagrams of F-formations (left) and Proxemics (right), based\non [6, 11]\n","bbox":[321.094,588.79927,564.1630600000002,605.73527],"page":"2"},"2":{"caption":"Figure 2. Diagram showing the placement of recording equipment\n","bbox":[63.282,590.85127,287.6454700000001,598.82127],"page":"5"},"3":{"caption":"Figure 3. Distance between Red and Blue characters compared to the\naverage distance between Green/Blue and Green/Red dyads\n","bbox":[321.094,457.49727,564.1630600000001,474.43426999999997],"page":"6"},"4":{"caption":"Figure 4. Mean interpersonal distance between dyads for each group\nover time\n","bbox":[53.929,528.8412699999999,296.9980600000001,545.7772699999999],"page":"7"},"5":{"caption":"Figure 5. Distance between Red character and centre compared to the\naverage distance from centre of Blue and Green characters\n","bbox":[53.929,329.17127,296.99806,346.10826999999995],"page":"7"},"6":{"caption":"Figure 6. Mean ranks of distance from centre in theatre and 360-degree\n","bbox":[54.061,581.2382699999999,296.8670500000001,589.20827],"page":"8"},"7":{"caption":"Figure 7. Mean ranks of angles in staging for theatre and 360-degree\n","bbox":[59.098,421.86527,291.8299700000002,429.83527],"page":"8"},"8":{"caption":"Figure 8. Normalized instances of audience being inside or outside the\nactors’ F-formations\n","bbox":[321.094,293.25727,564.1630600000001,310.19426999999996],"page":"8"},"9":{"caption":"Figure 9. Group 4’s ‘huddle’ in theatre (left) and 360-degree (right)\n","bbox":[61.102,498.81627000000003,289.8250600000002,506.78627],"page":"9"}},"crops":{"1":{"crop_coord":[886.9277613888887,212.30957027777777,1227.7764127777778,509.4722238888889],"bbox":[321.0939941,610.3899994,440.19950859999994,713.7685547],"page":"2"},"2":{"crop_coord":[1224.5277913888888,168.22968361111126,1565.373179166667,509.4722238888889],"bbox":[442.6300049,610.3899994,561.7343445,729.6373139],"page":"2"},"3":{"crop_coord":[144.80278027777777,168.2311247222224,829.9919469444444,492.7027722222222],"bbox":[53.9290009,616.427002,296.99710089999996,729.6367951],"page":"5"},"4":{"crop_coord":[886.9277613888887,393.90530888888907,1572.0888263888887,843.7555438888888],"bbox":[321.0939941,490.0480042,564.1519774999999,648.3940888],"page":"6"},"5":{"crop_coord":[144.80278027777777,267.35836444444453,829.9831305555555,645.5805374999999],"bbox":[53.9290009,561.3910065,296.993927,693.9509888],"page":"7"},"6":{"crop_coord":[144.80278027777777,775.0941469444443,829.977875,1200.2194638888889],"bbox":[53.9290009,361.720993,296.992035,511.16610710000003],"page":"7"},"7":{"crop_coord":[225.74442555555555,168.22057083333323,742.1551091666666,549.8472341666668],"bbox":[83.0679932,595.8549957,265.3758393,729.6405945],"page":"8"},"8":{"crop_coord":[225.74442555555555,616.3376363888889,742.1258205555555,992.550015],"bbox":[83.0679932,436.4819946,265.3652954,568.3184509],"page":"8"},"9":{"crop_coord":[886.9277613888887,980.410393611111,1572.0840791666667,1299.9777813888888],"bbox":[321.0939941,325.8079987,564.1502685,437.2522583],"page":"8"},"10":{"crop_coord":[171.64722861111113,603.5269502777778,485.46323972222217,778.7972258333333],"bbox":[63.5930023,513.4329987,172.9667663,572.9302979],"page":"9"},"11":{"crop_coord":[482.40001249999995,602.0771533333334,796.2573919444444,778.7972258333333],"bbox":[175.4640045,513.4329987,284.8526611,573.4522248],"page":"9"}}}},{"filename":"The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World ","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World .pdf","paper_id":"The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World ","venue":"VR_2017","keywords":["Mixed Reality","Augmented Reality","Embodied interaction","Embodied cognition","Meaning making","Window-on-theWorld","World-as-Support"],"paragraph_containing_keyword":"ABSTRACT \nCurrent  technical  capabilities  of  mobile  technologies  are \nconsolidating  the  interest  in  developing  context-aware \nAugmented/Mixed  Reality  applications.  Most  of  these \napplications  are  designed  based  on  the  Window-on-the-\nWorld (WoW) interaction paradigm. A significant decrease \nin cost of projection technology and advances in pico-sized \nprojectors  have \nspurred  applications  of  Projective \nAugmented  Reality.  This  research  has  focused  mainly  on \ntechnological development. However, there is still a need to \nits  communicational  and  expressive \nfully  understand \npotential. Hence, we define a conceptual paradigm that we \ncall  World-as-Support  (WaS).  We  compare  the  WaS  and \nWoW  paradigms  by  contrasting  their  assumptions  and \ncultural values, as well as through a study of an application \naimed at supporting the collaborative improvisation of site-\nspecific  narratives  by  children.  Our  analysis  of  children’s \nunderstanding  of  the  physical  and  social  environment  and \nof  their  imaginative  play  allowed  us  to  identify  the \ntwo \naffordances,  strengths  and  weaknesses  of \nparadigms. \nAuthor Keywords \nMixed Reality; Augmented Reality; embodied interaction; \nembodied cognition; meaning making; Window-on-the-\nWorld; World-as-Support. \n \nACM Classification Keywords \nH.5.1.  Information  interfaces  and  presentation  (e.g.,  HCI): \nMultimedia Information Systems \nINTRODUCTION \nThe rich opportunities offered by the blending of the digital \nand the material worlds have become a mainstream research \nfield  in  HCI.  From  a  broad  perspective,  this  hybrid  space \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies \nbear this notice and the full citation on the first page. Copyrights for \ncomponents of this work owned by others than ACM must be honored. \nAbstracting with credit is permitted. To copy otherwise, or republish, to \npost on servers or to redistribute to lists, requires prior specific permission \nand/or a fee. Request permissions from Permissions@acm.org. \nCHI 2017, May 06-11, 2017, Denver, CO, USA  \n© 2017 ACM. ISBN 978-1-4503-4655-9/17/05…$15.00  \nDOI: http://dx.doi.org/10.1145/3025453.3025955","doi":"10.1145/3025453.3025955","paragraph_after_keyword":"these","sections":[{"word_count":402,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":625,"figure_citations":{},"section_index":1,"title":"PHYSICAL WORLDS"},{"word_count":2824,"figure_citations":{"1":["Figure 1)."],"2":["Figure 2)."],"3":["Figure 3), which allows visualizing both spatial and embodied resources as well as the time-based unfolding of the experience."]},"section_index":2,"title":"PERSPECTIVE"},{"word_count":2021,"figure_citations":{"6":["Figure 6), the two paradigms may affect the unfolding social relationships and the instances for participation and coconstruction of meaning."]},"section_index":3,"title":"RESULTS"},{"word_count":1265,"figure_citations":{"7":["Figure 7), trying to catch him, etc."]},"section_index":4,"title":"DISCUSSION AND FUTURE WORK"},{"word_count":238,"figure_citations":{},"section_index":5,"title":"CONCLUSIONS"},{"word_count":25,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGEMENTS"},{"word_count":1444,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"The World-as-Support: Embodied Exploration, Understanding and Meaning-Making of the Augmented World","authors":"Laura Malinverni, Julian Maya, Marie-Monique Schaper, Narcis Pares","abstract":"Current technical capabilities of mobile technologies are consolidating the interest in developing context-aware Augmented/Mixed Reality applications. Most of these applications are designed based on the Window-on-the-World (WoW) interaction paradigm. A significant decrease in cost of projection technology and advances in pico-sized projectors have spurred applications of Projective Augmented Reality. This research has focused mainly on technological development. However, there is still a need to fully understand its communicational and expressive potential. Hence, we define a conceptual paradigm that we call World-as-Support (WaS). We compare the WaS and WoW paradigms by contrasting their assumptions and cultural values, as well as through a study of an application aimed at supporting the collaborative improvisation of site-specific narratives by children. Our analysis of children's understanding of the physical and social environment and of their imaginative play allowed us to identify the affordances, strengths and weaknesses of these two paradigms.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. Child playing with the WoW based system \n","bbox":[74.7,232.23610000000002,276.5951689500001,241.23610000000002],"page":"5"},"2":{"caption":"Figure 2. Children playing with the WaS based system \n","bbox":[70.2,113.976,281.2173038999999,122.976],"page":"5"},"3":{"caption":"Figure 3. Example of a transcription storyboard \n","bbox":[82.26,368.01599999999996,269.16360714,377.01599999999996],"page":"6"},"4":{"caption":"Figure  4. Children focusing on the screen while playing with \nthe WoW based system \n","bbox":[320.76,246.75310000000002,556.2826551,266.0761],"page":"6"},"5":{"caption":"Figure 5. Distribution of ways of using space \n","bbox":[352.14,454.536,524.8721840999999,463.536],"page":"7"},"6":{"caption":"Figure 6. Distribution of ways of participating \n","bbox":[349.02,164.556,528.0249431699999,173.556],"page":"8"},"7":{"caption":"Figure 7. Child playing with the virtual character in WaSc \n","bbox":[62.34,577.416,289.032,586.416],"page":"9"}},"crops":{"1":{"crop_coord":[254.33333500000003,1189.0000322222224,718.9999813888888,1515.0000169444447],"bbox":[93.3600006,248.3999939,257.0399933,362.1599884],"page":"5"},"2":{"crop_coord":[243.66667861111108,1566.3333130555557,728.9999983333333,1843.6666616666666],"bbox":[89.5200043,130.0800018,260.6399994,226.3200073],"page":"5"},"3":{"crop_coord":[190.50001361111114,708.1666819444443,783.5000272222221,1060.8333502777778],"bbox":[70.3800049,411.8999939,280.2600098,535.2599945000001],"page":"6"},"4":{"crop_coord":[190.50001361111114,1050.8333077777777,783.5000272222221,1145.8333247222222],"bbox":[70.3800049,381.3000031,280.2600098,411.9000092],"page":"6"},"5":{"crop_coord":[878.3333333333334,947.6666852777778,1559.6666463888887,1446.3333469444444],"bbox":[318,273.1199951,559.6799927,449.0399933],"page":"6"},"6":{"crop_coord":[974.3333266666665,692.9999966666668,1057.000003611111,772.3333486111113],"bbox":[352.5599976,515.7599945,378.7200013,540.7200012],"page":"7"},"7":{"crop_coord":[1066.999995,638.333350277778,1150.333311388889,772.3333486111113],"bbox":[385.9199982,515.7599945,412.31999210000004,560.3999938999999],"page":"7"},"8":{"crop_coord":[1160.3333452777779,649.0000066666668,1243.0000222222222,772.3333486111113],"bbox":[419.5200043,515.7599945,445.680008,556.5599976],"page":"7"},"9":{"crop_coord":[1253.0000136111112,690.3333536111113,1336.3333300000002,772.3333486111113],"bbox":[452.8800049,515.7599945,479.27999880000004,541.6799927],"page":"7"},"10":{"crop_coord":[972.3333230555555,1469.6666802777777,1038.3333333333333,1598.333350277778],"bbox":[351.8399963,218.3999939,372,261.1199951],"page":"8"},"11":{"crop_coord":[1035.666665,1345,1102.3333147222222,1598.333350277778],"bbox":[374.6399994,218.3999939,395.0399933,306],"page":"8"},"12":{"crop_coord":[1099.666646388889,1382.3333400000001,1166.3332961111112,1598.333350277778],"bbox":[397.6799927,218.3999939,418.0799866,292.5599976],"page":"8"},"13":{"crop_coord":[1163.66667,1438.3333502777778,1229.6666802777777,1598.333350277778],"bbox":[420.7200012,218.3999939,440.8800049,272.3999939],"page":"8"},"14":{"crop_coord":[1227.666651388889,1475.666690277778,1293.6666616666666,1598.333350277778],"bbox":[443.7599945,218.3999939,463.9199982,258.9599915],"page":"8"},"15":{"crop_coord":[1290.9999933333331,1475.666690277778,1357.6666430555556,1598.333350277778],"bbox":[466.5599976,218.3999939,486.9599915,258.9599915],"page":"8"},"16":{"crop_coord":[253.00001361111111,164.99999166666692,721.1666869444443,555.8333419444446],"bbox":[92.8800049,593.6999969,257.8200073,730.800003],"page":"9"}}}},{"filename":"ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays.pdf","paper_id":"ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays","venue":"VR_2017","keywords":["Thermal display","Thermal haptics","Head mounted display"],"doi":"10.1145/3025453.3025824","paragraph_containing_keyword":"Author Keywords\nthermal display; thermal haptics; head mounted display","paragraph_after_keyword":"INTRODUCTION\nHead Mounted Displays (HMDs) have been on the rise as a\ndisplay that lets users immerse themselves in the virtual world.\nBesides visual output, recent research demonstrated further\nenriching VR experience by adding the headset with various\nsensory outputs [11, 3]. This allows for improved immersion\nwhile keeping the device compact and centralized.\nHaptic research, in particular, presented major driving forces\n[3] implemented\ntoward this trend [4, 16]. De Jesus et al.\nspatial vibrotactile cues by embedding the headset with a\nset of vibrotactors around the head. GyroVR [5] enables\nsimulating inertial with augmenting a ﬂywheel on the headset.\nIn this work, we explored adding a new dimension of feedback,\nthermal haptic feedback.\nThermal haptics has been independently explored on various\nlocations, such as the wrist [6], palm [13], ear [1] for contexts\nsuch as thermal displays [9] and communication [10, 7]. As far\nas we know, this paper presents a ﬁrst attempt at integrating an\nHMD with thermal haptics for spatial and immersive thermal\nexperience.","sections":[{"word_count":448,"figure_citations":{"1":["Figure 1(a)-(c) demonstrates ThermoVR being used to provide thermal cues for a virtual reality game and applications."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":350,"figure_citations":{"1":["Figure 1(c)."],"2":["Figure 2 (a).","Figure 2 (b).","Figure 2 (c)) are used for the closed loop control.","Figure 2 (a))."]},"section_index":1,"title":"IMPLEMENTATION"},{"word_count":847,"figure_citations":{"1":["Figure 1 (b-4) as the visual that is related to the hot stimulation and, Figure 1 (b-2) for the cold."],"4":["Figure 4 (a)) and the HMD displayed the user interface developed using Unity3 depicted in Figure 4 (b)."]},"section_index":2,"title":"EVALUATIONS"},{"word_count":354,"figure_citations":{"5":["Figure 5(a) shows the overall accuracy for the perception of thermal directional cues.","Figure 5(a)).","Figure 5(b)(c) indicates the perception accuracy for each cue in hot and cold conditions.","Figure 5(d)(e) shows the confusion matrix that depicts which cues were misidentified.","Figure 5(f), the study results of visual involvement, Q1, reported 4."]},"section_index":3,"title":"RESULTS"},{"word_count":560,"figure_citations":{"5":["Figure 5(d)), all the cues on the forehead (Left Up, Up, Right Up) were heavily misidentified with each other and the Front cue.","Figure 5(e)) indicated that the Right Up cue was the most difficult to understand as it was confused as Up, Front, Right, Left Down cues.","Figure 5(f)), participants rated visual involvement and thermal aspects for no-stimulation, hot and cold stimulations conditions."]},"section_index":4,"title":"DISCUSSION"},{"word_count":260,"figure_citations":{"1":["Figure 1(a)) is a first-person point of view game.","Figure 1(b)) we designed for enhancing virtual reality experience with thermal haptic feedback."]},"section_index":5,"title":"APPLICATIONS"},{"word_count":194,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":50,"figure_citations":{},"section_index":7,"title":"ACKNOWLEDGEMENTS"},{"word_count":547,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"ThermoVR: Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays","authors":"Roshan Lalintha Peiris, Wei Peng, Zikun Chen, Liwei Chan, Kouta Minamizawa","abstract":"Head Mounted Displays (HMDs) provide a promising opportunity for providing haptic feedback on the head for an enhanced immersive experience. In ThermoVR, we integrated five thermal feedback modules on the HMD to provide thermal feedback directly onto the user's face. We conducted evaluations with 15 participants using two approaches: Firstly, we provided simultaneously actuated thermal stimulations (hot and cold) as directional cues and evaluated the accuracy of recognition; secondly, we evaluated the overall immersive thermal experience that the users experience when provided with thermal feedback on the face. Results indicated that the recognition accuracy for cold stimuli were of approx. 89.5% accuracy while the accuracy for hot stimuli were 68.6%. Also, participants reported that they felt a higher level of immersion on the face when all modules were simultaneously stimulated (hot and cold). The presented applications demonstrate the ThermoVR's directional cueing and immersive experience.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. ThermoVR applications (insets depict example stimulations) (a) “Where is my camp?”: This game provides immersive or directional cues\nin the game as players try to locate their camp before it gets too cold. (a-1) Receiving immersive cold feedback in the winter night (a-2,3) Receiving\ndirectional cues as hot stimulations (a-4) Users immersively feel the warmth of the campﬁre; (b) “What is the weather?” Application provides thermally\nimmersive weather feedback to let users explore the weather of different locations (b-1) Users can select the location to explore weather (b-2,3,4)\nImmersively explore the weather (c) User wearing the ThermoVR\n","bbox":[53.929,424.44626999999997,564.6944200000007,468.28126999999995],"page":"1"},"2":{"caption":"Figure 2. (a) The Prototype System of ThermoVR that integrates ﬁve\nthermal modules (b) Contact locations of the thermal modules on the\nface (c) Close up of thermal module. T1, T2 and T3 are temperature\nsensors where T1 contacts the skin during use\n","bbox":[53.929,591.7252699999999,296.9980600000002,626.5942699999999],"page":"2"},"3":{"caption":"Figure 3. Stimulation patterns for directional cues. The modules in black\ndenote the thermally (hot or cold) stimulated modules\n","bbox":[321.094,613.57827,564.3862200000002,630.51427],"page":"2"},"4":{"caption":"Figure 4. The study setup (a)The evaluation setup for identifying ther-\nmal cues (b) Interface displayed on the HMD for selecting perceived cues\nduring the evaluation\n","bbox":[321.094,466.75027,565.48608,492.65326999999996],"page":"2"},"5":{"caption":"Figure 5. Results: (L-Left, U-Up, D-Down, R-Right) (a) Perception accuracy and Average Task Completion Times for thermal cues: Hot and Cold cues;\nHeat map visualization of the accuracy of perception for individual cues for (b) hot and (c) cold stimulation; Confusion matrix for the (d) hot and (e)\nfor cold stimulations (f) Qualitative study results\n","bbox":[53.929,611.7382699999999,564.9574300000007,637.64127],"page":"3"}},"crops":{"1":{"crop_coord":[144.80278027777777,557.6002655555554,1572.1375247222222,891.288876111111],"bbox":[53.9290009,472.9360046,564.1695089,589.4639044],"page":"1"},"2":{"crop_coord":[152.67779888888887,170.99371333333323,837.8568516666667,449.2833369444446],"bbox":[56.7640076,632.0579987,299.8284666,728.6422632],"page":"2"},"3":{"crop_coord":[894.8027802777779,170.9948144444444,1579.9818330555556,438.3944533333333],"bbox":[323.9290009,635.9779968,566.9934599000001,728.6418668],"page":"2"},"4":{"crop_coord":[886.9277613888887,520.1101547222223,1572.1271547222225,823.5888925],"bbox":[321.0939941,497.3079987,564.1657757,602.9603443],"page":"2"},"5":{"crop_coord":[144.80386947693054,-286.04417663436084,1711.2778181209303,420.84393177613896],"bbox":[53.929393011695,642.29618456059,614.2600145235349,893.1759035883699],"page":"3"}}}},{"filename":"Using Presence Questionnaires in Virtual Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/Using Presence Questionnaires in Virtual Reality.pdf","paper_id":"Using Presence Questionnaires in Virtual Reality","venue":"chi2019xr","keywords":["Virtual reality","Presence","Questionnaire","Evaluation"],"paragraph_containing_keyword":"Using Presence Questionnaires in Virtual Reality\nValentin Schwind\nNiels Henze\nUniversity of Stuttgart\nUniversity of Regensburg\nStuttgart, Germany\nRegensburg, Germany\nvalentin.schwind@acm.org\nniels.henze@ur.de\nABSTRACT\nVirtual Reality (VR) is gaining increasing importance in sci-\nence, education, and entertainment. A fundamental charac-\nteristic of VR is creating presence, the experience of ’being’\nor ’acting’, when physically situated in another place. Mea-\nsuring presence is vital for VR research and development.\nIt is typically repeatedly assessed through questionnaires\ncompleted after leaving a VR scene. Requiring participants to\nleave and re-enter the VR costs time and can cause disorien-\ntation. In this paper, we investigate the effect of completing\npresence questionnaires directly in VR. Thirty-six partici-\npants experienced two immersion levels and filled three stan-\ndardized presence questionnaires in the real world or VR. We\nfound no effect on the questionnaires’ mean scores; however,\nwe found that the variance of those measures significantly\ndepends on the realism of the virtual scene and if the subjects\nhad left the VR. The results indicate that, besides reducing\na study’s duration and reducing disorientation, completing\nquestionnaires in VR does not change the measured presence\nbut can increase the consistency of the variance.\nCCS CONCEPTS\n• Human-centered computing → HCI design and eval-\nuation methods; Virtual reality; User studies;\nKEYWORDS\nVirtual reality; presence; questionnaire; evaluation.\nACM Reference Format:\nValentin Schwind, Pascal Knierim, Nico Haas, and Niels Henze. 2019.\nUsing Presence Questionnaires in Virtual Reality. In CHI Conference\non Human Factors in Computing Systems Proceedings (CHI 2019),\nMay 4–9, 2019, Glasgow, Scotland UK. ACM, New York, NY, USA,\n12 pages. https://doi.org/10.1145/3290605.3300590","doi":"10.1145/3290605.3300590","paragraph_after_keyword":"Nico Haas\nUniversity of Stuttgart\nStuttgart, Germany\nnicohaasni@gmx.de\n1 INTRODUCTION\nThe key characteristic of virtual reality (VR) is the ability to\ncreate a sense of presence [11, 26, 43], the feeling of being\nor acting in a place, even when one is physically situated in\nanother location [1, 24]. To create immersive VR experiences\nand to study the interaction in VR, it is, therefore, crucial to\nreliably measure presence. Previous work developed increas-\ningly sophisticated approaches to assess presence. While\nmultiple physiological measures have been proposed [18],\nvalidated questionnaires are still the most common method\nfor measuring this construct [10]. Using different items and\nsubscales such questionnaires provide scores, which reflect\nthe level of felt presence in the virtual world (cf. Table 1).\nThe use of validated presence questionnaires is omnipre-\nsent in research and industry. These standardized question-\nnaires are filled in using pen and paper. Typical studies re-\npeatedly assess presence, especially when multiple VR ex-\nperiences are compared. As current VR experiences are pre-\nsented through head-mounted displays (HMDs), users must\nremove the headset and leave the VR before completing the\nquestionnaire. In doing so, the person has to re-orientate in\nthe real-world which causes a so-called “break-in-presence\n(BIP)” [11]. Slater and Steed describe it as a moment when\n“a report can be given that a break has occurred without this\nin itself disturbing the sense of presence, which of course\nhas already been disturbed” [36]. This means that surveying\nsubjects about their feeling of presence potentially causes\nthe BIP and compromises the phenomenon that the ques-\ntionnaire is supposed to be measuring [26, 34, 36].\nLeaving VR can cause BIPs which distort the phenomenon\nthat presence questionnaires measure [26, 34, 36]. Further-\nmore, leaving and re-entering the VR takes time not only\nbecause it requires removing and putting on the HMDs but\nalso reorientation in the real-world and when entering the\nVR experience again.\nInstead of requiring participants to leave the VR to fill in\nquestionnaires, we propose to survey participants directly\nwithin the VR using existing questionnaires. Surveying par-\nticipants during the VR experience have a number of poten-\ntial advantages:","sections":[{"word_count":601,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":1329,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1024,"figure_citations":{"1":["Figure 1).","Figure 1).","Figure 1)."],"2":["Figure 2)."]},"section_index":2,"title":"METHOD"},{"word_count":1841,"figure_citations":{"4":["Figure 4 shows the Gaussian distribution curve fits of those differences."]},"section_index":3,"title":"RESULTS"},{"word_count":29,"figure_citations":{},"section_index":4,"title":"REAL"},{"word_count":6,"figure_citations":{},"section_index":5,"title":"HAPTC"},{"word_count":163,"figure_citations":{"6":["Figure 6, F (2, 203) = 150."]},"section_index":6,"title":"IFQUAL"},{"word_count":1278,"figure_citations":{"6":["Figure 6)."]},"section_index":7,"title":"DISCUSSION"},{"word_count":238,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":35,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGMENTS"},{"word_count":1446,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"Using Presence Questionnaires in Virtual Reality","authors":"Valentin Schwind, Pascal Knierim, Nico Haas, Niels Henze","abstract":"Virtual Reality (VR) is gaining increasing importance in science, education, and entertainment. A fundamental characteristic of VR is creating presence, the experience of 'being' or 'acting', when physically situated in another place. Measuring presence is vital for VR research and development. It is typically repeatedly assessed through questionnaires completed after leaving a VR scene. Requiring participants to leave and re-enter the VR costs time and can cause disorientation. In this paper, we investigate the effect of completing presence questionnaires directly in VR. Thirty-six participants experienced two immersion levels and filled three standardized presence questionnaires in the real world or VR. We found no effect on the questionnaires' mean scores; however, we found that the variance of those measures significantly depends on the realism of the virtual scene and if the subjects had left the VR. The results indicate that, besides reducing a study's duration and reducing disorientation, completing questionnaires in VR does not change the measured presence but can increase the consistency of the variance.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Virtual (l) and real (r) environment with question-\nnaire and input controller.\n","bbox":[53.798,507.886854,295.637918,527.8108540000001],"page":"4"},"2":{"caption":"Figure 2: Abstract and realistic scene of a first person shooter game developed to induce different levels of presence.\n","bbox":[71.866,538.573854,540.1153499999997,547.539854],"page":"5"},"3":{"caption":"Figure 3: Means and score variances of the presence measures. While presence scores of the three questionnaires did not differ\nsignificantly between in- and outside the VR, variance measures show significant interaction effects between the questionnaire\nenvironment and the realism during the VR experience. Error bars show 95% confidence intervals (CI95)\n","bbox":[53.797999999999995,496.01285399999995,558.3694199999999,526.895854],"page":"6"},"4":{"caption":"Figure 4: Histogram, means, and fitted Gaussian distribution using the average scores of all questionnaires. While the distribu-\ntions remain nearly constant when the questionnaires are used in VR, there is a significant difference between the variances\nin the real-world responses after BIP using abstract and after BIP using a realistic virtual scene.\n","bbox":[53.798,459.568854,559.7852439999999,490.452854],"page":"7"},"5":{"caption":"Figure 5: Subscales of the IPQ and WS presence questionnaires. IPQ: General Presence (GP), spatial presence (SP), involvement\n(INV), and realism (REAL); WS: Involvement (INV), natural (NAT), auditory (AUD), haptics (HAPTC), resolution (RES), and\ninterface quality (IFQUAL).\n","bbox":[53.52,298.740854,558.1803300000001,329.624854],"page":"8"},"6":{"caption":"Figure 6: Completion times of the SUS, IPQ, and WS ques-\ntionnaire. Error bars show CI95.\n","bbox":[53.798,506.523854,295.63791799999996,526.448854],"page":"9"}},"crops":{"1":{"crop_coord":[144.43890055555556,252.3659683333335,821.8000116666666,703.4916686111111],"bbox":[53.7980042,540.5429993,294.0480042,699.3482514],"page":"4"},"2":{"crop_coord":[154.98331694444445,252.97072527777777,851.4722102777779,648.6916775],"bbox":[57.5939941,560.2709961,304.7299957,699.1305389],"page":"5"},"3":{"crop_coord":[848.4583197222222,252.36995277777737,1544.9881997222224,648.6916775],"bbox":[307.2449951,560.2709961,554.3957519,699.3468170000001],"page":"5"},"4":{"crop_coord":[154.98331694444445,252.3688127555555,851.55807788,635.9916602777778],"bbox":[57.5939941,564.8430023,304.7609080368,699.347227408],"page":"6"},"5":{"crop_coord":[848.4583197222222,252.3688127555555,1545.0330806577779,635.9916602777778],"bbox":[307.2449951,564.8430023,554.4119090368,699.347227408],"page":"6"},"6":{"crop_coord":[501.7222172222223,628.7572586844444,1198.2969781577779,706.0333166666666],"bbox":[182.4199982,539.628006,429.58691213680004,563.8473868736],"page":"6"},"7":{"crop_coord":[144.43890055555556,252.3676808333334,1555.6076805555558,807.2666508333334],"bbox":[53.7980042,503.1840057,558.2187650000001,699.3476349],"page":"7"},"8":{"crop_coord":[144.43890055555556,280.04134032472234,1555.559508050278,725.8222283333333],"bbox":[53.7980042,532.5039978,558.2014228981001,689.3851174831],"page":"8"},"9":{"crop_coord":[144.43890055555556,718.5923958386114,1555.559508050278,1254.0111033333333],"bbox":[53.7980042,342.3560028,558.2014228981001,531.5067374980999],"page":"8"},"10":{"crop_coord":[144.43890055555556,252.3681380555558,821.8033905555556,707.2777980555556],"bbox":[53.7980042,539.1799927,294.0492206,699.3474702999999],"page":"9"}}}},{"filename":"VRRRRoom- Virtual Reality for Radiologists in the Reading Room","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/VRRRRoom- Virtual Reality for Radiologists in the Reading Room.pdf","paper_id":"VRRRRoom- Virtual Reality for Radiologists in the Reading Room","venue":"VR_2017","keywords":["Virtual Reality","Multitouch Surfaces","Medical Visualization","Interaction Design"],"paragraph_containing_keyword":"ABSTRACT\nReading room conditions such as illumination, ambient light,\nhuman factors and display luminance, play an important role\non how radiologists analyze and interpret images. Indeed,\nserious diagnostic errors can appear when observing images\nthrough everyday monitors. Typically, these occur whenever\nprofessionals are ill-positioned with respect to the display or\nvisualize images under improper light and luminance condi-\ntions. In this work, we show that virtual reality can assist\nradiodiagnostics by considerably diminishing or cancel out\nthe effects of unsuitable ambient conditions. Our approach\ncombines immersive head-mounted displays with interactive\nsurfaces to support professional radiologists in analyzing med-\nical images and formulating diagnostics. We evaluated our\nprototype with two senior medical doctors and four seasoned\nradiology fellows. Results indicate that our approach consti-\ntutes a viable, ﬂexible, portable and cost-efﬁcient option to\ntraditional radiology reading rooms.\nACM Classiﬁcation Keywords\nH.5.2 Information Interfaces and Presentation (e.g. HCI):\nInteraction styles, Graphical User Interfaces\nAuthor Keywords\nVirtual Reality; Multitouch Surfaces; Medical Visualization;\nInteraction Design","paragraph_after_keyword":"Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for  personal  or \nclassroom use is granted without fee provided that copies are not made or distributed \nfor proﬁt or commercial advantage and that copies bear this notice and the full citation \non the ﬁrst p age. Copyrights for components of this work owned by others than the \nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or \nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission \nand/or a fee. Request permissions from Permissions@acm.org.\nCHI 2017, May 06 - 11, 2017, Denver, CO, USA\nCopyright is held by the owner/author(s). Publication rights licensed to ACM. \nACM 978-1-4503-4655-9/17/05...$15.00\nDOI: http://dx.doi.org/10.1145/3025453.3025566","doi":"10.1145/3025453.3025566","sections":[{"word_count":620,"figure_citations":{"1":["Figure 1A illustrates a typical radiology reading room.","Figure 1B."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":433,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":601,"figure_citations":{"2":["Figure 2).","Figure 2 on the left.","Figure 2 on the right)."]},"section_index":2,"title":"INTERACTION DESIGN"},{"word_count":603,"figure_citations":{"4":["Figure 4 displays the graphical elements of our prototype that the user can interact with."]},"section_index":3,"title":"IMPLEMENTATION"},{"word_count":708,"figure_citations":{},"section_index":4,"title":"EVALUATION"},{"word_count":199,"figure_citations":{},"section_index":5,"title":"CONCLUSIONS"},{"word_count":49,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":884,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"VRRRRoom: Virtual Reality for Radiologists in the Reading Room","authors":"Maurício Sousa, Daniel Mendes, Soraia Paulo, Nuno Matela, Joaquim Jorge, Daniel Simões Lopes","abstract":"Reading room conditions such as illumination, ambient light, human factors and display luminance, play an important role on how radiologists analyze and interpret images. Indeed, serious diagnostic errors can appear when observing images through everyday monitors. Typically, these occur whenever professionals are ill-positioned with respect to the display or visualize images under improper light and luminance conditions. In this work, we show that virtual reality can assist radiodiagnostics by considerably diminishing or cancel out the effects of unsuitable ambient conditions. Our approach combines immersive head-mounted displays with interactive surfaces to support professional radiologists in analyzing medical images and formulating diagnostics. We evaluated our prototype with two senior medical doctors and four seasoned radiology fellows. Results indicate that our approach constitutes a viable, flexible, portable and cost-efficient option to traditional radiology reading rooms.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. A) A typical radiology reading room; B) Our approach combining virtual reality and desktop touch interactions.\n","bbox":[101.611,470.12127,516.4813800000007,478.09126999999995],"page":"1"},"2":{"caption":"Figure 2. Desk surface gestures for left and right hands.\n","bbox":[347.998,86.96227,537.2615900000001,94.93227],"page":"2"},"3":{"caption":"Figure 3. The setup consists of an Oculus Rift HMD with a head track-\ning sensor and a multitouch frame to detect touches on the desk surface.\n","bbox":[53.929,577.0862699999999,298.3928100000001,594.0222699999999],"page":"3"},"4":{"caption":"Figure 4. Virtual desk and volume rendered from medical images.\n","bbox":[330.738,86.96227,554.51966,94.93227],"page":"3"}},"crops":{"1":{"crop_coord":[276.8583169444445,557.599490105319,826.945780519759,858.5027822222222],"bbox":[101.4689941,484.7389984,295.9004809871132,589.4641835620852],"page":"1"},"2":{"crop_coord":[889.9111091666666,557.599490105319,1439.9985727419814,858.5027822222222],"bbox":[322.1679993,484.7389984,516.5994861871133,589.4641835620852],"page":"1"},"3":{"crop_coord":[920.6916469444444,1582.172353734647,1538.354981359457,1922.8333283333334],"bbox":[333.2489929,101.5800018,552.0077932894045,220.6179526555271],"page":"2"},"4":{"crop_coord":[229.2027875,168.22779083111118,745.4925556155555,542.0083277777777],"bbox":[84.3130035,598.677002,266.5773200216,729.6379953008],"page":"3"},"5":{"crop_coord":[886.9277613888887,1504.9760858172222,1571.981519185,1922.8333283333334],"bbox":[321.0939941,101.5800018,564.1133469066,248.4086091058],"page":"3"}}}},{"filename":"VaiR- Simulating 3D Airflows in Virtual Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/VaiR- Simulating 3D Airflows in Virtual Reality.pdf","paper_id":"VaiR- Simulating 3D Airflows in Virtual Reality","venue":"VR_2017","keywords":["Virtual reality","Airﬂow","Presence","Evaluation"],"paragraph_containing_keyword":"Author Keywords\nVirtual reality; airﬂow; presence; evaluation.","paragraph_after_keyword":"INTRODUCTION\nEnhancing presence and immersion is one of the major goals\nof Virtual Reality (VR) research. The sense of being there [7]\nboth occurs and can be supported in three dimensions: the\npersonal, the environmental and the social forms of presence.\nThe main focus of this paper is to enhance the personal feeling\nof presence. This can be achieved by simulating sensory stim-\nuli as close as possible to the capability of a sensor regarding\nrange and intensity. Appealing multiple senses ampliﬁes the\nfeeling of presence and immersion in VR applications [3, 6].\nBesides the inherent integration of visual and audio [9, 13]\nin most VR systems, a large body of work exists to include\nfurther channels in the VR experience. Those channels include\nhaptics [1, 19], warmth [4, 5] and smell [21, 20]. Regarding","doi":"10.1145/3025453.3026009","sections":[{"word_count":925,"figure_citations":{"1":["Figure 1, can easily be combined with current head mounted displays."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":1517,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1325,"figure_citations":{},"section_index":2,"title":"IMPLEMENTATION"},{"word_count":2004,"figure_citations":{},"section_index":3,"title":"EVALUATION"},{"word_count":590,"figure_citations":{},"section_index":4,"title":"CONCLUSION"},{"word_count":691,"figure_citations":{},"section_index":5,"title":"REFERENCES"}],"title":"VaiR: Simulating 3D Airflows in Virtual Reality","authors":"Michael Rietzler, Katrin Plaumann, Taras Kränzle, Marcel Erath, Alexander Stahl, Enrico Rukzio","abstract":"The integration of multi-sensory stimuli, e.g. haptic airflow, in virtual reality (VR) has become an important topic of VR research and proved to enhance the feeling of presence. VaiR focuses on an accurate and realistic airflow simulation that goes far beyond wind. While previous works on the topic of airflow in VR are restricted to wind, while focusing on the feeling of presence, there is to the best of our knowledge no work considering the conceptual background or on the various application areas. Our pneumatic prototype emits short and long term flows with a minimum delay and is able to animate wind sources in 3D space around the user's head. To get insights on how airflow can be used in VR and how such a device should be designed, we arranged focus groups and discussed the topic. Based on the gathered knowledge, we developed a prototype which proved to increase presence, as well as enjoyment and realism, while not disturbing the VR experience.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1. The VaiR Prototype worn in combination with a HTC Vive\nhead mounted display. Fixed to frame mounted onto the Vive’s straps\nare two bows. These bows are moved by two separate motors, allowing\nboth bows to each rotate 135◦. Fixes on each bow are ﬁve nozzles (each\n36◦ apart), were the air streams come out. Several nozzles can be used at\nthe same time. Due to the modular design, nozzles can be moved along\nthe bows and changed as needed. That way, angle and dispersion of the\nair streams can be customized. VaiR is so designed that a container of\npressurized air, valves, power source and an arduino controller can be\nworn as a backpack.\n","bbox":[320.8070000000001,262.08827,564.16726,350.75527],"page":"1"},"2":{"caption":"Figure 2. Overview of the focus group results.\n","bbox":[365.197,574.8442699999999,520.0620700000001,582.81427],"page":"3"},"3":{"caption":"Figure 3. Examples of different nozzles to vary spray angle and intensity.\n","bbox":[53.929,660.1342699999999,298.3928100000002,668.1042699999999],"page":"4"},"4":{"caption":"Figure 4. The VaiR setup: Two bows with nozzles having a angular\ndistance of 36◦ and two servo motors to control the angle of each bow\nseperately (left). Each bow can be rotated 135◦, overall 270◦ around the\nuser’s head and neck can be displayed (right).\n","bbox":[321.094,576.6762699999999,564.44605,611.54527],"page":"4"},"5":{"caption":"Figure 5. The Backpack consists of a compressed air bottle and 14 valves\n(ten for the nozzles, and four to vary the intensity) as well as the control\nunit.\n","bbox":[320.831,329.08127,564.1630700000002,354.98427],"page":"4"},"6":{"caption":"Figure 6. The ﬁrst and second scene used for the evaluation. a) The cliff scene looking towards the sea and b) view when turned around. c) The fair\nscene regarded from outside and d) the ﬁrst person view. e) The golf cart scene from ﬁrst person view.\n","bbox":[53.929,623.0132699999999,564.3357700000012,639.94927],"page":"6"},"7":{"caption":"Figure 7. The third and fourth scene used for the evaluation. a) The cart scene in ﬁrst person view. b) The horror scene: bats ﬂying over the head, c) a\nrotating saw runs through the viewer’s neck and d) a Zombie appears.\n","bbox":[53.929,505.47727000000003,564.160430000001,522.41327],"page":"6"},"8":{"caption":"Figure 8. The results of the enjoyment scores per scene.\n","bbox":[348.643,520.5572699999999,536.61545,528.5272699999999],"page":"7"},"9":{"caption":"Figure 9. The results of the presence scores per scene.\n","bbox":[351.815,296.48727,533.44333,304.45727],"page":"7"},"10":{"caption":"Figure 10. The results of the E2I presence and enjoyment scores.\n","bbox":[66.185,520.5572699999999,284.74264999999997,529.8027519999999],"page":"8"}},"crops":{"1":{"crop_coord":[886.9277613888887,499.48731305555543,1572.119767777778,1217.7499897222222],"bbox":[321.0939941,355.4100037,564.1631164,610.3845673000001],"page":"1"},"2":{"crop_coord":[886.9277613888887,168.22697111111125,1572.1244725,567.6083375000003],"bbox":[321.0939941,589.4609985,564.1648101,729.6382904],"page":"3"},"3":{"crop_coord":[144.80278027777777,168.2229869444445,830.0250075,336.2249925000001],"bbox":[53.9290009,672.7590027,297.0090027,729.6397247],"page":"4"},"4":{"crop_coord":[886.9277613888887,168.22748638888913,1572.154594722222,493.33334194444444],"bbox":[321.0939941,616.1999969,564.1756541,729.6381048999999],"page":"4"},"5":{"crop_coord":[988.2083130555557,629.4404900000001,1470.8405197222223,1206.0027566666668],"bbox":[357.5549927,359.6390076,527.7025871000001,563.6014236],"page":"4"},"6":{"crop_coord":[144.80278027777777,168.2273224999999,1572.038780277778,414.4333225000002],"bbox":[53.9290009,644.6040039,564.1339609,729.6381639],"page":"6"},"7":{"crop_coord":[144.80278027777777,494.7162411111109,1572.038780277778,740.9222411111109],"bbox":[53.9290009,527.0679932,564.1339609,612.1021532000001],"page":"6"},"8":{"crop_coord":[886.9277613888887,168.22745583333327,1437.105872222222,718.4055666666667],"bbox":[321.0939941,535.173996,515.5581139999999,729.6381159],"page":"7"},"9":{"crop_coord":[886.9277613888887,790.6441005555556,1437.105872222222,1340.8222113888887],"bbox":[321.0939941,311.1040039,515.5581139999999,505.5681238],"page":"7"},"10":{"crop_coord":[144.80278027777777,168.22745583333327,694.9808911111112,718.4055666666667],"bbox":[53.9290009,535.173996,248.39312080000002,729.6381159],"page":"8"}}}},{"filename":"Vremiere- In-Headset Virtual Reality Video Editing","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/Vremiere- In-Headset Virtual Reality Video Editing.pdf","paper_id":"Vremiere- In-Headset Virtual Reality Video Editing","venue":"VR_2017","keywords":["Virtual reality","Video editing"],"paragraph_containing_keyword":"Author Keywords\nVirtual reality; video editing","paragraph_after_keyword":"INTRODUCTION\nVirtual Reality (VR) video is emerging as a new medium for\nshared, creative experiences. VR video speciﬁcally means\nspherical panorama (full or partial) video viewed within a\nhead-mounted display. Filmmakers use multi-camera rigs to\ncapture spherical videos and then edit them with standard\nvideo production software such as Adobe Premiere or Apple\nFinal Cut Pro. However, these tools only support editing VR\nvideo in its ﬂattened equirectangular projection format (Figure\n1). One prominent director describes her ﬁrst experience with\nediting spherical video:","doi":"10.1145/3025453.3025675","sections":[{"word_count":618,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":234,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":1364,"figure_citations":{},"section_index":2,"title":"VR PROFESSIONAL INTERVIEWS"},{"word_count":2961,"figure_citations":{"2":["Figure 2 presents the main timeline interface for our system, which is overlaid on a spherical video view.","Figure 2D.","Figure 2D.","Figure 2D.","Figure 2C \"Graphics\" track), and bookmarks can be placed on the video as well (Figure 2D.","Figure 2B).","Figure 2 D.","Figure 2 D."],"3":["Figure 3).","Figure 3, the user quickly maps her current view in the video with the view in the visualization; she can also spot the jumping lady behind her that she might have not noticed before."],"4":["Figure 4)."],"5":["Figure 5B).","Figure 5C, the user has aligned the ski lift before the cut to the skier after the clip, rather than transitioning from the ski lift to an empty view.","Figure 5B).","Figure 5B, if the toggle is off, the user can change the rotation of each video individually; otherwise, rotating the blue (ski_1) video will also rotate the red video (ski_2) by the same amount."],"6":["Figure 6 left).","Figure 6, the editor can easily position the logo image on the parachute (Figure 6 right), which is difficult on the desktop due to distortion (Figure 6 left)."],"7":["Figure 7, Figure 2 D."]},"section_index":3,"title":"THE VREMIERE SYSTEM"},{"word_count":1430,"figure_citations":{},"section_index":4,"title":"EXPERT REVIEW"},{"word_count":716,"figure_citations":{},"section_index":5,"title":"DISCUSSION"},{"word_count":185,"figure_citations":{},"section_index":6,"title":"CONCLUSION"},{"word_count":66,"figure_citations":{"1":["Figure 1, 4 and 5 use images from YouTube users TOYO TIRES JAPAN and Ábaco Digital Zaragoza under a Creative Commons license."],"6":["Figure 6 and 7 use images with permission from Youtube users P J Orravan and Jacob Phillips."]},"section_index":7,"title":"ACKNOWLEDGMENTS"},{"word_count":877,"figure_citations":{},"section_index":8,"title":"REFERENCES"}],"title":"Vremiere: In-Headset Virtual Reality Video Editing","authors":"Cuong Nguyen, Stephen DiVerdi, Aaron Hertzmann, Feng Liu","abstract":"Creative professionals are creating Virtual Reality (VR) experiences today by capturing spherical videos, but video editing is still done primarily in traditional 2D desktop GUI applications such as Premiere. These interfaces provide limited capabilities for previewing content in a VR headset or for directly manipulating the spherical video in an intuitive way. As a result, editors must alternate between editing on the desktop and previewing in the headset, which is tedious and interrupts the creative process. We demonstrate an application that enables a user to directly edit spherical video while fully immersed in a VR headset. We first interviewed professional VR filmmakers to understand current practice and derived a suitable workflow for in-headset VR video editing. We then developed a prototype system implementing this new workflow. Our system is built upon a familiar timeline design, but is enhanced with custom widgets to enable intuitive editing of spherical video inside the headset. We conducted an expert review study and found that with our prototype, experts were able to edit videos entirely within the headset. Experts also found our interface and widgets useful, providing intuitive controls for their editing needs.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: The same shot, viewed in a headset versus on the\nscreen. VR views look very different than on the desktop\nscreen, and provide a much stronger sense of immersion.\n© TOYO TIRES JAPAN\n","bbox":[320.413,495.85234399999996,565.9130056600001,538.4769920000001],"page":"1"},"2":{"caption":"Figure 2: The Vremiere interface contains a video view in the background (A) and ﬂoating UI components such as the timeline\n(B), editing tracks (C) and editing widgets (D). By using familiar mouse and keyboard interaction, an editor can edit and view the\nspherical video directly inside the VR headset.\n","bbox":[53.6,528.434992,564.1892837239996,560.3149920000001],"page":"4"},"3":{"caption":"Figure 3: Left: the Little Planet is shown in a small window\nabove the timeline to aid in-headset video navigation. Right:\nclose-up view of the visualization; it can be used both as a\nminimap and a compass of the 360° image. As noted by the\nyellow arrow, the user can quickly spot an interesting event\noutside her ﬁeld of view (indicated by the yellow fan at the\ncenter) that would be otherwise difﬁcult to ﬁnd.\n","bbox":[53.68016000000003,477.734992,298.39735787499995,553.450992],"page":"5"},"4":{"caption":"Figure 4: Our system dynamically contracts a pair of vignettes\nin front of the user’s eyes based on the perceived motion in\nthe video, to help reduce discomfort during video playback\nor scrubbing. Note, on the left, the default diameter of the\nvignette is 120°, which corresponds to no vignetting. © Ábaco\nDigital Zaragoza\n","bbox":[320.845,578.232992,564.4191534400002,644.136992],"page":"5"},"5":{"caption":"Figure 5: A: an example of a jarring cut in VR video. The cut transitions from the ski lift to an empty view of the mountain in the\nski_2 video. B: Our Rotation Alignment tool is shown above the timeline. In the close-up view, the user can visualize how the cut\ntransitions from one clip to another given a speciﬁc viewpoint in the video (visualized by yellow bars); she can also directly drag\non the clips to align events before and after a cut, thereby helping the viewer to follow key elements in the video. Here, the Ripple\nRotation mode is shown currently as an Unlink toggle. C: after rotation, the skier in clip ski_2 is aligned to the previous shot,\nresulting in a much better cut. © Ábaco Digital Zaragoza\n","bbox":[53.929,524.413992,565.4163589,590.316992],"page":"6"},"6":{"caption":"Figure 6: Left: On desktop, adding 2D text or images to a\nspherical video is often unintuitive. Right: in the headset, our\nsystem provides a natural view of the image (Sample Logo)\nand enables users to directly place it anywhere in the scene.\n© P J Orravan\n","bbox":[53.172,574.715992,298.7480056600001,628.5149919999999],"page":"7"},"7":{"caption":"Figure 7: The user can directly click on the video to add\nmarkers. These markers are visualized on the timeline and\ncan be reviewed quickly using keyboard shortcuts. © Jacob\nPhillips\n","bbox":[321.094,518.177992,564.1752592,561.017992],"page":"7"}},"crops":{"1":{"crop_coord":[886.9277613888887,454.60201694444436,1222.894787222222,640.4333241666665],"bbox":[321.0939941,563.2440033,438.44212339999996,626.5432739],"page":"1"},"2":{"crop_coord":[1236.1472319444445,454.7935994444445,1572.1257441666667,640.4333241666665],"bbox":[446.8130035,563.2440033,564.1652679,626.4743042],"page":"1"},"3":{"crop_coord":[170.13334500000002,168.22777638888888,1546.800011666667,621.2833319444446],"bbox":[63.0480042,570.1380005,555.0480042,729.6380005],"page":"4"},"4":{"crop_coord":[148.1527625,355.0178408333335,826.6498975,640.3499941666666],"bbox":[55.1349945,563.2740021,295.7939631,662.3935773],"page":"5"},"5":{"crop_coord":[897.8527577777779,168.22857000000025,1561.2027769444446,388.44722333333345],"bbox":[325.0269928,653.9589996,560.2329997,729.6377147999999],"page":"5"},"6":{"crop_coord":[145.84999944444445,168.2269877777779,1571.0812569444447,537.9444461111112],"bbox":[54.3059998,600.1399994,563.7892525000001,729.6382844],"page":"6"},"7":{"crop_coord":[149.34443166666668,168.22640166666656,825.4591761111111,431.8388961111113],"bbox":[55.5639954,638.3379974,295.3653034,729.6384954],"page":"7"},"8":{"crop_coord":[971.1972216666668,168.22616194444444,1487.8561616666668,619.3305461111113],"bbox":[351.4309998,570.8410034,533.8282182,729.6385817],"page":"7"}}}},{"filename":"WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/VR_2017/WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality.pdf","paper_id":"WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality","venue":"VR_2017","keywords":["Smartwatches","Micro-Interaction","Wearable Devices"],"paragraph_containing_keyword":"Author Keywords\nSmartwatches; Micro-Interaction; Wearable Devices","paragraph_after_keyword":"INTRODUCTION\nSmartwatches are designed to support micro-interactions.\nTheir small screens enable compact form factors, but result in\na limited visual and interaction space. While most work on ad-\ndressing these limitations explores additional input techniques\nor modalities, this work focuses on the output challenges that\narise from small screen sizes of currently 1.5–1.8 (cid:48)(cid:48) (10–25%\nthe size of typical 5 (cid:48)(cid:48) smartphone screens).\nWatchThru extends the output capabilities of current smart-\nwatches into 3D with an additional 1.8 (cid:48)(cid:48) transparent screen\nthat complements the main touchscreen with graphics that can\nbe displayed ﬂoating in mid-air (see Figure 1). WatchThru\ncan show different information on the two screens, based on","doi":"10.1145/3025453.3025852","sections":[{"word_count":334,"figure_citations":{"1":["Figure 1)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":280,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":704,"figure_citations":{"1":["Figure 1 (top, right) shows a conceptual app using the WatchThru screen to display static and animated notifications."],"2":["Figure 2 shows a prototypical app with a 2D map on the main screen and an oriented 3D arrow on the WatchThru screen."],"3":["Figure 3 (c, d) show a smart home scenario where objects in a room, e.","Figure 3 (e), a learning scenario, a virtual globe is presented in the middle of a room.","Figure 3 (f) shows an assembly scenario; WatchThru shows the user how to connect electronic components."]},"section_index":2,"title":"WATCHTHRU INTERACTIONS"},{"word_count":601,"figure_citations":{"1":["Figure 1, bottom)."],"3":["Figure 3 a,b)."]},"section_index":3,"title":"IMPLEMENTATION OF PROTOTYPES"},{"word_count":648,"figure_citations":{"4":["Figure 4) such that the WatchThru screen could be pulled out (manually or automatically) when needed."]},"section_index":4,"title":"LIMITATIONS AND FUTURE WORK"},{"word_count":208,"figure_citations":{},"section_index":5,"title":"CONCLUSIONS"},{"word_count":85,"figure_citations":{},"section_index":6,"title":"ACKNOWLEDGMENTS"},{"word_count":760,"figure_citations":{},"section_index":7,"title":"REFERENCES"}],"title":"WatchThru: Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality","authors":"Dirk Wenig, Johannes Schöning, Alex Olwal, Mathias Oben, Rainer Malaka","abstract":"We introduce WatchThru, an interactive method for extended wrist-worn display on commercially-available smartwatches. To address the limited visual and interaction space, WatchThru expands the device into 3D through a transparent display. This enables novel interactions that leverage and extend smartwatch glanceability. We describe three novel interaction techniques, Pop-up Visuals, Second Perspective and Peek-through, and discuss how they can complement interaction on current devices. We also describe two types of prototypes that helped us to explore standalone interactions, as well as, proof-of-concept AR interfaces using our platform.","publication":{"venue":"CHI '17","venue_full":"Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems","year":"2017","date":"2017/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: WatchThru is a lightweight and easy-to-build display\nextension for existing smartwatches.\n","bbox":[321.094,346.582992,564.5235656800002,367.504992],"page":"1"},"2":{"caption":"Figure 2: Second Perspective navigation with a 2D map (left)\nand directional arrows (right).\n","bbox":[321.09399999999994,506.02899199999996,564.8403189999999,527.1801409999999],"page":"2"},"3":{"caption":"Figure 3: The Peek-through prototype uses external cameras\nto track retroreﬂective markers on the (a) device and (b) user,\nwhich enabled the several implemented AR scenarios (c–f).\n","bbox":[320.736,424.42299199999997,565.4201360500001,456.533141],"page":"3"},"4":{"caption":"Figure 4: In the future for WatchThru, we envision a transpa-\nrent screen that can be folded out when needed.\n","bbox":[53.929,628.733992,298.66768395400004,649.655992],"page":"4"}},"crops":{"1":{"crop_coord":[886.9277613888887,626.441116388889,1572.0744152777777,1156.8666586111112],"bbox":[321.0939941,377.3280029,564.1467895,564.6811981],"page":"1"},"2":{"crop_coord":[886.9277613888887,324.19112305555575,1572.0744152777777,713.961105277778],"bbox":[321.0939941,536.7740021,564.1467895,673.4911956999999],"page":"2"},"3":{"crop_coord":[900.2666558333333,195.90001416666667,1227.6000044444443,384.3999988888887],"bbox":[325.8959961,655.4160004,440.1360016,719.6759949],"page":"3"},"4":{"crop_coord":[1231.4444308333334,195.90001416666667,1558.7777794444441,384.3999988888887],"bbox":[445.1199951,655.4160004,559.3600005999999,719.6759949],"page":"3"},"5":{"crop_coord":[900.2666558333333,440.81392083333316,1227.6000044444443,629.3139055555556],"bbox":[325.8959961,567.246994,440.1360016,631.5069885],"page":"3"},"6":{"crop_coord":[1231.4444308333334,440.81392083333316,1558.7777794444441,629.3139055555556],"bbox":[445.1199951,567.246994,559.3600005999999,631.5069885],"page":"3"},"7":{"crop_coord":[900.2666558333333,685.727785,1227.6000044444443,874.2277697222221],"bbox":[325.8959961,479.0780029,440.1360016,543.3379974],"page":"3"},"8":{"crop_coord":[1231.4444308333334,685.727785,1558.7777794444441,874.2277697222221],"bbox":[445.1199951,479.0780029,559.3600005999999,543.3379974],"page":"3"},"9":{"crop_coord":[161.68332416666664,168.23443111111112,813.0470613888889,373.1166755555554],"bbox":[60.0059967,659.4779968,290.8969421,729.6356048],"page":"4"}}}},{"filename":"What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics.pdf","paper_id":"What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics","venue":"chi2019xr","keywords":["Augmented Reality","Physics Education","Collaborative Learning"],"paragraph_containing_keyword":"KEYWORDS \nAugmented Reality; Physics Education; Collaborative Learning","paragraph_after_keyword":"ACM Reference Format \nIulian  Radu  and  Bertrand  Schneider.  2019. What  Can  We  Learn  from \nAugmented  Reality  (AR)?  Benefits  and  Drawbacks  of  AR  for  Inquiry-\nbased Learning of Physics. In 2019 CHI Conference on Human Factors in \nPermission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed \nfor  profit  or  commercial  advantage  and  that  copies  bear  this  notice  and  the  full \ncitation on the first page. Copyrights for components of this work owned by others \nthan ACM must be honored. Abstracting with credit is permitted. To copy otherwise, \nor  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior  specific \npermission and/or a fee. Request permissions from Permissions@acm.org. \nCHI 2019, May 4–9, 2019, Glasgow, Scotland Uk  \n© 2019 Association for Computing Machinery. \nACM ISBN 978-1-4503-5970-2/19/05...$15.00  \nhttps://doi.org/10.1145/3290605.3300774","doi":"10.1145/3290605.3300774","sections":[{"word_count":995,"figure_citations":{},"section_index":0,"title":"INTRODUCTION"},{"word_count":290,"figure_citations":{},"section_index":1,"title":"RELATED WORK"},{"word_count":668,"figure_citations":{},"section_index":2,"title":"SYSTEM DESIGN"},{"word_count":397,"figure_citations":{},"section_index":3,"title":"RESEARCH QUESTIONS AND STUDY DESIGN"},{"word_count":380,"figure_citations":{},"section_index":4,"title":"METHODS"},{"word_count":665,"figure_citations":{"3":["Figure 3 question 5), electricity and movement (e.","Figure 3 question 6), electricity and magnetic field (e.","Figure 3 question 3) The concept of sequential reasoning indicates the style in which participants answered the open-ended question of “How is electrical energy turned into sound inside the speaker?” A large number of responses included a narrative which explained the connection between different components as a sequence (Figure 3 q1 top) rather than directly explaining the core physics phenomena driving the speaker."]},"section_index":5,"title":"AR"},{"word_count":1323,"figure_citations":{},"section_index":6,"title":"RESULTS"},{"word_count":1452,"figure_citations":{},"section_index":7,"title":"DISCUSSION"},{"word_count":60,"figure_citations":{},"section_index":8,"title":"ACKNOWLEDGEMENTS"},{"word_count":1185,"figure_citations":{},"section_index":9,"title":"REFERENCES"}],"title":"What Can We Learn from Augmented Reality (AR)?: Benefits and Drawbacks of AR for Inquiry-based Learning of Physics","authors":"Iulian Radu, Bertrand Schneider","abstract":"Emerging technologies such as Augmented Reality (AR), have the potential to radically transform education by making challenging concepts visible and accessible to novices. In this project, we have designed a Hololens-based system in which collaborators are exposed to an unstructured learning activity in which they learned about the invisible physics involved in audio speakers. They learned topics ranging from spatial knowledge, such as shape of magnetic fields, to abstract conceptual knowledge, such as relationships between electricity and magnetism. We compared participants' learning, attitudes and collaboration with a tangible interface through multiple experimental conditions containing varying layers of AR information. We found that educational AR representations were beneficial for learning specific knowledge and increasing participants' self-efficacy (i.e., their ability to learn concepts in physics). However, we also found that participants in conditions that did not contain AR educational content, learned some concepts better than other groups and became more curious about physics. We discuss learning and collaboration differences, as well as benefits and detriments of implementing augmented reality for unstructured learning activities.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure  1.  The  Augmented  Reality  system  developed  for \nthis  project  (top  image:  two  users  interacting  with  the \nspeaker activity; bottom image: the magnetic fields around \nthe  coil  and  the  magnet  that  are  generating  the  sound \nwaves). \n","bbox":[318,94.44599999999997,560.3415744900003,144.486],"page":"3"},"2":{"caption":"Figure  2.  Changing  AR  visualizations  of  electricity, \namplification, and magnetic fields overlaid on the physical \nobject.  (Note,  AR  visualization  misalignments  are  due  to \nthe photo camera, and not visible to participants) \n","bbox":[53.99630729999997,485.03508,296.29296728999986,528.1859999999999],"page":"4"},"3":{"caption":"Figure 3. Examples of questions from the learning test.  \n","bbox":[54.00371639999997,115.62299999999998,281.8855167600002,124.62299999999998],"page":"6"},"4":{"caption":"Figure  4.  Information  shown  on  the  physical  poster  in \nfront of the participants.  \n","bbox":[318.00361639999994,216.72600000000003,560.3146490299999,235.98600000000002],"page":"6"},"6":{"caption":"Figure  6.  Group  differences  in  relative  learning  gains \n(Range 0-1; Red = EdAR group, Blue = NoEdAR group; Bars \n= standard error) \n","bbox":[318.00376070000004,154.69500000000005,560.3740804699999,186.24900000000002],"page":"8"}},"crops":{"1":{"crop_coord":[877.6666666666666,997,1559.0000000000002,1799],"bbox":[317.76,146.16,559.44,431.28],"page":"3"},"2":{"crop_coord":[144.33333333333334,865.6666666666664,823.6666666666667,1236.333333333333],"bbox":[53.76,348.72,294.72,478.56000000000006],"page":"4"},"3":{"crop_coord":[144.33333333333334,1227.6666666666667,823.6666666666667,1652.3333333333333],"bbox":[53.76,198.96,294.72,348.24],"page":"4"},"4":{"crop_coord":[144.33333333333334,393.6666666666666,825.6666666666666,588.3333333333333],"bbox":[53.76,582,295.44,648.48],"page":"6"},"5":{"crop_coord":[144.33333333333334,579.6666666666666,825.6666666666666,764.3333333333335],"bbox":[53.76,518.64,295.44,581.52],"page":"6"},"6":{"crop_coord":[144.33333333333334,757.6666666666664,825.6666666666666,862.3333333333333],"bbox":[53.76,483.36,295.44,517.44],"page":"6"},"7":{"crop_coord":[144.33333333333334,853.6666666666666,825.6666666666666,954.9999999999999],"bbox":[53.76,450,295.44,482.88],"page":"6"},"8":{"crop_coord":[144.33333333333334,948.3333333333334,825.6666666666666,1043],"bbox":[53.76,418.32,295.44,448.8],"page":"6"},"9":{"crop_coord":[144.33333333333334,1034.3333333333333,825.6666666666666,1287.6666666666665],"bbox":[53.76,330.24,295.44,417.84000000000003],"page":"6"},"10":{"crop_coord":[144.33333333333334,1279.0000000000002,825.6666666666666,1557],"bbox":[53.76,233.28,295.44,329.76],"page":"6"},"11":{"crop_coord":[144.33333333333334,1548.3333333333333,825.6666666666666,1853],"bbox":[53.76,126.72,295.44,232.8],"page":"6"},"12":{"crop_coord":[877.6666666666666,885,1559.0000000000002,1505.0000000000002],"bbox":[317.76,252,559.44,471.6],"page":"6"},"13":{"crop_coord":[144.33333333333334,811.6666666666666,822.3333333333334,1343.6666666666667],"bbox":[53.76,310.08,294.24,498],"page":"8"},"14":{"crop_coord":[878.3333333333334,1237.1666666666665,1555,1664.6666666666667],"bbox":[318,194.52,558,344.82000000000005],"page":"8"}}}},{"filename":"What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom","data":{"fullpath":"/Users/takyeonlee/Documents/work/_수업_2021/ID503_Design_Project_1/_PaperVis/papers/chi2019xr/What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom.pdf","paper_id":"What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom","venue":"chi2019xr","keywords":["Augmented reality","Pen-based interactions","Annotation","Projection mapping","Educational system"],"doi":"10.1145/3290605.3300464","paragraph_containing_keyword":"KEYWORDS\nAugmented reality, pen-based interactions, annotation, pro-\njection mapping, educational system.\nACM Reference Format:\nHasan Shahid Ferdous, Thuong Hoang, Zaher Joukhadar, Martin N\nReinoso, Frank Vetere, David Kelly, Louisa Remedios. 2019. “What’s\nHappening at that Hip?”: Evaluating an On-body Projection based\nAugmented Reality System for Physiotherapy Classroom. In Pro-\nceedings of CHI Conference on Human Factors in Computing Sys-\ntems Proceedings (CHI 2019). ACM, New York, NY, USA, 12 pages.\nhttps://doi.org/10.1145/3290605.3300464","paragraph_after_keyword":"1 INTRODUCTION\nAugmented reality (AR) systems have been adopted as a pow-\nerful tool to enhance the teaching and learning experience\nin many classrooms. AR systems overlay virtual information\ndirectly on the physical world, offering great potential to im-\nprove training and educational outcomes [6, 20, 25]. These\nsystems often leverage virtual 3D visualizations to make diffi-\ncult ideas more comprehensible [25]. They can transform the\nphysical space into interactive venues, where otherwise inac-\ncessible digital information can be observed and analyzed [6].\nAR visualization in instructional settings can also improve\ntask completion times and lead to fewer errors [20].\nWhile providing a novel visualization capability through\noverlaying digital information with real world artifacts, these\nsystems typically prioritize display capability rather than in-\nteractivity. A systematic review of AR trends in education\n[3] shows that the majority of AR educational systems focus\non visualization by aiming to “augment information” (40.6%)\nand/or “explain a topic” (43.7%), as compared to support “lab\nexperiments” (12.5%) or “exploration” (3.13%) where interac-\ntions with the contents are encouraged.\nBillinghurst et al. [4] outline the promise of hybrid user\ninterfaces [7] that combine AR with other interactive tech-\nnologies such as virtual reality (VR) and pen-based devices.","sections":[{"word_count":667,"figure_citations":{"1":["Figure 1)."]},"section_index":0,"title":"INTRODUCTION"},{"word_count":752,"figure_citations":{"1":["Figure 1(b)) as an interaction mechanism alongside the projection mapping system to create a hybrid user interface.","Figure 1(c))."],"2":["Figure 2(c)), which was then projected on the human model."]},"section_index":1,"title":"RELATED WORK"},{"word_count":216,"figure_citations":{},"section_index":2,"title":"RESEARCH DESIGN"},{"word_count":1143,"figure_citations":{"1":["Figure 1(a)).","Figure 1(b))."],"2":["Figure 2(a)).","Figure 2(c)).","Figure 2(c))."]},"section_index":3,"title":"INTERFACE"},{"word_count":1699,"figure_citations":{"2":["Figure 2(c))."]},"section_index":4,"title":"AUGMENTED BODY SYSTEM"},{"word_count":750,"figure_citations":{"2":["Figure 2(b)) and used retroreflective markers with Velcro stickers to put on the volunteer student’s clothing (Figure 1(a))."],"3":["Figure 3(a)), thus enabling the teacher and students to draw directly on the volunteer’s body and to select muscle(s) or bone(s).","Figure 3(b)) which the teacher used to select brush size, color, eraser, create groups of muscles/bones, and to selectively turn on/off the muscle and skeleton view as well as remove specific muscles to reveal the inner layers (Figure 4(a)) using the developed pen."],"4":["Figure 4(b))."]},"section_index":5,"title":"CUSTOM PEN INTERFACE"},{"word_count":2277,"figure_citations":{},"section_index":6,"title":"OUTCOMES OF AUGMENTED BODY SYSTEM"},{"word_count":1020,"figure_citations":{},"section_index":7,"title":"DISCUSSION"},{"word_count":144,"figure_citations":{},"section_index":8,"title":"CONCLUSION"},{"word_count":66,"figure_citations":{},"section_index":9,"title":"ACKNOWLEDGMENTS"},{"word_count":753,"figure_citations":{},"section_index":10,"title":"REFERENCES"}],"title":"\"What's Happening at that Hip?\": Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom","authors":"Hasan Shahid Ferdous, Thuong Hoang, Zaher Joukhadar, Martin N. Reinoso, Frank Vetere, David Kelly, Louisa Remedios","abstract":"We present two studies to discuss the design, usability analysis, and educational outcome resulting from our system Augmented Body in physiotherapy classroom. We build on prior user-centric design work that investigates existing teaching methods and discuss opportunities for intervention. We present the design and implementation of a hybrid system for physiotherapy education combining an on-body projection based virtual anatomy supplemented by pen-based tablets to create real-time annotations. We conducted a usability evaluation of this system, comparing with projection only and traditional teaching conditions. Finally, we focus on a comparative study to evaluate learning outcome among students in actual classroom settings. Our studies showed increased usage of visual representation techniques in students'11 note taking behavior and statistically significant improvement in some learning aspects. We discuss challenges for designing augmented reality systems for education, including minimizing attention split, addressing text-entry issues, and digital annotations on a moving physical body.","publication":{"venue":"N/A","venue_full":"Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","year":"2019","date":"2019/5/2"},"version":4},"fig":{"captions":{"1":{"caption":"Figure 1: Augmented Body system for physiotherapy educa-\ntion - (a) projection mapping on a student’s body, (b) pen-\nbased tablet interface for annotation, and (c) pen-based selec-\ntion/drawing on the physical body.\n","bbox":[53.79799999999997,499.9658540000001,295.642788,541.808854],"page":"2"},"2":{"caption":"Figure 2: Schematic diagram of the setup and tablet interface (a) Study one setup, (b) Study two setup, and (c) Hand drawing\nannotations (green and red strokes on the upper thigh) and text annotation (green window) in the tablet device.\n","bbox":[53.797999999999945,487.194854,558.1803299999999,507.11985400000003],"page":"4"},"3":{"caption":"Figure 3: (a) 3D printed pen, (b) Projected palette, and (c) Sam-\nple of notes taken during projection-based classes.\n","bbox":[317.95500000000004,555.8278540000001,559.794918,575.7528540000001],"page":"5"},"4":{"caption":"Figure 4: (a) Teacher could remove upper layer to reveal inner\nlayer of muscles and bones, (b) Part of the classroom.\n","bbox":[317.95500000000004,368.223854,558.37829,388.14885400000003],"page":"5"}},"crops":{"1":{"crop_coord":[149.9750011111111,281.1081441666667,336.81898333333334,623.1000180555556],"bbox":[55.7910004,569.4839935,119.454834,689.0010681],"page":"2"},"2":{"crop_coord":[372.4333530555556,280.04002888888886,616.0000186111112,623.1000180555556],"bbox":[135.8760071,569.4839935,219.9600067,689.3855896],"page":"2"},"3":{"crop_coord":[646.0888841666667,283.08881972222235,816.2616644444445,623.1000180555556],"bbox":[234.3919983,569.4839935,292.0541992,688.2880249],"page":"2"},"4":{"crop_coord":[149.9750011111111,280.6043500000001,580.3116186111112,719.4555411111112],"bbox":[55.7910004,534.7960052,207.1121827,689.182434],"page":"4"},"5":{"crop_coord":[596.3027613888888,280.2659438888891,1040.6598322222223,719.4555411111112],"bbox":[216.4689941,534.7960052,372.8375396,689.3042601999999],"page":"4"},"6":{"crop_coord":[1056.6360983333334,280.0410463888888,1550.064968611111,719.4555411111112],"bbox":[382.1889954,534.7960052,556.2233887,689.3852233],"page":"4"},"7":{"crop_coord":[883.7444388888888,280.35499555555555,980.4613663888888,528.8083308333333],"bbox":[319.947998,603.4290009,351.16609189999997,689.2722016],"page":"5"},"8":{"crop_coord":[986.6527641666667,280.034985,1216.8849861111112,528.8083308333333],"bbox":[356.9949951,603.4290009,436.278595,689.3874054],"page":"5"},"9":{"crop_coord":[1223.030565,281.11234027777766,1549.958580833333,528.8083308333333],"bbox":[442.0910034,603.4290009,556.1850890999999,688.9995575],"page":"5"},"10":{"crop_coord":[883.7444388888888,738.9934625000001,1123.9575616666668,1049.9333191666667],"bbox":[319.947998,415.8240051,402.8247222,524.1623535],"page":"5"},"11":{"crop_coord":[1139.6111044444442,739.6208105555555,1550.0277541666667,1049.9333191666667],"bbox":[412.0599976,415.8240051,556.2099915,523.9365082],"page":"5"}}}}]