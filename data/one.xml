<items>
    <article>
        <filename>2662155.2662246</filename>
        <data>
            <paper_id>2662155.2662246</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376794</doi>
            <title>Body Follows Eye: Unobtrusive Posture Manipulation Through a Dynamic Content Position in Virtual Reality</title>
            <abstract>
                While virtual objects are likely to be a part of future interfaces, we lack knowledge of how the dynamic position of virtual objects influences users' posture. In this study, we investigated users' posture change following the unobtrusive and swift motions of a content window in virtual reality (VR). In two perception studies, we estimated the perception threshold on undetectable slow motions and displacement during an eye blink. In a formative study, we compared users' performance, posture change as well as subjective responses on unobtrusive, swift, and no motions. Based on the result, we designed concept applications and explored potential design space of moving virtual content for unobtrusive posture change. With our study, we discuss the interfaces that control users and the initial design guidelines of unobtrusive posture manipulation.
            </abstract>
            <sections>
                <section>
                    <word_count>75</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>KAIST</title>
                </section>
                <section>
                    <word_count>893</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>921</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>3666</word_count>
                    <figure_citations>Figure 2).Figure 3, we placed the target object 2 m from the eyes and six degrees below the eye level based on an ergonomic recommendation [49].Figure 5 shows the positive responses means and standard errors, and the ﬁtted psychometric func- Paper 665 tion.Figure 7), we compared the three conditions (unnoticeable slow motion, single swift motion, and no motion) on three distinct manipulations left (1.</figure_citations>
                    <section_index>3</section_index>
                    <title>PERCEPTION THRESHOLD STUDY</title>
                </section>
                <section>
                    <word_count>913</word_count>
                    <figure_citations>Figure 8).</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>458</word_count>
                    <figure_citations>Figure 9) to explore the design space of interfaces that induce posture changes.</figure_citations>
                    <section_index>5</section_index>
                    <title>APPLICATIONS</title>
                </section>
                <section>
                    <word_count>636</word_count>
                    <figure_citations>Figure 8, there is a range that the participants engage continuous neck rotation and beyond that range with abrupt body movements (e.</figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>330</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>34</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>273</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
                <section>
                    <word_count>2098</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>CANADIAN INFORMATION PROCESSING</title>
                </section>
            </sections>
            <keywords>
                <li>Posture change</li>
                <li>Unobtrusive interaction</li>
                <li>Virtual Reality</li>
            </keywords>
            <authors>
                <li>Joon Gi Shin</li>
                <li> Doheon Kim</li>
                <li> Chaehan So</li>
                <li> Daniel Saakes</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>2662155.2662246_crop_1.jpg</url>
                <caption>Figure 1. A virtual window moved at slow speed and during an eye blink to unobtrusively manipulate users’ posture. With two perception threshold
                    studies, We estimated unnoticeable motion speed and displacement per blink. The formative study showed that unobtrusive motions triggered body
                    movements in a sequence (neck, torso, and chair) without affecting users’ performance.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>2662155.2662246_crop_2.jpg</url>
                <caption>Figure 2. We tested six blocks of motion: three translations {BTX
                    (left, right), BTY (up, down), BTZ (forward, backward)} and three ro-
                    tations {BRX (up, down), BRY (left, right), BRZ (clockwise, counter-
                    clockwise)}.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>2662155.2662246_crop_3.jpg</url>
                <caption>Figure 3. Based on ergonomics guidelines, we positioned the virtual window within a comfortable viewing distance and angle. The 0.73 by 0.73 m
                    window was placed at 6 degrees below and 2 meters away from the eyes.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>2662155.2662246_crop_4.jpg</url>
                <caption>Figure 4. The plotted result of users’ detection threshold on constant slow translations (up) and rotations (bottom): The x-axis shows motion speeds,
                    and the y-axis shows the probability of the participants’ responses for observed motion direction. The psychometric function shows upper threshold (p
                    = 75%) for positive direction and lower threshold (p = 25%) for negative direction. Speeds within the blue area are considered unobtrusive.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>2662155.2662246_crop_5.jpg</url>
                <caption>Figure 5. The plotted result of users’ detection threshold on the translations (up) and rotations (down) during a blink: Displacements within the blue
                    area are considered unobtrusive.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>2662155.2662246_crop_6.jpg</url>
                <caption>Figure 6. We measured the participants’ IPD with a wearable ruler and
                    adjusted the HMD (top). We logged the participants’ motion with HMD
                    and each tracker on their chest as well as the chair (bottom).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>2662155.2662246_crop_7.jpg</url>
                <caption>Figure 7. We manipulated participants’ posture from upright sitting
                    to leaning left (A), forward (B), and backward (C). For each posture
                    manipulation, we compared three conditions: not-moving as the baseline
                    behavior, swift motion, and unobtrusive motion.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>8</id>
                <url>2662155.2662246_crop_8.jpg</url>
                <caption>Figure 8. We plotted the motion data of a representative participant for each motion: Left translation (top), receding (middle), and rotating up (bottom).
                    During the left translation and up rotation, unobtrusive motions triggered continuous neck and back movements. The swift motion triggered abrupt
                    and mixed body movements. During the receding motions, the swift motion triggered a couple of chair movements in big steps, whereas the unobtrusive
                    motion triggered several chair movements in smaller steps.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>9</id>
                <url>2662155.2662246_crop_9.jpg</url>
                <caption>Figure 9. We designed three concept applications. The ﬁrst application
                    makes a user rollover from prone to supine and to lying position (top).
                    The second application coerces a user to stand up (left). The third appli-
                    cation triggers active sitting (right).
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173587</filename>
        <data>
            <paper_id>3173574.3173587</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173587</doi>
            <title>MABLE: Mediating Young Children's Smart Media Usage with Augmented Reality</title>
            <abstract>
                There has been a growing concern over the huge increase in use of smart media by young children. This study explores the possibility of using augmented-reality(AR) for regulat-ing preschoolers' media usage behavior. With MABLE (mobile application for behavioral learning and education), parents can provide AR-assisted feedback by changing facial expressions and sound effects. When overlaying a smart media, which has MABLE running, in front of a QR marker on a puppet, a facial expression is displayed on top of the puppet's face. A two-week long experiment with 36 parent-child pairs showed that compared to using just the puppet, using MABLE showed higher amount of engage-ment among preschoolers. For the effectiveness of parental mediation in terms of self-control, our data showed mixed results. MABLE had positive effects in that the amount of rule-compliance increased and problematic behaviors de-creased, whereas the level of behavioral dependency on smart media was not influenced.
            </abstract>
            <sections>
                <section>
                    <word_count>526</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1486</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>712</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>METHODS</title>
                </section>
                <section>
                    <word_count>1356</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1411</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSIONS</title>
                </section>
                <section>
                    <word_count>492</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>LIMITATIONS OF THE STUDY</title>
                </section>
                <section>
                    <word_count>18</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>273</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
                <section>
                    <word_count>841</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
            </sections>
            <keywords>
                <li>Preschoolers</li>
                <li>Rule compliance</li>
                <li>Engagement</li>
                <li>Parental mediation</li>
                <li>Augmented reality (AR)</li>
                <li>Smart media usage</li>
            </keywords>
            <authors>
                <li>Gahgene Gweon</li>
                <li> Bugeun Kim</li>
                <li> Jinyoung Kim</li>
                <li> Kung Jin Lee</li>
                <li> Jungwook Rhim</li>
                <li> Jueun Choi</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173587_crop_1.jpg</url>
                <caption>Figure 1. Experimental Study Procedure and Measurement
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173587_crop_2.jpg</url>
                <caption>Figure 2. Facial expressions that provided by MABLE
                </caption>
                <page>3</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173616</filename>
        <data>
            <paper_id>3173574.3173616</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173616</doi>
            <title>Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality</title>
            <abstract>
                Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.
            </abstract>
            <sections>
                <section>
                    <word_count>748</word_count>
                    <figure_citations>Figure 1 illustrates such an interaction.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>5118</word_count>
                    <figure_citations>Figure 1, we sampled 60 positions on a circular sphere surface around the user’s body.Figure 2 shows the setting of the experiment and the task interface.Figure 3 summarizes the result on different horizontal and vertical angle levels.Figure 3) and participants could see the targets when acquiring them.Figure 4 shows, the minimum distance between targets and comfort levels distributed symmetrically on both sides.Figure 4, we found a negative correlation between the distance and the rating, supported by a Point-Biserial test (correlation = -0.Figure 5, the user rotated from the left to the right, and the relative horizontal angle of the target to the user changed from alpha to phi.Figure 6 visualized the main results of this experiment.Figure 7(c) visualizes the difference of the averaged offsets for twelve horizontal angles of the target positions.Figure 7(d) shows, the spatial offset of the acquisition increased with the number of rotations users performed before the acquisition.Figure 7(a), along the horizontal angle of the target position, the horizontal angular offset distributed symmetrically about the point of 0 degrees.Figure 7(c) shows, similar to the spatial offset, the movement amplitude increased as the target positions changed from 0 degrees in the front to both sides.Figure 8 shows the interpolation result of the standard deviations, which designers could refer to arrange target locations for eyes-free target acquisitions.</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>2009</word_count>
                    <figure_citations>Figure 9 shows, the targets were rendered as two 3×3 grids of spheres out of view, with one on each side of the user’s body.Figure 9 shows, the spheres were arranged as two three by three grids on both sides of users, which had equal distance (65cm) to users’ chest.Figure 11 visualizes the subjective ratings in all dimensions.</figure_citations>
                    <section_index>2</section_index>
                    <title>TARGET ACQUISITION</title>
                </section>
                <section>
                    <word_count>457</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>197</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>LIMITATION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>191</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>59</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGEMENT</title>
                </section>
                <section>
                    <word_count>1780</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Target acquisition</li>
                <li>Eyes-free</li>
                <li>Proprioception</li>
            </keywords>
            <authors>
                <li>Yukang Yan</li>
                <li> Chun Yu</li>
                <li> Xiaojuan Ma</li>
                <li> Shuai Huang</li>
                <li> Hasan Iqbal</li>
                <li> Yuanchun Shi</li>
            </authors>
        </data>
        <figures>
            <figure>
                <url>3173574.3173616_crop_1.jpg</url>
                <caption>Figure 1. An illustrative scenario of eyes-free target acquisition in vir-
                    tual reality. A user is doing design and he fetches tools in the interaction
                    space around the body in an eyes-free manner. The FOV (ﬁeld of view)
                    size of his HMD is visualized (110 - 120 degrees).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173616_crop_2.jpg</url>
                <caption>Figure 2. The setting of the experiment and the task interface in Study1.
                    A user moved the controller to acquire the virtual sphere in the 5 × 5
                    grid. Then he turned head to see which sphere he had acquired (the red
                    one).
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173616_crop_3.jpg</url>
                <caption>Figure 3. The circles summarize the comfort level (color) and the mini-
                    mum distance between targets (radius) for different target positions (cen-
                    ters) when users acquired them.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173616_crop_4.jpg</url>
                <caption>Figure 4. The averaged distances between targets and comfort ratings of
                    the target positions with different horizontal angles.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173616_crop_5.jpg</url>
                <caption>Figure 5. The concept of the experiment settings. The furniture indicates
                    the virtual surroundings of the participants. The red spheres indicate
                    the twelve directions that the participants rotate to, and the green sphere
                    indicates the positioned target.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3173616_crop_6.jpg</url>
                <caption>Figure 6. Summary of the main results of this experiment. The centers of
                    the circles are the averaged positions of twelve acquisitions of 24 users
                    and radiuses are the standard deviations. The blue lines visualize the
                    offsets of the average positions from the target’s actual positions. All
                    coordinates and lengths are converted to angles in degrees.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3173616_crop_7.jpg</url>
                <caption>Figure 7. (a) The horizontal angular offset of the acquisitions at differ-
                    ent horizontal angles; (b) The vertical angular offset of the acquisitions
                    at different horizontal angles; (c) The spatial offset and head movement
                    amplitudes of the acquisitions at different horizontal angles; (d) The spa-
                    tial offset of the acquisitions after different times of rotations.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3173616_crop_8.jpg</url>
                <caption>Figure 8. The interpolation of the standard deviations of the target ac-
                    quisition points on the whole surface, visualized into a heat map.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3173574.3173616_crop_9.jpg</url>
                <caption>Figure 9. The concept of the experiment setting. Targets were located on
                    both sides of users’ body. A shortcut of the target layout was visualized
                    in the front, as well as the character for the second task.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3173574.3173616_crop_10.jpg</url>
                <caption>Figure 10. The speed (average acquisition time) and accuracy (rate) re-
                    sults of the acquisitions in different conditions. The error bars represent
                    the standard deviations.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3173574.3173616_crop_11.jpg</url>
                <caption>Figure 11. User’s subjective ratings for the experience using two ap-
                    proaches to acquire targets. The error bars represent the standard devi-
                    ations.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173620</filename>
        <data>
            <paper_id>3173574.3173620</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173620</doi>
            <title>Mini-Me: An Adaptive Avatar for Mixed Reality Remote Collaboration</title>
            <abstract>
                We present Mini-Me, an adaptive avatar for enhancing Mixed Reality (MR) remote collaboration between a local Augmented Reality (AR) user and a remote Virtual Reality (VR) user. The Mini-Me avatar represents the VR user's gaze direction and body gestures while it transforms in size and orientation to stay within the AR user's field of view. A user study was conducted to evaluate Mini-Me in two collaborative scenarios: an asymmetric remote expert in VR assisting a local worker in AR, and a symmetric collaboration in urban planning. We found that the presence of the Mini-Me significantly improved Social Presence and the overall experience of MR collaboration.
            </abstract>
            <sections>
                <section>
                    <word_count>502</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>3685</word_count>
                    <figure_citations>Figure 2a and 2b).Figure 2c and 2d).Figure 2e and 2f).Figure 3 shows an example of this with an application named “Snow Dome,” a remote MR collaboration application that demonstrates how AR and VR collaboration can be enhanced with multi-scale interaction.Figure 3a.Figure 3b.</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>3526</word_count>
                    <figure_citations>Figure 3d and 3e show the original AR space and the result of the reconstruction showed to the VR user.Figure 4).Figure 4).Figure 4).</figure_citations>
                    <section_index>2</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>610</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>257</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>CONCLUSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>23</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1623</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Mixed reality</li>
                <li>Remote collaboration</li>
                <li>Augmented reality</li>
                <li>Virtual reality</li>
                <li>Remote embodiment</li>
                <li>Avatar</li>
                <li>Redirected</li>
                <li>Gaze</li>
                <li>Gesture</li>
                <li>Awareness</li>
            </keywords>
            <authors>
                <li>Thammathip Piumsomboon</li>
                <li> Gun A. Lee</li>
                <li> Jonathon D. Hart</li>
                <li> Barrett Ens</li>
                <li> Robert W. Lindeman</li>
                <li> Bruce H. Thomas</li>
                <li> Mark Billinghurst</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173620_crop_1.jpg</url>
                <caption>Figure 1. Mini-Me Concept: (a) MR view through the HoloLens of a remote VR user’s Mini-Me avatar standing on a table and a
                    life-size avatar standing on the right, both avatars gaze toward the same place and point at the same target with gaze and pointing
                    of the Mini-Me avatar redirected, (b) A male (VR user) and a female (AR user) avatars in VR, (c) Design features of the Mini-Me
                    avatar, (d) The local AR user moving a tea box, (e) + (f) The remote VR user is pointing at a box and the Mini-Me in the AR user’s
                    FOV has its gaze and pointing gesture redirected to the same box (g) A remote VR user is pointing.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173620_crop_2.jpg</url>
                <caption>Figure 2: The AR user views the Mini-Me from two different perspectives showing Mini-Me consistently gazing and pointing at the
                    same location: a) in front of the whiteboard, b) side view of the whiteboard, c) As the AR user gazes at the remote VR user’s life-
                    size avatar, the Mini-Me moves toward this avatar and d) fuses with it and disappears, e) AR user can gaze at the Mini-Me and
                    perform an air-tap to pin it in place or f) Tap again to unpin it from that location.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173620_crop_3.jpg</url>
                <caption>Figure 3: a) Scaled down VR user’s perspective seeing the AR user as a giant, b) VR user shrunk down interacting inside the
                    miniature dome, c) VR user is a giant looking down at the AR reconstructed space, d) The real experimental space for the AR user,
                    and e) its virtual reconstruction for the VR user
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173620_crop_4.jpg</url>
                <caption>Figure 4. Baseline condition illustrating how the remote VR user’s avatar look through a HoloLens. (a-d) Tea Party task (a) VR
                    user asks the AR user to pick up a tea box (b) the AR user follows the pointing ray from the avatar’s hand to a correct tea box with
                    a virtual replica overlaid on the physical box (c) VR user points at the shelf to place the box (d) AR user adjusts the orientation of
                    the box as the VR user instructs (e-h) Urban Planner task (e) VR view showing VR and AR users sitting side by side (f) AR user
                    looks at the VR user’s avatar (g) VR user is pointing at the building at the top while AR user is pointing at the one at the bottom (h)
                    AR user looks at a physical model that the VR user is pointing. The gaze cursors of both users can be seen here.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173620_crop_5.jpg</url>
                <caption>Figure 5. Task completion time.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173628</filename>
        <data>
            <paper_id>3173574.3173628</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173628</doi>
            <title>FaceDisplay: Towards Asymmetric Multi-User Interaction for Nomadic Virtual Reality</title>
            <abstract>
                Mobile VR HMDs enable scenarios where they are being used in public, excluding all the people in the surrounding (Non-HMD Users) and reducing them to be sole bystanders. We present FaceDisplay, a modified VR HMD consisting of three touch sensitive displays and a depth camera attached to its back. People in the surrounding can perceive the virtual world through the displays and interact with the HMD user via touch or gestures. To further explore the design space of FaceDisplay, we implemented three applications (FruitSlicer, SpaceFace and Conductor) each presenting different sets of aspects of the asymmetric co-located interaction (e.g. gestures vs touch). We conducted an exploratory user study (n=16), observing pairs of people experiencing two of the applications and showing a high level of enjoyment and social interaction with and without an HMD. Based on the findings we derive design considerations for asymmetric co-located VR applications and argue that VR HMDs are currently designed having only the HMD user in mind but should also include Non-HMD Users.
            </abstract>
            <sections>
                <section>
                    <word_count>1075</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>783</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1714</word_count>
                    <figure_citations>Figure 4 shows the interaction/engagement gradient starting from the most engaged (touch) to the least engaged (observing).Figure 4 a,b).Figure 4 c,d) enables additional applications without the need to battle Paper 54 exclusion and isolation (e.</figure_citations>
                    <section_index>2</section_index>
                    <title>FACEDISPLAY</title>
                </section>
                <section>
                    <word_count>1312</word_count>
                    <figure_citations>Figure 7 a).</figure_citations>
                    <section_index>3</section_index>
                    <title>APPLICATIONS</title>
                </section>
                <section>
                    <word_count>829</word_count>
                    <figure_citations>Figure 8 summarizes the scores of the GEQ, SUS and SAM and Figure 9 shows responses for our own questions.</figure_citations>
                    <section_index>4</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>54</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>HMD</title>
                </section>
                <section>
                    <word_count>1148</word_count>
                    <figure_citations>Figure 10).</figure_citations>
                    <section_index>6</section_index>
                    <title>NHMD</title>
                </section>
                <section>
                    <word_count>999</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>334</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>44</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1732</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Nomadic virtual reality</li>
                <li>Asymmetric virtual reality</li>
                <li>Multi-user virtual reality</li>
                <li>Co-located multiplayer</li>
            </keywords>
            <authors>
                <li>Jan Gugenheimer</li>
                <li> Evgeny Stemasov</li>
                <li> Harpreet Sareen</li>
                <li> Enrico Rukzio</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173628_crop_1.jpg</url>
                <caption>Figure 1. FaceDisplay is a modified VR HMD consisting of three touch sensitive displays and a depth camera attached to its back (a-c). This allows people in
                    the surrounding to perceive the virtual world through the displays and interact with the HMD user either through touch (e) or gestures (d).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173628_crop_2.jpg</url>
                <caption>Figure 2. The hardware prototype of FaceDisplay, consisting of three
                    touchscreens and a Leap Motion depth camera attached to the back and
                    the sides of an Oculus Rift DK2
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173628_crop_3.jpg</url>
                <caption>Figure 3. The technical setup used to reduce the weight of the FaceDisplay
                    prototype, using a key retractor (a) and a pair of springs (b).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173628_crop_4.jpg</url>
                <caption>Figure 4. Interaction Gradient for FaceDisplay. Starting from the most engaged a: touch to b:gesture, c: external device and d: observing.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173628_crop_5.jpg</url>
                <caption>Figure 5. The FruitSlicer application with its outside view (a), inside view
                    (b), interaction concepts (c) and visualization metaphor (d).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3173628_crop_6.jpg</url>
                <caption>Figure 6. The SpaceFace application and its outside view (a), inside view (b)
                    interaction and visualization concept (c) and physical interaction scenario
                    (d).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3173628_crop_7.jpg</url>
                <caption>Figure 7. The Conductor application showing its outside view (a), inside
                    view (b), hand tracking region (c) and interaction scenario (d).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3173628_crop_8.jpg</url>
                <caption>Figure 8. The distribution of our data from (a) the GEQ In-Game Module, GEQ Social Module, the SUS and (b) the SAM questionnaire. All bar charts
                    showing the mean with standard deviation.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3173574.3173628_crop_9.jpg</url>
                <caption>Figure 9. Boxplots of our own questions on discomfort "I felt uncomfortable
                    touching/being touched/gesturing/being gestured at", understanding "I was
                    always able to understand the current state of the game" and agency "I was
                    always able to influence the outcome of the game"
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3173574.3173628_crop_10.jpg</url>
                <caption>Figure 10. A variety of physical interaction poses participants used during the study emphasizing the vast possibilities of physical interaction arising from
                    SpaceFace: (a) The Kraken: The Non-HMD User abused his power and wraps around the HMD User to restrict his motions. (b) The Leg-press: the HMD User
                    utilizes his legs to either find or push the Non-HMD User away. (d) The Hedgehog: the HMD User rolls in like a hedgehog to hide from the attacks.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173638</filename><data>
            <paper_id>3173574.3173638</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173638</doi>
            <title>Depth Conflict Reduction for Stereo VR Video Interfaces</title>
            <abstract>
                Applications for viewing and editing 360° video often render user interface (UI) elements on top of the video. For stereoscopic video, in which the perceived depth varies over the image, the perceived depth of the video can conflict with that of the UI elements, creating discomfort and making it hard to shift focus. To address this problem, we explore two new techniques that adjust the UI rendering based on the video content. The first technique dynamically adjusts the perceived depth of the UI to avoid depth conflict, and the second blurs the video in a halo around the UI. We conduct a user study to assess the effectiveness of these techniques in two stereoscopic VR video tasks: video watching with subtitles, and video search.
            </abstract>
            <sections>
                <section>
                    <word_count>665</word_count>
                    <figure_citations>Figure 1), there is a conflict between the stereopsis depth cue and the occlusion depth cue.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>447</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>382</word_count>
                    <figure_citations>Figure 1 Left).</figure_citations>
                    <section_index>2</section_index>
                    <title>DEPTH CONFLICTS IN VR VIDEO INTERFACES</title>
                </section>
                <section>
                    <word_count>118</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>TECHNIQUES CONSIDERED</title>
                </section>
                <section>
                    <word_count>10</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>A</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>B</title>
                </section>
                <section>
                    <word_count>1400</word_count>
                    <figure_citations>Figure 1, the current scene is still not geometrically consistent.Figure 2: Overview of Dynamic Depth.Figure 2A).Figure 2C).Figure 3: Halo Blur blurs the video content around the UI.Figure 3).Figure 4: Illustration of the Subtitles task (Left) and the Search task (Right).</figure_citations>
                    <section_index>6</section_index>
                    <title>C</title>
                </section>
                <section>
                    <word_count>2558</word_count>
                    <figure_citations>Figure 4 Left), participants were asked to watch videos with subtitles.Figure 4 Right), participants were asked to search for a target scene as quickly and accurately as possible.Figure 5 summarizes the ratings in both tasks.Figure 5: (Left) Summary of participants’ ratings to the subjective questionnaire in both tasks.Figure 5).</figure_citations>
                    <section_index>7</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>173</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>39</word_count>
                    <figure_citations>Figure 1, 2, and 3 use images from YouTube users Kevin Kunze under a Creative Commons license.</figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGEMENT</title>
                </section>
                <section>
                    <word_count>1462</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Stereoscopic</li>
                <li>360</li>
                <li>Subtitles</li>
                <li>Video interface</li>
            </keywords>
            <authors>
                <li>Cuong Nguyen</li>
                <li> Stephen DiVerdi</li>
                <li> Aaron Hertzmann</li>
                <li> Feng Liu</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173638_crop_1.jpg</url>
                <caption>Figure 1: Depth conﬂict illustration. An interface (the video
                    player) overlays a video object (the actress) but is actually
                    behind her in depth. Left: The resulting graphics are uncom-
                    fortable to view in VR (e.g., the areas behind the text 00:01 in
                    the insets are different between the left and right views). View-
                    ers may also experience difﬁculty changing focus between the
                    interface and the video. Right: illustration of the conﬂicting
                    depth cues perceived by the same viewer in the same view.
                    © Kevin Kunze
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173638_crop_2.jpg</url>
                <caption>Figure 2: Overview of Dynamic Depth. (A) We pre-process the input video to ﬁnd feature points and left/right disparities (e.g.,
                    green lines on the actress) (B) Features points are mapped to the VR view and shown as the green dots (only for illustrative
                    purposes). Dynamic Depth estimates the perceived depth of the video based on these points. It detects when depth conﬂict occurs
                    by comparing the depths between the UI and the video. (C) Dynamic Depth moves the UI closer to the viewer to reduce depth
                    conﬂicts. Notice in the insets that the areas around the interface’s corner are more geometrically consistent compared to the same
                    scene in Figure 1. © Kevin Kunze
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173638_crop_3.jpg</url>
                <caption>Figure 3: Halo Blur blurs the video content around the UI.
                    Insets: compared to the same scene in Figure 1, the current
                    scene is still not geometrically consistent. However, the blur
                    effects mask high-frequency spatial information in the video
                    and make the details from the UI clearer. © Kevin Kunze
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173638_crop_4.jpg</url>
                <caption>Figure 4: Illustration of the Subtitles task (Left) and the Search
                    task (Right). Please refer to the video demo for a better assess-
                    ment. © Kevin Kunze
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173638_crop_5.jpg</url>
                <caption>Figure 5: (Left) Summary of participants’ ratings to the subjective questionnaire in both tasks. (Middle and Right) Task time and
                    task error summary of the Search task.
                </caption>
                <page>6</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173673</filename>
        <data>
            <paper_id>3173574.3173673</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173673</doi>
            <title>Object Manipulation in Virtual Reality Under Increasing Levels of Translational Gain</title>
            <abstract>
                Room-scale Virtual Reality (VR) has become an affordable consumer reality, with applications ranging from entertainment to productivity. However, the limited physical space available for room-scale VR in the typical home or office environment poses a significant problem. To solve this, physical spaces can be extended by amplifying the mapping of physical to virtual movement (translational gain). Although amplified movement has been used since the earliest days of VR, little is known about how it influences reach-based interactions with virtual objects, now a standard feature of consumer VR. Consequently, this paper explores the picking and placing of virtual objects in VR for the first time, with translational gains of between 1x (a one-to-one mapping of a 3.5m*3.5m virtual space to the same sized physical space) and 3x (10.5m*10.5m virtual mapped to 3.5m*3.5m physical). Results show that reaching accuracy is maintained for up to 2x gain, however going beyond this diminishes accuracy and increases simulator sickness and perceived workload. We suggest gain levels of 1.5x to 1.75x can be utilized without compromising the usability of a VR task, significantly expanding the bounds of interactive room-scale VR.
            </abstract>
            <sections>
                <section>
                    <word_count>827</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1556</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED RESEARCH</title>
                </section>
                <section>
                    <word_count>1987</word_count>
                    <figure_citations>Figure 1) consisted of a warehouse with floor, walls, ceiling lights and structural pillars.Figure 2).Figure 2).Figure 3).Figure 4).</figure_citations>
                    <section_index>2</section_index>
                    <title>TRANSLATIONAL GAIN</title>
                </section>
                <section>
                    <word_count>2062</word_count>
                    <figure_citations>Figure 6, where the fixed position of the target evident in Figure 6.Figure 8) and on all the TLX subscales, with differences predominantly arising between {1.</figure_citations>
                    <section_index>3</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1396</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>135</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>15</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1852</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Amplified movement</li>
                <li>Object manipulation</li>
                <li>Translational gain</li>
                <li>Redirected walking</li>
            </keywords>
            <authors>
                <li>Graham Wilson</li>
                <li> Mark McGill</li>
                <li> Matthew Jamieson</li>
                <li> Julie R. Williamson</li>
                <li> Stephen A. Brewster</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173673_crop_1.jpg</url>
                <caption>Figure 1: Left: Virtual environment with 1.0x (no) gain.
                    Right: Virtual environment with 3.0x gain.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173673_crop_2.jpg</url>
                <caption>Figure 2: Left: Highlighted wall for next target. Right: Table.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173673_crop_3.jpg</url>
                <caption>Figure 3: Left: User reaching to select target cube, with inset
                    real-world environment. Right: Alignment cube.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173673_crop_4.jpg</url>
                <caption>Figure 4: The three walking routes. Left: Straight. Middle:
                    Zig-zag. Right: Curve. Obstacles were scaled based on gain.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173673_crop_5.jpg</url>
                <caption>Figure 5. Scatterplot of controller attach point at target
                    grab/release, front-on for Wall, top-down for Table (mean-
                    ing both are effectively front-on relative to the target). The
                    spread increases with increasing gain.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3173673_crop_6.jpg</url>
                <caption>Figure 6. Side-on view of reach trajectory in real-world space
                    when within 1m virtually of the target.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3173673_crop_7.jpg</url>
                <caption>Figure 7. Real-world walking velocity whilst navigating ob-
                    stacles mid-trial.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3173673_crop_8.jpg</url>
                <caption>Figure 8. Overall NASA TLX workload score across different
                    levels of gain.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173702</filename>
        <data>
            <paper_id>3173574.3173702</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173702</doi>
            <title>Breaking the Tracking: Enabling Weight Perception using Perceivable Tracking Offsets</title>
            <abstract>
                Virtual reality (VR) technology strives to enable a highly immersive experience for the user by including a wide variety of modalities (e.g. visuals, haptics). Current VR hardware however lacks a sufficient way of communicating the perception of weight of an object, resulting in scenarios where users can not distinguish between lifting a bowling ball or a feather. We propose a solely software based approach of simulating weight in VR by deliberately using perceivable tracking offsets. These tracking offsets nudge users to lift their arm higher and result in a visual and haptic perception of weight. We conducted two user studies showing that participants intuitively associated them with the sensation of weight and accept them as part of the virtual world. We further show that compared to no weight simulation, our approach led to significantly higher levels of presence, immersion and enjoyment. Finally, we report perceptional thresholds and offset boundaries as design guidelines for practitioners.
            </abstract>
            <sections>
                <section>
                    <word_count>567</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1250</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1480</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>WEIGHT MODELING APPROACH</title>
                </section>
                <section>
                    <word_count>798</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>FIRST STUDY</title>
                </section>
                <section>
                    <word_count>2478</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>SECOND STUDY</title>
                </section>
                <section>
                    <word_count>1061</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>LIMITATIONS OF THE APPROACH</title>
                </section>
                <section>
                    <word_count>215</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>31</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1142</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Weight perception</li>
                <li>Virtual reality</li>
                <li>Pseudo haptics</li>
            </keywords>
            <authors>
                <li>Michael Rietzler</li>
                <li> Florian Geiselhart</li>
                <li> Jan Gugenheimer</li>
                <li> Enrico Rukzio</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173702_crop_1.jpg</url>
                <caption>Figure 1. We propose a solely software based approach of simulating weight in VR by deliberately using perceivable tracking offsets. These tracking
                    offsets nudge users to lift their arm higher and result in a visual and haptic perception of weight.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173702_crop_2.jpg</url>
                <caption>Figure 2. a) When an object is grabbed it is pulled down by the weight
                    force (F(g)). The imaginary force (F(o)) is working against the weight
                    force and increases with the offset between visual and tracked position.
                    b) When an object is grabbed, the visual position ﬁrst remains on the
                    tracked position. While lifting, the visual position is shifted towards the
                    object one’s. c) The faster an object is moved, the more the visual posi-
                    tion is shifted towards the tracked one.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173702_crop_3.jpg</url>
                <caption>Figure 3. Plot of the participant ratings of different presented offsets as
                    well as the trend line. The green area includes ratings where at least 50%
                    of the participants rated the offsets as a good metaphor, the yellow area
                    includes offsets which were accepted by all participants. The red area
                    includes values which were not accepted by all participants.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173702_crop_4.jpg</url>
                <caption>Figure 4. A screenshot of the bowling scene.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173702_crop_5.jpg</url>
                <caption>Figure 5. A screenshot of the detection threshold task scene.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3173702_crop_6.jpg</url>
                <caption>Figure 6. Boxplots of the Presence, immersion and enjoyment scores
                    split by condition.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3173702_crop_7.jpg</url>
                <caption>Figure 7. Boxplots of the results of our own questions regarding weight
                    perception and estimation.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3173702_crop_8.jpg</url>
                <caption>Figure 8. Illustration of the participants’ rankings of the two conditions.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3173574.3173702_crop_9.jpg</url>
                <caption>Figure 9. The probability and variances of correct estimations as well as
                    a trend line when comparing two weights using different offsets.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3173574.3173702_crop_10.jpg</url>
                <caption>Figure 10. Participants’ weight associations of different presented off-
                    sets.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173739</filename>
        <data>
            <paper_id>3173574.3173739</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173739</doi>
            <title>VR-OOM: Virtual Reality On-rOad driving siMulation</title>
            <abstract>
                Researchers and designers of in-vehicle interactions and interfaces currently have to choose between performing evaluation and human factors experiments in laboratory driving simulators or on-road experiments. To enjoy the benefit of customizable course design in controlled experiments with the immediacy and rich sensations of on-road driving, we have developed a new method and tools to enable VR driving simulation in a vehicle as it travels on a road. In this paper, we describe how the cost-effective and flexible implementation of this platform allows for rapid prototyping. A preliminary pilot test (N = 6), centered on an autonomous driving scenario, yields promising results, illustrating proof of concept and indicating that a basic implementation of the system can invoke genuine responses from test participants.
            </abstract>
            <sections>
                <section>
                    <word_count>545</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1096</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1777</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>498</word_count>
                    <figure_citations>Figure 5 shows how this was done for the validation study.</figure_citations>
                    <section_index>3</section_index>
                    <title>REPLICATION SETUP CONSIDERATIONS</title>
                </section>
                <section>
                    <word_count>569</word_count>
                    <figure_citations>Figure 4 for the participant view of the virtual driving simulation environment).Figure 5).</figure_citations>
                    <section_index>4</section_index>
                    <title>VALIDATION STUDY</title>
                </section>
                <section>
                    <word_count>2417</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>102</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>FUTURE WORK</title>
                </section>
                <section>
                    <word_count>147</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>85</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1382</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Autonomous vehicles</li>
                <li>Prototyping</li>
                <li>Design evaluation</li>
            </keywords>
            <authors>
                <li>David Goedicke</li>
                <li> Jamy Li</li>
                <li> Vanessa Evers</li>
                <li> Wendy Ju</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173739_crop_1.jpg</url>
                <caption>Figure 1. VR-OOM allows participants to experience the physical sen-
                    sations of the real-world with the controlled virtual environments and
                    events. Photo by Arjan Reef.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173739_crop_2.jpg</url>
                <caption>Figure 2. VR-OOM System Diagram (excluding the wizard driver)
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173739_crop_3.jpg</url>
                <caption>Figure 3. View from inside vehicle with participant on left, Driving Wizard on right, and Interaction Wizard in backseat, top. Participant’s view with
                    hand in foreground, left. Bird’s eye view of virtual vehicle in virtual world, right.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173739_crop_4.jpg</url>
                <caption>Figure 4. Participant view at the start of the experiment (excluding the tracked hands).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173739_crop_5.jpg</url>
                <caption>Figure 5. The virtual route overlaid on a satellite map. The black box
                    is the starting location, conditions one and two are marked by the yel-
                    low cars and the white outline is the shape of the road. The gray boxes
                    indicated the approximate locations of buildings in the virtual scene.
                </caption>
                <page>6</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173759</filename>
        <data>
            <paper_id>3173574.3173759</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173759</doi>
            <title>SymbiosisSketch: Combining 2D &amp; 3D Sketching for Designing Detailed 3D Objects in Situ</title>
            <abstract>
                We present SymbiosisSketch, a hybrid sketching system that combines drawing in air (3D) and on a drawing surface (2D) to create detailed 3D designs of arbitrary scale in an augmented reality (AR) setting. SymbiosisSketch leverages the complementary affordances of 3D (immersive, unconstrained, life-sized) and 2D (precise, constrained, ergonomic) interactions for in situ 3D conceptual design. A defining aspect of our system is the ongoing creation of surfaces from unorganized collections of 3D curves. These surfaces serve a dual purpose: as 3D canvases to map strokes drawn on a 2D tablet, and as shape proxies to occlude the physical environment and hidden curves in a 3D sketch. SymbiosisSketch users draw interchangeably on a 2D tablet or in 3D within an ergonomically comfortable canonical volume, mapped to arbitrary scale in AR. Our evaluation study shows this hybrid technique to be easy to use in situ and effective in transcending the creative potential of either traditional sketching or drawing in air.
            </abstract>
            <sections>
                <section>
                    <word_count>635</word_count>
                    <figure_citations>Figure 2).Figure 4) of illustration styles, textures, and rendering techniques to depict geometric detail, material, and lighting [26, 42].Figure 6).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1276</word_count>
                    <figure_citations>Figure 3).Figure 4).Figure 14a), mechanical wing augmentation (Figure 14c), mini car (participant creation), Flintstones’ house (author creation), and large fan (Figure 14d).</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>378</word_count>
                    <figure_citations>Figure 6a).Figure 6b).Figure 8).</figure_citations>
                    <section_index>2</section_index>
                    <title>SYSTEM OVERVIEW AND SETUP</title>
                </section>
                <section>
                    <word_count>2184</word_count>
                    <figure_citations>Figure 1 demonstrates the overall workflow of our system.Figure 2).Figure 7c–d).Figure 9).Figure 10).Figure 10).Figure 11) shows an orthographic projection of the active drawing canvas, a color palette, and other functionality and configuration settings (also see Figure 8).Figure 11a).Figure 12).Figure 12).Figure 13).Figure 14a).Figure 14).Figure 15c), softness (drapery or clothes, see Figure 17), or size and reachability (buildings, ceilings, or cars, Figure 14d).</figure_citations>
                    <section_index>3</section_index>
                    <title>COMPONENTS</title>
                </section>
                <section>
                    <word_count>834</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>IMPLEMENTATION DETAILS</title>
                </section>
                <section>
                    <word_count>1393</word_count>
                    <figure_citations>Figure 14 shows some of the resulting artifacts.Figure 15a).</figure_citations>
                    <section_index>5</section_index>
                    <title>USER EVALUATION</title>
                </section>
                <section>
                    <word_count>310</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>113</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>38</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1769</word_count>
                    <figure_citations>Figure 14c), war helicopter (Figure 14a), large wall-fan (Figure 14d), and some additional results: CHInosaur as the Toronto Raptor (participant creation) and skirt drawn over a human model (author creation).</figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>3D drawing</li>
                <li>Design sketching</li>
                <li>Augmented reality</li>
            </keywords>
            <authors>
                <li>Rahul Arora</li>
                <li> Rubaiat Habib Kazi</li>
                <li> Tovi Grossman</li>
                <li> George Fitzmaurice</li>
                <li> Karan Singh</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173759_crop_1.jpg</url>
                <caption>Figure 1. SymbiosisSketch combines mid-air 3D interactions (a) with constrained sketching. Users can create planar or curved canvases (b), and use a
                    tablet (c) to sketch onto them. Designs are created in situ, in context of physical objects in the scene (d), allowing quick post-processing operations to
                    seamlessly blend the virtual objects into the real world (e).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173759_crop_2.jpg</url>
                <caption>Figure 2. Lacking surfaces, Tilt Brush users employ many strokes to de-
                    pict texture or the illusion of a surface. ©Kamikakushi de Sen et Chihiro
                    (left) and Christopher Watson (right). Used under CC-Attribution 3.0.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173759_crop_3.jpg</url>
                <caption>Figure 3. Design workﬂow using 3D and 2D tools. Rustam Hasanov mod-
                    eled an underlay of an interior scene in a 3D modeling tool (SketchUp),
                    and then drew details over the top of those underlays using a 2D inter-
                    face and tools. ©Rustam Hasanov. Used with permission.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173759_crop_4.jpg</url>
                <caption>Figure 4. Details in 2D drawings: structural details, architectural em-
                    bellishments, and textures. © Jake Parker (left) and Johannes Figlhuber
                    (right). Used with permission.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173759_crop_5.jpg</url>
                <caption>Figure 5. Post-processed results. HoloLens hardware limitations restrict us to basic Gouraud shading. With appropriate shading and occlusion,
                    SymbiosisSketch designs can seamlessly blend into the real world. (Clockwise from top) war helicopter shooting at Captain America (Figure 14a),
                    mechanical wing augmentation (Figure 14c), mini car (participant creation), Flintstones’ house (author creation), and large fan (Figure 14d).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3173759_crop_6.jpg</url>
                <caption>Figure 6. Setup: the user puts on the HoloLens and draws with a motion-
                    tracked stylus, on a tablet (left), or mid-air (right) using a mouse afﬁxed
                    to the back of the tablet.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3173759_crop_7.jpg</url>
                <caption>Figure 7. Strokes drawn using the 2D tablet are projected onto drawing
                    canvases. (a) A planar drawing canvas is a rectangle with the same aspect
                    ratio as the tablet. (b) Surfaces form curved drawing canvases. (c) Users
                    can draw closed curves on canvases to deﬁne (d) solid surfaces which also
                    lend occlusion, lighting, and shadows to the design.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3173759_crop_8.jpg</url>
                <caption>Figure 8. 2D UI toolbar. (left to right) save/load, initial canvas, create canvas, explore and select bookmarks, color tools (palette, HSV sliders, indicator),
                    ﬁll, pencil/ink toggle, symmetry plane, UI and pencil toggles, workspace scaling and reset, clear all and undo stroke. Buttons are shown only if currently
                    useful. For example, selecting the pencil hides color tools. Icons © icons8.com. Used under CC BY-ND 3.0.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3173574.3173759_crop_9.jpg</url>
                <caption>Figure 9. Creating a curved drawing canvas. (a) The user draws a few
                    strokes using the 2D and/or 3D interface. (b) A surface patch is ﬁt to
                    these strokes.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3173574.3173759_crop_10.jpg</url>
                <caption>Figure 10. Widgets for direct 3D manipulation: translation and rotation
                    (all canvases), and scaling (planar canvases): shown at the center, edges,
                    and corners, respectively.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3173574.3173759_crop_11.jpg</url>
                <caption>Figure 11. (a) The user sketches in the 2D view of the canvas. (b) The
                    corresponding strokes in 3D.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>12</id>
                <url>3173574.3173759_crop_12.jpg</url>
                <caption>Figure 12. Workspace scaling tool for drawing a car. The user uses a
                    large workspace to draw the shape of the car in a small, comfortable,
                    scale (left). She then deﬁnes a small workspace centered on a headlight
                    (right). This “zoom in” effect allows her to capture the details of the
                    headlight’s shape, deﬁne a new canvas on it, and draw highlights.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>13</id>
                <url>3173574.3173759_crop_13.jpg</url>
                <caption>Figure 13. Physical planes detected by the HoloLens are automatically
                    bookmarked (shown overlaid on the scene, left). With workspace scaling,
                    users can visualize and work even with room-scale planes in an accessi-
                    ble position and comfortable scale (right).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>14</id>
                <url>3173574.3173759_crop_14.jpg</url>
                <caption>Figure 14. Sample results showing SymbiosisSketch’s creative potential. Author creations: war helicopter shooting at a Captain America ﬁgure (a), logo
                    drawn on a person’s clothing (b), mechanical wing augmenting a robot ﬁgure (c), and a large wall-fan (d). Participant creations: scary ﬁgure utilizing
                    a mannequin head (e) and genie emanating out of a teapot (f) with details (g, h).
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>15</id>
                <url>3173574.3173759_crop_15.jpg</url>
                <caption>Figure 15. Task-1: drawing London’s Gherkin building (a). Task-2:
                    drawing a logo (b) on a curved physical surface of a vase (c).
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>16</id>
                <url>3173574.3173759_crop_16.jpg</url>
                <caption>Figure 16. Representative results for the ﬁxed tasks. Task 1: Gherkin
                    building, using symbiosis (left) and mid-air only (center-left). Task 2:
                    logo on physical object, symbiosis (center-right), mid-air only (right).
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>17</id>
                <url>3173574.3173759_crop_17.jpg</url>
                <caption>Figure 17. (Clockwise from top-left) close-ups of selected results showing details of designs: mechanical wing (Figure 14c), war helicopter (Figure 14a),
                    large wall-fan (Figure 14d), and some additional results: CHInosaur as the Toronto Raptor (participant creation) and skirt drawn over a human model
                    (author creation).
                </caption>
                <page>14</page>
            </figure>
            <figure>
                <id>18</id>
                <url>3173574.3173759_crop_18.jpg</url>
                <caption>Figure 18. All participants’ drawings for the ﬁxed tasks. Row-wise from top: task 1 (Gherkin building) in symbiosis condition, the in mid-air only
                    condition; task 2 (logo on a physical surface) in symbiosis condition, and in mid-air only condition.
                </caption>
                <page>15</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173778</filename>
        <data>
            <paper_id>3173574.3173778</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173778</doi>
            <title>vrSocial: Toward Immersive Therapeutic VR Systems for Children with Autism</title>
            <abstract>
                Social communication frequently includes nuanced nonverbal communication cues, including eye contact, gestures, facial expressions, body language, and tone of voice. This type of communication is central to face-to-face interaction, but can be challenging for children and adults with autism. Innovative technologies can provide support by augmenting human-delivered cuing and automated prompting. Specifically, immersive virtual reality (VR) offers an option to generalize social skill interventions by concretizing nonverbal information in real-time social interactions. In this work, we explore the design and evaluation of three nonverbal communication applications in immersive VR. The results of this work indicate that delivering real-time visualizations of proximity, speaker volume, and duration of one's speech is feasible in immersive VR and effective for real-time support for proximity regulation for children with autism. We conclude with design considerations for therapeutic VR systems.
            </abstract>
            <sections>
                <section>
                    <word_count>30</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>AUTHOR KEYWORDS</title>
                </section>
                <section>
                    <word_count>660</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>796</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1458</word_count>
                    <figure_citations>Figure 1-time spent talking bar).Figure 2B) did not result in changing one’s proximity when needed; and c.Figure 2A).Figure 2B), and yellow representing an intermediate warning between the two zones (see Figure 2D).Figure 2E).Figure 2D) or ‘step away’ respectively (see Figure 2 E).Figure 2C).</figure_citations>
                    <section_index>3</section_index>
                    <title>SYSTEM DESIGN</title>
                </section>
                <section>
                    <word_count>830</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>2223</word_count>
                    <figure_citations>Figure 3, left).Figure 3, middle), yet at the Paper 204 individual level, three participants improved in the intervention condition (P1, P9, P11), three performed well in baseline and intervention (P2, P6, P8), and three showed reduced performance in the intervention condition (P5, P7, P10).Figure 3, right).</figure_citations>
                    <section_index>5</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1200</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>DESIGN CONSIDERATIONS FOR THERAPUETIC VR</title>
                </section>
                <section>
                    <word_count>351</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>169</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>48</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1288</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Visualization</li>
                <li>Assistive technology</li>
                <li>Autism</li>
                <li>Proximity</li>
                <li>Prosody</li>
                <li>Immersive VR</li>
                <li>Accessibility</li>
            </keywords>
            <authors>
                <li>LouAnne E. Boyd</li>
                <li> Saumya Gupta</li>
                <li> Sagar B. Vikmani</li>
                <li> Carlos M. Gutierrez</li>
                <li> Junxiang Yang</li>
                <li> Erik Linstead</li>
                <li> Gillian R. Hayes</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173778_crop_1.jpg</url>
                <caption>Figure 1: Interfaces for Voice Condition. On the left, the bar is balanced between speakers and the volume icon is
                    greyed out as the user is not speaking. In the middle, the green speaker indicates the user’s volume is in the correct
                    range, and on the right the larger red speaker indicates the user is speaking too loud.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173778_crop_2.jpg</url>
                <caption>Figure 2: Images of the five interfaces for Proximity Condition: A.) Baseline, B.) Proximity correct zone, C.) Too far away zone, D.) A bit
                    too close for acquaintances zone, E.) Intrusion into personal space.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173778_crop_3.jpg</url>
                <caption>Figure 3: Split plot graphs depicting percent correct proximity (left), percent correct volume (middle) and time spent talking (right)
                    across four conditions, over five sessions. The purple line represents the combination condition, the green line represents the
                    proximity condition, the yellow line represents the volume condition and the blue represents the baseline condition.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173792</filename>
        <data>
            <paper_id>3173574.3173792</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173792</doi>
            <title>Projective Windows: Bringing Windows in Space to the Fingertip</title>
            <abstract>
                In augmented and virtual reality (AR and VR), there may be many 3D planar windows with 2D texts, images, and videos on them. However, managing the position, orientation, and scale of such a window in an immersive 3D workspace can be difficult. Projective Windows strategically uses the absolute and apparent sizes of the window at various stages of the interaction to enable the grabbing, moving, scaling, and releasing of the window in one continuous hand gesture. With it, the user can quickly and intuitively manage and interact with windows in space without any controller hardware or dedicated widget. Through an evaluation, we demonstrate that our technique is performant and preferable, and that projective geometry plays an important role in the design of spatial user interfaces.
            </abstract>
            <sections>
                <section>
                    <word_count>403</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>61</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1205</word_count>
                    <figure_citations>Figure 2a).Figure 2b) the fingers, and completes the selection by making the tips of the fingers touch (Figure 2c).Figure 3a), i.Figure 3b) while maintaining the same apparent size, thus appearing the same to the user (Figure 3a, b insets) as in the image plane interaction proposed by Pierce et al.Figure 4a), or smaller by taking it away from the face (Figure 4b).Figure 4a, b), whereas the window is erected against a horizontal surface, perpendicular to the user’s gaze, to enforce the best viewing angle (Figure 4c).Figure 5a).</figure_citations>
                    <section_index>2</section_index>
                    <title>PROJECTIVE WINDOWS</title>
                </section>
                <section>
                    <word_count>115</word_count>
                    <figure_citations>Figure 6a) for tracking hand movement and posture within the user’s view frustum, the Leap Motion sensor’s front-facing camera for a video feed of the real world, the Unity 3D engine for integrating virtual and physical elements (Figure 6b), and an Intel quad-core i7 3.</figure_citations>
                    <section_index>3</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>428</word_count>
                    <figure_citations>Figure 5b), when the zoom is large, the slope is steep, and when the zoom is small, the slope is gentle, so the user has finer control when the zoom is smaller.Figure 7) to a widget-based ray-casting technique (RC) using the same controller.</figure_citations>
                    <section_index>4</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>1487</word_count>
                    <figure_citations>
                        Figure 8h), and others were move widgets (Figure 8f, g).Figure 8, 9).Figure 9) to simultaneously compare the discrete levels of various factors in the later statistical analyses: For example, moving a window from A to a target at C requires a linear movement of 4 m and an angular movement of -60°, whereas A to D requires 2 m and 30° respectively.Figure 9).Figure 10a).Figure 10b).Figure 11a■, F4,1675 = 45, p &lt; 0.Figure 11b■, F8,1671=11, p &lt; 0.Figure 11c■, F8,1671=17, p &lt; 0.Figure 11d■, F6,1673=54, p &lt; 0.Figure 11a■, F4,1675=19, p &lt; 0.Figure 11b■, F8,1671=13, p &lt; 0.Figure 11d■, F6,1673=23, p &lt; 0.Figure 12c■, F8,1671=59, p &lt; 0.Figure 12d■, F6,1673=30, p &lt; 0.Figure 12a■, F4,1675=8.Figure 12b■, F8,1671=5.Figure 12c■, F8,1761=15, p &lt; 0.Figure 12d■, F6,1673=51, p &lt; 0.Figure 12a■, F4,1675=15, p &lt; 0.Figure 12b■, F8,1671=17, p &lt; 0.Figure 13).
                    </figure_citations>
                    <section_index>5</section_index>
                    <title>EXPERIMENT SESSION</title>
                </section>
                <section>
                    <word_count>247</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>743</word_count>
                    <figure_citations>Figure 11d) and with an absolute size ratio of 1 for RC (Figure 11c), irrespective of the technique.Figure 11d, 12d).</figure_citations>
                    <section_index>7</section_index>
                    <title>EXPLORATION SESSION</title>
                </section>
                <section>
                    <word_count>415</word_count>
                    <figure_citations>Figure 14) using our implementation (Figure 6).Figure 14a), the user can pull picture windows out of a laptop screen and easily scale and place them anywhere on nearby walls for visual reference, just as he or she would sticky notes, but with the ability to freely change the size.Figure 14a), the user can pick up a window from a laptop screen and place it on a tablet device to quickly change the input from typing to drawing, without having to swap applications.Figure 14b), the user can perform the grab gesture to instantly scan a notebook page and then generate a projective window from it, to scale and place it anywhere for reference.Figure 14c), the user can pick up a small movie window from a nearby table, play the preview of the movie by bringing it closer to the face [1], and then start playing the movie by projecting it onto a vertical wall.Figure 14d), the user can use the entire unbounded scene as a workspace, even projecting windows across large distances.</figure_citations>
                    <section_index>8</section_index>
                    <title>USER SCENARIOS</title>
                </section>
                <section>
                    <word_count>607</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Augmented reality</li>
                <li>Virtual reality</li>
                <li>3D window management</li>
            </keywords>
            <authors>
                <li>Joon Hyub Lee</li>
                <li> Sang-Gyun An</li>
                <li> Yongkwan Kim</li>
                <li> Seok-Hyung Bae</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173792_crop_1.jpg</url>
                <caption>Figure 1. In a mock setup depicting the flow of Projective Windows, (a) the user wishing to adjust the position and scale of an AR
                    window (b) grabs the window, (c) moves it, (d) makes it bigger by bringing it closer, and (e) projects it to the desired position.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173792_crop_2.jpg</url>
                <caption>Figure 2. (a) The user creates a big area cursor, (b) specifies a
                    window in a cluttered situation by closing the fingers and
                    making the cursor smaller, and (c) selects it by pinching.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173792_crop_3.jpg</url>
                <caption>Figure 3. (a) The user makes a grab gesture on a window to (b)
                    projectively bring it to the grabbed point.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173792_crop_4.jpg</url>
                <caption>Figure 4. (a) The user makes the window appear bigger by
                    bringing it closer to the face, and (b) smaller by putting it
                    away. The user can project a window (a, b) onto a vertical
                    surface, or (c) make it stand on a horizontal surface.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173792_crop_5.jpg</url>
                <caption>Figure 5. (a) When the user grabs a window from a wall,
                    moves it relative to the face, and releases it to another wall, (b)
                    the zoom (ratio of the final to initial widths) is inversely
                    proportional to d.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3173792_crop_6.jpg</url>
                <caption>Figure 6. (a) Implementation hardware. (b) The hands, real
                    and virtual objects in the user’s view.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3173792_crop_7.jpg</url>
                <caption>Figure 7. With Projective Windows (PW) using a hardware
                    controller (PWC), (a-b) a finger pinch is substituted with (c-d)
                    the pull of the trigger (highlighted green) on the hardware
                    controller. Note the similarity between the bare hand gesture
                    and the controller-based gesture.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3173792_crop_8.jpg</url>
                <caption>Figure 8. The experiment was conducted with PW using a controller (PWC) and ray-casting (RC). Using PWC, the participant (a)
                    first searched for the target (white), (b) grabbed the window (blue), (c) moved the window while adjusting its apparent size, and (d)
                    released it onto the target when the apparent sizes matched. Using RC, the participant (e) first searched for the target (white), (f)
                    dragged the window (blue) to the target, (g) placed it in the target, and (h) scaled it so that the absolute sizes matched.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3173574.3173792_crop_9.jpg</url>
                <caption>Figure 9. Windows and targets appeared on A~L. The user
                    performed move and scale tasks using Projective Windows
                    and a widget-based ray-casting baseline technique.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>14</id>
                <url>3173574.3173792_crop_14.jpg</url>
                <caption>Figure 14. User scenarios of Projective Windows in a (a)
                    design studio, (b) study, (c) living room, and (d) VR scene.
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173793</filename>
        <data>
            <paper_id>3173574.3173793</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173793</doi>
            <title>Scenariot: Spatially Mapping Smart Things Within Augmented Reality Scenes</title>
            <abstract>The emerging simultaneous localizing and mapping (SLAM) based tracking technique allows the mobile AR device spatial awareness of the physical world. Still, smart things are not fully supported with the spatial awareness in AR. Therefore, we present Scenariot, a method that enables instant discovery and localization of the surrounding smart things while also spatially registering them with a SLAM based mobile AR system. By exploiting the spatial relationships between mobile AR systems and smart things, Scenariot fosters in-situ interactions with connected devices. We embed Ultra-Wide Band (UWB) RF units into the AR device and the controllers of the smart things, which allows for measuring the distances between them. With a one-time initial calibration, users localize multiple IoT devices and map them within the AR scenes. Through a series of experiments and evaluations, we validate the localization accuracy as well as the performance of the enabled spatial aware interactions. Further, we demonstrate various use cases through Scenariot.</abstract>
            <sections>
                <section>
                    <word_count>695</word_count>
                    <figure_citations>Figure 1), an AR system which provides fast estimation of the 3D locations of smart things and exploits the spatial relationships for location aware interactions.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>696</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1484</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>SCENARIOT</title>
                </section>
                <section>
                    <word_count>915</word_count>
                    <figure_citations>Figure 3, IoT controllers are deployed to smart things as well as to the AR device.Figure 3, the overall size of the board is 100mm × 100mm × 20mm with the units installed in position.</figure_citations>
                    <section_index>3</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>1425</word_count>
                    <figure_citations>Figure 4, we conducted 9 surveyings to collect the surveying data with r ∈ {2, 3, 5}m and n ∈ {1, 2, 4}.Figure 5 (right), we observed a mean error of 0.Figure 6 (left), the mean error for {1.Figure 7, with the condition of r = 5m, n = 2 and n = 4 presented a larger error (> 0.</figure_citations>
                    <section_index>4</section_index>
                    <title>TECHNICAL EVALUATION</title>
                </section>
                <section>
                    <word_count>1730</word_count>
                    <figure_citations>Figure 1 and Figure 12, we not only visualize the digital interfaces when the corresponding physical object is located inside the view, but also the ones outside.Figure 9, the overall average error decreased to 0.Figure 9, the average of the localization error over all 8 devices yielded 0.Figure 9, we suspect that the accuracy degradation was not just caused by the localization accuracy.Figure 10, we observed an average of 0.</figure_citations>
                    <section_index>5</section_index>
                    <title>TASK EVALUATION</title>
                </section>
                <section>
                    <word_count>440</word_count>
                    <figure_citations>Figure 14 (a).Figure 14 (b)).Figure 14 (c, d)).</figure_citations>
                    <section_index>6</section_index>
                    <title>EXAMPLE USE CASES</title>
                </section>
                <section>
                    <word_count>248</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>DISCUSSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>354</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>LIMITATION</title>
                </section>
                <section>
                    <word_count>123</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>65</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>ACKNOWLEDGEMENT</title>
                </section>
                <section>
                    <word_count>276</word_count>
                    <figure_citations></figure_citations>
                    <section_index>11</section_index>
                    <title>REFERENCES</title>
                </section>
                <section>
                    <word_count>1196</word_count>
                    <figure_citations></figure_citations>
                    <section_index>12</section_index>
                    <title>ELECTRICAL ENGINEERING AND COMPUTER</title>
                </section>
            </sections>
            <keywords>
                <li>Augmented Reality</li>
                <li>IoT</li>
                <li>Smart Environment</li>
                <li>Spatial Interactions</li>
                <li>Context Awareness</li>
                <li>Localization</li>
                <li>UWB</li>
                <li>SLAM</li>
            </keywords>
            <authors>
                <li>Ke Huo</li>
                <li> Yuanzhi Cao</li>
                <li> Sang Ho Yoon</li>
                <li> Zhuangying Xu</li>
                <li> Guiming Chen</li>
                <li> Karthik Ramani</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173793_crop_1.jpg</url>
                <caption>Figure 1. Scenariot is a method for discovering and localizing IoT devices with a SLAM-based AR device. We register the discovered devices spatially
                    in the AR scene to enable new spatial aware interactions.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173793_crop_2.jpg</url>
                <caption>Figure 2. Scenariot localization principle.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173793_crop_3.jpg</url>
                <caption>Figure 3. Overview of the Scenariot hardware. Deploy IoT controller
                    boards (right) to IoT devices and AR device (left).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173793_crop_4.jpg</url>
                <caption>Figure 4. Technical evaluation setup.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173793_crop_5.jpg</url>
                <caption>Figure 5. Effect of sampling space on the localization accuracy:
                    (left)assume a cubic volume, (right) varying h and set l = w = 1.6(m).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3173793_crop_6.jpg</url>
                <caption>Figure 6. Effect of sampling Number (m) on the localization accuracy.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3173793_crop_7.jpg</url>
                <caption>Figure 7. Effect of sampling distances (r) and number of devices (n) on
                    the localization accuracy.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3173793_crop_8.jpg</url>
                <caption>Figure 8. Task evaluation setup with 8 IoT devices in an ofﬁce.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3173574.3173793_crop_9.jpg</url>
                <caption>Figure 9. Localization accuracy study with users. Runtime: runtime lo-
                    calization result. Height Correction: results with height correction. Dis-
                    tance: the distances of IoT devices to the center of the surveying space.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3173574.3173793_crop_10.jpg</url>
                <caption>Figure 10. Distant pointing accuracy and completion time.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3173574.3173793_crop_11.jpg</url>
                <caption>Figure 11. Proximity based control accuracy and completion time.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>12</id>
                <url>3173574.3173793_crop_12.jpg</url>
                <caption>Figure 12. Discoverable World. The digital representations of the discov-
                    ered IoT devices are visualized within the AR scene with spatial PiPs.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>13</id>
                <url>3173574.3173793_crop_13.jpg</url>
                <caption>Figure 13. Proximity based Control. While users move closer to the
                    machine (a, b, c), the level of engagement is adjusted accordingly.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>14</id>
                <url>3173574.3173793_crop_14.jpg</url>
                <caption>Figure 14. Monitoring the IoT assets (a, b) and navigating the user to-
                    wards the assets by visualizing the direction on the screen(c, d).
                </caption>
                <page>10</page>
            </figure>
            <figure>
                <id>15</id>
                <url>3173574.3173793_crop_15.jpg</url>
                <caption>Figure 15. User creates a miniature world of the physical environment
                    enhanced by the digital interfaces of IoT devices.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173863</filename>
        <data>
            <paper_id>3173574.3173863</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173863</doi>
            <title>Communication Behavior in Embodied Virtual Reality</title>
            <abstract>Embodied virtual reality faithfully renders users' movements onto an avatar in a virtual 3D environment, supporting nuanced nonverbal behavior alongside verbal communication. To investigate communication behavior within this medium, we had 30 dyads complete two tasks using a shared visual workspace: negotiating an apartment layout and placing model furniture on an apartment floor plan. Dyads completed both tasks under three different conditions: face-to-face, embodied VR with visible full-body avatars, and no embodiment VR, where the participants shared a virtual space, but had no visible avatars. Both subjective measures of users' experiences and detailed annotations of verbal and nonverbal behavior are used to understand how the media impact communication behavior. Embodied VR provides a high level of social presence with conversation patterns that are very similar to face-to-face interaction. In contrast, providing only the shared environment was generally found to be lonely and appears to lead to degraded communication.</abstract>
            <sections>
                <section>
                    <word_count>689</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1196</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1417</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>METHODS</title>
                </section>
                <section>
                    <word_count>2274</word_count>
                    <figure_citations>Figure 2e.Figure 2f.Figure 2a).Figure 2g.Figure 2c).</figure_citations>
                    <section_index>3</section_index>
                    <title>ANNOTATED PARTICIPANT BEHAVIOR</title>
                </section>
                <section>
                    <word_count>313</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>SEMANTIC DIFFERENCE MEASURE OF SOCIAL PRESENCE</title>
                </section>
                <section>
                    <word_count>475</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>NETWORKED MINDS MEASURE OF SOCIAL PRESENCE</title>
                </section>
                <section>
                    <word_count>1783</word_count>
                    <figure_citations>Figure 2 the interaction.</figure_citations>
                    <section_index>6</section_index>
                    <title>PARTICIPANT PREFERENCES AND EXIT INTERVIEWS</title>
                </section>
                <section>
                    <word_count>342</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>15</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
                <section>
                    <word_count>906</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>APPEARANCE TO CREATE ENERGY SAVINGS</title>
                </section>
                <section>
                    <word_count>311</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>INDUSTRIAL NEGOTIATION AT THE PLANT</title>
                </section>
            </sections>
            <keywords>
                <li>Computer-mediated communication</li>
                <li>Virtual reality</li>
                <li>Embodiment</li>
                <li>Social presence</li>
            </keywords>
            <authors>
                <li>Harrison Jesse Smith</li>
                <li> Michael Neff</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173863_crop_1.jpg</url>
                <caption>Figure 1. Dyads performed the ﬁrst (A) and second (B) tasks in the face-
                    to-face conditions. In virtual reality conditions, avatars appeared across
                    the table from each other (C), but were actually positioned on opposite
                    sides of the motion capture stage (D). In the embodVR condition, partic-
                    ipants were able to see both avatars (E). In the no_embodVR condition,
                    participants were unable to see their partner and could only see their
                    hands in the second task, to assist with furniture manipulation (F).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173863_crop_2.jpg</url>
                <caption>Figure 2. Sub ﬁgure a shows the average number of gestures performed per minute for each condition. Subﬁgure b shows the percentage of gestures that
                    fall into each annotated category (note that, because some gestures ﬁt multiple categories, totals for each condition can add up to over 100%). Subﬁgure
                    c shows the rate and percentage of gestures which introduced novel content into the discussions (for example, point at a location while referring to it by
                    a referential pronoun). Subﬁgure d shows the mean number of conversational turns taken per minute. Subﬁgure e shows the percent of utterances that
                    fall in each annotated category (note that, because some gestures ﬁt multiple categories, totals for each condition can add up to over 100%). Subﬁgure
                    f shows the frequencies of the manners by which conversational turns were started. Subﬁgure g shows the ratio of gestures performed by the more
                    frequent gesturer and less frequent gesturer in each dyad. Subﬁgure h shows the mean social presence scores, with standard errors of the mean, as
                    measured by the semantic difference questionaire. Subﬁgure i shows the most and least favorite conditions, as reported by participants at the end of the
                    experiment. All error bars show standard error of the mean.
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173902</filename>
        <data>
            <paper_id>3173574.3173902</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173902</doi>
            <title>Vanishing Importance: Studying Immersive Effects of Game Audio Perception on Player Experiences in Virtual Reality</title>
            <abstract>Sound and virtual reality (VR) are two important output modalities for creating an immersive player experience (PX). While prior research suggests that sounds might contribute to a more immersive experience in games played on screens and mobile displays, there is not yet evidence of these effects of sound on PX in VR. To address this, we conducted a within-subjects experiment using a commercial horror-adventure game to study the effects of a VR and monitor-display version of the same game on PX. Subsequently, we explored, in a between-subjects study, the effects of audio dimensionality on PX in VR. Results indicate that audio has a more implicit influence on PX in VR because of the impact of the overall sensory experience and that audio dimensionality in VR may not be a significant factor contributing to PX. Based on our findings and observations, we provide five design guidelines for VR games.</abstract>
            <sections>
                <section>
                    <word_count>1014</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>6060</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>230</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>IEQ</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>VSA</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>VSM</title>
                </section>
                <section>
                    <word_count>701</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>VSAM</title>
                </section>
                <section>
                    <word_count>268</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>GENERAL DISCUSSION AND DESIGN GUIDELINES</title>
                </section>
                <section>
                    <word_count>463</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>FUTURE WORK</title>
                </section>
                <section>
                    <word_count>204</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>60</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1729</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Games</li>
                <li>Audio</li>
                <li>Virtual reality</li>
                <li>Player experience</li>
                <li>Ambient noises</li>
                <li>Background music</li>
                <li>Sound effects</li>
            </keywords>
            <authors>
                <li>Katja Rogers</li>
                <li> Giovanni Ribeiro</li>
                <li> Rina R. Wehbe</li>
                <li> Michael Weber</li>
                <li> Lennart E. Nacke</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173902_crop_1.jpg</url>
                <caption>Figure 1. Factors of IEQ for the study comparing monitor-display version of the game to the HMD-VR version. Emotional involvement was signiﬁcantly
                    higher for HMD-VR, while challenge was signiﬁcantly higher for monitor-display (p&lt;0.05). </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173902_crop_2.jpg</url>
                <caption>Figure 2. Factors of IEQ compared by audio condition showed no signiﬁcant differences (p>0.05) for increased audio dimensionality.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173919</filename>
        <data>
            <paper_id>3173574.3173919</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173919</doi>
            <title>Physical Keyboards in Virtual Reality: Analysis of Typing Performance and Effects of Avatar Hands</title>
            <abstract>Entering text is one of the most common tasks when interacting with computing systems. Virtual Reality (VR) presents a challenge as neither the user's hands nor the physical input devices are directly visible. Hence, conventional desktop peripherals are very slow, imprecise, and cumbersome. We developed a apparatus that tracks the user's hands, and a physical keyboard, and visualize them in VR. In a text input study with 32 participants, we investigated the achievable text entry speed and the effect of hand representations and transparency on typing performance, workload, and presence. With our apparatus, experienced typists benefited from seeing their hands, and reach almost outside-VR performance. Inexperienced typists profited from semi-transparent hands, which enabled them to type just 5.6 WPM slower than with a regular desktop setup. We conclude that optimizing the visualization of hands in VR is important, especially for inexperienced typists, to enable a high typing performance.</abstract>
            <sections>
                <section>
                    <word_count>698</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>477</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>280</word_count>
                    <figure_citations>Figure 1: Side by side illustration of the real environment (left) and the virtual reality replica (right).</figure_citations>
                    <section_index>2</section_index>
                    <title>REALIZING TYPING IN VIRTUAL REALITY</title>
                </section>
                <section>
                    <word_count>513</word_count>
                    <figure_citations>Figure 2: Hand with 23 retroreflective markers (left) and the hardware setup for finger and keyboard tracking (right).</figure_citations>
                    <section_index>3</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>708</word_count>
                    <figure_citations>Figure 3: Pictures of the eight hand visualizations used in the study.</figure_citations>
                    <section_index>4</section_index>
                    <title>METHOD</title>
                </section>
                <section>
                    <word_count>404</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>595</word_count>
                    <figure_citations>Figure 4: Mean values of words per minute and corrected error rate for each condition.</figure_citations>
                    <section_index>6</section_index>
                    <title>WPM</title>
                </section>
                <section>
                    <word_count>8</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>TRANSPARENCY</title>
                </section>
                <section>
                    <word_count>5</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REAL WORLD</title>
                </section>
                <section>
                    <word_count>66</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>M</title>
                </section>
                <section>
                    <word_count>1204</word_count>
                    <figure_citations>Figure 5: Subjective assessments of task load and presence.</figure_citations>
                    <section_index>10</section_index>
                    <title>SD</title>
                </section>
                <section>
                    <word_count>801</word_count>
                    <figure_citations></figure_citations>
                    <section_index>11</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>36</word_count>
                    <figure_citations></figure_citations>
                    <section_index>12</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>132</word_count>
                    <figure_citations></figure_citations>
                    <section_index>13</section_index>
                    <title>REFERENCES</title>
                </section>
                <section>
                    <word_count>839</word_count>
                    <figure_citations></figure_citations>
                    <section_index>14</section_index>
                    <title>CONCLUSION</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Text entry</li>
                <li>Physical keyboard</li>
                <li>Hands</li>
            </keywords>
            <authors>
                <li>Pascal Knierim</li>
                <li> Valentin Schwind</li>
                <li> Anna Maria Feit</li>
                <li> Florian Nieuwenhuizen</li>
                <li> Niels Henze</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173919_crop_1.jpg</url>
                <caption>Figure 1: Side by side illustration of the real environment (left)
                    and the virtual reality replica (right).
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173919_crop_2.jpg</url>
                <caption>Figure 2: Hand with 23 retroreﬂective markers (left) and the
                    hardware setup for ﬁnger and keyboard tracking (right).
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173919_crop_3.jpg</url>
                <caption>Figure 3: Pictures of the eight hand visualizations used in the study. Realistic, abstract, ﬁngertips with no transparency and real
                    hands (1st row) as well as 50% transparency and no hands (2nd row).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173919_crop_4.jpg</url>
                <caption>Figure 4: Mean values of words per minute and corrected error rate for each condition. Error bars show standard error of the
                    mean (SE). Exact values are also listed in Table 1.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173919_crop_5.jpg</url>
                <caption>Figure 5: Subjective assessments of task load and presence. Error bars show standard error of the mean (SE).
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173927</filename><data>
            <paper_id>3173574.3173927</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173927</doi>
            <sections></sections>
            <keywords>
                <li>H.5.2. Information interfaces and presentation: Input devices and strategies</li>
                <li>Interaction styles</li>
            </keywords>
            <authors>
                <li>undefined</li>
            </authors>
        </data>
    </article>
    <article>
        <filename>3173574.3173937</filename>
        <data>
            <paper_id>3173574.3173937</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173937</doi>
            <title>Understanding Users' Capability to Transfer Information between Mixed and Virtual Reality: Position Estimation across Modalities and Perspectives</title>
            <abstract>Mixed Reality systems combine physical and digital worlds, with great potential for the future of HCI. It is possible to design systems that support flexible degrees of virtuality by combining complementary technologies. In order for such systems to succeed, users must be able to create unified mental models out of heterogeneous representations. In this paper, we present two studies focusing on the users' accuracy on heterogeneous systems using Spatial Augmented Reality (SAR) and immersive Virtual Reality (VR) displays, and combining viewpoints (egocentric and exocentric). The results show robust estimation capabilities across conditions and viewpoints.</abstract>
            <sections>
                <section>
                    <word_count>493</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>739</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1377</word_count>
                    <figure_citations>Figure 1).Figure 2).Figure 3-left), and to confirm the estimation by pressing controller’s trigger (using the soft "hair-trigger" trigger mode to mitigate unintended movement when clicking).Figure 3-right).Figure 7).</figure_citations>
                    <section_index>2</section_index>
                    <title>STUDY DESIGN</title>
                </section>
                <section>
                    <word_count>23</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>CONDITION</title>
                </section>
                <section>
                    <word_count>4050</word_count>
                    <figure_citations>Figure 3).Figure 4).Figure 5 presents the error distribution per region and condition.Figure 6).Figure 6).Figure 7).Figure 7, 7-Likert scale) and the comments obtained during the interview were similar.Figure 8-left).Figure 8-left).Figure 8).Figure 9-top).Figure 9bottom).Figure 10).Figure 10 and Figure 6: both studies present similar distributions from the egocentric perspective, yet there is a shift towards zero for the Study 2.Figure 11).Figure 12).Figure 12).Figure 12) shows that both conditions present positive mean scores (MR: 1.</figure_citations>
                    <section_index>4</section_index>
                    <title>SAR</title>
                </section>
                <section>
                    <word_count>898</word_count>
                    <figure_citations>Figure 13 show three cases of distances between target and estimation.Figure 13).</figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>388</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>15</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1445</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Mixed Reality</li>
                <li>Spatial Augmented Reality</li>
                <li>Virtual Reality</li>
                <li>Evaluation</li>
                <li>Quantitative Methods</li>
            </keywords>
            <authors>
                <li>Joan Sol Roo</li>
                <li> Jean Basset</li>
                <li> Pierre-Antoine Cinquin</li>
                <li> Martin Hachet</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173937_crop_1.jpg</url>
                <caption>Figure 1. Setup and Tasks involved in the studies presented in this paper. Participants interacted with a spatially augmented mock-up and its virtual
                    counterpart (left). They were iteratively presented with a spherical target from either egocentric (Study 1) and exocentric perspectives (Study 2), and
                    then asked to estimate the position of the target (right) using an estimator attached to a controller. The objective of this protocol is to quantitatively
                    measure the participants’ capability to transfer information between physical and virtual spaces, and between egocentric and exocentric perspectives.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173937_crop_2.jpg</url>
                <caption>Figure 2. Experiment setup: HTC Vive lighthouses were placed in front
                    of the participant, while the Optitrack cameras and the LG projector
                    where placed around and over the participant.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173937_crop_3.jpg</url>
                <caption>Figure 3. Controller with estimation attached to it (left). Controller
                    used to align Optitrack and HTC Vive spaces, markers placed to easily
                    identify origin and orientation of the controller (right).
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173937_crop_4.jpg</url>
                <caption>Figure 4. 48 target locations (left), 6 regions (center), and the 48 targets
                    clustered by region (right). The participant (not represented) would be
                    placed at the bottom of the picture.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173937_crop_5.jpg</url>
                <caption>Figure 5. Absolute error: The effect of region by condition. The charts
                    present both mean with conﬁdence 95% intervals at one sample per-
                    participant (black), and distribution at one sample per-trial (colored).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3173937_crop_6.jpg</url>
                <caption>Figure 6. Depth error: A positive error in depth implies that the partici-
                    pants estimated closer to themselves (presented this way for clarity).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3173937_crop_7.jpg</url>
                <caption>Figure 7. Subjective experience based on a 7-Likert scale questionnaire,
                    values between -3 and 3. Error bars indicate conﬁdence intervals.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3173937_crop_8.jpg</url>
                <caption>Figure 8. target locations (48) and 6 POVs, regions (6).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3173574.3173937_crop_9.jpg</url>
                <caption>Figure 9. Estimation error per condition in comparison with Study 1
                    (top), and the tendency towards order effect (bottom).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3173574.3173937_crop_10.jpg</url>
                <caption>Figure 10. Depth error: The error per condition for Study 2, computed
                    from both the egocentric (left) and exocentric (right) viewpoints. Note
                    the distribution around zero, in contrast with Study 1 (see Figure 6).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3173574.3173937_crop_11.jpg</url>
                <caption>Figure 11. Workload results from the NASA-TLX questionnaire. Both
                    the results for the ﬁrst and second study are presented.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>12</id>
                <url>3173574.3173937_crop_12.jpg</url>
                <caption>Figure 12. Results obtained for the subjective experience questionnaire
                    for the second study, using a 7-Likert scale. The results are presented on
                    a scale between -3 to 3 for clarity.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>13</id>
                <url>3173574.3173937_crop_13.jpg</url>
                <caption>Figure 13. Three instances of estimation: touching the center of the
                    target (A), touching the target (B), and the failure threshold (C).
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3173982</filename>
        <data>
            <paper_id>3173574.3173982</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3173982</doi>
            <title>Interactive Feedforward for Improving Performance and Maintaining Intrinsic Motivation in VR Exergaming</title>
            <abstract>Exergames commonly use low to moderate intensity exercise protocols. Their effectiveness in implementing high intensity protocols remains uncertain. We propose a method for improving performance while maintaining intrinsic motivation in high intensity VR exergaming. Our method is based on an interactive adaptation of the feedforward method: a psychophysical training technique achieving rapid improvement in performance by exposing participants to self models showing previously unachieved performance levels. We evaluated our method in a cycling-based exergame. Participants competed against (i) a self model which represented their previous speed; (ii) a self model representing their previous speed but increased resistance therefore requiring higher performance to keep up; or (iii) a virtual competitor at the same two levels of performance. We varied participants' awareness of these differences. Interactive feedforward led to improved performance while maintaining intrinsic motivation even when participants were aware of the interventions, and was superior to competing against a virtual competitor.</abstract>
            <sections>
                <section>
                    <word_count>1368</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1119</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>693</word_count>
                    <figure_citations>Figure 1a.Figure 1c).Figure 1d).Figure 1 b-d), similar to [74].Figure 1b).</figure_citations>
                    <section_index>2</section_index>
                    <title>EXERGAME DESIGN</title>
                </section>
                <section>
                    <word_count>1712</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>EXPERIMENTAL DESIGN</title>
                </section>
                <section>
                    <word_count>1064</word_count>
                    <figure_citations>Figure 2 top-left).Figure 2 top-right).Figure 2 bottom-left).Figure 2 bottom-right) showed that SC led to a significantly higher performance, t(22) = 3.Figure 3 top-left).</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>26</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>VA</title>
                </section>
                <section>
                    <word_count>13</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>NA</title>
                </section>
                <section>
                    <word_count>15</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>FA</title>
                </section>
                <section>
                    <word_count>8</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>NSC</title>
                </section>
                <section>
                    <word_count>823</word_count>
                    <figure_citations>Figure 3: IMI Interest/Enjoyment (left column) and IMI Pressure/Tension (right column) scores for the baseline (B), equal challenge (E) and harder challenge (H) conditions for 1) self competition (SC) framing in different levels of resistance awareness (top row) and 2) without resistance awareness (NA) in self competition (SC) vs.Figure 3 bottom-left).Figure 3 bottomright).Figure 4 top-left).Figure 4: FSQ Balance of Challenges and Skills (left column) and FSQ Absorption in the Task (right column) scores for the baseline (B), equal challenge (E) and harder challenge (H) conditions for 1) self competition (SC) framing in different levels of resistance awareness (top row) and 2) without resistance awareness (NA) in self competition (SC) vs.Figure 4 top-right).Figure 4 bottom-left).Figure 4 bottom-right).Figure 5: IEQ scores in different levels of resistance awareness (left) and competition framing (right).Figure 5 left).Figure 5 right) showed that there was a significant difference between SC and NSC, t(22) = 2.</figure_citations>
                    <section_index>9</section_index>
                    <title>H</title>
                </section>
                <section>
                    <word_count>21</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>E</title>
                </section>
                <section>
                    <word_count>375</word_count>
                    <figure_citations>Figure 2: ∆Power (difference from baseline B) for the equal challenge (E) and harder challenge (H) conditions in different levels of resistance awareness (top-left) and in non-self (NSC) vs.Figure 3 top-right).</figure_citations>
                    <section_index>11</section_index>
                    <title>SC</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>12</section_index>
                    <title>B</title>
                </section>
                <section>
                    <word_count>8</word_count>
                    <figure_citations></figure_citations>
                    <section_index>13</section_index>
                    <title>IEQ</title>
                </section>
                <section>
                    <word_count>1049</word_count>
                    <figure_citations></figure_citations>
                    <section_index>14</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>142</word_count>
                    <figure_citations></figure_citations>
                    <section_index>15</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>77</word_count>
                    <figure_citations></figure_citations>
                    <section_index>16</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>2744</word_count>
                    <figure_citations></figure_citations>
                    <section_index>17</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Feedforward</li>
                <li>Exergame</li>
                <li>Virtual reality (VR)</li>
                <li>Performance</li>
                <li>Intrinsic motivation</li>
            </keywords>
            <authors>
                <li>Soumya C. Barathi</li>
                <li> Daniel J. Finnegan</li>
                <li> Matthew Farrow</li>
                <li> Alexander Whaley</li>
                <li> Pippa Heath</li>
                <li> Jude Buckley</li>
                <li> Peter W. Dowrick</li>
                <li> Burkhard C. Wuensche</li>
                <li> James L. J. Bilzon</li>
                <li> Eamonn O'Neill</li>
                <li> Christof Lutteroth</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3173982_crop_1.jpg</url>
                <caption>Figure 1: a) The exergame is based on a computer-controlled stationary exercycle and played while wearing a head-mounted display.
                    b) A “self modelling cue” helps the player to identify a “ghost” avatar with their own previous performance. c) Low-intensity
                    cycling and avoiding trucks during warm-up, recovery and cool-down phases. d) High-intensity race against the “ghost.”
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3173982_crop_2.jpg</url>
                <caption>Figure 2: ∆Power (difference from baseline B) for the equal
                    challenge (E) and harder challenge (H) conditions in different
                    levels of resistance awareness (top-left) and in non-self (NSC)
                    vs. self competition (SC) framing (bottom-left). ∆Power for
                    H in different levels of resistance awareness (top-right) and
                    competition framing (bottom-right).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3173982_crop_3.jpg</url>
                <caption>Figure 3: IMI Interest/Enjoyment (left column) and IMI Pres-
                    sure/Tension (right column) scores for the baseline (B), equal
                    challenge (E) and harder challenge (H) conditions for 1) self
                    competition (SC) framing in different levels of resistance
                    awareness (top row) and 2) without resistance awareness (NA)
                    in self competition (SC) vs. non-self competition (NSC) fram-
                    ing (bottom row).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3173982_crop_4.jpg</url>
                <caption>Figure 4: FSQ Balance of Challenges and Skills (left column)
                    and FSQ Absorption in the Task (right column) scores for the
                    baseline (B), equal challenge (E) and harder challenge (H)
                    conditions for 1) self competition (SC) framing in different
                    levels of resistance awareness (top row) and 2) without resis-
                    tance awareness (NA) in self competition (SC) vs. non-self
                    competition (NSC) framing (bottom row).
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3173982_crop_5.jpg</url>
                <caption>Figure 5: IEQ scores in different levels of resistance awareness
                    (left) and competition framing (right).
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3174020</filename>
        <data>
            <paper_id>3173574.3174020</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3174020</doi>
            <title>Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation</title>
            <abstract>We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered-not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMS-constraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMS-detents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline.</abstract>
            <sections>
                <section>
                    <word_count>595</word_count>
                    <figure_citations>Figure 1: (a) In this Mixed Reality game that uses a physical tray as prop, our mobile system renders shifts in the tray’s center of gravity as the marble moves.Figure 1 illustrates this at the example of our Mixed Reality balance marble game using a physical tray as a game prop, which our approach augments via EMS-based force feedback.Figure 3 shows the user’s view through the HoloLens.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1298</word_count>
                    <figure_citations>Figure 1a shows the system she is wearing, i.Figure 2 shows a user wearing our EMS for Mixed Reality system.Figure 2: Using a regular cup as an impromptu tangible brightness dial.Figure 3: The previous scene through the HoloLens.Figure 4: (a) This user physically drags the couch and feels the simulated friction against the floor.Figure 4 shows the user exploring different placements in the room by pushing a couch with her two hands.Figure 4a).Figure 4b).Figure 5, the user now explores a lamp from the catalog.Figure 5b).Figure 5: Turning on the virtual lamp.Figure 6: The user configures the intensity of the desired light bulb using a cup as a stand-in for a dial (passive prop).Figure 6, the user picks up a cup to serve as a tangible brightness dial.Figure 6b shows how our system also adds detents to the dial.Figure 7a).Figure 7: The user manipulates two cups to control the light temperature and intensity simultaneously.Figure 7b).Figure 7c, for example, the user chooses a “colder” light, causing the system to switch to a less intense bulb by actuating the user’s wrist as to reach that option (Figure 7d).Figure 8: Here, our system enhances a fully functional thermostat with detents.Figure 10 shows a simple MR game featuring a virtual catapult that appears in the user’s physical surroundings.</figure_citations>
                    <section_index>1</section_index>
                    <title>WALKTHROUGH OF A MIXED REALITY EXPERIENCE</title>
                </section>
                <section>
                    <word_count>793</word_count>
                    <figure_citations>Figure 1 depicted a classic MR marble maze, which was in fact inspired by that of Ohan and Feiner [48] and complemented with EMS-based force feedback for added realism.Figure 9: Walkthrough examples mapped to the realityvirtuality continuum by Milgram et al.Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult’s spring.Figure 11: (a) At the start of the game the marble falls from the sky.Figure 12 illustrates the user solving this room’s puzzle.Figure 12, the user finds that when moved the lamps have detents, rendered using our system, hence they can be only in one of three positions.</figure_citations>
                    <section_index>2</section_index>
                    <title>SUMMARY OF WALKTHROUGH</title>
                </section>
                <section>
                    <word_count>1037</word_count>
                    <figure_citations>Figure 12: (a) These gooseneck lamps are repurposed as levers, with force feedback, allowing the user to input the secret combination to (b) unlock the door.</figure_citations>
                    <section_index>3</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1174</word_count>
                    <figure_citations>Figure 3b.Figure 13: The hardware components and electrode placement (one arm only).Figure 13 details how 10 electrodes are placed on the user’s right arm and shoulder; the user’s left arm is equipped the same way.Figure 14: Stimulation parameters per haptic effect at the example of one study participant: amplitude (in mA), pulse-width (in µs) and duration (in ms).</figure_citations>
                    <section_index>4</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>497</word_count>
                    <figure_citations>Figure 13, which allowed for untethered use.Figure 15 shows participant’s average ratings in both conditions regarding perceived realism.</figure_citations>
                    <section_index>5</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1014</word_count>
                    <figure_citations>Figure 15: Participants rated their experience as more realistic in the EMS conditions.Figure 16, participants rated the enjoyment significantly higher when in the EMS condition for the furniture and the catapult tasks.Figure 16: Participants rated their experience as more enjoyable in most of the EMS conditions.Figure 17 summarizes participants’ preferences for each of the interface conditions.Figure 17: Most participants preferred the EMS to the no-EMS interface condition across tasks.Figure 18) polarized participants in that only 7 of them expressed a preference for experiencing it with EMS.Figure 18: Participant balancing the marble (image from the study, with consent of the participant).</figure_citations>
                    <section_index>6</section_index>
                    <title>EMS</title>
                </section>
                <section>
                    <word_count>419</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSIONS AND OUTLOOK</title>
                </section>
                <section>
                    <word_count>1976</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Games</li>
                <li>Mixed reality</li>
                <li>EMS</li>
            </keywords>
            <authors>
                <li>Pedro Lopes</li>
                <li> Sijing You</li>
                <li> Alexandra Ion</li>
                <li> Patrick Baudisch</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3174020_crop_1.jpg</url>
                <caption>Figure 1: (a) In this Mixed Reality game that uses a
                    physical tray as prop, our mobile system renders shifts
                    in the tray’s center of gravity as the marble moves.
                    (b) Our system creates the necessary forces by applying
                    electrical muscle stimulation to users’ triceps muscles.
                    (c) Our approach leaves users’ hands free at all times,
                    allowing the user to interact with the tray.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3174020_crop_3.jpg</url>
                <caption>Figure 3: The previous scene through the HoloLens.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3174020_crop_4.jpg</url>
                <caption>Figure 4: (a) This user physically drags the couch and
                    feels the simulated friction against the floor. (b) As the
                    couch collides with a real wall, the system stops the user
                    by pushing the user’s shoulders and wrists backwards.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3174020_crop_5.jpg</url>
                <caption>Figure 5: Turning on the virtual lamp. Here our EMS
                    system renders the forces of the button’s mechanism.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3174020_crop_6.jpg</url>
                <caption>Figure 6: The user configures the intensity of the de-
                    sired light bulb using a cup as a stand-in for a dial (pas-
                    sive prop). Using EMS force feedback, our system aug-
                    ments the tangible with constraints and detents.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3174020_crop_7.jpg</url>
                <caption>Figure 7: The user manipulates two cups to control the
                    light temperature and intensity simultaneously.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3174020_crop_8.jpg</url>
                <caption>Figure 8: Here, our system enhances a fully functional
                    thermostat with detents.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3173574.3174020_crop_9.jpg</url>
                <caption>Figure 9: Walkthrough examples mapped to the reality-
                    virtuality continuum by Milgram et al. [43].
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3173574.3174020_crop_10.jpg</url>
                <caption>Figure 10: While the user pulls the lever of this virtual
                    catapult, our system provides force feedback simulating
                    the catapult’s spring.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3173574.3174020_crop_11.jpg</url>
                <caption>Figure 11: (a) At the start of the game the marble falls
                    from the sky. (b) As it hits the tray, the EMS pulls the
                    user’s arms down quickly so as to represent the impact.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>12</id>
                <url>3173574.3174020_crop_12.jpg</url>
                <caption>Figure 12: (a) These gooseneck lamps are repurposed as
                    levers, with force feedback, allowing the user to input
                    the secret combination to (b) unlock the door.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>13</id>
                <url>3173574.3174020_crop_13.jpg</url>
                <caption>Figure 13: The hardware components and electrode
                    placement (one arm only).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>14</id>
                <url>3173574.3174020_crop_14.jpg</url>
                <caption>Figure 14: Stimulation parameters per haptic effect at
                    the example of one study participant: amplitude (in
                    mA), pulse-width (in µs) and duration (in ms).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>15</id>
                <url>3173574.3174020_crop_15.jpg</url>
                <caption>Figure 15: Participants rated their experience as more
                    realistic in the EMS conditions.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>16</id>
                <url>3173574.3174020_crop_16.jpg</url>
                <caption>Figure 16: Participants rated their experience as more
                    enjoyable in most of the EMS conditions.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>17</id>
                <url>3173574.3174020_crop_17.jpg</url>
                <caption>Figure 17: Most participants preferred the EMS to the
                    no-EMS interface condition across tasks.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>18</id>
                <url>3173574.3174020_crop_18.jpg</url>
                <caption>Figure 18: Participant balancing the marble (image
                    from the study, with consent of the participant).
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3174039</filename>
        <data>
            <paper_id>3173574.3174039</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3174039</doi>
            <title>CatAR: A Novel Stereoscopic Augmented Reality Cataract Surgery Training System with Dexterous Instruments Tracking Technology</title>
            <abstract>We propose CatAR, a novel stereoscopic augmented reality (AR) cataract surgery training system. It provides dexterous instrument tracking ability using a specially designed infrared optical system with 2 cameras and 1 reflective marker. The tracking accuracy on the instrument tip is 20 µm, much higher than previous simulators. Moreover, our system allows trainees to use and to see real surgical instruments while practicing. Five training modules with 31 parameters were designed and 28 participants were enrolled to conduct efficacy and validity tests. The results revealed significant differences between novice and experienced surgeons. Improvements in surgical skills after practicing with CatAR were also significant.</abstract>
            <sections>
                <section>
                    <word_count>924</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>BACKGROUND AND RELATED WORK</title>
                </section>
                <section>
                    <word_count>1655</word_count>
                    <figure_citations>Figure 1b, 2) which helps meet user requirements for using real surgical instruments instead of fake props.Figure 3a), which helped users judge the depth of the tip with greater ease.Figure 3c) and dot markers on 3 different planes were designed for CatAR calibration (Figure 3b) to solve the extrinsic and intrinsic camera parameters.Figure 4), which provided a realistic visual environment for microsurgery training.Figure 5b).Figure 5a, 5b).Figure 5c).Figure 5d).</figure_citations>
                    <section_index>1</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>369</word_count>
                    <figure_citations>Figure 6).Figure 7a).</figure_citations>
                    <section_index>2</section_index>
                    <title>SYSTEM EVALUATION</title>
                </section>
                <section>
                    <word_count>294</word_count>
                    <figure_citations>Figure 7b).Figure 7c).</figure_citations>
                    <section_index>3</section_index>
                    <title>RMSE</title>
                </section>
                <section>
                    <word_count>445</word_count>
                    <figure_citations>Figure 1c, 7e).Figure 7d).Figure 7f).</figure_citations>
                    <section_index>4</section_index>
                    <title>TRAINING MODULE DESIGN</title>
                </section>
                <section>
                    <word_count>1096</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1419</word_count>
                    <figure_citations>Figure 9).</figure_citations>
                    <section_index>6</section_index>
                    <title>RESULT</title>
                </section>
                <section>
                    <word_count>633</word_count>
                    <figure_citations>Figure 8) in our study.Figure 9).Figure 10).Figure 11a), the difference was not statistically significant.Figure 11b).</figure_citations>
                    <section_index>7</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>134</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>187</word_count>
                    <figure_citations>Figure 11c).</figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>599</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
                <section>
                    <word_count>694</word_count>
                    <figure_citations></figure_citations>
                    <section_index>11</section_index>
                    <title>OF SURGICAL INSTRUMENTS IN THE EYE</title>
                </section>
            </sections>
            <keywords>
                <li>Augmented reality</li>
                <li>Microsurgery</li>
                <li>Surgical simulator</li>
                <li>Surgical training</li>
                <li>Cataract</li>
                <li>Instrument tracking</li>
                <li>Dexterous input</li>
            </keywords>
            <authors>
                <li>Yu-Hsuan Huang</li>
                <li> Hao-Yu Chang</li>
                <li> Wan-ling Yang</li>
                <li> Yu-Kai Chiu</li>
                <li> Tzu-Chieh Yu</li>
                <li> Pei-Hsuan Tsai</li>
                <li> Ming Ouhyoung</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3174039_crop_1.jpg</url>
                <caption>Figure 1. (a) A surgeon operating a CatAR system. (b) System overview: AR microscope platform, dual 4K displays, tracking area and surgical
                    mannequin are shown. (c) A real surgical instrument interacting with the virtual object in a training module. The iris, blue guidance curve, and white
                    rectangle are virtual objects overlaid on a real scene.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3174039_crop_2.jpg</url>
                <caption>Figure 2. CatAR system structure diagram.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3174039_crop_3.jpg</url>
                <caption>Figure 3. (a) Components and structure of the lower camera module.
                    (b) Dot markers on 3 different plans. (c) Three different types of micro
                    calibration boards.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3174039_crop_4.jpg</url>
                <caption>Figure 4. Stereoscopic display system in CatAR. The distance between
                    the eye and screen is 32 cm, the viewing angle is 50 degrees and has 2160
                    pixels in this field (43 PPD).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3174039_crop_5.jpg</url>
                <caption>Figure 5. (a) Spatula and (b) Capsule forceps used in cataract surgery.
                    Arrow: reflective tracker. (c) Standard hand posture on the face model
                    while holding forceps. (d) Dimensions of eye model, the soft part is made
                    using EVA material. (e) The soft part allows the forceps to be tilted in
                    the small artificial wound. (f) The elasticity will constrain the opening
                    distance of forceps while tilting laterally.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3174039_crop_6.jpg</url>
                <caption>Figure 6. This spatula is fixed on the linear translation stage and its tip
                    is placed inside the eye model through the wound. * Micrometer drive.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3174039_crop_7.jpg</url>
                <caption>Figure 7. Instructor’s 3D view (black star) and user’s AR view (white star) of 5 modules. (a) Antitremor module, 6 blue virtual balls are placed in the
                    pupil area, and turn to red when touched. Arrow: insertion point of the instrument. (b) Anterior chamber navigation module. Arrow: starting point
                    in the pupil center. (c) Circular tracing module, green line represents the curve drawn by the user along the white reference circle. (d) Forceps
                    training module: six blue balls are dragged to the small white point in the pupil center. Two white balls beside the blue ball represent the forceps tips.
                    Arrow: wound touch warning indicators, the lower one is touched and turned to red. (e) Capsulorhexis module: white thin box represents the proximal
                    end of the capsule flap. The box is dragged along the blue curve to the white point. (f) Capsulorhexis in the human eye.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3174039_crop_8.jpg</url>
                <caption>Figure 8. Wound touch counts and accumulated time of every
                    participants’ pre- and post-intervention in the forceps training module.
                    The IDs are sorted in ascending order by their total training experience.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3173574.3174039_crop_9.jpg</url>
                <caption>Figure 9. Correlation between experience and search time in the
                    anterior chamber navigation module. Linear regression lines are
                    presented.
                </caption>
                <page>10</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3173574.3174039_crop_10.jpg</url>
                <caption>Figure 10. 3D presentation of the results in circular tracing module.
                    Novice: R2, Experienced: Attending physician
                </caption>
                <page>10</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3173574.3174039_crop_11.jpg</url>
                <caption>Figure 11. Tracking plots on the XY and XZ planes in 3 different modules (unit of axes: mm). (a) Pre-intervention results of novice (R2) and
                    experienced surgeons (R4) in the antitremor module. (b) Pre-intervention results of novice (R2) and experienced surgeons (Fellow) in the forceps
                    training module. The pink circles represent the position of virtual balls. (c) Pre- and post-intervention results of the same novice participant (R2).
                    The circles in 4 different colors represent the starting and releasing point of 4 tears. The curves connecting 2 circles are the guidance tracts for the
                    trainee to follow.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3174057</filename>
        <data>
            <paper_id>3173574.3174057</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3174057</doi>
            <title>The Dream is Collapsing: The Experience of Exiting VR</title>
            <abstract>Research on virtual reality (VR) has studied users' experience of immersion, presence, simulator sickness, and learning effects. However, the momentary experience of exiting VR and transitioning back to the real-world is not well understood. Do users become self-conscious of their actions upon exit? Are users nervous of their surroundings? Using explicitation interviews, we explore the moment of exit from VR across four applications. Analysis of the interviews reveals five components of experience: space, control, sociality, time, and sensory adaptation. Participants described spatial disorientation, for example, regardless of the complexity of the VR scene. Participants also described a window across which they exit VR, for example mentally first and then physically. We present six designs for easing or heightening the exit experience, as described by the participants. Based on these findings, we further discuss the ?moment of exit' as an opportunity for designing engaging and enhanced VR experiences.</abstract>
            <sections>
                <section>
                    <word_count>667</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>985</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>4083</word_count>
                    <figure_citations>Figure 1, top left).Figure 1, bottom right).Figure 1, bottom left).Figure 2).</figure_citations>
                    <section_index>2</section_index>
                    <title>EXPLORING THE MOMENT OF EXIT</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>EXPERIENCE OF EXITING VR</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>LESSENING</title>
                </section>
                <section>
                    <word_count>1144</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>THE</title>
                </section>
                <section>
                    <word_count>1122</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>185</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>20</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1430</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>User Experience</li>
            </keywords>
            <authors>
                <li>Jarrod Knibbe</li>
                <li> Jonas Schjerlund</li>
                <li> Mathias Petraeus</li>
                <li> Kasper Hornbæk</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3174057_crop_1.jpg</url>
                <caption>Figure 1. The four VR scenarios. Top: Gaming and Illusions scenarios. Bottom: Cognitive and Perception scenarios.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3174057_crop_2.jpg</url>
                <caption>Figure 2. Illustration of the Illusion Scenario. The player
                    starts in the green location in both the virtual and real
                    environments. By slightly varying the player’s rotation in VR,
                    the player ends up in one corner of the room in VR (orange),
                    but in the opposite corner, facing the opposite direction in
                    reality (blue).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3174057_crop_3.jpg</url>
                <caption>Figure 3. Illustrating Abrupt vs Open Endings. Left: an
                    abrupt ending (fade to black), forces the player to exit VR.
                    Right: the game world remains visible while suggesting an
                    exit, leaving the player in control.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3174057_crop_4.jpg</url>
                <caption>Figure 4. Illustrating Soft Transitions. From left to right, the
                    game world softly fades into the real world, allowing the
                    player to re-orient themselves before removing the headset.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3174057_crop_5.jpg</url>
                <caption>Figure 5. Spatial Alignment illustration. Towards the end of
                    the game, objects of focus are moved to the extremities of the
                    real-space, helping the player adapt to the real-world
                    proportions before exiting VR.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3174057_crop_6.jpg</url>
                <caption>Figure 6. Illustration of Ability Dashboards. A heads-up-
                    display shows the participant special capabilities that are
                    enabled in VR (left) and shows them disabled before removing
                    the headset (right).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3174057_crop_7.jpg</url>
                <caption>Figure 7. Spatial Reconfiguration illustration. Left: the room
                    layout as the player enters VR. Right: The re-arranged layout
                    (where furnishings have moved) as the player exits VR.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3174057_crop_8.jpg</url>
                <caption>Figure 8. Illustration of Social Setting Changes. Left: the
                    social setting of the space as the player enters VR. Right: The
                    social setting upon exiting VR, where multiple people are
                    present.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3174088</filename>
        <data>
            <paper_id>3173574.3174088</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3174088.</doi>
            <sections></sections>
            <keywords>
                <li>Dementia</li>
                <li>Care</li>
                <li>Virtual reality</li>
                <li>Augmented reality</li>
                <li>Creativity</li>
                <li>Experience</li>
                <li>Expression</li>
            </keywords>
            <authors>
                <li>undefined</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3174088_crop_1.jpg</url>
                <caption>Figure 1: From sketching to final environment
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3174088_crop_2.jpg</url>
                <caption>Figure 2: creating the concert venue for Janet
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3174151</filename>
        <data>
            <paper_id>3173574.3174151</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3174151</doi>
            <title>Season Traveller: Multisensory Narration for Enhancing the Virtual Reality Experience</title>
            <abstract>In the same way that we experience the real-world through a range of senses, experiencing a virtual environment through multiple sensory modalities may augment both our presence within a scenario and our reaction to it. In this paper, we present Season Traveller, a multisensory virtual reality (VR) narration of a journey through four seasons within a mystical realm. By adding olfactory and haptic (thermal and wind) stimuli, we extend traditional audio-visual VR technologies to achieve enhanced sensory engagement within interactive experiences. Using both subjective measures of presence and elicited physiological responses, we evaluated the impact of different modalities on the virtual experience. Our results indicate that 1) the addition of any singular modality improves sense of presence with respect to traditional audio-visual experiences and 2) providing a combination of these modalities produces a further significant enhancement over the aforementioned improvements. Furthermore, insights into participants' psychophysiology were extrapolated from electrodermal activity (EDA) and heart rate (HR) measurements during each of the VR experiences.</abstract>
            <sections>
                <section>
                    <word_count>906</word_count>
                    <figure_citations>Figure 1: Season Traveller is a multisensory VR experience integrated with Samsung Gear VR HMD.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1029</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>BACKGROUND AND RELATED WORK</title>
                </section>
                <section>
                    <word_count>2372</word_count>
                    <figure_citations>Figure 1) that aims to distribute the weight of the system evenly between the front and the back of the user’s head.Figure 1).Figure 1).Figure 2, the chamber contained the smell emitting mechanisms, a control module, a fume extractor for clearing scented air between participant exposures, and a nose rest that ensured participants maintained a consistent distance from the stimuli.Figure 2: The experimental setup of the smell chamber used to compare olfactory delivery methods.Figure 3: Average sensation scores based on Labeled Magnitude Scale (LMS).Figure 4: Main modules of Season Traveller: (a) Control Module, (b) Olfactory Simulation Module, (c) Front Casing and Wind Simulation Module, (d) Thermal Simulation Module participants (10 males, average age = 26, SD = 2.</figure_citations>
                    <section_index>2</section_index>
                    <title>DESIGN</title>
                </section>
                <section>
                    <word_count>615</word_count>
                    <figure_citations>Figure 1, both the Control Module and all of the Simulation Modules were mounted on the Samsung Gear VR Paper 577 CHI 2018, April 21–26, 2018, Montréal, QC, Canada (a) Spring (b) Summer (c) Autumn (d) Winter Figure 5: The four seasons and their stimuli: a) Spring (Jasmine scent, medium wind strength and no thermal stimulation), b) Summer (Lemon scent, mild wind strength and heating stimulation), c) Autumn (Cinnamon scent, strong wind strength and no thermal stimulation) and d) Winter (Mint scent, medium wind strength and cooling stimulation).Figure 4).Figure 5).</figure_citations>
                    <section_index>3</section_index>
                    <title>SYSTEM IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>1854</word_count>
                    <figure_citations>Figure 6 for study setup).Figure 6: The experimental setup of Season Traveller.Figure 7: Normalized average Sensory sub-factors scores for five system configurations (Error bars represent 95% CI, n = 20).Figure 8: Normalized average Sensory Factors scores for five system configurations (Error bars represent 95% CI, n = 20).Figure 9: Normalized average scores for item 30 (richness of experience) (Error bars represent 95% CI, n = 20).Figure 9, post-hoc tests further indicate a significant improvement (p = 0.Figure 10 presents the baseline adjusted HR of all the participants recorded through the different system configurations.Figure 11 presents the baseline adjusted electrical skin conductivity of all the participants.</figure_citations>
                    <section_index>4</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>685</word_count>
                    <figure_citations>Figure 10: Baseline adjusted HR of all the participants recorded through the five different system configurations.Figure 11: Baseline adjusted EDA of all the participants recorded through the five different system configurations.</figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION AND CONCLUSION</title>
                </section>
                <section>
                    <word_count>29</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>42</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
                <section>
                    <word_count>1496</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ESSEX CORP WESTLAKE VILLAGE CA HUMAN</title>
                </section>
            </sections>
            <keywords>
                <li>Multisensory VR</li>
                <li>Virtual Reality</li>
                <li>Multimodal Interaction</li>
            </keywords>
            <authors>
                <li>Nimesha Ranasinghe</li>
                <li> Pravar Jain</li>
                <li> Nguyen Thi Ngoc Tram</li>
                <li> Koon Chuan Raymond Koh</li>
                <li> David Tolley</li>
                <li> Shienny Karwita</li>
                <li> Lin Lien-Ya</li>
                <li> Yan Liangkun</li>
                <li> Kala Shamaiah</li>
                <li> Chow Eason Wai Tung</li>
                <li> Ching Chiuan Yen</li>
                <li> Ellen Yi-Luen Do</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3174151_crop_1.jpg</url>
                <caption>Figure 1: Season Traveller is a multisensory VR experience
                    integrated with Samsung Gear VR HMD. It enhances partici-
                    pants’ sense of presence using visual, auditory, wind, thermal,
                    and olfactory stimuli.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3174151_crop_2.jpg</url>
                <caption>Figure 2: The experimental setup of the smell chamber used
                    to compare olfactory delivery methods.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3174151_crop_3.jpg</url>
                <caption>Figure 3: Average sensation scores based on Labeled Magni-
                    tude Scale (LMS). (Error bars represent 95% CI, n = 15).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3174151_crop_4.jpg</url>
                <caption>Figure 4: Main modules of Season Traveller: (a) Control
                    Module, (b) Olfactory Simulation Module, (c) Front Casing
                    and Wind Simulation Module, (d) Thermal Simulation Module
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3174151_crop_5.jpg</url>
                <caption>Figure 5: The four seasons and their stimuli: a) Spring (Jas-
                    mine scent, medium wind strength and no thermal stimula-
                    tion), b) Summer (Lemon scent, mild wind strength and heat-
                    ing stimulation), c) Autumn (Cinnamon scent, strong wind
                    strength and no thermal stimulation) and d) Winter (Mint scent,
                    medium wind strength and cooling stimulation).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3174151_crop_6.jpg</url>
                <caption>Figure 6: The experimental setup of Season Traveller. Physio-
                    logical signals were recorded during each participant’s experi-
                    ence with the system.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3174151_crop_7.jpg</url>
                <caption>Figure 7: Normalized average Sensory sub-factors scores for ﬁve system conﬁgurations (Error bars represent 95% CI, n = 20).
                    Signiﬁcant differences are highlighted by asterisks.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3173574.3174151_crop_8.jpg</url>
                <caption>Figure 8: Normalized average Sensory Factors scores for ﬁve
                    system conﬁgurations (Error bars represent 95% CI, n = 20).
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3173574.3174151_crop_9.jpg</url>
                <caption>Figure 9: Normalized average scores for item 30 (richness of
                    experience) (Error bars represent 95% CI, n = 20).
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3173574.3174151_crop_10.jpg</url>
                <caption>Figure 10: Baseline adjusted HR of all the participants
                    recorded through the ﬁve different system conﬁgurations.
                    Solid lines represent the means and the ﬁlled areas represent
                    their respective standard deviations.
                </caption>
                <page>10</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3173574.3174151_crop_11.jpg</url>
                <caption>Figure 11: Baseline adjusted EDA of all the participants
                    recorded through the ﬁve different system conﬁgurations.
                    Solid lines represent the means and the ﬁlled areas represent
                    their respective standard deviations.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3173574.3174221</filename>
        <data>
            <paper_id>3173574.3174221</paper_id>
            <venue>CHI 18</venue>
            <doi>10.1145/3173574.3174221</doi>
            <title>Selection-based Text Entry in Virtual Reality</title>
            <abstract>In recent years, Virtual Reality (VR) and 3D User Interfaces (3DUI) have seen a drastic increase in popularity, especially in terms of consumer-ready hardware and software. While the technology for input as well as output devices is market ready, only a few solutions for text input exist, and empirical knowledge about performance and user preferences is lacking. In this paper, we study text entry in VR by selecting characters on a virtual keyboard. We discuss the design space for assessing selection-based text entry in VR. Then, we implement six methods that span different parts of the design space and evaluate their performance and user preferences. Our results show that pointing using tracked hand-held controllers outperforms all other methods. Other methods such as head pointing can be viable alternatives depending on available resources. We summarize our findings by formulating guidelines for choosing optimal virtual keyboard text entry methods in VR.</abstract>
            <sections>
                <section>
                    <word_count>13</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>NG</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>HAND</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>T</title>
                </section>
                <section>
                    <word_count>215</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>NUOUSCURSOR</title>
                </section>
                <section>
                    <word_count>508</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>2892</word_count>
                    <figure_citations>Figure 1): • Head Pointing (HP) — the participant selects a character by pointing to it with her head.</figure_citations>
                    <section_index>5</section_index>
                    <title>X</title>
                </section>
                <section>
                    <word_count>1092</word_count>
                    <figure_citations>Figure 1).Figure 1).Figure 1).Figure 1).</figure_citations>
                    <section_index>6</section_index>
                    <title>EVALUATED TEXT INPUT TECHNIQUES</title>
                </section>
                <section>
                    <word_count>35</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>EMPIRICAL STUDY</title>
                </section>
                <section>
                    <word_count>13</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>HP</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>CP</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>CT</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>11</section_index>
                    <title>FH</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>12</section_index>
                    <title>CC</title>
                </section>
                <section>
                    <word_count>6</word_count>
                    <figure_citations></figure_citations>
                    <section_index>13</section_index>
                    <title>DC</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>14</section_index>
                    <title>WHO</title>
                </section>
                <section>
                    <word_count>3</word_count>
                    <figure_citations></figure_citations>
                    <section_index>15</section_index>
                    <title>WANTS TO</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>16</section_index>
                    <title>VER</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>17</section_index>
                    <title>TS TO LIVE</title>
                </section>
                <section>
                    <word_count>746</word_count>
                    <figure_citations>Figure 3).</figure_citations>
                    <section_index>18</section_index>
                    <title>FO</title>
                </section>
                <section>
                    <word_count>508</word_count>
                    <figure_citations></figure_citations>
                    <section_index>19</section_index>
                    <title>S</title>
                </section>
                <section>
                    <word_count>174</word_count>
                    <figure_citations>Figure 4).</figure_citations>
                    <section_index>20</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>486</word_count>
                    <figure_citations>Figure 5).</figure_citations>
                    <section_index>21</section_index>
                    <title>WPM</title>
                </section>
                <section>
                    <word_count>961</word_count>
                    <figure_citations></figure_citations>
                    <section_index>22</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>23</section_index>
                    <title>START</title>
                </section>
                <section>
                    <word_count>11</word_count>
                    <figure_citations></figure_citations>
                    <section_index>24</section_index>
                    <title>YES</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>25</section_index>
                    <title>HEAD POINTING</title>
                </section>
                <section>
                    <word_count>10</word_count>
                    <figure_citations></figure_citations>
                    <section_index>26</section_index>
                    <title>NO</title>
                </section>
                <section>
                    <word_count>138</word_count>
                    <figure_citations></figure_citations>
                    <section_index>27</section_index>
                    <title>L</title>
                </section>
                <section>
                    <word_count>14</word_count>
                    <figure_citations></figure_citations>
                    <section_index>28</section_index>
                    <title>H</title>
                </section>
                <section>
                    <word_count>97</word_count>
                    <figure_citations></figure_citations>
                    <section_index>29</section_index>
                    <title>CURSOR CONTROL</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>30</section_index>
                    <title>POINTING</title>
                </section>
                <section>
                    <word_count>290</word_count>
                    <figure_citations>Figure 7).</figure_citations>
                    <section_index>31</section_index>
                    <title>FREEHAND</title>
                </section>
                <section>
                    <word_count>287</word_count>
                    <figure_citations></figure_citations>
                    <section_index>32</section_index>
                    <title>CONCLUSION AND OUTLOOK</title>
                </section>
                <section>
                    <word_count>1719</word_count>
                    <figure_citations></figure_citations>
                    <section_index>33</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Text entry</li>
                <li>Pointing</li>
                <li>Mid-air</li>
                <li>User experience</li>
                <li>Task performance</li>
            </keywords>
            <authors>
                <li>Marco Speicher</li>
                <li> Anna Maria Feit</li>
                <li> Pascal Ziegler</li>
                <li> Antonio Krüger</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3173574.3174221_crop_1.jpg</url>
                <caption>Figure 1. This ﬁgure illustrates our implemented selection-based text entry candidates for VR. From left to right: Head Pointing (HP, red), Controller
                    Pointing (CP, yellow), Controller Tapping (CT, blue), Freehand (FH, green), Discrete (DC, orange) and Continuous Cursor (CC, light blue).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3173574.3174221_crop_2.jpg</url>
                <caption>Figure 2. This ﬁgure illustrates the experimental setup. (1) HTC Vive
                    optical tracker (at 2.5m) and tracking space with 4 × 4m2. (2) Virtual
                    keyboard, stimulus and text input ﬁeld. (3) Participant wearing HTC
                    Vive and tracked hand-held controllers. (4) PC for experiment control
                    and ﬁlling out questionnaires.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3173574.3174221_crop_3.jpg</url>
                <caption>Figure 3. This ﬁgure shows the virtual environment including stimulus
                    (purple), text input ﬁeld, and the virtual keyboard.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3173574.3174221_crop_4.jpg</url>
                <caption>Figure 4. The upper ﬁgure show the speed measurements, given in words
                    per minute (WPM). Below, the corrected error rate measurements, given
                    in percent (%).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3173574.3174221_crop_5.jpg</url>
                <caption>Figure 5. User Experience Questionnaire (UEQ) ratings with respect to
                    comparison benchmarks.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3173574.3174221_crop_6.jpg</url>
                <caption>Figure 6. This ﬁgure shows the NASA subscales with signiﬁcant differences between the six text input methods. Non-signiﬁcant subscales (mental
                    demand, temporal demand, and effort) are not shown for better clarity.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3173574.3174221_crop_7.jpg</url>
                <caption>Figure 7. Decision support tool for VR text input on a virtual keyboard.
                    Discrete Cursor is not considered, because of the bad results across all
                    measurements. Controller Tapping performed slightly worse than Point-
                    ing, so it is not considered due to its higher technical requirements.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376195</filename>
        <data>
            <paper_id>3313831.3376195</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376195</doi>
            <title>Therminator: Understanding the Interdependency of Visual and On-Body Thermal Feedback in Virtual Reality</title>
            <abstract>Recent advances have made Virtual Reality (VR) more realistic than ever before. This improved realism is attributed to today's ability to increasingly appeal to human sensations, such as visual, auditory or tactile. While research also examines temperature sensation as an important aspect, the interdependency of visual and thermal perception in VR is still underexplored. In this paper, we propose Therminator, a thermal display concept that provides warm and cold on-body feedback in VR through heat conduction of flowing liquids with different temperatures. Further, we systematically evaluate the interdependency of different visual and thermal stimuli on the temperature perception of arm and abdomen with 25 participants. As part of the results, we found varying temperature perception depending on the stimuli, as well as increasing involvement of users during conditions with matching stimuli.</abstract>
            <sections>
                <section>
                    <word_count>760</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1376</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1117</word_count>
                    <figure_citations>Figure 2a).Figure 2b).Figure 3c depicts a closeup perspective of the actuator tubes, as well as an example thermal camera view at 43 ◦C.Figure 3b shows a user wearing two actuators on the right arm and the abdomen.</figure_citations>
                    <section_index>2</section_index>
                    <title>THERMINATOR CONCEPTS AND SYSTEM</title>
                </section>
                <section>
                    <word_count>2192</word_count>
                    <figure_citations>Figure 1a) shows a participant while exposed to the snow visualization.Figure 3a.</figure_citations>
                    <section_index>3</section_index>
                    <title>METHODOLOGY</title>
                </section>
                <section>
                    <word_count>1447</word_count>
                    <figure_citations>Figure 5a.Figure 5b.Figure 5c, the median rating as well as the minimum and maximum ratings show differences with regards to the visuals.</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>895</word_count>
                    <figure_citations>Figure 5a), the thermal stimuli have the highest impact on the perceived temperature.Figure 5b and Figure 5c), the involvement resulted in higher medians for both stimuli.Figure 5a), they only slightly affect the involvement if there is no visual stimulus displayed.Figure 6).</figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>433</word_count>
                    <figure_citations>Figure 1d).Figure 1c), discovered on a treasure map.Figure 1b).</figure_citations>
                    <section_index>6</section_index>
                    <title>EXAMPLE APPLICATIONS</title>
                </section>
                <section>
                    <word_count>421</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>LIMITATIONS AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>105</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>41</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>2593</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Haptics</li>
                <li>Temperature</li>
                <li>Thermal Feedback</li>
                <li>Virtual Reality</li>
            </keywords>
            <authors>
                <li>Sebastian Günther</li>
                <li> Florian Müller</li>
                <li> Dominik Schön</li>
                <li> Omar Elmoghazy</li>
                <li> Max Mühlhäuser</li>
                <li> Martin Schmitz</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376195_crop_1.jpg</url>
                <caption>Figure 1. Therminator concepts and example VR applications showing (a) a user during our experiment with a snow visual stimulus, (b) a cold game
                    environment with a user throwing snowballs, (c) a warm tropical islands, and (d) a ﬁreﬁghting simulation with a user extinguishing ﬂames.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376195_crop_2.jpg</url>
                <caption>Figure 2. Concept of our Therminator system. a) We use heat conducting tubes which can be ﬂexibly formed to ﬁt various shapes of different body
                    parts. We let liquids with adjustable temperatures ﬂow through the tubes which are then emitting their temperature. b) We have four parameters to
                    adjust the temperature, arrangement and number, shape, and size of each tube.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376195_crop_4.jpg</url>
                <caption>Figure 4. Visual stimuli based on their expected temperature from cold to hot: snow, rain cloud, neutral / no visualization, heating lamp, ﬁre.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376195_crop_6.jpg</url>
                <caption>Figure 6. Comfort rating of the participants with regards to the thermal
                    stimuli (from very uncomfortable to very comfortable). Each temperature
                    level is split into both body parts.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376228</filename>
        <data>
            <paper_id>3313831.3376228</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376228</doi>
            <title>Bridging the Virtual and Real Worlds: A Preliminary Study of Messaging Notifications in Virtual Reality</title>
            <abstract>Virtual reality (VR) platforms provide their users with immersive virtual environments, but disconnect them from real-world events. The increasing length of VR sessions can therefore be expected to boost users' needs to obtain information about external occurrences such as message arrival. Yet, how and when to present these real-world notifications to users engaged in VR activities remains underexplored. We conducted an experiment to investigate individuals' receptivity during four VR activities (Loading, 360 Video, Treasure Hunt, Rhythm Game) to message notifications delivered using three types of displays (head-mounted, controller, and movable panel). While higher engagement generally led to higher perceptions that notifications were ill-timed and/or disruptive, the suitability of notification displays to VR activities was influenced by the time-sensitiveness of VR content, overlapping use of modalities for delivering alerts, the display locations, and a requirement that the display be moved for notifications to be seen. Specific design suggestions are also provided.</abstract>
            <sections>
                <section>
                    <word_count>646</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>525</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1922</word_count>
                    <figure_citations>Figure 1, top left) was inspired by the Steam VR Home.Figure 1, top right) is a common VR activity for individuals wearing HMDs.Figure 1 bottom left) is a popular VR gaming format.Figure 1, bottom right) was inspired by a popular VR game called Beat Saber1.Figure 2).Figure 2a) was fixed to the upper left corner in the user’s field of view: specifically, in the near-peripheral region, about 25 degrees from the line of sight [61].Figure 2b) was inspired by NotifiVR [20].Figure 2).Figure 2c) was inspired by Facebook Space’s2 information pad.Figure 3, top).Figure 3, bottom), in each of which participant experienced one of the three VR activities (360 Video, Treasure Hunt, and Rhythm Game) preceded by a Loading activity to simulate system loading.</figure_citations>
                    <section_index>2</section_index>
                    <title>THE EXPERIMENT</title>
                </section>
                <section>
                    <word_count>273</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>DATA ANALYSIS</title>
                </section>
                <section>
                    <word_count>2191</word_count>
                    <figure_citations>Figure 4 indicates, they were the most engaged in Rhythm Game (M=6.Figure 5, showing the percentages of notifications the participants actually looked at, indicates that they failed to see a significant portion of notifications in Rhythm Game, especially when alerts were sent via the controller (59.Figure 6, top left), followed by Treasure Hunter (M=3.Figure 6, bottom left), all the previously noted differences held true, but in Rhythm Game, alerts presented via controller display (M=5.Figure 6, top center): i.Figure 6, top right).Figure 6, bottom right), we observed a significant increase in the recall of notifications for all displays (HMD: 75%=> 77.</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>729</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>1096</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>DESIGN RECOMMENDATIONS</title>
                </section>
                <section>
                    <word_count>390</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>LIMITATIONS</title>
                </section>
                <section>
                    <word_count>266</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>56</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>2205</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Notification systems</li>
                <li>Interruptibility</li>
                <li>Receptivity</li>
                <li>Eye-tracking</li>
            </keywords>
            <authors>
                <li>Ching-Yu Hsieh</li>
                <li> Yi-Shyuan Chiang</li>
                <li> Hung-Yu Chiu</li>
                <li> Yung-Ju Chang</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>3</id>
                <url>3313831.3376228_crop_3.jpg</url>
                <caption>Figure 3. A sample of the study procedure (top) and the timeline
                    of each block (bottom).
                    one 11-20 times. The remaining 11 (27.5%) had used VR
                    more than 20 times. The participants were balanced in terms
                    of their self-reported receptivity to message notifications,
                    with 10 individuals fitting into each of the following four
                    categories: 1) tend to ignore notifications; 2) tend not to deal
                    with notifications immediately, but check and respond later;
                    3) tend to check notifications immediately, but respond later;
                    and 4) tend to check and respond immediately.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376228_crop_4.jpg</url>
                <caption>Figure 4. Means and SDs of engagement, by VR activity.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376228_crop_6.jpg</url>
                <caption>Figure 6. All notifications’ means and SDs of the outcome variables: Perceived Disruptiveness (top left), Perceived Timeliness (top
                    center) and Recall (top right); seen notifications’ means and SDs of the outcome variables: Perceived Disruptiveness (bottom left),
                    Perceived Timeliness (bottom center) and Recall (bottom right).
                </caption>
                <page>6</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376243</filename>
        <data>
            <paper_id>3313831.3376243</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376243</doi>
            <title>JumpVR: Jump-Based Locomotion Augmentation for Virtual Reality</title>
            <abstract>
                One of the great benefits of virtual reality (VR) is the implementation of features that go beyond realism. Common "unrealistic" locomotion techniques (like teleportation) can avoid spatial limitation of tracking, but minimize potential benefits of more realistic techniques (e.g. walking). As an alternative that combines realistic physical movement with hyper-realistic virtual outcome, we present JumpVR, a jump-based locomotion augmentation technique that virtually scales users' physical jumps. In a user study (N=28), we show that jumping in VR (regardless of scaling) can significantly increase presence, motivation and immersion compared to teleportation, while largely not increasing simulator sickness. Further, participants reported higher immersion and motivation for most scaled jumping variants than forward-jumping. Our work shows the feasibility and benefits of jumping in VR and explores suitable parameters for its hyper-realistic scaling. We discuss design implications for VR experiences and research.
            </abstract>
            <sections>
                <section>
                    <word_count>476</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1216</word_count>
                    <figure_citations>Figure 1c) [3].</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>154</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>RESEARCH FOCUS</title>
                </section>
                <section>
                    <word_count>717</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>1026</word_count>
                    <figure_citations>Figure 1).Figure 2b).Figure 2a).Figure 2a), MoveInPlace was also enabled.Figure 2a.Figure 3 (all shown jumps were executed with a mean airtime of 250ms).</figure_citations>
                    <section_index>4</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>770</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>C ONDITION PAIR</title>
                </section>
                <section>
                    <word_count>116</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>SSQ</title>
                </section>
                <section>
                    <word_count>25</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>IMI</title>
                </section>
                <section>
                    <word_count>281</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>IPQ</title>
                </section>
                <section>
                    <word_count>3</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>C ONDITION</title>
                </section>
                <section>
                    <word_count>3</word_count>
                    <figure_citations></figure_citations>
                    <section_index>11</section_index>
                    <title>J UMPS</title>
                </section>
                <section>
                    <word_count>1981</word_count>
                    <figure_citations></figure_citations>
                    <section_index>12</section_index>
                    <title>FALLS</title>
                </section>
                <section>
                    <word_count>105</word_count>
                    <figure_citations></figure_citations>
                    <section_index>13</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>24</word_count>
                    <figure_citations></figure_citations>
                    <section_index>14</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1485</word_count>
                    <figure_citations></figure_citations>
                    <section_index>15</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>VR</li>
                <li>Virtual reality</li>
                <li>Jumping</li>
                <li>Super human</li>
                <li>Immersion</li>
                <li>Hyper realism</li>
            </keywords>
            <authors>
                <li>Dennis Wolf</li>
                <li> Katja Rogers</li>
                <li> Christoph Kunder</li>
                <li> Enrico Rukzio</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376243_crop_1.jpg</url>
                <caption>Figure 1: We use physical jumps to augment locomotion in VR, by applying a scaling factor to extend the natural jumping parabola
                    by forward motion (a). The range of the previous jump is indicated to users by a radius indicator (b). We compared this scaled
                    jumping to a teleportation baseline (c).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376243_crop_2.jpg</url>
                <caption>Figure 2: Topviews of the course (a) in the main study (all conditions except forward-jumping) and (b) the smaller second
                    course used only for the forward-jumping condition. The brown blocks in the main-study course were checkpoints; upon falling,
                    participants were re-set to the last passed checkpoint.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376243_crop_3.jpg</url>
                <caption>Figure 3: The different scaled jumping conditions in comparison, visualizing the range of each scaling for an average jump on the
                    yellow square to the left.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376243_crop_4.jpg</url>
                <caption>Figure 4: The main states the system is able to detect: On-
                    Ground (a), KneesBent (b), Rising/Falling (c), Landing (d).
                    The dashed line symbolises the measured baseline.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376243_crop_5.jpg</url>
                <caption>Figure 5: Immersion, interest/enjoyment, and presence were rated higher for the scaled jumping conditions than compared to
                    are highlighted with ∗ (p&lt;.05), ∗∗ (p&lt;.01) teleportation (and largely also compared to forward-jumping). Signiﬁcant differences and ∗ ∗ ∗ (p&lt;.001). </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376243_crop_6.jpg</url>
                <caption>Figure 6: Results of participants’ most preferred condition.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376243_crop_7.jpg</url>
                <caption>Figure 7: Comfort with HMD.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376260</filename>
        <data>
            <paper_id>3313831.3376260</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376260</doi>
            <title>Examining Design Choices of Questionnaires in VR User Studies</title>
            <abstract>
                Questionnaires are among the most common research tools in virtual reality (VR) user studies. Transitioning from virtuality to reality for giving self-reports on VR experiences can lead to systematic biases. VR allows to embed questionnaires into the virtual environment which may ease participation and avoid biases. To provide a cohesive picture of methods and design choices for questionnaires in VR (inVRQ), we discuss 15 inVRQ studies from the literature and present a survey with 67 VR experts from academia and industry. Based on the outcomes, we conducted two user studies in which we tested different presentation and interaction methods of inVRQs and evaluated the usability and practicality of our design. We observed comparable completion times between inVRQs and questionnaires outside VR (nonVRQs) with higher enjoyment but lower usability for \inVRQs. These findings advocate the application of inVRQs and provide an overview of methods and considerations that lay the groundwork for inVRQ design.
            </abstract>
            <sections>
                <section>
                    <word_count>867</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>818</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>STATE OF THE ART</title>
                </section>
                <section>
                    <word_count>390</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>LITERATURE REVIEW</title>
                </section>
                <section>
                    <word_count>868</word_count>
                    <figure_citations>Figure 1 depicts 7 different realizations of IN VRQ S.</figure_citations>
                    <section_index>3</section_index>
                    <title>HUD</title>
                </section>
                <section>
                    <word_count>1590</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>EXPERT SURVEY</title>
                </section>
                <section>
                    <word_count>1018</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DESIGN STUDY</title>
                </section>
                <section>
                    <word_count>538</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>989</word_count>
                    <figure_citations>Figure 5b.</figure_citations>
                    <section_index>7</section_index>
                    <title>UMUX</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>INV</title>
                </section>
                <section>
                    <word_count>1085</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REAL</title>
                </section>
                <section>
                    <word_count>595</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>67</word_count>
                    <figure_citations></figure_citations>
                    <section_index>11</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>7378</word_count>
                    <figure_citations></figure_citations>
                    <section_index>12</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>VR</li>
                <li>User studies</li>
                <li>In-VR questionnaires</li>
                <li>InVRQs</li>
                <li>Research methods</li>
            </keywords>
            <authors>
                <li>Dmitry Alexandrovsky</li>
                <li> Susanne Putze</li>
                <li> Michael Bonfert</li>
                <li> Sebastian Höffner</li>
                <li> Pitt Michelmann</li>
                <li> Dirk Wenig</li>
                <li> Rainer Malaka</li>
                <li> Jan David Smeddinck</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376260_crop_1.jpg</url>
                <caption>Figure 1. Examples of different realizations of INVRQ: (a) and (b) present the questionnaire using a HUD, (c)-(f) use a world-referenced questionnaire,
                    and (g) presents the questionnaire attached to the body.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376260_crop_2.jpg</url>
                <caption>Figure 2. Ratings of the usefulness of INVRQS (Q17, scale: 0 to 5, 5
                    being highest) separated by INVRQ experience (Yes/No).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376260_crop_3.jpg</url>
                <caption>Figure 3. Screenshots of the archery task. The world space-anchored
                    INVRQ is ﬁlled out using the HTC Vive controller as a laser pointer.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376260_crop_4.jpg</url>
                <caption>Figure 4. Ratings of usability on UMUX [155] (left) and of completion
                    times (right) for both conditions.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376260_crop_5.jpg</url>
                <caption>Figure 5. Boxplots of presence and workload split by questionnaire type
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376265</filename>
        <data>
            <paper_id>3313831.3376265</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376265</doi>
            <title>Virtual Reality Games for People Using Wheelchairs</title>
            <abstract>Virtual Reality (VR) holds the promise of providing engaging embodied experiences, but little is known about how people with disabilities engage with it. We explore challenges and opportunities of VR gaming for wheelchair users. First, we present findings from a survey that received 25 responses and gives insights into wheelchair users' motives to (non-) engage with VR and their experiences. Drawing from this survey, we derive design implications which we tested through implementation and qualitative evaluation of three full-body VR game prototypes with 18 participants. Our results show that VR gaming engages wheelchair users, though nuanced consideration is required for the design of embodied immersive experiences for minority bodies, and we illustrate how designers can create meaningful, positive experiences.</abstract>
            <sections>
                <section>
                    <word_count>599</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>3556</word_count>
                    <figure_citations>Figure 2).Figure 3).</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>147</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>PENS</title>
                </section>
                <section>
                    <word_count>2588</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>TLX</title>
                </section>
                <section>
                    <word_count>836</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>189</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>LIMITATIONS AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>144</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>41</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1056</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Accessibility</li>
                <li>Games</li>
                <li>Virtual reality</li>
            </keywords>
            <authors>
                <li>Kathrin Gerling</li>
                <li> Patrick Dickinson</li>
                <li> Kieran Hicks</li>
                <li> Liam Mason</li>
                <li> Adalberto L. Simeone</li>
                <li> Katta Spiel</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376265_crop_1.jpg</url>
                <caption>Figure 1. Screenshot of Karamaisu Slope.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376265_crop_2.jpg</url>
                <caption>Figure 2. Screenshot of Dungeon Spell.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376265_crop_3.jpg</url>
                <caption>Figure 3. Screenshot of Space Travel.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376265_crop_4.jpg</url>
                <caption>Figure 4. Sketch of a player engaging with Space Travel.
                </caption>
                <page>6</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376286</filename>
        <data>
            <paper_id>3313831.3376286</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376286</doi>
            <title>MoveVR: Enabling Multiform Force Feedback in Virtual Reality using Household Cleaning Robot</title>
            <abstract>Haptic feedback can significantly enhance the realism and immersiveness of virtual reality (VR) systems. In this paper, we propose MoveVR, a technique that enables realistic, multiform force feedback in VR leveraging commonplace cleaning robots. MoveVR can generate tension, resistance, impact and material rigidity force feedback with multiple levels of force intensity and directions. This is achieved by changing the robot's moving speed, rotation, position as well as the carried proxies. We demonstrated the feasibility and effectiveness of MoveVR through interactive VR gaming. In our quantitative and qualitative evaluation studies, participants found that MoveVR provides more realistic and enjoyable user experience when compared to commercially available haptic solutions such as vibrotactile haptic systems.</abstract>
            <sections>
                <section>
                    <word_count>518</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>610</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1206</word_count>
                    <figure_citations>Figure 1 shows the setup of our prototype.Figure 3 B).Figure 3 A and 3 C).Figure 3 D shows, users can tie shared proxies (strings, ropes or sewing threads) to the robot to simulate force feedback such as tension.Figure 3 E indicates.Figure 3 F shows.Figure 3 H shows.Figure 3 I shows, users can attach the VR controller with tape onto a user-driving proxy simulating a stick or a ﬁshing rod.Figure 3 J shows.Figure 3 D).Figure 4A shows.</figure_citations>
                    <section_index>2</section_index>
                    <title>MOVEVR IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>3883</word_count>
                    <figure_citations>Figure 4A shows, different levels of tensile strength can be simulated through changing the distance between the user and MoveVR: the further they are apart, the stronger the tension.Figure 5 A.Figure 5 C illustrates.Figure 5 D illustrates.Figure 5 D illustrates, the robot bumps into the user to simulate a dog rubbing up against his/her leg or a strike on his/her shoulder.Figure 5 B).Figure 5 B simulates two virtual objects built with different materials.Figure 5B.Figure 6 shows the accuracy and confusion matrix of each benchmark study.Figure 6 A), 96.Figure 6 B), 99.Figure 6 C), 97.Figure 6 D), and 96.Figure 6 E).Figure 7, the user has a joint experience with a dog leading her/him to the front door.Figure 8, the user notices a box blocking the entry.Figure 9 B where an enemy escapes from the user’s hit.Figure 10 shows that participants perceived the virtual world as more realistic and enjoyable in the MoveVR condition compared to the barehand (p = 0.Figure 11A indicates.Figure 11 indicates.</figure_citations>
                    <section_index>3</section_index>
                    <title>MOVEVR FORCE EXPRESSIONS</title>
                </section>
                <section>
                    <word_count>888</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>143</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>249</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>REFERENCES</title>
                </section>
                <section>
                    <word_count>1511</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGEMENT</title>
                </section>
            </sections>
            <keywords>
                <li>Force feedback</li>
                <li>Haptic feedback</li>
                <li>Virtual reality</li>
                <li>VR</li>
                <li>Robotics</li>
                <li>Cleaning robot</li>
                <li>Human-robot interaction</li>
            </keywords>
            <authors>
                <li>Yuntao Wang</li>
                <li> Zichao (Tyson) Chen</li>
                <li> Hanchuan Li</li>
                <li> Zhengyi Cao</li>
                <li> Huiyi Luo</li>
                <li> Tengxiang Zhang</li>
                <li> Ke Ou</li>
                <li> John Raiti</li>
                <li> Chun Yu</li>
                <li> Shwetak Patel</li>
                <li> Yuanchun Shi</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376286_crop_1.jpg</url>
                <caption>Figure 1. Overview of MoveVR. A: Hardware setup. B: Adjusting
                    tension intensities and directions by conﬁguring the relative position
                    between the robot and the user. C: Multi-level resistance force feedback
                    by conﬁguring the robot’s direction of motion. D: Simulating material
                    rigidity through rotating different material proxies within reach of users.
                    E: Applying different levels of impact force to users by varying the speed.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376286_crop_2.jpg</url>
                <caption>Figure 2. MoveVR system architecture.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376286_crop_3.jpg</url>
                <caption>Figure 3. Fabricating and assembling MoveVR. A: Example of
                    fabrication tools. B: Example of everyday objects as proxies. C: Touch
                    fasteners or double-sided tape to ﬁx the carry-on proxy. D: Attaching the
                    shared-proxy onto the robot. E: Fixing the carry-on proxy using touch
                    fasteners. F: Fixing the carry-on proxy using tapes. H - I: Attaching the
                    VR controller with the user-driving proxy. G/J: Examples of MoveVR
                    carrying proxies.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376286_crop_4.jpg</url>
                <caption>Figure 4. Force expression explanation. A: Tension force through
                    an elastic string. B: Reaction force simulating material rigidity. C:
                    Resistance force against user’s force. D: Impact force that the robot
                    actively applies to the user.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376286_crop_5.jpg</url>
                <caption>Figure 5. MoveVR can simulate a variety of force feedback with everyday
                    objects as proxies. A: Tension using string/rope proxies between users
                    and robots. B: Reaction with proxies built by different materials. C:
                    Resistance by conﬁguring the wheels’ status. D: Impact from the robot
                    crashing on users with varying speeds.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376286_crop_6.jpg</url>
                <caption>Figure 6. Results of force perception accuracy. A: Tension force intensity.
                    B: Resistive force intensity. C: Impact force intensity. D: Material
                    rigidity. E: Tension force direction.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376286_crop_7.jpg</url>
                <caption>Figure 7. Dog walking VR scenario with four haptic conditions.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376286_crop_8.jpg</url>
                <caption>Figure 8. Box Pushing VR scenario with four haptic conditions.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376286_crop_9.jpg</url>
                <caption>Figure 9. Enemy hitting VR scenario with four haptic conditions. A:
                    The user strikes the enemy successfully. B: The enemy avoids a strike
                    from the user. C: The enemy attacks the user and the user striking back.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3313831.3376286_crop_10.jpg</url>
                <caption>Figure 10. Overall Likert score of four conditions. Error bar indicates
                    the standard error of the mean. * indicates signiﬁcant difference
                    between two conditions with p &lt;0.05. </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3313831.3376286_crop_11.jpg</url>
                <caption>Figure 11. A: Likert scores of continuous pulling force feedback (dog
                    leading scenario). B: Likert scores of resistive force feedback (box
                    pushing scenario). C: Likert scores of reaction force feedback (enemy
                    hitting scenario). D: Likert scores of impact force feedback (enemy
                    attacking scenario). Error bar indicates the standard error of the mean.
                    * indicates signiﬁcant difference between two conditions with p &lt;0.05. </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376358</filename>
        <data>
            <paper_id>3313831.3376358</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376358</doi>
            <title>PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality</title>
            <abstract>
                We introduce PoCoPo, the first handheld pin-based shape display that can render various 2.5D shapes in hand in realtime. We designed the display small enough for a user to hold it in hand and carry it around, thereby enhancing the haptic experiences in a virtual environment. PoCoPo has 18 motor-driven pins on both sides of a cuboid, providing the sensation of skin contact on the user's palm and fingers. We conducted two user studies to understand the capability of PoCoPo. The first study showed that the participants were generally successful in distinguishing the shapes rendered by PoCoPo with an average success rate of 88.5%. In the second study, we investigated the acceptable visual size of a virtual object when PoCoPo rendered a physical object of a certain size. The result led to a better understanding of the acceptable differences between the perceptions of visual size and haptic size.
            </abstract>
            <sections>
                <section>
                    <word_count>708</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>784</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1766</word_count>
                    <figure_citations>Figure 2 shows a system schematic of PoCoPo.Figure 3).Figure 4).Figure 6).Figure 7 demonstrates how the user can hold several objects on the table as seen in VR.Figure 8).</figure_citations>
                    <section_index>2</section_index>
                    <title>DESIGN AND IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>2791</word_count>
                    <figure_citations>Figure 10) were chosen based on the previous studies [41, 46].Figure 11).Figure 12).</figure_citations>
                    <section_index>3</section_index>
                    <title>INFORMATION</title>
                </section>
                <section>
                    <word_count>818</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>LIMITATION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>412</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>27</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>2317</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Shape Display</li>
                <li>Haptic Device</li>
                <li>Handheld Device</li>
            </keywords>
            <authors>
                <li>Shigeo Yoshida</li>
                <li> Yuqian Sun</li>
                <li> Hideaki Kuzuoka</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376358_crop_1.jpg</url>
                <caption>Figure 1. a: PoCoPo is a handheld pin-based shape display that has 18 pins on two opposite faces of the device (36 pins in total). The display area
                    where the pins move up and down touch the user’s ﬁngers and palm base. By controlling the length of the 36 pins, PoCoPo can render various shapes,
                    including rectangular and curved shapes (e.g., b-1 and b-2 show a user holding a glass). Moreover, PoCoPo can render dynamic transformations of
                    graspable objects (e.g., c-1 and c-2 show the pins moving up and down to represent the heartbeat of a small animal).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376358_crop_2.jpg</url>
                <caption>Figure 2. Top and side views with the dimensions of PoCoPo.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376358_crop_3.jpg</url>
                <caption>Figure 3. Worm gear mechanism used to change the force direction and
                    increase the power. By actuating the worm gear (blue: worm, red: worm
                    wheel with lead screw) by a DC motor (black), the pin moves with up-
                    down motion.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376358_crop_4.jpg</url>
                <caption>Figure 4. Variations in pin length depending on their position: 3, 9, and
                    14 mm. A rendered CG image of the worm gear mechanism is overlaid.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376358_crop_5.jpg</url>
                <caption>Figure 5. Overview of the software and hardware architecture of
                    PoCoPo. Four types of custom two-layer PCBs were designed for
                    PoCoPo. The microcontroller (Teensy 3.6) and a Bluetooth module are
                    attached to the main board. The motor driver board is connected to
                    the main board via I2C. The board with two multiplexers (multiplexer
                    board) is used to switch the optical encoders to read by time division.
                    The encoder board is placed at the bottom of the pin to measure the
                    length of its extrusion.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376358_crop_6.jpg</url>
                <caption>Figure 6. Shape surface represented by the blue areas (a circle here).
                    Each pin length is determined so that the midpoint on the outer edge of
                    each pin is close to the surface of the target shape.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376358_crop_7.jpg</url>
                <caption>Figure 7. Example applications of PoCoPo regarding the rendering of
                    static objects. The user can freely hold a: a glass (linear surface), b: a
                    matryoshka (convex surface), or c: a trophy (concave surface) on the
                    table in VR.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376358_crop_8.jpg</url>
                <caption>Figure 8. Example applications of PoCoPo regarding the rendering of
                    dynamic objects. The user can freely hold animals in VR. a: Popping up
                    of the pins when holding a hedgehog. b: Expanding or shrinking of the
                    device representing hamster pulse. c: Movement of the pins correspond-
                    ing to the movement of a snake.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376358_crop_9.jpg</url>
                <caption>Figure 9. Example setup of Study 1. The image on the right side shows
                    how the paricipant held the device.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3313831.3376358_crop_10.jpg</url>
                <caption>Figure 10. Shapes used in the shape prediction study. The images at the
                    bottom show examples of how the shapes appeared when rendered by
                    PoCoPo.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3313831.3376358_crop_11.jpg</url>
                <caption>Figure 11. Confusion matrix summarizing the results from the shape
                    prediction study (N=10). Each row shows the total number of predicted
                    objects in the ﬁnal two sets of the study.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>12</id>
                <url>3313831.3376358_crop_12.jpg</url>
                <caption>Figure 12. Shapes used in the visual size acceptance range study. The
                    images at the bottom show examples of how the shapes appeared when
                    rendered by PoCoPo.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376438</filename>
        <data>
            <paper_id>3313831.3376438</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376438</doi>
            <title>Outline Pursuits: Gaze-assisted Selection of Occluded Objects in Virtual Reality</title>
            <abstract>
                In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded
            </abstract>
            <sections>
                <section>
                    <word_count>700</word_count>
                    <figure_citations>Figure 1, the Page 1 CHI 2020 Paper concept is to display the outlines of objects that lie in the direction in which the user points, and to generate a distinct motion around each of the outlines.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>722</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1019</word_count>
                    <figure_citations>Figure 2 shows the cone casting technique.</figure_citations>
                    <section_index>2</section_index>
                    <title>OUTLINE PURSUITS</title>
                </section>
                <section>
                    <word_count>1355</word_count>
                    <figure_citations>Figure 5 illustrates Controller-based Outline Pursuits in a room planner setting.</figure_citations>
                    <section_index>3</section_index>
                    <title>TECHNIQUES</title>
                </section>
                <section>
                    <word_count>307</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>USER STUDIES</title>
                </section>
                <section>
                    <word_count>3736</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>ANOVA</title>
                </section>
                <section>
                    <word_count>714</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>162</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>1993</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Occlusion</li>
                <li>Eye tracking</li>
                <li>Smooth pursuits</li>
            </keywords>
            <authors>
                <li>Ludwig Sidenmark</li>
                <li> Christopher Clarke</li>
                <li> Xuesong Zhang</li>
                <li> Jenny Phu</li>
                <li> Hans Gellersen</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376438_crop_1.jpg</url>
                <caption>Figure 1. Outline Pursuits support selection in occluded 3D scenes. A: The user points at an object of interest but the selection is ambiguous due to
                    occlusion by other objects. B: Potential targets are outlined, with each outline presenting a moving stimulus that the user can follow with their gaze.
                    C: Matching of the user’s smooth pursuit eye movement completes the selection. Note that outline pursuits can augment manual pointing as shown, or
                    support hands-free input using the head or gaze for initial pointing.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376438_crop_2.jpg</url>
                <caption>Figure 2. Cone casting with a primary pointer is used to select Nc candi-
                    date targets within a visual angle radius rc. In this example, with Nc = 2,
                    four objects are within the cone (A) and objects 2 and 3 are selected as
                    they are closest to the centre of the cone.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376438_crop_3.jpg</url>
                <caption>Figure 3. Movement paths for occluded objects. A: Whole: the target
                    moves along the whole outline. B: Shared: the target moves along the
                    visible part of the object. C: Cut: the target moves along the shortest
                    path to the next visible part. D: Jump: the target jumps to the next
                    visible part.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376438_crop_4.jpg</url>
                <caption>Figure 4. Hands-free Outline Pursuits in an interactive virtual cityscape
                    environment. A: The user selects candidate buildings by pointing with
                    their head; B: The user follows the outline motion with their gaze (red)
                    to select the building. The outline shows feedback of selection progress
                    via colour change. C: The building is selected and contextual feedback
                    is displayed.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376438_crop_5.jpg</url>
                <caption>Figure 5. Controller-based Outline Pursuits in a Room Planner setting.
                    A: The user wants to move a partially occluded book to a different shelf
                    but does not know which book is which. B: The user points with the
                    controller to pre-select the book and other nearby items, showing which
                    books are available for interaction via their outline and stimuli move-
                    ments. C: The user follows the motion of a book with their eyes, which
                    in turn highlights it. D: The user selects the book via a button click and
                    can now manipulate it in 3D space, moving it to another shelf.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376438_crop_6.jpg</url>
                <caption>Figure 6. A: Participants were tasked to select the yellow object. B:
                    Participants were tasked to select the yellow object at a higher level of
                    occlusion. C: The user was tasked to fnish the memory game.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376438_crop_7.jpg</url>
                <caption>Figure 7. Study 2 task performance at different object densities and
                    levels of occlusion. Error bars represents mean 95% confdence interval.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376438_crop_8.jpg</url>
                <caption>Figure 8. Study 2 average translational head movements during trials.
                    Error bars represents 95% confdence interval for means.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376438_crop_9.jpg</url>
                <caption>Figure 9. Median scores on a 5-point Likert scale with error bars repre-
                    senting interquartile ranges.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376470</filename>
        <data>
            <paper_id>3313831.3376470</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376470</doi>
            <title>Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics</title>
            <abstract>
                Today's virtual reality (VR) systems allow users to explore immersive new worlds and experiences through sight. Unfortunately, most VR systems lack haptic feedback, and even high-end consumer systems use only basic vibration motors. This clearly precludes realistic physical interactions with virtual objects. Larger obstacles, such as walls, railings, and furniture are not simulated at all. In response, we developed Wireality, a self-contained worn system that allows for individual joints on the hands to be accurately arrested in 3D space through the use of retractable wires that can be programmatically locked. This allows for convincing tangible interactions with complex geometries, such as wrapping fingers around a railing. Our approach is lightweight, low-cost, and low-power, criteria important for future, worn consumer uses. In our studies, we further show that our system is fast-acting, spatially-accurate, high-strength, comfortable, and immersive.
            </abstract>
            <sections>
                <section>
                    <word_count>572</word_count>
                    <figure_citations>Figure 1).Figure 1).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>560</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1002</word_count>
                    <figure_citations>Figure 2).Figure 3).Figure 4).</figure_citations>
                    <section_index>2</section_index>
                    <title>WIREALITY IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>1302</word_count>
                    <figure_citations>Figure 1).Figure 2), we included a 10-turn potentiometer that allowed us to precisely track a point’s distance from the module, but not the azimuth or altitude.Figure 6).</figure_citations>
                    <section_index>3</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>427</word_count>
                    <figure_citations>Figure 7 shows our main results.</figure_citations>
                    <section_index>4</section_index>
                    <title>QUALITATIVE STUDY</title>
                </section>
                <section>
                    <word_count>968</word_count>
                    <figure_citations>Figure 8, A-D).Figure 9, A-D).Figure 10 shows four example scenarios: an ATM screen, a button, a lever, and a piano.Figure 11, A-D).</figure_citations>
                    <section_index>5</section_index>
                    <title>EXAMPLE USES</title>
                </section>
                <section>
                    <word_count>127</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>1975</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Haptics</li>
                <li>Force Feedback</li>
                <li>String-Driven</li>
                <li>Touch</li>
                <li>Grasp</li>
            </keywords>
            <authors>
                <li>Cathy Fang</li>
                <li> Yang Zhang</li>
                <li> Matthew Dworman</li>
                <li> Chris Harrison</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376470_crop_1.jpg</url>
                <caption>Figure 1. Wireality enables strong, whole-hand haptic feed-
                    back for complex objects in VR experiences.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376470_crop_2.jpg</url>
                <caption>Figure 2. An earlier prototype using a motor for string retrac-
                    tion and a potentiometer for tracking hand positions.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376470_crop_3.jpg</url>
                <caption>Figure 3. Exploded illustration of one haptic module.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376470_crop_4.jpg</url>
                <caption>
                    Figure 4. Locking mechanism. Our driver board actuates the solenoid pawl for 40 milliseconds, which locks the ratchet gear, stopping a joint from further forward movement (A).String tension keeps the gear latched even after the solenoid is not powered (B). When the user pulls back from touching the virtual surface, slack is created in the string, which then re-leases the ratchet (C).
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376470_crop_5.jpg</url>
                <caption>Figure 5. Example Wireality setup with seven haptic modules.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376470_crop_6.jpg</url>
                <caption>
                    Figure 6. Exemplary objects used in the study. Left to right: a wall, tilted flat surface, sphere, pole, and irregular object.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376470_crop_8.jpg</url>
                <caption>Figure 8. Our system restrains a user’s hand collided with a
                    wall (A), railings (B), a roadblock (C), and a fire hydrant (D).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376470_crop_9.jpg</url>
                <caption>Figure 9. Our system provides force feedback when touching
                    a virtual sofa (A), stereo (B), sculpture (C), and car (D).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3313831.3376470_crop_10.jpg</url>
                <caption>Figure 10. Our system provides force feedback when a user’s
                    hand interacts with an ATM touchscreen (A), a button (B), a
                    lever (C), and a piano (D).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3313831.3376470_crop_11.jpg</url>
                <caption>Figure 11. Our system provides haptic feedback when a user’s
                    hand interacts with virtual characters: high five (A), tap on
                    shoulder (B), shaking hands (C), and face slapping (D).
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376481</filename>
        <data>
            <paper_id>3313831.3376481</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376481</doi>
            <title>FaceHaptics: Robot Arm based Versatile Facial Haptics for Immersive Environments</title>
            <abstract>
                This paper introduces FaceHaptics, a novel haptic display based on a robot arm attached to a head-mounted virtual reality display. It provides localized, multi-directional and movable haptic cues in the form of wind, warmth, moving and single-point touch events and water spray to dedicated parts of the face not covered by the head-mounted display.The easily extensible system, however, can principally mount any type of compact haptic actuator or object. User study 1 showed that users appreciate the directional resolution of cues, and can judge wind direction well, especially when they move their head and wind direction is adjusted dynamically to compensate for head rotations. Study 2 showed that adding FaceHaptics cues to a VR walkthrough can significantly improve user experience, presence, and emotional responses.
            </abstract>
            <sections>
                <section>
                    <word_count>1409</word_count>
                    <figure_citations>Figure 1 and Figure 2).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION AND MOTIVATION</title>
                </section>
                <section>
                    <word_count>759</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>792</word_count>
                    <figure_citations>Figure 2) consists of a custom-made robot arm attached to a commercial HMD, currently an Oculus Rift CV1.</figure_citations>
                    <section_index>2</section_index>
                    <title>FACEHAPTICS SYSTEM</title>
                </section>
                <section>
                    <word_count>5156</word_count>
                    <figure_citations>Figure 3 and highest for the condition with oscillating head and static wind (M = 14.Figure 3 (middle)): The standard deviation of the signed point error differed signiﬁcantly between movement conditions (F(3, 45) = 3.Figure 3 (right).Figure 4) but from a static location.Figure 4) that contained 16 events along a 3 minute pre-deﬁned walkthrough.</figure_citations>
                    <section_index>3</section_index>
                    <title>USER STUDIES</title>
                </section>
                <section>
                    <word_count>512</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>TECHNICAL CONSIDERATIONS AND LIMITATIONS</title>
                </section>
                <section>
                    <word_count>734</word_count>
                    <figure_citations>Figure 5), underlining the easy extensibility of the system, but also the need for a reloading mechanism or dispensing system (e.</figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>19</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>2386</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Haptics</li>
                <li>Robot arm</li>
                <li>Immersive environments</li>
                <li>Virtual reality</li>
                <li>User study</li>
                <li>Perception</li>
                <li>Presence</li>
                <li>Emotion</li>
            </keywords>
            <authors>
                <li>Alexander Wilberz</li>
                <li> Dominik Leschtschow</li>
                <li> Christina Trepkowski</li>
                <li> Jens Maiero</li>
                <li> Ernst Kruijff</li>
                <li> Bernhard Riecke</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376481_crop_1.jpg</url>
                <caption>Figure 1. The FaceHaptics system, showing a side and frontal view of the setup for face haptic feedback, affording various sensations including touch,
                    texture, warmth, air ﬂow, or wetness. The left image depict one of many possible touch/texture feedback elements, which can easily be exchanged.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376481_crop_2.jpg</url>
                <caption>Figure 2. System overview: elements of the robot arm with different
                    feedback elements. The close up shows a frontal view of lower robot
                    arm with fan, heat wire in front of fan, and spray nozzle. The blue
                    overlay over the face shows the approximate area that can be reached
                    using touch events (wind can be sensed over the whole face).
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376481_crop_3.jpg</url>
                <caption>Figure 3. Mean performance for the different conditions, averaged over
                    the two repetitions. Gray dots indicate participant mean data, whiskers
                    indicate 95% conﬁdence intervals.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376481_crop_4.jpg</url>
                <caption>Figure 4. Feedback elements with sample events: ventilator depicts fan,
                    wire for heat, leaf is the rubber tip and spray the water-air spray.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376481_crop_5.jpg</url>
                <caption>Figure 5. Adding biting functionality to FaceHaptics - showing the po-
                    tential of extensibility, but also the limitations regarding reloading.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376523</filename><data>
            <paper_id>3313831.3376523</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376523</doi>
            <title>RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</title>
            <abstract>
                RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.
            </abstract>
            <sections>
                <section>
                    <word_count>809</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>2552</word_count>
                    <figure_citations>Figure 3 illustrates the mechanical design of each RoomShift robot.Figure 5) ranges from 3.Figure 5 illustrates various static props that the RoomShift robot can actuate.Figure 6 depicts the space and mounted cameras on the ceiling (left) and tracking software (right).Figure 7).Figure 7).Figure 9 illustrates the schematic of RoomShift’s circuit.</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>9</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>MCU</title>
                </section>
                <section>
                    <word_count>432</word_count>
                    <figure_citations>Figure 8).</figure_citations>
                    <section_index>3</section_index>
                    <title>GND</title>
                </section>
                <section>
                    <word_count>718</word_count>
                    <figure_citations>Figure 10 illustrates the architecture of the RoomShift software.</figure_citations>
                    <section_index>4</section_index>
                    <title>UDP</title>
                </section>
                <section>
                    <word_count>920</word_count>
                    <figure_citations>Figure 1).Figure 12).Figure 13).Figure 14).</figure_citations>
                    <section_index>5</section_index>
                    <title>INTERACTION WITH ROOMSHIFT</title>
                </section>
                <section>
                    <word_count>294</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>559</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>LIMITATIONS AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>110</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>43</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1939</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Haptic interfaces</li>
                <li>Room-scale haptics</li>
                <li>Virtual reality</li>
                <li>Swarm robots</li>
            </keywords>
            <authors>
                <li>Ryo Suzuki</li>
                <li> Hooman Hedayati</li>
                <li> Clement Zheng</li>
                <li> James L. Bohn</li>
                <li> Daniel Szafir</li>
                <li> Ellen Yi-Luen Do</li>
                <li> Mark D. Gross</li>
                <li> Daniel Leithinger</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376523_crop_1.jpg</url>
                <caption>Figure 1. RoomShift is composed of a swarm of shape-changing robots for haptic feedback in VR. RoomShift robots move beneath a piece of furniture
                    to lift, move and place it. Multiple robots move furniture to construct a physical haptic environment collectively. The corresponding virtual scene is
                    shown, with a human silhouette added for a reference.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376523_crop_2.jpg</url>
                <caption>Figure 2. A RoomShift robot drives beneath a desk, lifts it by extending
                    the scissor structure, and moves it.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376523_crop_3.jpg</url>
                <caption>Figure 3. Mechanical design of the robot and the scissor structure.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376523_crop_4.jpg</url>
                <caption>Figure 4. Each robot can extend from 30 cm to 100 cm to lift objects.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376523_crop_5.jpg</url>
                <caption>Figure 5. Different types of furniture moved by the system.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376523_crop_6.jpg</url>
                <caption>Figure 6. Photo of tracked space and screenshot of tracking software.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376523_crop_7.jpg</url>
                <caption>Figure 7. Retro-reﬂective markers mounted to parallel lift bars, high-
                    lighted in pink.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376523_crop_8.jpg</url>
                <caption>Figure 8. The system ﬁrst navigates the robot to a user-deﬁned entry
                    point to avoid the collision with the legs of furniture.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376523_crop_9.jpg</url>
                <caption>Figure 9. Hardware schematic of the robot.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3313831.3376523_crop_10.jpg</url>
                <caption>Figure 10. The communication software.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3313831.3376523_crop_11.jpg</url>
                <caption>Figure 11. The interaction design space of RoomShift.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>12</id>
                <url>3313831.3376523_crop_12.jpg</url>
                <caption>Figure 12. Simulating a larger table by moving a smaller surface.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>13</id>
                <url>3313831.3376523_crop_13.jpg</url>
                <caption>Figure 13. When teleporting, the robots move furniture to match the
                    new scene position.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>14</id>
                <url>3313831.3376523_crop_14.jpg</url>
                <caption>Figure 14. Pointing and moving with a gesture.
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376550</filename><data>
            <paper_id>3313831.3376550</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376550</doi>
            <title>A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing</title>
            <abstract>
                Supporting natural communication cues is critical for people to work together remotely and face-to-face. In this paper we present a Mixed Reality (MR) remote collaboration system that enables a local worker to share a live 3D panorama of his/her surroundings with a remote expert. The remote expert can also share task instructions back to the local worker using visual cues in addition to verbal communication. We conducted a user study to investigate how sharing augmented gaze and gesture cues from the remote expert to the local worker could affect the overall collaboration performance and user experience. We found that by combing gaze and gesture cues, our remote collaboration system could provide a significantly stronger sense of co-presence for both the local and remote users than using the gaze cue alone. The combined cues were also rated significantly higher than the gaze in terms of ease of conveying spatial actions.
            </abstract>
            <sections>
                <section>
                    <word_count>669</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1483</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1668</word_count>
                    <figure_citations>Figure 1 shows an overview of our system.Figure 2a).Figure 2a.Figure 2b with the red frames.Figure 3) that could be shared from the remote VR mode to the local AR mode: • Eye Gaze A virtual raycast line of the remote user’s eye gaze overlaid onto the local user’s AR view from a third-person perspective.Figure 4).</figure_citations>
                    <section_index>2</section_index>
                    <title>SYSTEM OVERVIEW</title>
                </section>
                <section>
                    <word_count>1347</word_count>
                    <figure_citations>Figure 5b).Figure 5a.Figure 5c.Figure 5a.Figure 6a).Figure 6b).</figure_citations>
                    <section_index>3</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1797</word_count>
                    <figure_citations>Figure 7 shows the average CP rating.Figure 8 shows the average rating results of each condition for TLX.Figure 9).</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1918</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>254</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSIONS AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>2057</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Mixed Reality</li>
                <li>Augmented Reality</li>
                <li>Virtual Reality</li>
                <li>Remote collaboration</li>
                <li>3D panorama</li>
                <li>Scene reconstruction</li>
                <li>Eye gaze</li>
                <li>Hand gesture</li>
            </keywords>
            <authors>
                <li>Huidong Bai</li>
                <li> Prasanth Sasikumar</li>
                <li> Jing Yang</li>
                <li> Mark Billinghurst</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376550_crop_1.jpg</url>
                <caption>Figure 1. The 3D panorama unit reconstructs the local environment,
                    and then streams the stitched point-cloud data to the remote VR expert
                    via the network. The eye gaze and hand gesture information are shared
                    back to the local AR worker from the remote VR expert synchronously.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376550_crop_2.jpg</url>
                <caption>Figure 2. a) The sensor cluster with eight depth camera units; b) The
                    calibration result of the sensor cluster.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376550_crop_3.jpg</url>
                <caption>Figure 3. Natural visual cues shared from the remote to the local user:
                    the purple gaze raycast line, and the grey hand mesh in a) the remote
                    VR mode and b) the local AR mode.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376550_crop_4.jpg</url>
                <caption>Figure 4. Auxiliary awareness cues for the local worker: the simple
                    avatar has the purple sphere head and half-transparent yellow view frus-
                    tum, and the pin arrow in orange always points to the head sphere.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376550_crop_5.jpg</url>
                <caption>Figure 5. Experiment environment: a) The local section with Lego
                    bricks placed around desks and the live 3D panorama capture unit in-
                    stalled on the ceiling; b) The remote part with the VR headset installed;
                    c) A screenshot of the reconstructed live 3D panorama in VR.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376550_crop_6.jpg</url>
                <caption>Figure 6. a) Lego bricks on the tags; b) The overlaid cubes with numbers
                    to indicate the brick for picking up, and the overlaid cubes with numbers
                    and arrows to indicate the target position and orientation of the Lego
                    brick with the same pickup number.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376550_crop_7.jpg</url>
                <caption>Figure 7. Results of Co-presence questionnaires (7-point Likert scale
                    from 1 to 7, the higher the better).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376550_crop_8.jpg</url>
                <caption>Figure 8. Results of the TLX questionnaire (100-points range with 5-
                    point steps, 0: very low~100: very high, the lower the better).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376550_crop_9.jpg</url>
                <caption>Figure 9. User preference based ranking results (Rank1 is the most pre-
                    ferred, *: statistically signiﬁcant).
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376574</filename><data>
            <paper_id>3313831.3376574</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376574</doi>
            <title>Walking by Cycling: A Novel In-Place Locomotion User Interface for Seated Virtual Reality Experiences</title>
            <abstract>
                We introduce VR Strider, a novel locomotion user interface (LUI) for seated virtual reality (VR) experiences, which maps cycling biomechanics of the user's legs to virtual walking movements. The core idea is to translate the motion of pedaling on a mini exercise bike to a corresponding walking animation of a virtual avatar while providing audio-based tactile feedback on virtual ground contacts. We conducted an experiment to evaluate the LUI and our novel anchor-turning rotation control method regarding task performance, spatial cognition, VR sickness, sense of presence, usability and comfort in a path-integration task. The results show that VR Strider has a significant positive effect on the participants' angular and distance estimation, sense of presence and feeling of comfort compared to other established locomotion techniques, such as teleportation and joystick-based navigation. A confirmatory study further indicates the necessity of synchronized avatar animations for virtual vehicles that rely on pedalling.
            </abstract>
            <sections>
                <section>
                    <word_count>623</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>576</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>2456</word_count>
                    <figure_citations>Figure 1): 1.Figure 2 A.Figure 2 B).Figure 2 C).Figure 2 D), Page 3 CHI 2020 Paper CHI 2020, April 25–30, 2020, Honolulu, HI, USA Figure 2: A-F: Iterations of the IK layer of the walking animation.Figure 2 E).Figure 2 F).Figure 2 G as four discrete phases of a single step, where the ﬁlled and the blank pedals represent the right and left foot respectively.</figure_citations>
                    <section_index>2</section_index>
                    <title>VR STRIDER</title>
                </section>
                <section>
                    <word_count>1649</word_count>
                    <figure_citations>Figure 3).</figure_citations>
                    <section_index>3</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1168</word_count>
                    <figure_citations>Figure 4(left).Figure 4(center).Figure 4(right).Figure 5(left).Figure 5(center).Figure 5(right).Figure 6 illustrates the distribution of responses for items with signiﬁcant differences.</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1612</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>815</word_count>
                    <figure_citations>Figure 7).</figure_citations>
                    <section_index>6</section_index>
                    <title>CONFIRMATORY STUDY</title>
                </section>
                <section>
                    <word_count>343</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>66</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1477</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Navigation techniques</li>
                <li>Locomotion user interface</li>
                <li>Virtual walking</li>
            </keywords>
            <authors>
                <li>Jann Philipp Freiwald</li>
                <li> Oscar Ariza</li>
                <li> Omar Janeh</li>
                <li> Frank Steinicke</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376574_crop_1.jpg</url>
                <caption>Figure 1: The VR Strider device including mini exercise bike and electronics for pressure tracking and vibrotactile feedback.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376574_crop_2.jpg</url>
                <caption>Figure 2: A-F: Iterations of the IK layer of the walking animation. G: The result superimposed on the reference diagram [43].
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376574_crop_3.jpg</url>
                <caption>Figure 3: A view to the VE for the path-integration task.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376574_crop_4.jpg</url>
                <caption>Figure 4: Task Time (left), Distance Error (center), and Angular Error (right). Lower is better in all plots.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376574_crop_5.jpg</url>
                <caption>Figure 5: Results for SSQ (left, lower is better), IPQ (center, higher is better), and SUS (right, higher is better).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376574_crop_6.jpg</url>
                <caption>Figure 6: Results for the Device Assessment (DAQ).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376574_crop_7.jpg</url>
                <caption>Figure 7: A translation of real pedalling to a virtual GoKart.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376582</filename><data>
            <paper_id>3313831.3376582</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376582</doi>
            <title>Reflexive VR Storytelling Design Beyond Immersion: Facilitating Self-Reflection on Death and Loneliness</title>
            <abstract>
                This research examines the reflexive dimensions of cinematic virtual reality (CVR) storytelling. We created Anonymous, an interactive CVR piece that employs a reflexive storytelling method. This method is based on distancing effects and is used to elicit audience awareness and self-reflection about loneliness and death. To understand the audience's experiences, we conducted in-depth interviews to study which design factors and elements prompted reflexive thoughts and feelings. Our findings highlight how the audience experience was impacted by four reflexive dimensions: abstract and minimal aesthetics, everyday materials and textures, the restriction of control, and multiple, disembodied points of view. We use our findings to discuss how these dimensions can inform the design of VR storytelling experiences that provoke self and social reflection.
            </abstract>
            <sections>
                <section>
                    <word_count>934</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>3011</word_count>
                    <figure_citations>Figure 2 displays the diorama.</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>488</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>METHOD</title>
                </section>
                <section>
                    <word_count>1896</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>FINDINGS</title>
                </section>
                <section>
                    <word_count>1318</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION AND IMPLICATIONS</title>
                </section>
                <section>
                    <word_count>132</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>92</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1700</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Alienation</li>
                <li>Cinematic VR</li>
                <li>Distancing Effect</li>
                <li>Estrangement</li>
                <li>Immersive Storytelling</li>
                <li>Reﬂexivity</li>
                <li>Virtual Reality</li>
            </keywords>
            <authors>
                <li>Sojung Bahng</li>
                <li> Ryan M. Kelly</li>
                <li> Jon McCormack</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376582_crop_1.jpg</url>
                <caption>Figure 1: Scenes from Anonymous, an interactive cinematic virtual reality story about death and loneliness.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376582_crop_2.jpg</url>
                <caption>Figure 2: Initial diorama of Anonymous using physical card-
                    board.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376582_crop_3.jpg</url>
                <caption>Figure 3: Objects in Anonymous.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376582_crop_4.jpg</url>
                <caption>Figure 4: The virtual environment of Anonymous.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376582_crop_5.jpg</url>
                <caption>Figure 5: The widower character from Anonymous.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376582_crop_6.jpg</url>
                <caption>Figure 6: Screenshot of the viewer inhabiting an object within
                    Anonymous. In this image, the viewer’s perspective is shown
                    from inside the main character’s telephone. The foreground of
                    the image shows the telephone’s number pad and handset, and
                    the main character appears in the background.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376582_crop_7.jpg</url>
                <caption>Figure 7: Screenshots of becoming the butterﬂy. The upper
                    half of the image shows the viewer’s perspective when gazing
                    at the butterﬂy. The lower half shows the viewer’s perspective
                    when they are embodied as the butterﬂy.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376582_crop_8.jpg</url>
                <caption>Figure 8: Screenshots of the ﬁnal scene in Anonymous.
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376614</filename><data>
            <paper_id>3313831.3376614</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376614</doi>
            <title>ARchitect: Building Interactive Virtual Experiences from Physical Affordances by Bringing Human-in-the-Loop</title>
            <abstract>
                Automatic generation of Virtual Reality (VR) worlds which adapt to physical environments have been proposed to enable safe walking in VR. However, such techniques mainly focus on the avoidance of physical objects as obstacles and overlook their interaction affordances as passive haptics. Current VR experiences involving interaction with physical objects in surroundings still require verbal instruction from an assisting partner. We present ARchitect, a proof-of-concept prototype that allows flexible customization of a VR experience with human-in-the-loop. ARchitect brings in an assistant to map physical objects to virtual proxies of matching affordances using Augmented Reality (AR). In a within-subjects study (9 user pairs) comparing ARchitect to a baseline condition, assistants and players experienced decreased workload and players showed increased VR presence and trust in the assistant. Finally, we defined design guidelines of ARchitect for future designers and implemented three demonstrative experiences.
            </abstract>
            <sections>
                <section>
                    <word_count>1062</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>648</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1730</word_count>
                    <figure_citations>Figure 2).Figure 3a-e).Figure 4).Figure 5a).Figure 5b).Figure 5c), rotated by two-ﬁnger twirl (Figure 5d), and resized by two-ﬁnger pinch (Figure 5e) [2].Figure 5f).</figure_citations>
                    <section_index>2</section_index>
                    <title>DESIGN AND IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>1284</word_count>
                    <figure_citations>Figure 3).Figure 6).Figure 7).</figure_citations>
                    <section_index>3</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1804</word_count>
                    <figure_citations>Figure 8 shows an overview for comparison between ARchitect and the baseline.</figure_citations>
                    <section_index>4</section_index>
                    <title>EXPERIMENTAL RESULT AND ANALYSIS</title>
                </section>
                <section>
                    <word_count>1412</word_count>
                    <figure_citations>Figure 9a).Figure 9b-d).Figure 10b).Figure 10a).Figure 10c).Figure 11a).Figure 11b).Figure 11c-d).</figure_citations>
                    <section_index>5</section_index>
                    <title>DESIGN GUIDELINES AND EXAMPLE EXPERIENCES</title>
                </section>
                <section>
                    <word_count>395</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>LIMITATIONS AND FURTHER WORK</title>
                </section>
                <section>
                    <word_count>105</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>69</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1744</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>ARchitect</li>
                <li>Virtual reality</li>
                <li>Affordance</li>
                <li>Passive haptics</li>
                <li>Asymmetric</li>
            </keywords>
            <authors>
                <li>Chuan-en Lin</li>
                <li> Ta Ying Cheng</li>
                <li> Xiaojuan Ma</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376614_crop_1.jpg</url>
                <caption>Figure 1. (a) ARchitect allows physical objects to be mapped to virtual proxies offering matching affordances (e.g. both “chair” and “tree stump” afford
                    a “sitting” interaction). A physical scene (b) may be translated using ARchitect’s user interface (c) to an interactive virtual experience (d).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376614_crop_2.jpg</url>
                <caption>Figure 2. Overview of the ARchitect system
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376614_crop_3.jpg</url>
                <caption>Figure 3. Components of the virtual world being added by the Assistant: (a) scene, (b) barriers, (c) obstacles, (d) interactables, (e) game objects.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376614_crop_4.jpg</url>
                <caption>Figure 4. The Affordance Recommender detects predeﬁned interactable
                    classes (e.g. “chair” class corresponds to “sitting on” interactable).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376614_crop_5.jpg</url>
                <caption>Figure 5. Operations for conﬁguring a virtual proxy over the physical
                    scene using ARchitect: (a) scan the ﬂoor, (b) place the virtual proxy, (c)
                    translate the virtual proxy, (d) rotate the virtual proxy, (e) resize the
                    virtual proxy, (f) remove the virtual proxy.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376614_crop_6.jpg</url>
                <caption>Figure 6. The virtual view of the Player in baseline (a) and in ARchitect
                    (b) while bending down to collect virtual mushrooms (c).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376614_crop_7.jpg</url>
                <caption>Figure 7. The virtual view of the Player in baseline (a) and in ARchitect
                    (b) while sitting down to take a rest (c).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376614_crop_8.jpg</url>
                <caption>Figure 8. Means and standard deviations of presence, trust, workload
                    (Assistant), and workload (Player) scores comparing ARchitect and the
                    baseline. Error bars are standard errors.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376614_crop_9.jpg</url>
                <caption>Figure 9. (a) In LavaEscape, the Assistant ﬁrst maps physical furniture
                    to rocks of various shapes and sizes (and heights). The Player steps onto
                    the lowest piece of furniture (b) and continues moving onto higher furni-
                    ture as the lava plane rises (c) until reaching the higher ground (d).
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3313831.3376614_crop_10.jpg</url>
                <caption>Figure 10. A Player attempting to locate the treasure chest (3D audio
                    source) (a) while playing Maze. The virtual maze corresponds to the
                    physical room conﬁguration (walls and table) with the addition of a fake
                    virtual wall (b). (c) shows an example completion path that may be taken
                    by the Player.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3313831.3376614_crop_11.jpg</url>
                <caption>Figure 11. A Player playing with a skateboard simulator conﬁgured with
                    Sandbox (a) using a physical skateboard controller (b). The experience
                    may be reconﬁgured to a different theme (e.g. aquatic (c) or space (d)).
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376626</filename>
        <data>
            <paper_id>3313831.3376626</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376626</doi>
            <title>Podoportation: Foot-Based Locomotion in Virtual Reality</title>
            <abstract>
                Virtual Reality (VR) allows for infinitely large environments. However, the physical traversable space is always limited by real-world boundaries. This discrepancy between physical and virtual dimensions renders traditional locomotion methods used in real world unfeasible. To alleviate these limitations, research proposed various artificial locomotion concepts such as teleportation, treadmills, and redirected walking. However, these concepts occupy the user's hands, require complex hardware or large physical spaces. In this paper, we contribute nine VR locomotion concepts for foot-based locomotion, relying on the 3D position of the user's feet and the pressure applied to the sole as input modalities. We evaluate our concepts and compare them to state-of-the-art point &amp; teleport technique in a controlled experiment with 20 participants. The results confirm the viability of our approaches for foot-based and engaging locomotion. Further, based on the findings, we contribute a wireless hardware prototype implementation.
            </abstract>
            <sections>
                <section>
                    <word_count>458</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>2047</word_count>
                    <figure_citations>Figure 2 a).Figure 2 b).Figure 2 c).Figure 3 a) shows a schematic representation of this input modality.Figure 3 b)).Figure 3 c) further illustrates how this input method works.Figure 4 shows the internals of the prototype.Figure 5), with a light blue line ending in a circle with upwards fading walls and a downward pointing arrow in the center.</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1241</word_count>
                    <figure_citations>Figure 5).Figure 8).</figure_citations>
                    <section_index>2</section_index>
                    <title>METHODOLOGY</title>
                </section>
                <section>
                    <word_count>2266</word_count>
                    <figure_citations>Figure 6 depicts the measured mean errors for all conditions.Figure 7a depicts the measured mean TCTs for all conditions.Figure 7b depicts the measured mean numbers of teleports for individual conditions.Figure 7c depicts the measured mean RTLX values the individual conditions.Figure 8 depicts all answers of the participants.Figure 8 depicts all answers of the participants.Figure 8 depicts all answers given.</figure_citations>
                    <section_index>3</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1756</word_count>
                    <figure_citations>Figure 7a, we found no signiﬁcant differences between directional input modalities.Figure 8).</figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>180</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>22</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGEMENT</title>
                </section>
                <section>
                    <word_count>2410</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Locomotion</li>
                <li>Foot-based input</li>
            </keywords>
            <authors>
                <li>Julius von Willich</li>
                <li> Martin Schmitz</li>
                <li> Florian Müller</li>
                <li> Daniel Schmitt</li>
                <li> Max Mühlhäuser</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376626_crop_1.jpg</url>
                <caption>Figure 1. A person in Virtual Reality, using their feet for teleportation while simultaneously using their hands for interaction.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376626_crop_2.jpg</url>
                <caption>Figure 2. Illustration of the different direction input methods. a) inter
                    feet direction b) foot direction c) Point and lean
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376626_crop_3.jpg</url>
                <caption>Figure 3. Illustration of the different distance input methods. a) forefoot
                    lift b) inter feet distance c) intra foot pressure
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376626_crop_4.jpg</url>
                <caption>Figure 4. The inside of our prototype with two visible sensors (left), the
                    top sole with the four pins (middle) and an example of how the prototype
                    is worn (right)
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376626_crop_5.jpg</url>
                <caption>Figure 5. Screenshot of our testing environment, showing a target and a
                    teleport being performed.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376626_crop_6.jpg</url>
                <caption>Figure 6. Error in position, measured between the target center and the
                    participants’ active foot.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376626_crop_7.jpg</url>
                <caption>Figure 7. Efﬁciency and convenience metric of all presented input modalities including point &amp; teleport as baseline.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376626_crop_8.jpg</url>
                <caption>Figure 8. The results of our custom questionnaire on a 5-point Likert scale (1:fully disagree, 5:fully agree, percentage of answers)
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376628</filename>
        <data>
            <paper_id>3313831.3376628</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376628</doi>
            <title>VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality</title>
            <abstract>
                Sketching in virtual reality (VR) enhances perception and understanding of 3D volumes, but is currently a challenging task, as spatial input devices (e.g., tracked controllers) do not provide any scaffolding or constraints for mid-air interaction. We present VRSketchIn, a VR sketching application using a 6DoF-tracked pen and a 6DoF-tracked tablet as input devices, combining unconstrained 3D mid-air with constrained 2D surface-based sketching. To explore what possibilities arise from this combination of 2D (pen on tablet) and 3D input (6DoF pen), we present a set of design dimensions and define the design space for 2D and 3D sketching interaction metaphors in VR. We categorize prior art inside our design space and implemented a subset of metaphors for pen and tablet sketching in our prototype. To gain a deeper understanding which specific sketching operations users perform with 2D and which with 3D metaphors, we present findings of usability walkthroughs with six participants.
            </abstract>
            <sections>
                <section>
                    <word_count>639</word_count>
                    <figure_citations>Figure 1b) and the combination of pen and tablet as 6DoF-tracked 3D input devices (e.Figure 1d/8a).Figure 7a) and sketch on them using pen on tablet (see Figure 1c/7b).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1309</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>901</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>PRELIMINARY EXPERT INTERVIEWS</title>
                </section>
                <section>
                    <word_count>2439</word_count>
                    <figure_citations>Figure 2) known as Zwicky box [64], which is a common design space tool (e.Figure 2/3).Figure 2).Figure 2).Figure 2).Figure 2).Figure 2).Figure 3), we will show how to place an interaction metaphor from SymbiosisSketch [3].Figure 3 for implementation and avoided functional gaps (2).Figure 3; see [23]).Figure 3 are not complete and were created and positioned during joint brainstorming sessions.Figure 5a).Figure 7a).Figure 8a).Figure 9a) that allows users to see distant and hidden objects inside the space.Figure 9b) in the direction of one axis [13].</figure_citations>
                    <section_index>3</section_index>
                    <title>DESIGN SPACE</title>
                </section>
                <section>
                    <word_count>1880</word_count>
                    <figure_citations>Figure 3).Figure 3), we implemented drawing surfaces as drawing aids that can be placed in space by the tablet position and orientation.Figure 3).Figure 3) that it is constrained to 2D surfaces.Figure 3).Figure 4a) on the pen to increase and the left to decrease.Figure 4a).Figure 4a).Figure 4b).Figure 4c).Figure 4a/5a).Figure 5a).Figure 5b).Figure 6a).Figure 6b) and scaling are supported as well.Figure 7a).Figure 7b).Figure 8a).Figure 8b).Figure 9a).Figure 9b).Figure 10).Figure 10).Figure 10).Figure 10).Figure 10).</figure_citations>
                    <section_index>4</section_index>
                    <title>THE VRSKETCHIN SYSTEM</title>
                </section>
                <section>
                    <word_count>133</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1114</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>USABILITY WALKTHROUGH</title>
                </section>
                <section>
                    <word_count>337</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>DISCUSSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>196</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>26</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>2129</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Sketching</li>
                <li>Pen and tablet</li>
                <li>Mid-air painting</li>
                <li>Virtual reality</li>
                <li>Interaction metaphors</li>
                <li>Design space</li>
            </keywords>
            <authors>
                <li>Tobias Drey</li>
                <li> Jan Gugenheimer</li>
                <li> Julian Karlbauer</li>
                <li> Maximilian Milo</li>
                <li> Enrico Rukzio</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376628_crop_1.jpg</url>
                <caption>Figure 1. VRSketchIn is an (a) immersive sketching application combining (b) unconstrained 3D mid-air sketching with a pen and (c) constrained
                    surface-based sketching with pen on tablet. We created a design space and describe multiple interaction metaphors such as drawing surfaces (c) or World
                    In Miniature (d) to enable a combination of 2D and 3D mid-air sketching.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376628_crop_2.jpg</url>
                <caption>Figure 2. A design space for pen- and tablet-based VR sketching applica­
                    tions. Interaction metaphors are categorized into cells of this space by se­
                    lecting the appropriate input devices (D1) and sketching operations (D2)
                    parameters (P). This matrix was created during a morphological analysis.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376628_crop_3.jpg</url>
                <caption>Figure 3. A populated version of our design space for pen- and tablet-based sketching that shows (1) the classiﬁcation of interaction metaphors, (2) the
                    classiﬁcation of prior art, (3) the deﬁned interaction metaphor groups, and (4) the interaction metaphors implemented in VRSketchIn. Due to space
                    constraints, we had to split the design space into two parts. This is only a visualization.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376628_crop_4.jpg</url>
                <caption>Figure 4. Our setup (c) consisted of an HMD, a tablet, and a pen (a). We
                    3D printed a pen case and combined the graphical pen with wireless
                    buttons (b). All devices were motion tracked using OptiTrack [42].
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376628_crop_5.jpg</url>
                <caption>Figure 5. 3D Mid-Air Sketching. a) Pen-based mid-air drawing in the
                    immersive environment. b) Pen-based mid-air object selection.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376628_crop_6.jpg</url>
                <caption>Figure 6. Gizmos. a) The translation gizmo activated for the selected
                    stroke. b) The rotation gizmo for the same stroke.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376628_crop_7.jpg</url>
                <caption>Figure 7. Drawing Surfaces. a) New drawing surfaces can be created by
                    placing them in space with the tablet position and orientation. The scale
                    and the distance are set by sliding the pen on the tablet. A preview of the sli­
                    ced objects is provided on the tablet. b) Sketching on the tablet creates sto­
                    kes in mid-air that lie on the deﬁned surface. Gridlines can be activated.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376628_crop_8.jpg</url>
                <caption>Figure 8. World In Miniature. a) WIM provides a miniaturized view of
                    the whole space in the vicinity of the user and attaches it to the tablet.
                    b) It is possible to use all interaction metaphors inside WIM such as the
                    rotation gizmo.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376628_crop_9.jpg</url>
                <caption>Figure 9. a) Portals can be places in space and provide a virtual camera
                    that is visible on the tablet. b) Primitives can be extruded to create solid
                    objects.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3313831.3376628_crop_10.jpg</url>
                <caption>Figure 10. Conﬁguration Menu. Besides the permanently available
                    physical tablet with two hardware buttons and the ﬂoating buttons, all
                    other controls are accessible via the conﬁguration menu on the tablet.
                    Icons © icons8.com. Used under CC BY-ND 3.0. [26]
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3313831.3376628_crop_11.jpg</url>
                <caption>Figure 11. Islands with beach houses created by our participants during the usability walkthrough (P2: (a), P1: (b), P5: (c)).
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376639</filename><data>
            <paper_id>3313831.3376639</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376639</doi>
            <title>Performance and Experience of Throwing in Virtual Reality</title>
            <abstract>
                Throwing is a fundamental movement in many sports and games. Given this, accurate throwing in VR applications today is surprisingly difficult. In this paper we explore the nature of the difficulties of throwing in VR in more detail. We present the results of a user study comparing throwing in VR and in the physical world. In a short pre-study with 3 participants we determine an optimal number of throwing repetitions for the main study by exploring the learning curve and subjective fatigue of throwing in VR. In the main study, with 12 participants, we find that throwing precision and accuracy in VR are lower particularly in the distance and height dimensions. It also requires more effort and exhibits different kinematic patterns.
            </abstract>
            <sections>
                <section>
                    <word_count>411</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>563</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>435</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>PHYSICAL BACKGROUND</title>
                </section>
                <section>
                    <word_count>380</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>IMPLEMENTATION OF THROWING IN VR</title>
                </section>
                <section>
                    <word_count>711</word_count>
                    <figure_citations>Figure 1).Figure 2 illustrates the accuracy of each hit point for the three participants over the course of the session.</figure_citations>
                    <section_index>4</section_index>
                    <title>LEARNING CURVE</title>
                </section>
                <section>
                    <word_count>729</word_count>
                    <figure_citations>Figure 1 shows the three stations in the real world and a throw in VR.Figure 1).</figure_citations>
                    <section_index>5</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1228</word_count>
                    <figure_citations>Figure 3).Figure 4, the precision is not only more than 2-3 times higher in the real world than in VR, it is also more stable between participants, namely they have more similar precision levels in real world, than in VR.Figure 5) highlight, that while the scattering of hit points is generally higher in all VR throwing styles, the hit points are also much more scattered along the vertical/longitudal axis, than the horizontal.Figure 6 shows the “darts-like” throw (left) and the underhand throw (right) for a representative participant.Figure 7 we can observe Page 5 CHI 2020 Paper CHI 2020, April 25–30, 2020, Honolulu, HI, USA Figure 7.</figure_citations>
                    <section_index>6</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>341</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>81</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>1080</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Throwing</li>
                <li>Virtual Reality</li>
                <li>User Study</li>
            </keywords>
            <authors>
                <li>Tim Zindulka</li>
                <li> Myroslav Bachynskyi</li>
                <li> Jörg Müller</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376639_crop_1.jpg</url>
                <caption>Figure 1. We show that throwing in reality (2-4) is more than twice as accurate compared to VR (1). We investigate overhand throws at short range
                    with vertical targets (1, 2), underhand throws at short range with horizontal targets (3), and overhand throws at long range (4).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376639_crop_2.jpg</url>
                <caption>Figure 2. Throwing accuracy for three participants shows no improve-
                    ment after the initial 10-15 throws.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376639_crop_3.jpg</url>
                <caption>Figure 3. Throwing accuracy in the real world is almost twice as high as
                    in VR.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376639_crop_4.jpg</url>
                <caption>Figure 4. Throwing precision in the real world is 2-3 times higher, and
                    more stable between the participants compared to VR.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376639_crop_5.jpg</url>
                <caption>Figure 5. Scatter plots of hit points with kernel density estimator. A
                    small number of virtual hit points are located outside the viewport
                    shown here. The ﬁrst row shows the real setting and the second row
                    VR. Stations 1, 2, and 3 are shown from left to right. The black circle
                    represents the outline of the target (All units are meters).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376639_crop_6.jpg</url>
                <caption>Figure 6. Comparison of kinematic patterns for Station 1 (left) and 2
                    (right). The real movement data recorded with OptiTrack is displayed
                    in red. Data points recorded in VR with Unity are shown in blue. The
                    throwing direction followed the green arrow along the red axis to the
                    right.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376639_crop_7.jpg</url>
                <caption>Figure 7. Boxplot of the average perceived workload according to the
                    Raw NASA TLX scores.
                </caption>
                <page>6</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376642</filename><data>
            <paper_id>3313831.3376642</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376642</doi>
            <title>Again, Together: Socially Reliving Virtual Reality Experiences When Separated</title>
            <abstract>
                To share a virtual reality (VR) experience remotely together, users usually record videos from an individual's point of view and then co-watch these videos. However, co-watching recorded videos limits users to reliving their memories from the perspective from which the video was captured. In this paper, we describe ReliveInVR, a new time-machine-like VR experience sharing method. ReliveInVR allows multiple users to immerse themselves in the relived experience together and independently view the experience from any perspective. We conducted a 1x3 within-subject study with 26 dyads to compare ReliveInVR with (1) co-watching 360-degree videos on desktop, and (2) co-watching 360-degree videos in VR. Our results suggest that participants reported higher levels of immersion and social presence in ReliveInVR. Participants in ReliveInVR also understood the shared experience better, discovered unnoticed things together and found the sharing experience more fulfilling. We discuss the design implications for sharing VR experiences over time and space.
            </abstract>
            <sections>
                <section>
                    <word_count>615</word_count>
                    <figure_citations>Figure 1, ReliveInVR allows users to relive the virtual experience together.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>453</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>900</word_count>
                    <figure_citations>Figure 2).Figure 3, All events were different on each island.</figure_citations>
                    <section_index>2</section_index>
                    <title>PROTOTYPES AND VR ARCHERY GAME</title>
                </section>
                <section>
                    <word_count>1420</word_count>
                    <figure_citations>Figure 5 (similar to co-watching 360-degree video in Facebook Spaces).Figure 6, we implemented a VR networked environment and a state-based replay system with SteamVR plugins and Photon Networking framework in Unity.</figure_citations>
                    <section_index>3</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>2132</word_count>
                    <figure_citations>Figure 7 (a), the ANOVA for the linear mixed model of Shared Cognition yielded a signiﬁcant effect of the sharing condition, F(2, 102) = 13.Figure 7(c) shows a signiﬁcant effect of the sharing condition, F(2, 102) = 47.Figure 7 (e) shows that the the Paper 513 ANOVA for the linear mixed model of Conversation Engagement yielded a signiﬁcant effect of the sharing condition, F(2, 102) = 19.Figure 7 (f).Figure 7 (g) and (h), when asked about most prefered sharing tool for sharing VR experience remotely together, 96% of the participants chose ReliveInVR, 2% of the participants chose Co-watchVR and 2% chose Co-watchDT.Figure 8, the means and standard deviations for the sharing time ratios for each sharing condition are: Co-watchDT=1.Figure 9, a post-hoc test using Wilcoxon signed-rank with Bonferroni correction showed signiﬁcant differences between ReliveInVR and Co-watchDT Spaces (p &lt; 0.</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1048</word_count>
                    <figure_citations>Figure 10 (a) demonstrates the percentage of where participants viewed their replayed avatars during Review session in 4 different areas around the replayed avatar.</figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>586</word_count>
                    <figure_citations>Figure 10(b), participants only had average 18.</figure_citations>
                    <section_index>6</section_index>
                    <title>DESIGN IMPLICATIONS</title>
                </section>
                <section>
                    <word_count>206</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>LIMITATIONS AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>136</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>1292</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Shared experience</li>
                <li>Virtual reality</li>
                <li>Social</li>
                <li>Replay</li>
                <li>Shared experience</li>
                <li>Presence</li>
                <li>Immersion</li>
            </keywords>
            <authors>
                <li>Cheng Yao Wang</li>
                <li> Mose Sakashita</li>
                <li> Upol Ehsan</li>
                <li> Jingjin Li</li>
                <li> Andrea Stevenson Won</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376642_crop_1.jpg</url>
                <caption>Figure 1. Three prototypes for sharing virtual reality experiences over distance: (1) Co-watching 360-degrees videos on desktop; (2) Co-watching
                    360-degrees videos in VR; (3) ReliveInVR: fully recreating the experience to relive it socially.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376642_crop_2.jpg</url>
                <caption>Figure 2. The basic interactions in the archery game; (1) picking up
                    a bow, (2) grabbing an arrow, releasing an arrow, and (3) teleporting.
                    ReliveInVR provides controls such as play, pause, seek to certain time
                    stamp (4).
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376642_crop_3.jpg</url>
                <caption>Figure 3. All the surprising events in the both islands.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376642_crop_4.jpg</url>
                <caption>Figure 4. Co-watching 360-degree videos on desktop (Co-watchDT) pro-
                    totype: (a) Sharer can have the video control. (b) Sharee can view a
                    synchronized video with the sharer over distance. Sharer and sharee
                    can have different perspectives of the 360-degree video
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376642_crop_5.jpg</url>
                <caption>Figure 5. Co-watching 360-degree videos in VR prototype (CowatchVR).
                    Users can watch the 360-degree video together and see each other’s
                    avatar at the same time.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376642_crop_6.jpg</url>
                <caption>Figure 6. Reliving experience in VR prototype (ReliveInVR).(a) P2’s
                    point of view when P1 and P2 relive P1’s experience through ReliveInVR.
                    (b) P1 and P2 can see each other and P1’s replayed avatar. (c) VR
                    networked environment and record-replay technique in the ReliveInVR
                    prototype.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376642_crop_7.jpg</url>
                <caption>Figure 7. (a)-(e) boxplots for each factor across sharing conditions. (f) Self-reported emotion ratings for each condition. (g) Most preferred sharing
                    tools. (h) Least preferred sharing tools.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376642_crop_8.jpg</url>
                <caption>Figure 8. The result of normalized sharing time consists of the Review
                    session and the Exploration session.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376642_crop_9.jpg</url>
                <caption>Figure 9. Head orientation result across all sharing conditions.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3313831.3376642_crop_10.jpg</url>
                <caption>Figure 10. (a) The distribution of relative positions between partici-
                    pant’s avatar and replayed avatar when the participant viewed the re-
                    played avatar in the ReliveInVR condition. (b) The percentage of time
                    when two participants were within the area of social space during the
                    Exploration session in the ReliveInVR condition.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376652</filename>
        <data>
            <paper_id>3313831.3376652</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376652</doi>
            <title>"In VR, everything is possible!": Sketching and Simulating Spatially-Aware Interactive Spaces in Virtual Reality</title>
            <abstract>We propose using virtual reality (VR) as a design tool for sketching and simulating spatially-aware interactive spaces. Using VR, designers can quickly experience their envisioned spaces and interactions by simulating technologies such as motion tracking, multiple networked devices, or unusual form factors such as spherical touchscreens or bezel-less display tiles. Design ideas can be rapidly iterated without restrictions by the number, size, or shape and availability of devices or sensors in the lab. To understand the potentials and challenges of designing in VR, we conducted a user study with 12 interaction designers. As their tool, they used a custom-built virtual design environment with finger tracking and physics simulations for natural interactions with virtual devices and objects. Our study identified the designers' experience of space in relation to their own bodies and playful design explorations as key opportunities. Key challenges were the complexities of building a usable yet versatile VR-based "World Editor".</abstract>
            <sections>
                <section>
                    <word_count>1106</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1499</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1761</word_count>
                    <figure_citations>Figure 2).</figure_citations>
                    <section_index>2</section_index>
                    <title>DESIGN AND IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>1446</word_count>
                    <figure_citations>Figure 4).Figure 5, A and B).Figure 5, A), we allow users to conﬁgure the VE with displays of different shapes, as well as real-world and abstract objects.Figure 5 B), only objects with collision status turned on remain graspable and can be rescaled or moved around in virtual space.</figure_citations>
                    <section_index>3</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>2687</word_count>
                    <figure_citations>Figure 6).Figure 7).Figure 7).</figure_citations>
                    <section_index>4</section_index>
                    <title>OBSERVED OPPORTUNITIES AND CHALLENGES</title>
                </section>
                <section>
                    <word_count>461</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>LIMITATIONS AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>116</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>40</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>3693</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Interactive Spaces</li>
                <li>Spatial Awareness</li>
                <li>Interaction Design</li>
                <li>Virtual Reality</li>
                <li>Design tools</li>
            </keywords>
            <authors>
                <li>Hans-Christian Jetter</li>
                <li> Roman Rädle</li>
                <li> Tiare Feuchtner</li>
                <li> Christoph Anthes</li>
                <li> Judith Friedl</li>
                <li> Clemens Nylandsted Klokmose</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376652_crop_1.jpg</url>
                <caption>Figure 1. We created a virtual environment for designers, in which they can generate and arrange an arbitrary number of devices that execute real-
                    world web applications (A). This allows simulation of existing interactive spaces and multi-device systems (B, C) [71], as well as sketching of new
                    interactions with diverse tracking systems or futuristic devices, e.g., a cylindrical touch screen (D).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376652_crop_2.jpg</url>
                <caption>Figure 2. Our experimental design tool uses ﬁnger and hand tracking to
                    let users naturally interact with devices and objects in the VE.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376652_crop_3.jpg</url>
                <caption>Figure 3. Simulations of existing systems in VR (right) and the original
                    system (left). 1) Window Manager by Klokmose et al. [49] for 75 LCD
                    tiles. 2) VisTiles by Langner et al. [51]. 3) Peephole navigation with
                    HuddleLamp by Rädle et al. [73]. 4) Edge Bubbles by Rädle et al. [74].
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376652_crop_4.jpg</url>
                <caption>Figure 4. VR-based design tools can enable designers to experience fu-
                    turistic touchscreen devices, e.g., Google Maps on a cylindrical touch-
                    screen or drawing on a spherical display.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376652_crop_5.jpg</url>
                <caption>Figure 5. In design mode (A), users can manipulate device properties and resize them. In use mode (B), users can draw on devices and interact with
                    real-world applications using (multi-)touch.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376652_crop_6.jpg</url>
                <caption>Figure 6. P6 tiptoes (left) and P9 jumps (right) to reach overhead targets.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376652_crop_7.jpg</url>
                <caption>Figure 7. P1 throwing a ball at the targets on the display (left). P6
                    picking up a tablet she dropped (right).
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376687</filename>
        <data>
            <paper_id>3313831.3376687</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376687</doi>
            <title>Improving Virtual Reality Ergonomics Through Reach-Bounded Non-Linear Input Amplification</title>
            <abstract>Input amplification enables easier movement in virtual reality (VR) for users with mobility issues or in confined spaces. However, current techniques either do not focus on maintaining feelings of body ownership, or are not applicable to general VR tasks. We investigate a general purpose non-linear transfer function that keeps the user's reach within reasonable bounds to maintain body ownership. The technique amplifies smaller movements from a user-definable neutral point into the expected larger movements using a configurable Hermite curve. Two experiments evaluate the approach. The first establishes that the technique has comparable performance to the state-of-the-art, increasing physical comfort while maintaining task performance and body ownership. The second explores the characteristics of the technique over a wide range of amplification levels. Using the combined results, design and implementation recommendations are provided with potential applications to related VR transfer functions.</abstract>
            <sections>
                <section>
                    <word_count>688</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>2670</word_count>
                    <figure_citations>Figure 2a).Figure 2b.Figure 3 illustrates the curve confgurations used for Experiment 1.Figure 3).Figure 4).Figure 5a), and within each layout, the three amplifcation levels produced similar times.</figure_citations>
                    <section_index>1</section_index>
                    <title>BACKGROUND AND RELATED WORK</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>NONE</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>LOW</title>
                </section>
                <section>
                    <word_count>948</word_count>
                    <figure_citations>Figure 5b), with HIGH amplifcation reducing RULA more than LOW.Figure 5c).Figure 5d).</figure_citations>
                    <section_index>4</section_index>
                    <title>HIGH</title>
                </section>
                <section>
                    <word_count>1000</word_count>
                    <figure_citations>Figure 6).Figure 7).</figure_citations>
                    <section_index>5</section_index>
                    <title>AMPLIFICATION</title>
                </section>
                <section>
                    <word_count>1777</word_count>
                    <figure_citations>Figure 8a).Figure 8b).Figure 8c).Figure 8d).Figure 8e).Figure 9).Figure 10 shows responses for questions 1 to 4.</figure_citations>
                    <section_index>6</section_index>
                    <title>TANCES</title>
                </section>
                <section>
                    <word_count>1066</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>GENERAL DISCUSSION</title>
                </section>
                <section>
                    <word_count>168</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>36</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1198</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Interaction techniques</li>
                <li>Ergonomics</li>
                <li>Input re-mapping</li>
            </keywords>
            <authors>
                <li>Johann Wentzel</li>
                <li> Greg d'Eon</li>
                <li> Daniel Vogel</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376687_crop_1.jpg</url>
                <caption>Figure 1. The physical controller position (green), relative to a calibrated
                    neutral position, is amplifed using a non-linear function so the virtual
                    controller (blue) appears farther away. The transfer function keeps the
                    physical-to-virtual hand offset small near the body, but maximum vir-
                    tual reach can be achieved with the real controller moving 30% less.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376687_crop_2.jpg</url>
                <caption>Figure 2. Key geometric points captured in calibration and used for the
                    amplifcation technique: (a) the user’s maximum reach Pmax, at distance
                    rmax from their shoulder point PS; (b) P∗
                    H is calculated from the user’s
                    hand position PH .
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376687_crop_3.jpg</url>
                <caption>Figure 3. The Low and High amplifcation functions used in Experiment
                    1. These functions modify the relationship between the physical offset r
                    and the virtual offset f (r) from PN .
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376687_crop_4.jpg</url>
                <caption>Figure 4. The ERGONOMIC, LIMITS, and FIXED target layouts used in
                    Experiment 1.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376687_crop_5.jpg</url>
                <caption>Figure 5. (a) Time, (b) Comfort, (c) Physical Path Length, and (d) Virtual Path Length by AMPLIFICATION and LAYOUT. Error bars are 95% CI.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376687_crop_6.jpg</url>
                <caption>Figure 6. Levels of AMPLIFICATION used in Experiment 2. These curves
                    modify the relationship between the physical offset r and the virtual off-
                    set f (r) from PN .
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376687_crop_7.jpg</url>
                <caption>Figure 7. The target layout used in Experiment 2, coloured by DIS-
                    TANCE, from the side (a) and the front (b). Purple targets are CLOSE,
                    blue are MID, green are FAR. Only two targets are visible at a time (c).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376687_crop_8.jpg</url>
                <caption>Figure 8. (a) Time, (b) Comfort, (c) Error, (d) Physical Path Length, and (e) Virtual Path Length by AMPLIFICATION. Error bars are 95% CI.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376687_crop_9.jpg</url>
                <caption>Figure 9. The proportion of participants who noticed that amplifcation
                    was taking place at each level.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3313831.3376687_crop_10.jpg</url>
                <caption>Figure 10. Proportion of questionnaire answers by AMPLIFICATION. Answers were inverted for Q2 for visual comparison.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376698</filename>
        <data>
            <paper_id>3313831.3376698</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376698</doi>
            <title>Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen</title>
            <abstract>
                The use of Virtual Reality (VR) in applications such as data analysis, artistic creation, and clinical settings requires high precision input. However, the current design of handheld controllers, where wrist rotation is the primary input approach, does not exploit the human fingers' capability for dexterous movements for high precision pointing and selection. To address this issue, we investigated the characteristics and potential of using a pen as a VR input device. We conducted two studies. The first examined which pen grip allowed the largest range of motion---we found a tripod grip at the rear end of the shaft met this criterion. The second study investigated target selection via 'poking' and ray-casting, where we found the pen grip outperformed the traditional wrist-based input in both cases. Finally, we demonstrate potential applications enabled by VR pen input and grip postures.
            </abstract>
            <sections>
                <section>
                    <word_count>876</word_count>
                    <figure_citations>Figure 1).Figure 7b).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1048</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>4817</word_count>
                    <figure_citations>Figure 2 illustrates the ﬁve grip postures that were investigated in this study, their descriptions follow: Tripod at Front End (TFE): This is the most common grip posture for precise writing and drawing on a surface (Figure 2a).Figure 2c).Figure 2d).Figure 2e).Figure 3a.Figure 3b).Figure 5).Figure 6b and Figure 7b.Figure 6b and 7b) that ran the experimental (Unity) application.Figure 9 revealed a strong correlation between the trial time and ID for Tilt gestures, with both R2 values above 0.Figure 10).</figure_citations>
                    <section_index>2</section_index>
                    <title>AND FINGER MOTION</title>
                </section>
                <section>
                    <word_count>272</word_count>
                    <figure_citations>Figure 11).</figure_citations>
                    <section_index>3</section_index>
                    <title>INTERACTION TECHNIQUES AND APPLICATIONS</title>
                </section>
                <section>
                    <word_count>888</word_count>
                    <figure_citations>Figure 12), we show that the gestures can be utilized as interface widgets, which trigger scrolling or zooming operations on a web view with tilting and poking the pen respectively.Figure 13a, the user can select an object which is hidden by another object he can’t see with the Palm Grip, leading to misinterpretation.Figure 13b, the larger object is stacked below the target object.</figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>134</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>74</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>2413</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Pen input</li>
                <li>FInger and wrist dexterity</li>
                <li>Grip postures</li>
                <li>Handheld controller</li>
                <li>Spatial target selection</li>
            </keywords>
            <authors>
                <li>Nianlong Li</li>
                <li> Teng Han</li>
                <li> Feng Tian</li>
                <li> Jin Huang</li>
                <li> Minghui Sun</li>
                <li> Pourang Irani</li>
                <li> Jason Alexander</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376698_crop_1.jpg</url>
                <caption>Figure 1. A user grips a pen controller to perform (a) poke gestures, and
                    (b) tilt gestures.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376698_crop_2.jpg</url>
                <caption>Figure 2. Candidate grip postures: (a) tripod at front end, (b) tripod at
                    rear end, (c) quadropod at rear end, (d) pinch, and (e) overhand.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376698_crop_3.jpg</url>
                <caption>Figure 3. (a) The 3D printed pen was used in the studies; (b) A user took
                    participant in Study 1.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376698_crop_4.jpg</url>
                <caption>Figure 4. The results of Study 1: (a) Pen tip travelled distance (mm); (b)
                    Pen shaft tilted angle (◦).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376698_crop_5.jpg</url>
                <caption>Figure 5. Participant responses to the workload of different grip pos-
                    tures. Graphs are centered around the neutral response, with the pro-
                    portion of positive and negative responses on the right and left side, re-
                    spectively.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376698_crop_6.jpg</url>
                <caption>Figure 6. (a) The user interface for selection with the poke; (b) A partic-
                    ipant uses the Pen Grip and selects with the poke in Study 2.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376698_crop_7.jpg</url>
                <caption>Figure 7. (a) The user interface for selection with tilt; (b) A participant
                    uses the Palm Grip and selects with tilt in Study 2.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>3313831.3376698_crop_8.jpg</url>
                <caption>Figure 8. Mean selection time for poke and tilt.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>3313831.3376698_crop_9.jpg</url>
                <caption>Figure 9. Fitts’ law models for the poke (solid line) and tilt (dash line)
                    tasks.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>3313831.3376698_crop_10.jpg</url>
                <caption>Figure 10. Participant responses to the workload of poke and tilt.
                    Graphs are centered around the neutral response, with the proportion
                    of positive and negative responses on the right and left side, respectively.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>11</id>
                <url>3313831.3376698_crop_11.jpg</url>
                <caption>Figure 11. (a) A participant ﬁrst selected a furniture using Palm Grip,
                    (b) then changed its color using Pen Grip.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>12</id>
                <url>3313831.3376698_crop_12.jpg</url>
                <caption>Figure 12. (a) A participant used tilt gesture to scroll the web page, (b)
                    and double poked to zoom in or zoom out.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>13</id>
                <url>3313831.3376698_crop_13.jpg</url>
                <caption>Figure 13. (a) A participant can select an invisible object; (b) A partici-
                    pant can see but cannot select the target.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376714</filename>
        <data>
            <paper_id>3313831.3376714</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376714</doi>
            <title>Touché: Data-Driven Interactive Sword Fighting in Virtual Reality</title>
            <abstract>
                VR games offer new freedom for players to interact naturally using motion. This makes it harder to design games that react to player motions convincingly. We present a framework for VR sword fighting experiences against a virtual character that simplifies the necessary technical work to achieve a convincing simulation. The framework facilitates VR design by abstracting from difficult details on the lower "physical" level of interaction, using data-driven models to automate both the identification of user actions and the synthesis of character animations. Designers are able to specify the character's behaviour on a higher "semantic" level using parameterised building blocks, which allow for control over the experience while minimising manual development work. We conducted a technical evaluation, a questionnaire study and an interactive user study. Our results suggest that the framework produces more realistic and engaging interactions than simple hand-crafted interaction logic, while supporting a controllable and understandable behaviour design.
            </abstract>
            <sections>
                <section>
                    <word_count>562</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1811</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>2390</word_count>
                    <figure_citations>Figure 1 shows a high-level view of Touché.</figure_citations>
                    <section_index>2</section_index>
                    <title>SYSTEM DESIGN</title>
                </section>
                <section>
                    <word_count>176</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>INTERACTION DESIGN METHODOLOGY</title>
                </section>
                <section>
                    <word_count>861</word_count>
                    <figure_citations>Figure 3 plots the prediction accuracy for gesture class and progress.Figure 4 visualises this formula for a single pair of bones.Figure 5 shows the distribution of the pose difference, measured in square meters.</figure_citations>
                    <section_index>4</section_index>
                    <title>TECHNICAL EVALUATION</title>
                </section>
                <section>
                    <word_count>1536</word_count>
                    <figure_citations>Figure 6 shows the results of the study.Figure 7 and Table 1.</figure_citations>
                    <section_index>5</section_index>
                    <title>USER EVALUATION</title>
                </section>
                <section>
                    <word_count>942</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>225</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>42</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>2441</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Sword ﬁghting</li>
                <li>Machine learning</li>
                <li>Animation</li>
                <li>Gesture recognition</li>
            </keywords>
            <authors>
                <li>Javier Dehesa</li>
                <li> Andrew Vidler</li>
                <li> Christof Lutteroth</li>
                <li> Julian Padget</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376714_crop_1.jpg</url>
                <caption>Figure 1: Left: Our framework splits the problem of simulating interactive VR sword ﬁghting characters into a “physical” level,
                    relying on data-driven models, and a “semantic” level, where designers can conﬁgure the behaviour of the character. Right:
                    The framework generates responsive animations against player attacks (top), avoiding nonreactive behaviour from the character
                    (bottom left). A neural network parameterised by the position of the player’s sword synthesises the animation (bottom right).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376714_crop_2.jpg</url>
                <caption>Figure 2: Character behaviour diagram. Transitions marked
                    with * depend on the conﬁguration of the framework.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376714_crop_3.jpg</url>
                <caption>Figure 3: Gesture class and progress prediction accuracy.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376714_crop_4.jpg</url>
                <caption>Figure 4: The pose difference measures the total distance
                    between pairs of corresponding bones (bone A and bone B),
                    computing the area of the four triangles from the bone ends
                    (A1, A2, B1, B2) to the mid point (M).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376714_crop_5.jpg</url>
                <caption>Figure 5: Normalised histogram and kernel density estima-
                    tion of the distribution of pose difference between the motion
                    predicted by our model and the evaluation data.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376714_crop_6.jpg</url>
                <caption>Figure 6: Interest / Enjoyment, Realism, Skill and Repetitive-
                    ness scores for the questionnaire study. The control condition
                    C was generally inferior, while differences between A and D
                    reﬂect the impact of the framework conﬁguration.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376714_crop_7.jpg</url>
                <caption>Figure 7: Interest / Enjoyment, Realism, Skill and Repetitive-
                    ness scores for the interactive study.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376719</filename><data>
            <paper_id>3313831.3376719</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/10.1145/3313831.3376719</doi>
            <sections></sections>
            <keywords>
                <li>Agency</li>
                <li>Social Touch</li>
                <li>Virtual Reality</li>
                <li>Human-likeness</li>
            </keywords>
            <authors>
                <li>undefined</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376719_crop_1.jpg</url>
                <caption>Figure 1. Is the presented character a human-controlled avatar or computer-controlled agent? If the character reaches out and physically performs a
                    social touch on a user, it will blur the boundaries between avatar and agent. For this we used a heat-able hand prototype with ﬂexible joints, to recreate
                    a touch sensation that is indistinguishable from a real hand.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376719_crop_2.jpg</url>
                <caption>Figure 2. The above-mentioned features were named by the participants
                    of the main and preliminary studies as features that inﬂuenced the deci-
                    sion whether they perceived the presented character as an agent or an
                    avatar. Social cues – such as the in the study induced social touch – were
                    selected as clear hints towards a human-controlled entity.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376719_crop_3.jpg</url>
                <caption>Figure 3. The silicone hand contains a baseplate for solidity, a wire skele-
                    ton, a heat-able foil element and a mount for tripods. The inner skeleton
                    provides adaptability for the hand posture. The mount allows the hand
                    to be connected to other mechanical devices such as levers.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376719_crop_4.jpg</url>
                <caption>Figure 4. The Figure shows the ratings of the participants regarding four measures and two factors (Entity and Touch). Higher ratings indicate for
                    all measures that the character was perceived as human-controlled. For all measures the avatar was rated signiﬁcantly higher compared to the agent
                    conditions. For Perceived Agency, Co-Presence and Embarrassment we see an signiﬁcant increase based on the introduction of Touch.
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376762</filename><data>
            <paper_id>3313831.3376762</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376762</doi>
            <title>Investigating Roleplaying and Identity Transformation in a Virtual Reality Narrative Experience</title>
            <abstract>In this paper we describe the design and evaluation of The Next Fairy Tale (TNFT) VR, a theatrical interactive storytelling system created in virtual reality and informed by performing arts theories. TNFT was designed to produce opportunities for interactors to experience role-taking and character identification using design principles drawn from actor training and theatrical performance. We report the results of a pilot qualitative study of interactors using TNFT to explore the elements of the design that supported or hindered roleplaying behavior. We identify four design patterns that supported roleplaying in the system: (1) using explicit roles to set player expectations, (2) embracing the "mask and the mirror" effect, (3) attending to visual and interactional details, and (4) easing the player gently into the roleplaying experience. These patterns speak to a broader need to support roleplay through explicit scaffolding of desired player behaviors in digital narrative experiences.</abstract>
            <sections>
                <section>
                    <word_count>649</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1373</word_count>
                    <figure_citations>Figure 1 – (a) Path to Minerva (b) Calliope’s Reflection in Mirror (c ) The main area not.</figure_citations>
                    <section_index>1</section_index>
                    <title>LITERATURE REVIEW</title>
                </section>
                <section>
                    <word_count>1408</word_count>
                    <figure_citations>Figure 1a).Figure 1b).Figure 2 – (a) The Emotion Spells (b) Interacting with the spells (c) Activating a spell with the wand Paper 633 Page 4 CHI 2020 Paper CHI 2020, April 25–30, 2020, Honolulu, HI, USA Figure 3 - The interactional logic of TNFT's dialogue system, the emotion spells, and Minerva's responses Figure 3 illustrates the interactional logic of our dialogue system.Figure 3), following the same color mappings as the spells.Figure 3 we visualize this and show how a player might navigate the expressive space of TNFT.</figure_citations>
                    <section_index>2</section_index>
                    <title>DESIGNING A PARTICIPATORY THEATER EXPERIENCE</title>
                </section>
                <section>
                    <word_count>386</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>METHODS</title>
                </section>
                <section>
                    <word_count>2504</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>FINDINGS</title>
                </section>
                <section>
                    <word_count>1830</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DESIGN RECOMMENDATIONS</title>
                </section>
                <section>
                    <word_count>413</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>51</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1593</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Interactive Digital Storytelling</li>
                <li>Interactive Performance</li>
                <li>Roleplaying</li>
                <li>Narrative</li>
                <li>Drama</li>
            </keywords>
            <authors>
                <li>Saumya Gupta</li>
                <li> Theresa Jean Tanenbaum</li>
                <li> Meena Devii Muralikumar</li>
                <li> Aparajita S. Marathe</li>
            </authors>
        </data>
        <figures></figures>
    </article>
    <article>
        <filename>3313831.3376788</filename>
        <data>
            <paper_id>3313831.3376788</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376788</doi>
            <title>Would you do it?: Enacting Moral Dilemmas in Virtual Reality for Understanding Ethical Decision-Making</title>
            <abstract>
                A moral dilemma is a decision-making paradox without unambiguously acceptable or preferable options. This paper investigates if and how the virtual enactment of two renowned moral dilemmas---the Trolley and the Mad Bomber---influence decision-making when compared with mentally visualizing such situations. We conducted two user studies with two gender-balanced samples of 60 participants in total that compared between paper-based and virtual-reality (VR) conditions, while simulating 5 distinct scenarios for the Trolley dilemma, and 4 storyline scenarios for the Mad Bomber's dilemma. Our findings suggest that the VR enactment of moral dilemmas further fosters utilitarian decision-making, while it amplifies biases such as sparing juveniles and seeking retribution. Ultimately, we theorize that the VR enactment of renowned moral dilemmas can yield ecologically-valid data for training future Artificial Intelligence (AI) systems on ethical decision-making, and we elicit early design principles for the training of such systems.
            </abstract>
            <sections>
                <section>
                    <word_count>784</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1753</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1776</word_count>
                    <figure_citations>Figure 1).Figure 1a).Figure 1c).</figure_citations>
                    <section_index>2</section_index>
                    <title>STUDY</title>
                </section>
                <section>
                    <word_count>3828</word_count>
                    <figure_citations>Figure 2).Figure 2).Figure 2).Figure 3).Figure 4).Figure 4).Figure 4).Figure 5).Figure 6).Figure 6).Figure 7).Figure 7).</figure_citations>
                    <section_index>3</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1172</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>DESIGNING FOR ETHICAL AI TRAINING</title>
                </section>
                <section>
                    <word_count>213</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>1232</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Ethics</li>
                <li>Moral dilemmas</li>
                <li>VR</li>
                <li>Decision-making</li>
                <li>Ethical AI</li>
            </keywords>
            <authors>
                <li>Evangelos Niforatos</li>
                <li> Adam Palma</li>
                <li> Roman Gluszny</li>
                <li> Athanasios Vourvopoulos</li>
                <li> Fotis Liarokapis</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376788_crop_1.jpg</url>
                <caption>Figure 1. (a) The train platform and the avatars approaching in the Trolley study, (b) A participant in the Trolley study wearing the Oculus DK2
                    and relying on Kinect for input, (c) Interacting with the bomber avatar in the Mad Bomber study, (d) A participant wearing Oculus DK2 and using a
                    mouse/keyboard for input.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376788_crop_2.jpg</url>
                <caption>Figure 2. Lever pulls (%) by scenario and condition in the Trolley study.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376788_crop_3.jpg</url>
                <caption>Figure 3. Tactics employed (%) by condition in the Mad Bomber study.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376788_crop_4.jpg</url>
                <caption>Figure 4. Tactics employed in all scenarios and both conditions in the
                    Mad Bomber study.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376788_crop_5.jpg</url>
                <caption>Figure 5. Median self-reported workload for all scenarios and both con-
                    ditions in the Mad Bomber study.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376788_crop_6.jpg</url>
                <caption>Figure 6. Lever pulls (%) by scenario for both genders and both condi-
                    tions in the Trolley study.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376788_crop_7.jpg</url>
                <caption>Figure 7. Tactics employed (%) for both genders and both conditions in
                    the Mad Bomber study.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376803</filename><data>
            <paper_id>3313831.3376803</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376803</doi>
            <title>HiveFive: Immersion Preserving Attention Guidance in Virtual Reality</title>
            <abstract>
                Recent advances in Virtual Reality (VR) technology, such as larger fields of view, have made VR increasingly immersive. However, a larger field of view often results in a user focusing on certain directions and missing relevant content presented elsewhere on the screen. With HiveFive, we propose a technique that uses swarm motion to guide user attention in VR. The goal is to seamlessly integrate directional cues into the scene without losing immersiveness. We evaluate HiveFive in two studies. First, we compare biological motion (from a prerecorded swarm) with non-biological motion (from an algorithm), finding further evidence that humans can distinguish between these motion types and that, contrary to our hypothesis, non-biological swarm motion results in significantly faster response times. Second, we compare HiveFive to four other techniques and show that it not only results in fast response times but also has the smallest negative effect on immersion.
            </abstract>
            <sections>
                <section>
                    <word_count>947</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1014</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>4150</word_count>
                    <figure_citations>Figure 2).Figure 4a).Figure 4d).Figure 4b).Figure 4f).Figure 4c).</figure_citations>
                    <section_index>2</section_index>
                    <title>GENERAL APPROACH</title>
                </section>
                <section>
                    <word_count>1435</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>SGD</title>
                </section>
                <section>
                    <word_count>445</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>GENERAL DISCUSSION</title>
                </section>
                <section>
                    <word_count>137</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>FUTURE WORK</title>
                </section>
                <section>
                    <word_count>132</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>1586</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Attention guidance</li>
                <li>Virtual reality</li>
                <li>Immersion</li>
                <li>Eye-tracking</li>
                <li>Particle swarms</li>
                <li>User studies</li>
            </keywords>
            <authors>
                <li>Daniel Lange</li>
                <li> Tim Claudius Stratmann</li>
                <li> Uwe Gruenefeld</li>
                <li> Susanne Boll</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376803_crop_1.jpg</url>
                <caption>Figure 1: Our proposed technique HiveFive, in which a swarm visualization is used as a diegetic cue for guiding attention in
                    Virtual Reality (in two different environments - left: forest, right: city
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376803_crop_2.jpg</url>
                <caption>Figure 2: The visible FOV of the HTC Vive is given as 110◦
                    (red line) under optimal conditions. However, since the visible
                    range depends on many factors such as the ﬁt of the headset,
                    facial geometry and the distance between eyes and lenses,
                    we have identiﬁed an average FOV of 65◦ (blue line). The
                    lemniscate is located in the centre with a total width of 45◦
                    (white line). Best seen in color.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376803_crop_3.jpg</url>
                <caption>Figure 3: Boxplot of times to ﬁrst ﬁxation for both swarm
                    motion types for each environment (forest, city).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376803_crop_4.jpg</url>
                <caption>Figure 4: The apple tree used in the second study as overview (a) and the compared techniques (b-f). Best seen in color.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376803_crop_5.jpg</url>
                <caption>Figure 5: Boxplot of times to ﬁrst ﬁxation for all techniques.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376803_crop_6.jpg</url>
                <caption>Figure 6: Results from the 5-point Likert-scale questionnaire.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376821</filename><data>
            <paper_id>3313831.3376821</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376821</doi>
            <sections></sections>
            <keywords>
                <li>Redirected Walking</li>
                <li>Virtual Reality</li>
                <li>Telewalk</li>
            </keywords>
            <authors>
                <li>undefined</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376821_crop_1.jpg</url>
                <caption>Figure 1: The concept of Telewalk: The combination of perceivable curvature and translation gains along with a head based camera
                    control allows to compress any virtual space to a pre-defned real world radius (in our case 1.5m). (Left) illustration of walking paths
                    and (right) plots of the virtual and real path walked in our study application.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376821_crop_2.jpg</url>
                <caption>Figure 2: The visualization of the optimal path including the
                    spot with the optimal distance to the tracking center and the
                    two lines indicating the optimal direction. The user’s viewing
                    direction is visualized by a single line.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376821_crop_3.jpg</url>
                <caption>Figure 3: Boxplots of presence (SUS), immersion and enjoy-
                    ment (E2I) and simulator sickness (SSQ) scores. The marked
                    comparisons were signifcant on the 5% level.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376821_crop_4.jpg</url>
                <caption>Figure 4: Diverging Stacked Bar Charts of the single item questions as described in the Method section.
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>3313831.3376847</filename>
        <data>
            <paper_id>3313831.3376847</paper_id>
            <venue>CHI 20</venue>
            <doi>10.1145/3313831.3376847</doi>
            <title>WalkingVibe: Reducing Virtual Reality Sickness and Improving Realism while Walking in VR using Unobtrusive Head-mounted Vibrotactile Feedback</title>
            <abstract>
                Virtual Reality (VR) sickness is common with symptoms such as headaches, nausea, and disorientation, and is a major barrier to using VR. We propose WalkingVibe, which applies unobtrusive vibrotactile feedback for VR walking experiences, and also reduces VR sickness and discomfort while improving realism. Feedback is delivered through two small vibration motors behind the ears at a frequency that strikes a balance in inducing vestibular response while minimizing annoyance. We conducted a 240-person study to explore how visual, audio, and various tactile feedback designs affect the locomotion experience of users walking passively in VR while seated statically in reality. Results showed timing and location for tactile feedback have significant effects on VR sickness and realism. With WalkingVibe, 2-sided step-synchronized design significantly reduces VR sickness and discomfort while significantly improving realism. Furthermore, its unobtrusiveness and ease of integration make WalkingVibe a practical approach for improving VR experiences with new and existing VR headsets.
            </abstract>
            <sections>
                <section>
                    <word_count>930</word_count>
                    <figure_citations>Figure 1 and Figure 2, uses two small vibration motors controlled by an Arduino microcontroller and is entirely integrated into and powered by the VR headset.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>793</word_count>
                    <figure_citations>Figure 3 (c)).Figure 3 (b)) vs.Figure 3 (a)).</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1945</word_count>
                    <figure_citations>Figure 2).Figure 3a and Figure 3b, respectively) were chosen based on the settings of the commercial bone-conducted earphone and other previous systems [61, 62].Figure 3c).Figure 3a).Figure 3b).Figure 4c) that were scattered in three designed VR scenes: a city in Figure 4(a)(d), a forest in Figure 4(b)(e), and a science ﬁction-themed passage in Figure 4(c)(f).Figure 5, and are stated directly in the following: </figure_citations>
                    <section_index>2</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>707</word_count>
                    <figure_citations>Figure 5, 6 and 7, where error bars in each ﬁgure denoted standard error of the mean value), and report our ﬁndings in the following.Figure 5).Figure 6).Figure 6 and are described as follows: • 2-sided tapping (synchronized) vs.</figure_citations>
                    <section_index>3</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1114</word_count>
                    <figure_citations>Figure 7).</figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>106</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>59</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGEMENT</title>
                </section>
                <section>
                    <word_count>2277</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality sickness</li>
                <li>Discomfort</li>
                <li>Realism</li>
                <li>Vestibular system</li>
                <li>Vibrotactile feedback</li>
            </keywords>
            <authors>
                <li>Yi-Hao Peng</li>
                <li> Carolyn Yu</li>
                <li> Shi-Hong Liu</li>
                <li> Chung-Wei Wang</li>
                <li> Paul Taele</li>
                <li> Neng-Hao Yu</li>
                <li> Mike Y. Chen</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>3313831.3376847_crop_1.jpg</url>
                <caption>Figure 1. WalkingVibe prototype with 2 vibration motors behind the
                    ears, which provide vibrotactile stimulation synchronized to footsteps
                    in VR. Our 240-person study showed that it signiﬁcantly reduced dis-
                    comfort and VR sickness, and signiﬁcantly improved the realism of the
                    virtual walking experience in VR.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>3313831.3376847_crop_2.jpg</url>
                <caption>Figure 2. WalkingVibe prototype with 2-sided vibrotactile design.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>3313831.3376847_crop_3.jpg</url>
                <caption>Figure 3. The setup for each category of tactile feedback: (a) 2-sided vibration, (b) backside vibration, (c) 2-sided tapping feedback replicated from
                    PhantomLegs [37] project.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>3313831.3376847_crop_4.jpg</url>
                <caption>Figure 4. Screenshots of three scenes and paths of the virtual walking
                    environment: (a)(d) the city, (b)(e) the forest, and (c)(f) the sci-ﬁ Passage.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>3313831.3376847_crop_5.jpg</url>
                <caption>Figure 5. The average and standard deviation (in parentheses) of dis-
                    comfort scores (from left to right) are: 1.81 (0.61), 1.53 (0.75), 1.70 (0.92),
                    1.36 (0.53), 1.07 (0.62), 1.45 (0.63), 1.86 (0.74), 1.61 (0.75).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>3313831.3376847_crop_6.jpg</url>
                <caption>Figure 6. The average and standard deviation (in parentheses) of RSS
                    (from left to right) are: 22.19 (18.60), 18.70 (19.17), 18.20 (15.47), 6.73
                    (15.46), 9.35 (16.75), 9.60 (14.18), 16.95 (12.16), 14.34 (20.55).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>3313831.3376847_crop_7.jpg</url>
                <caption>Figure 7. The average and standard deviation (in parentheses) of realism
                    score (from left to right) are: 4.17 (1.86), 4.93 (2.08), 5.70 (2.31), 4.27
                    (2.03), 5.67 (2.08), 3.50 (1.69), 4.30 (2.31), 4.23 (1.98).
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper</filename><data>
            <paper_id>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300826</doi>
            <title>360proto: Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper</title>
            <abstract>
                We explore 360 paper prototyping to rapidly create AR/VR prototypes from paper and bring them to life on AR/VR devices. Our approach is based on a set of emerging paper prototyping templates specifically for AR/VR. These templates resemble the key components of many AR/VR interfaces, including 2D representations of immersive environments, AR marker overlays and face masks, VR controller models and menus, and 2D screens and HUDs. To make prototyping with these templates effective, we developed 360proto, a suite of three novel physical--digital prototyping tools: (1) the 360proto Camera for capturing paper mockups of all components simply by taking a photo with a smartphone and seeing 360-degree panoramic previews on the phone or stereoscopic previews in Google Cardboard; (2) the 360proto Studio for organizing and editing captures, for composing AR/VR interfaces by layering the captures, and for making them interactive with Wizard of Oz via live video streaming; (3) the 360proto App for running and testing the interactive prototypes on AR/VR capable mobile devices and headsets. Through five student design jams with a total of 86 participants and our own design space explorations, we demonstrate that our approach with 360proto is useful to create relatively complex AR/VR applications.
            </abstract>
            <sections>
                <section>
                    <word_count>365</word_count>
                    <figure_citations>Figure 1 illustrates our method analogous to Rettig [32].</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>621</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>3689</word_count>
                    <figure_citations>Figure 1 the running example in this section.Figure 2).Figure 3).Figure 5).Figure 6): (1) the View pane shows a simulated or live preview of the AR/VR prototype running on a smartphone (live if used in combination with the 360proto App); (2) the Collect pane shows thumbnails of the live video feed of the Camera tool, previous captures of paper mockups, and PNG images imported from the file system; the Layers pane shows thumbnails of the 2D, 360, and AR/VR specific layers described below; the Capture pane shows a larger preview of content selected in the Collect pane and provides tools for chroma keying to make pixels of a selected color with a specified tolerance level made transparent in layers.Figure 7 (left) shows how our designer is now starting to add content and compose the butterfly scene from layers of paper mockups.Figure 7 (right) shows a 360 preview of the scene using spheres to render the 360 layers.Figure 8 shows our designer using the AR marker mode in the Camera tool to superimpose an image of a butterfly over a kanji marker attached to a stick made from paper.</figure_citations>
                    <section_index>2</section_index>
                    <title>INITIAL DESIGN JAMS</title>
                </section>
                <section>
                    <word_count>1185</word_count>
                    <figure_citations>Figure 10).Figure 11).</figure_citations>
                    <section_index>3</section_index>
                    <title>AR USER</title>
                </section>
                <section>
                    <word_count>2327</word_count>
                    <figure_citations>Figure 13 shows 29 students’ ratings for three questions.Figure 14).Figure 15).</figure_citations>
                    <section_index>4</section_index>
                    <title>FINAL DESIGN JAMS</title>
                </section>
                <section>
                    <word_count>304</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>141</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>866</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>AR/VR</li>
                <li>Physical–digital prototyping</li>
                <li>Wizard of Oz</li>
            </keywords>
            <authors>
                <li>Michael Nebeling</li>
                <li> Katy Madier</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_1.jpg</url>
                <caption>Figure 1: Inspired by Rettig [32], 360-degree paper proto-
                    typing involves participants in different roles: User tests an
                    AR/VR paper mockup of an animated butterfly scene; Facili-
                    tator streams the butterfly cut-out arranged on a 360◦ paper
                    template, while “Computer” moves the butterfly along the
                    360◦ grid; Observer records User’s behavior and feedback.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_2.jpg</url>
                <caption>Figure 2: Initial Daydream, Pokémon GO paper prototypes
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_3.jpg</url>
                <caption>Figure 3: 360 paper prototype versions viewed in Cardboard
                    took three hours with two hours focused on paper proto-
                    typing in groups. At the end of each round, groups were
                    asked to demonstrate the three required interactions with
                    their prototypes, which we recorded from the perspective
                    of the user. We concluded with a focus group discussion to
                    have participants reflect on the experience as well as exit
                    questionnaires to gather feedback in terms of two pros and
                    two cons comparing plain and 360 paper prototyping.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_4.jpg</url>
                <caption>Figure 4: 360 grid (red), range of motion (green), FOV (blue)
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_5.jpg</url>
                <caption>Figure 5: Camera tool creates static captures and live streams
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_6.jpg</url>
                <caption>Figure 6: 360proto Studio’s View pane (top left) shows a simulated or live preview of the 360 paper prototype; Collect pane
                    (bottom left) shows the live feed from the connected Camera tool and contains previous captures (here, three captures of
                    mountains, trees, and butterflies, a dog and a car drawn using the 360 paper template); Capture pane (bottom right) shows a
                    selected capture and provides color filters for chroma keying; Layers pane (top right) shows 2D, 360, and AR/VR specific layers
                    (here, only 360 layers are used to compose a basic butterfly scene from the running example).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_7.jpg</url>
                <caption>Figure 7: Butterfly scene composed from three 360 layers
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_8.jpg</url>
                <caption>Figure 8: AR marker to move virtual butterfly in AR view
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_9.jpg</url>
                <caption>Figure 9: AR mode with the user looking at a virtual butter-
                    fly from inside (left) and outside (right) the 360 video sphere
                    that is anchored at the starting physical location
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>10</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_10.jpg</url>
                <caption>Figure 10: Architecture of our suite of 360proto tools
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>11</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_11.jpg</url>
                <caption>Figure 11: Star Trek: two connected rooms made from paper
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>12</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_12.jpg</url>
                <caption>Figure 12: Androids on paper appear in the environment
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>13</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_13.jpg</url>
                <caption>Figure 13: Distribution of 29 responses for Q1–Q3
                </caption>
                <page>10</page>
            </figure>
            <figure>
                <id>14</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_14.jpg</url>
                <caption>Figure 14: Racquetball game (left), WOz version (right)
                </caption>
                <page>11</page>
            </figure>
            <figure>
                <id>15</id>
                <url>360proto Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper_crop_15.jpg</url>
                <caption>Figure 15: WOz version of Amazon Shopping’s AR view
                </caption>
                <page>11</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation</filename>
        <data>
            <paper_id>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300905</doi>
            <title>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation</title>
            <abstract>
                We present a small and lightweight wearable device that enhances virtual reality experiences and reduces cybersickness by means of galvanic vestibular stimulation (GVS). GVS is a specific way to elicit vestibular reflexes that has been used for over a century to study the function of the vestibular system. In addition to GVS, we support physiological sensing by connecting heart rate, electrodermal activity and other sensors to our wearable device using a plug and play mechanism. An accompanying Android app communicates with the device over Bluetooth (BLE) for transmitting the GVS stimulus to the user through electrodes attached behind the ears. Our system supports multiple categories of virtual reality applications with different types of virtual motion such as driving, navigating by flying, teleporting, or riding. We present a user study in which participants (N = 20) experienced significantly lower cybersickness when using our device and rated experiences with GVS-induced haptic feedback as significantly more immersive than a no-GVS baseline.
            </abstract>
            <sections>
                <section>
                    <word_count>1250</word_count>
                    <figure_citations>Figure 4,6) that can help reduce the initial time and effort and enable researchers to quickly focus on the design of novel applications and interaction techniques.Figure 5).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>958</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>380</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>CYBERSICKNESS VS SIMULATOR SICKNESS VS</title>
                </section>
                <section>
                    <word_count>2087</word_count>
                    <figure_citations>
                        Figure 1 shows four example application scenarios for GVS induced feedback.Figure 2) and tested it with six participants (5 females, average age 22.Figure 2).Figure 3 shows the second GVS device we created after receiving feedback from the pilot study with the first prototype (Figure 2).Figure 4).Figure 4).Figure 5) as zero-mean current noise of an imperceptible magnitude.Figure 5 shows the mastoid, which is the back part of the temporal bones situated at the sides and base of the skull.Figure 6 shows the opened up neckband with the PCB and the connected GVS electrodes, EDA electrodes and the HR sensor.Figure 6 shows EDA and HR sensors connected to the PCB through the two bands of black and white wires.</figure_citations>
                    <section_index>3</section_index>
                    <title>GVS FOR VR</title>
                </section>
                <section>
                    <word_count>1141</word_count>
                    <figure_citations>Figure 7).</figure_citations>
                    <section_index>4</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>1491</word_count>
                    <figure_citations>Figure 8 shows the boxplot of 20 participants’ scores for both trials.Figure 9 shows the results of Flow in the two conditions.Figure 11).Figure 12).</figure_citations>
                    <section_index>5</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>777</word_count>
                    <figure_citations>Figure 8).</figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>100</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>LIMITATIONS AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>136</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>2023</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Galvanic Vestibular Stimulation</li>
                <li>Wearables</li>
                <li>Interaction design</li>
                <li>Haptic feedback</li>
                <li>Virtual Reality</li>
                <li>Cybersickness</li>
            </keywords>
            <authors>
                <li>Misha Sra</li>
                <li> Abhinandan Jain</li>
                <li> Pattie Maes</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_1.jpg</url>
                <caption>Figure 1: Our GVS device induces proprioceptive feedback
                    for VR motions: (a) riding a roller coaster, (b) driving a car,
                    (c) leaping forward, and (d) navigating by flying.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_2.jpg</url>
                <caption>Figure 2: The first GVS prototype built using a Light Blue
                    Bean Board with a custom current driver circuit board.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_3.jpg</url>
                <caption>Figure 3: The second and final GVS prototype in the form of
                    a small wearable device that sits on the user’s neck, similar
                    to behind-the-head earphones. The red wire connects the cir-
                    cuit to an electrode behind the user’s ear. A similar wire con-
                    nects to a second electrode behind the other ear. The white
                    wires connect to EDA and HR sensors.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_4.jpg</url>
                <caption>Figure 4: GVS prototype with the 3D printed housing, PCB,
                    LiPo battery, sensor ports, and the electrode connectors.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_5.jpg</url>
                <caption>Figure 5: We attach electrodes to the mastoid process on the
                    temporal bone behind each ear to electrically stimulate the
                    vestibular system. Image courtesy: Wikipedia
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_6.jpg</url>
                <caption>Figure 6: Our GVS device without the 3D printed housing.
                    The image shows the EDA and HR sensors plugged into the
                    PCB using the onboard connection ports.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_7.jpg</url>
                <caption>Figure 7: VR scenes for the with-GVS and without-GVS trials.
                    (a) Full visibility, breezy sunny day. (b) Full visibility, over-
                    cast day with occasional thunder in the distance.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_8.jpg</url>
                <caption>Figure 8: Participants had significantly higher presence
                    when using the GVS device than without.
                </caption>
                <page>10</page>
            </figure>
            <figure>
                <id>9</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_9.jpg</url>
                <caption>Figure 9: A Pairwise Wilcoxon test shows Flow was signifi-
                    cantly higher in VR with the GVS device.
                </caption>
                <page>10</page>
            </figure>
            <figure>
                <id>10</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_10.jpg</url>
                <caption>Figure 10: Positive Affect was significantly higher and there
                    was no significant difference in the Negative Affect between
                    the with-GVS and without-GVS conditions.
                </caption>
                <page>10</page>
            </figure>
            <figure>
                <id>11</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_11.jpg</url>
                <caption>Figure 11: Perceived realism of experience is higher as indi-
                    cated by responses to related questions in the SUS inventory
                    for with-GVS and without-GVS conditions.
                </caption>
                <page>11</page>
            </figure>
            <figure>
                <id>12</id>
                <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_12.jpg</url>
                <caption>Figure 12: Results show that 85% of the users preferred the
                    VR experience with the GVS device.
                </caption>
                <page>12</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality</filename><data>
            <paper_id>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300628</doi>
            <title>Affinity Lens: Data-Assisted Affinity Diagramming with Augmented Reality</title>
            <abstract>Despite the availability of software to support Affinity Diagramming (AD), practitioners still largely favor physical sticky-notes. Physical notes are easy to set-up, can be moved around in space and offer flexibility when clustering un-structured data. However, when working with mixed data sources such as surveys, designers often trade off the physicality of notes for analytical power. We propose AffinityLens, a mobile-based augmented reality (AR) application for Data-Assisted Affinity Diagramming (DAAD). Our application provides just-in-time quantitative insights overlaid on physical notes. Affinity Lens uses several different types of AR overlays (called lenses) to help users find specific notes, cluster information, and summarize insights from clusters. Through a formative study of AD users, we developed design principles for data-assisted AD and an initial collection of lenses. Based on our prototype, we find that Affinity Lens supports easy switching between qualitative and quantitative 'views' of data, without surrendering the lightweight benefits of existing AD practice.</abstract>
            <sections>
                <section>
                    <word_count>674</word_count>
                    <figure_citations>Figure 1a).Figure 1b), the designer can use the phone to look at distributions of sleeping schedules for each cluster (Figure 1c).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1029</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1070</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>A DESIGN PROBE FOR DAAD</title>
                </section>
                <section>
                    <word_count>475</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>DESIGN GUIDELINES</title>
                </section>
                <section>
                    <word_count>547</word_count>
                    <figure_citations>Figure 2 captures the four main regions of the mobile interface: the largest, is dedicated to the camera and visualization augmentation (a), a contextual menu occupies the right edge of the display (b) and dynamically changes depending on what is present in the camera’s field of view, a data attribute Paper 398 CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK a b d cook eatout housing emp grade exercise c Figure 2: Affinity Lens User Interface.Figure 3) we follow a designer, Dave, as he uses DAAD to analyze the food choice dataset (this example is based on a combination of real use cases from our user studies).Figure 3a).Figure 3b).Figure 4 a).</figure_citations>
                    <section_index>4</section_index>
                    <title>USER EXPERIENCE</title>
                </section>
                <section>
                    <word_count>890</word_count>
                    <figure_citations>Figure 3 d) so he can continue working without pointing at the physical notes (D2, D3).Figure 3 e) and alerts him that all but one student in the on-campus sub-cluster are first years (D4).Figure 4 b).Figure 4 f).Figure 4) to support common tasks.</figure_citations>
                    <section_index>5</section_index>
                    <title>PRINT</title>
                </section>
                <section>
                    <word_count>2</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>STILL IMAGE</title>
                </section>
                <section>
                    <word_count>1140</word_count>
                    <figure_citations>Figure 1a).Figure 4i).Figure 4c) displays those data values that are the same and those that are different (a weak representation of affinity).Figure 4d) will generate an overlay of common words (sized by frequency) on top of the notes.Figure 4g).Figure 4h).</figure_citations>
                    <section_index>7</section_index>
                    <title>LIVE</title>
                </section>
                <section>
                    <word_count>979</word_count>
                    <figure_citations>Figure 5, Affinity Lens is comprised of five main components: (1) Scene Analyzer, (2) Lens Controller, (3) Dynamic View Configurator, (4) lenses, and (5) the Data Access and Analytics Module.</figure_citations>
                    <section_index>8</section_index>
                    <title>SYSTEM ARCHITECTURE</title>
                </section>
                <section>
                    <word_count>1834</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>462</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>DISCUSSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>130</word_count>
                    <figure_citations></figure_citations>
                    <section_index>11</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>475</word_count>
                    <figure_citations></figure_citations>
                    <section_index>12</section_index>
                    <title>REFERENCES</title>
                </section>
                <section>
                    <word_count>1068</word_count>
                    <figure_citations></figure_citations>
                    <section_index>13</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
            </sections>
            <keywords>
                <li>Affinity diagrams</li>
                <li>Visual analytics</li>
                <li>Augmented reality</li>
            </keywords>
            <authors>
                <li>Hariharan Subramonyam</li>
                <li> Steven M. Drucker</li>
                <li> Eytan Adar</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality_crop_1.jpg</url>
                <caption>Figure 1: Affinity Lens used to split a larger affinity cluster based on income level. (a) The user applies a heatmap lens to an
                    existing cluster which shows two sub-groups. (b) The designer regroups the notes. (c) A histogram lens compares sleeping
                    schedules for the two sub-clusters found in (a).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality_crop_2.jpg</url>
                <caption>Figure 2: Affinity Lens User Interface. (a) main camera view,
                    (b) contextual lens selector, (c) lens configuration options, (d)
                    lens modes
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality_crop_3.jpg</url>
                <caption>Figure 3: Affinity Lens workflow. Data is acquired (a) and automatically tagged for a Marker (b) for printing. Various forms of
                    DAAD (c, d, e) can be documented (f) along with associated insights (g).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality_crop_4.jpg</url>
                <caption>Figure 4: A sampling of Affinity Lens AR Lenses
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality_crop_5.jpg</url>
                <caption>Figure 5: System Architecture. (1) Scene analyzer extracts notes from camera feed, (2) lens controller determines set of lenses
                    applicable to notes in view, (3) dynamic view configurator updates the interface with available lenses, (4) lens queries for data
                    from the (5) Data access and analytics module, and renders the augmented visualization.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions</filename>
        <data>
            <paper_id>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025723</doi>
            <title>Ambiotherm: Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions</title>
            <abstract>In this paper, we present and evaluate Ambiotherm, a wearable accessory for Head Mounted Displays (HMD) that provides thermal and wind stimuli to simulate real-world environmental conditions, such as ambient temperatures and wind conditions, to enhance the sense of presence in Virtual Reality (VR). Ambiotherm consists of a Ambient Temperature Module that is attached to the user's neck, a Wind Simulation Module focused towards the user's face, and a Control Module utilizing Bluetooth communication. We demonstrate Ambiotherm with two VR environments, a hot desert, and a snowy mountain, to showcase the different types of simulated environmental conditions. We conduct several studies to 1) address design factors of the system and 2) evaluate Ambiotherm's effect on factors related to a user's sense of presence. Our findings show that the addition of wind and thermal stimuli significantly improves sensory and realism factors, contributing towards an enhanced sense of presence when compared to traditional VR experiences.</abstract>
            <sections>
                <section>
                    <word_count>675</word_count>
                    <figure_citations>Figure 1, the system is equipped with 1) an Ambient Temperature Simulation Module that utilizes Peltier elements to provide heating and cooling sensations (worn on the neck) and 2) a Wind Simulation Module that utilizes multidirectional fans focused towards the user’s face.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1326</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>BACKGROUND AND RELATED WORK</title>
                </section>
                <section>
                    <word_count>2078</word_count>
                    <figure_citations>Figure 2, were chosen for the study due to their proximity to the thermoregulatory centre of the central nervous system [15].Figure 3 (A)).Figure 3 (B)).Figure 4 (A): sensation scores).Figure 4 (B): sensation scores).Figure 4 (A): comfort scores).Figure 4 (B): comfort scores).</figure_citations>
                    <section_index>2</section_index>
                    <title>STIMULI DESIGN CONSIDERATIONS</title>
                </section>
                <section>
                    <word_count>383</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>SYSTEM IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>2972</word_count>
                    <figure_citations>Figure 7 (A)) and a snowy mountain environment (Figure 7 (B)) for 30 seconds each, presented in a randomized order.Figure 10).</figure_citations>
                    <section_index>4</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>761</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>146</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>34</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1106</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Presence</li>
                <li>Ambient Temperature</li>
                <li>Virtual Wind</li>
                <li>Virtual Reality</li>
                <li>Multimodal Interaction</li>
            </keywords>
            <authors>
                <li>Nimesha Ranasinghe</li>
                <li> Pravar Jain</li>
                <li> Shienny Karwita</li>
                <li> David Tolley</li>
                <li> Ellen Yi-Luen Do</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_1.jpg</url>
                <caption>Figure 1. A participant is using the Ambiotherm system with a Sam-
                    sung™ Gear VR HMD. The control module is attached behind the neck.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_2.jpg</url>
                <caption>Figure 2. Different locations selected for delivering thermal stimuli: (A)
                    Behind The Ear, (B) On The Throat, and (C) Behind The Neck.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_3.jpg</url>
                <caption>Figure 3. Normalized average rating scores: (A) heating sensation at
                    three locations and (B) cooling sensation at three locations. (Error bars
                    represent 95% CI, n = 15).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_4.jpg</url>
                <caption>Figure 4. Normalized average sensation and comfort scores: (A) four
                    heating stimuli and (B) four cooling stimuli. (Error bars represent 95%
                    CI, n = 15).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_5.jpg</url>
                <caption>Figure 5. Operational procedure of Ambiotherm: (1) VR environment
                    (Android Application) sends commands to (2) control module that ac-
                    tives (3a) thermal module and (3b) wind module.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_6.jpg</url>
                <caption>Figure 6. Ambiotherm: (A) Main components, (B) Control Module, (C)
                    Wind Simulation Module, and (D) Ambient Temperature Module.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_7.jpg</url>
                <caption>Figure 7. User’s ﬁrst-person view following the virtual guide in (A) the
                    hot desert and (B) the snowy mountain virtual environments.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_8.jpg</url>
                <caption>Figure 8. Normalized average Sensory and Realism Factors scores for
                    four stimuli conﬁgurations (Error bars represent 95% CI, n = 20).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_9.jpg</url>
                <caption>Figure 9. Normalized average engagement scores for four stimuli conﬁg-
                    urations (Error bars represent 95% CI, n = 20).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_10.jpg</url>
                <caption>Figure 10. Normalized average thermal, wind, and overall Involvement
                    scores for four stimuli conﬁgurations (Error bars represent 95% CI, n =
                    20).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>11</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_11.jpg</url>
                <caption>Figure 11. Normalized average consistency scores of VR Environment
                    compared to Real-World and consistency scores for sensory information
                    for four stimuli conﬁgurations (Error bars represent 95% CI, n = 20).
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>12</id>
                <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_12.jpg</url>
                <caption>Figure 12. Normalized average scores assessing the quickness to adjust
                    to the VR Environment with four stimuli conﬁgurations (Error bars rep-
                    resent 95% CI, n = 20).
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories</filename><data>
            <paper_id>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300377</doi>
            <title>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories</title>
            <abstract>Room-scale Virtual Reality (VR) systems have arrived in users' homes where tracked environments are set up in limited physical spaces. As most Virtual Environments (VEs) are larger than the tracked physical space, locomotion techniques are used to navigate in VEs. Currently, in recent VR games, point &amp; teleport is the most popular locomotion technique. However, it only allows users to select the position of the teleportation and not the orientation that the user is facing after the teleport. This results in users having to manually correct their orientation after teleporting and possibly getting entangled by the cable of the headset. In this paper, we introduce and evaluate three different point &amp; teleport techniques that enable users to specify the target orientation while teleporting. The results show that, although the three teleportation techniques with orientation indication increase the average teleportation time, they lead to a decreased need for correcting the orientation after teleportation.</abstract>
            <sections>
                <section>
                    <word_count>410</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>2227</word_count>
                    <figure_citations>Figure 2).Figure 3).Figure 4).Figure 5).Figure 6).</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>2822</word_count>
                    <figure_citations>Figure 8).</figure_citations>
                    <section_index>2</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>959</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>190</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>LIMITATIONS</title>
                </section>
                <section>
                    <word_count>257</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>18</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1672</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Locomotion</li>
                <li>Teleportation</li>
                <li>Orientation Indication</li>
                <li>Virtual Environments</li>
                <li>Point &amp; Teleport</li>
            </keywords>
            <authors>
                <li>Markus Funk</li>
                <li> Florian Müller</li>
                <li> Marco Fendrich</li>
                <li> Megan Shene</li>
                <li> Moritz Kolvenbach</li>
                <li> Niclas Dobbertin</li>
                <li> Sebastian Günther</li>
                <li> Max Mühlhäuser</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_1.jpg</url>
                <caption>Figure 1: A user is teleporting herself in a Virtual Environment using the Curved Teleport. It allows her to teleport around an
                    obstacle and graphically choose the orientation, which she wants to face after teleportation only by using the curved trajectory
                    visualization with orientation indication, and without having to turn her body in the physical world.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_2.jpg</url>
                <caption>Figure 2: A user is using the Linear Teleport to move onto a
                    target. The orientation that the user is facing is the forward
                    vector of the straight teleport line.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_3.jpg</url>
                <caption>Figure 3: The Parabola Teleport uses a parabola shaped visu-
                    alization to indicate the target position of the teleport. After
                    the teleport, users face the forward vector of the teleport.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_4.jpg</url>
                <caption>Figure 4: The AngleSelect Teleport uses a parabola visualiza-
                    tion to show the user the target position. Further it uses an
                    orientation indicator that lets the user select the orientation
                    that the user is facing after the teleport.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_5.jpg</url>
                <caption>Figure 5: The Curved Teleport visualizes the trajectory of the
                    teleportation in a curved line. The orientation that the user
                    is facing at the target location can be infuenced by adjusting
                    the steepness of the curve.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_6.jpg</url>
                <caption>Figure 6: The visualization of the HPCurved Teleport frst
                    uses a parabola that behaves like a state-of-the-art Parabola
                    Teleport, but at the high point of the parabola turns into a
                    Curved Teleport that lets the users chose the target orienta-
                    tion.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_7.jpg</url>
                <caption>Figure 7: We used round targets in our accuracy study. The
                    direction of the target is indicated by the green target area
                    and supported by the yellow target area. The rest of the tar-
                    get is colored in red indicating the wrong direction. In this
                    screenshot of our accuracy study, the participant is using the
                    Parabola Teleport. (Picture of participant added for clarity).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_8.jpg</url>
                <caption>Figure 8: The VE that we created as the environment for con-
                    ducting the study. The castle in the middle of the environ-
                    ment is the tutorial area, where participants can practice the
                    teleportation method before starting the study. Once partic-
                    ipants leave the tutorial castle, it disappears and becomes a
                    green lawn area.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>9</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_9.jpg</url>
                <caption>Figure 9: The average angle αcor r ect ed that was corrected by
                    the participants using each teleportation method. All error
                    bars indicate the standard error. All conditions are signif-
                    icantly diferent, except the two baseline conditions (indi-
                    cated with n.s.).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>10</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_10.jpg</url>
                <caption>Figure 10: The average time that participants took to tele-
                    port tt el epor t using each teleportation method. All error bars
                    indicate the standard error. All teleportation methods were
                    signifcantly diferent, except the ones indicated with n.s.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>11</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_11.jpg</url>
                <caption>Figure 11: The average time that participants took to correct
                    their position tcor r ect ed using natural walking for each tele-
                    portation method. All error bars indicate the standard error.
                    The asterisk (*) indicates a statistically signifcant diference
                    between the teleportation methods.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>12</id>
                <url>Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_12.jpg</url>
                <caption>Figure 12: The NASA-TLX scores of the VR locomotion tech-
                    niques that were assessed in the user study. All error bars
                    indicate the standard error. The asterisk (*) indicates a sig-
                    nifcant diference between the locomotion techniques.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Augmented Reality Views for Occluded Interaction</filename>
        <data>
            <paper_id>Augmented Reality Views for Occluded Interaction</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300676</doi>
            <title>Augmented Reality Views for Occluded Interaction</title>
            <abstract>
                We rely on our sight when manipulating objects. When objects are occluded, manipulation becomes difficult. Such occluded objects can be shown via augmented reality to re-enable visual guidance. However, it is unclear how to do so to best support object manipulation. We compare four views of occluded objects and their effect on performance and satisfaction across a set of everyday manipulation tasks of varying complexity. The best performing views were a see-through view and a displaced 3D view. The former enabled participants to observe the manipulated object through the occluder, while the latter showed the 3D view of the manipulated object offset from the object's real location. The worst performing view showed remote imagery from a simulated hand-mounted camera. Our results suggest that alignment of virtual objects with their real-world location is less important than an appropriate point-of-view and view stability.
            </abstract>
            <sections>
                <section>
                    <word_count>429</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>983</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>426</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>TYPES OF OCCLUDED INTERACTION</title>
                </section>
                <section>
                    <word_count>774</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>3</section_index>
                    <title>VISUALIZING OCCLUDED OBJECTS</title>
                </section>
                <section>
                    <word_count>1733</word_count>
                    <figure_citations>Figure 2) were as follows: Pressing a Light Switch: Required either pressing (changing the state of) the left button, right button or both of the buttons on a two-button light switch.Figure 2).Figure 3a).Figure 3b).Figure 3c), to the left of the participants.Figure 3a).Figure 4).</figure_citations>
                    <section_index>4</section_index>
                    <title>EVALUATING OCCLUDED INTERACTION VIEWS</title>
                </section>
                <section>
                    <word_count>2314</word_count>
                    <figure_citations>Figure 5, the manipulation delay differed between the views.Figure 5 also shows how this delay differed depending on the task.Figure 6).Figure 6).Figure 7 shows how the view influenced the two kinds of manipulation Paper 446 Dynamic camera Cloned 3D See-through −0.Figure 7 also shows the interactions between task and error.Figure 8).Figure 8).</figure_citations>
                    <section_index>5</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>869</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>107</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>25</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1343</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Augmented reality</li>
                <li>Manipulation task</li>
                <li>Finger-camera</li>
            </keywords>
            <authors>
                <li>Klemen Lilija</li>
                <li> Henning Pohl</li>
                <li> Sebastian Boring</li>
                <li> Kasper Hornbæk</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Augmented Reality Views for Occluded Interaction_crop_1.jpg</url>
                <caption>Figure 1: In some situations, we need to manipulate objects out of our sight. We investigate how different views of occluded
                    objects support users during manipulation tasks. An example of such a task is plugging in an HDMI cable. While the port is
                    normally out of sight, see-through view (middle) and displaced 3D view (right) enable visual feedback during interactions.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Augmented Reality Views for Occluded Interaction_crop_2.jpg</url>
                <caption>Figure 2: During the study participants performed five different tasks (left to right): pressing one of two light switches, rotating
                    a dial, dragging a slider, plugging an HDMI stick into one of four ports, and placing a key fob onto one of three hooks.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Augmented Reality Views for Occluded Interaction_crop_3.jpg</url>
                <caption>Figure 3: Four of the views used in the experiment: A) Static
                    camera, b) Dynamic camera, c) Cloned 3D and d) See-through
                    view. No visualization view is not shown, as it does not render
                    anything in user’s field of view.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Augmented Reality Views for Occluded Interaction_crop_4.jpg</url>
                <caption>Figure 4: During the study, participants were seated in front
                    of a 1 m3 frame. They reached in from the right side and
                    interacted with objects placed on a wall, that was tilted at a
                    30 ° angle. Movement inside the frame was tracked with an
                    Optitrack setup and participants received visual feedback in
                    a HoloLens headset.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Augmented Reality Views for Occluded Interaction_crop_5.jpg</url>
                <caption>Figure 5: Delay till start of manipulation per view (top) and
                    per view and object (bottom). Error bars show bootstrapped
                    95 % confidence intervals.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Augmented Reality Views for Occluded Interaction_crop_6.jpg</url>
                <caption>Figure 6: Duration of manipulation per view (top) and per
                    view and object (bottom). Error bars show bootstrapped 95 %
                    confidence intervals.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Augmented Reality Views for Occluded Interaction_crop_7.jpg</url>
                <caption>Figure 7: Relative error and absolute errors per view (top
                    two) and per view and object (bottom two). Error bars show
                    bootstrapped 95 % confidence intervals.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Augmented Reality Views for Occluded Interaction_crop_8.jpg</url>
                <caption>Figure 8: At the end of the study, participants provided an
                    overall rating of each view, indicating on a 7-point Likert
                    scale how well the view supported them in the tasks. Shown
                    here are the number of ratings for each level of the scale,
                    stacked horizontally to highlight overall trends.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences</filename>
        <data>
            <paper_id>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300736</doi>
            <title>Behind the Curtain of the "Ultimate Empathy Machine": On the Composition of Virtual Reality Nonfiction Experiences</title>
            <abstract>
                Virtual Reality nonfiction (VRNF) is an emerging form of immersive media experience created for consumption using panoramic "Virtual Reality" headsets. VRNF promises nonfiction content producers the potential to create new ways for audiences to experience "the real"; allowing viewers to transition from passive spectators to active participants. Our current project is exploring VRNF through a series of ethnographic and experimental studies. In order to document the content available, we embarked on an analysis of VR documentaries produced to date. In this paper, we present an analysis of a representative sample of 150 VRNF titles released between 2012-2018. We identify and quantify 64 characteristics of the medium over this period, discuss how producers are exploiting the affordances of VR, and shed light on new audience roles. Our findings provide insight into the current state of the art in VRNF and provide a digital resource for other researchers in this area.
            </abstract>
            <sections>
                <section>
                    <word_count>1651</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>464</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>741</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>THE CHARACTERISTICS OF VRNF</title>
                </section>
                <section>
                    <word_count>849</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>METHOD</title>
                </section>
                <section>
                    <word_count>2541</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1329</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>178</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>251</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>LIMITATIONS AND FURTHER WORK</title>
                </section>
                <section>
                    <word_count>37</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1107</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Nonfiction</li>
                <li>Immersive media</li>
                <li>Interaction</li>
            </keywords>
            <authors>
                <li>Chris Bevan</li>
                <li> David Philip Green</li>
                <li> Harry Farmer</li>
                <li> Mandy Rose</li>
                <li> Kirsten Cater</li>
                <li> Danaë Stanton Fraser</li>
                <li> Helen Brown</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences_crop_1.jpg</url>
                <caption>Figure 1: Hunger In Los Angeles (2012). Viewer on right is us-
                    ing prototypical VR hardware that would later become the
                    Oculus Rift.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences_crop_2.jpg</url>
                <caption>Figure 2: Characteristics of VRNF titles (n=150) by frequency of occurrence.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences_crop_3.jpg</url>
                <caption>Figure 3: Diegetic visual annotations. 6x9: A Virtual Experi-
                    ence of Solitary Confinement, 2016.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences_crop_4.jpg</url>
                <caption>Figure 4: Branching narrative gaze selection. Decisions:
                    Party’s Over (2018)
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality</filename>
        <data>
            <paper_id>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300589</doi>
            <title>Beyond The Force: Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality</title>
            <abstract>
                Quadcopters have been used as hovering encountered-type haptic devices in virtual reality. We suggest that quadcopters can facilitate rich haptic interactions beyond force feedback by appropriating physical objects and the environment. We present HoverHaptics, an autonomous safe-to-touch quadcopter and its integration with a virtual shopping experience. HoverHaptics highlights three affordances of quadcopters that enable these rich haptic interactions: (1) dynamic positioning of passive haptics, (2) texture mapping, and (3) animating passive props. We identify inherent challenges of hovering encountered-type haptic devices, such as their limited speed, inadequate control accuracy, and safety concerns. We then detail our approach for tackling these challenges, including the use of display techniques, visuo-haptic illusions, and collision avoidance. We conclude by describing a preliminary study (n = 9) to better understand the subjective user experience when interacting with a quadcopter in virtual reality using these techniques.
            </abstract>
            <sections>
                <section>
                    <word_count>730</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>3992</word_count>
                    <figure_citations>Figure 2a) and 1.Figure 2b).Figure 2c).Figure 3a).Figure 3b).Figure 3c).Figure 3b.Figure 5b.</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>529</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>TECHNICAL EVALUATION</title>
                </section>
                <section>
                    <word_count>2598</word_count>
                    <figure_citations>Figure 10, consisted of a safe-to-touch quadcopter, Vicon motion capture cameras, two HTC Vive lighthouses, and the HTC Vive Head-Mounted Display (HMD).Figure 11).Figure 13, and placed it in their shopping basket.Figure 14a.Figure 14b.</figure_citations>
                    <section_index>3</section_index>
                    <title>USER EVALUATION</title>
                </section>
                <section>
                    <word_count>89</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>39</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1689</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Quadcopter</li>
                <li>Drone</li>
                <li>UAV</li>
                <li>Encountered-Type</li>
                <li>Human-Drone Interaction</li>
                <li>Robotic Graphics</li>
                <li>Haptics</li>
                <li>Virtual Reality</li>
            </keywords>
            <authors>
                <li>Parastoo Abtahi</li>
                <li> Benoit Landry</li>
                <li> Jackie (Junrui) Yang</li>
                <li> Marco Pavone</li>
                <li> Sean Follmer</li>
                <li> James A. Landay</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_1.jpg</url>
                <caption>Figure 1: Haptic interactions using a quadcopter. Left: user touching fabrics attached to the quad for texture rendering. Middle:
                    user picking up a physical hanger attached to the quad. Right: user picking up the turned off quad as a passive haptic device.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_2.jpg</url>
                <caption>Figure 2: Affordances of quadcopters as haptic devices in VR,
                    focused on force feedback: (a) weight simulation (b) surface
                    stiffness simulation (c) lateral force feedback.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_3.jpg</url>
                <caption>Figure 3: We present three techniques for appropriating ob-
                    jects and the environment: (a) dynamic positioning of pas-
                    sive haptics (b) texture mapping (c) animating passive props.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_4.jpg</url>
                <caption>Figure 4: Based on the prediction algorithm, the closest ren-
                    derable patch is shown to the user.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_5.jpg</url>
                <caption>Figure 5: Visuo-haptic illusions are used to compensate for
                    lack of control accuracy. (a) Quad is accurately positioned
                    and no illusion is needed. (b) To correct for the position off-
                    set, as the virtual hand touches the virtual target, body warp-
                    ing is used to retarget the hand towards the quad.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_6.jpg</url>
                <caption>Figure 6: The emergency scene: the box in the center rep-
                    resents the current position of the quadcopter and warning
                    signs pointing towards the quad are placed around the user.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_7.jpg</url>
                <caption>Figure 7: Upon the quad’s arrival the patch is highlighted in
                    green, indicating that the user can touch the virtual object.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_8.jpg</url>
                <caption>Figure 8: Safe-to-touch quadcopter with the protective cage.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_9.jpg</url>
                <caption>Figure 9: Virtual boutique scene.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_10.jpg</url>
                <caption>Figure 10: Experimental Setup.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>11</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_11.jpg</url>
                <caption>Figure 11: Texture rendering: user feeling the material of a
                    scarf and a shirt.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>12</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_12.jpg</url>
                <caption>Figure 12: Animating passive props: user picking up a
                    hanger and dropping it in the shopping basket.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>13</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_13.jpg</url>
                <caption>Figure 13: Dynamic positioning of passive haptics: quad
                    landing on a table in the room to render a shoebox, and user
                    picking up the shoebox.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>14</id>
                <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_14.jpg</url>
                <caption>Figure 14: Affordances of multiple quads for haptics in VR:
                    (a) complex geometry and (b) continuous surface rendering.
                </caption>
                <page>11</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Can Mobile Augmented Reality Stimulate a Honeypot</filename>
        <data>
            <paper_id>Can Mobile Augmented Reality Stimulate a Honeypot</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300515</doi>
            <title>Can Mobile Augmented Reality Stimulate a Honeypot Effect?: Observations from Santa's Lil Helper</title>
            <abstract>
                In HCI, the honeypot effect describes a form of audience engagement in which a person's interaction with a technology stimulates passers-by to observe, approach and engage in an interaction themselves. In this paper we explore the potential for honeypot effects to arise in the use of mobile augmented reality (AR) applications in urban spaces. We present an observational study of Santa's Lil Helper, a mobile AR game that created a Christmas-themed treasure hunt in a metropolitan area. Our study supports a consideration of three factors that may impede the honeypot effect: the presence of people in relation to the game and its interactive components; the visibility of gameplay in urban space; and the extent to which the game permits a shared experience. We consider how these factors can inform the design of future AR experiences that are capable of stimulating honeypot effects in public space.
            </abstract>
            <sections>
                <section>
                    <word_count>3224</word_count>
                    <figure_citations>Figure 1a).Figure 1b).Figure 1c).Figure 1d).Figure 1a), which is a bustling shopping area and tourist destination, and Federation Square (location F in Figure 1a), which is a major venue for arts, culture and public events.Figure 1b).Figure 1c).Figure 1c).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>793</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>METHODS</title>
                </section>
                <section>
                    <word_count>3147</word_count>
                    <figure_citations>Figure 1b).Figure 1d).Figure 1c), which required users to point their device at a marker positioned above a large red throne.Figure 2d).</figure_citations>
                    <section_index>2</section_index>
                    <title>FINDINGS</title>
                </section>
                <section>
                    <word_count>1352</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>DISCUSSION AND IMPLICATIONS</title>
                </section>
                <section>
                    <word_count>134</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>22</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>2432</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Audience</li>
                <li>Augmented reality</li>
                <li>Honeypot efect</li>
                <li>Public space</li>
            </keywords>
            <authors>
                <li>Ryan M. Kelly</li>
                <li> Hasan Shahid Ferdous</li>
                <li> Niels Wouters</li>
                <li> Frank Vetere</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Can Mobile Augmented Reality Stimulate a Honeypot_crop_1.jpg</url>
                <caption>Figure 1: (a) The map of locations used by Santa’s Lil Helper;

                    (b) A column location marker; (c) A signpost location marker;


                    (d) A family interacting with the application.


                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Can Mobile Augmented Reality Stimulate a Honeypot_crop_2.jpg</url>
                <caption>Figure 2: (a) AR content in SLH; (b) Users interacting in city

                    space; (c) A marker at Federation Square; (d) Markers ‘blend-

                    ing in’ and being blocked within urban space.

                </caption>
                <page>5</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>CarVR- Enabling In-Car Virtual Reality Entertainment</filename>
        <data>
            <paper_id>CarVR- Enabling In-Car Virtual Reality Entertainment</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025665</doi>
            <title>CarVR: Enabling In-Car Virtual Reality Entertainment</title>
            <abstract>
                Mobile virtual reality (VR) head-mounted displays (HMDs) allow users to experience highly immersive entertainment whilst being in a mobile scenario. Long commute times make casual gaming in public transports and cars a common occupation. However, VR HMDs can currently not be used in moving vehicles since the car's rotation affects the HMD's sensors and simulator sickness occurs when the visual and vestibular system are stimulated with incongruent information. We present CarVR, a solution to enable VR in moving vehicles by subtracting the car's rotation and mapping vehicular movements with the visual information. This allows the user to actually feel correct kinesthetic forces during the VR experience. In a user study (n = 21), we compared CarVR inside a moving vehicle with the baseline of using VR without vehicle movements. We show that the perceived kinesthetic forces caused by CarVR increase enjoyment and immersion significantly while simulator sickness is reduced compared to a stationary VR experience. Finally, we explore the design space of in-car VR entertainment applications using real kinesthetic forces and derive design considerations for practitioners.
            </abstract>
            <sections>
                <section>
                    <word_count>558</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1609</word_count>
                    <figure_citations>Figure 2).</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1410</word_count>
                    <figure_citations>Figure 3).Figure 4).Figure 5).Figure 6).</figure_citations>
                    <section_index>2</section_index>
                    <title>DESIGN SPACE</title>
                </section>
                <section>
                    <word_count>851</word_count>
                    <figure_citations>Figure 7).Figure 8).</figure_citations>
                    <section_index>3</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>3349</word_count>
                    <figure_citations>Figure 9).Figure 10).Figure 11 shows the results of the final comparison regarding general discomfort between both conditions.</figure_citations>
                    <section_index>4</section_index>
                    <title>STUDY</title>
                </section>
                <section>
                    <word_count>193</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>39</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENT</title>
                </section>
                <section>
                    <word_count>1102</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Force-feedback</li>
                <li>Motion platform</li>
                <li>Immersion</li>
                <li>Virtual reality</li>
                <li>Automotive</li>
                <li>Entertainment</li>
                <li>Gaming</li>
            </keywords>
            <authors>
                <li>Philipp Hock</li>
                <li> Sebastian Benedikter</li>
                <li> Jan Gugenheimer</li>
                <li> Enrico Rukzio</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_1.jpg</url>
                <caption>Figure 1. A player is sitting on the front passenger seat playing the game
                    while the car is moving. Kinesthetic forces caused by the car match the
                    movements in VR.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_2.jpg</url>
                <caption>Figure 2. Schematic view of the position of devices and people involved
                    in the apparatus. The driver acts as normal driver, following a route to a
                    destination, the co-driver is playing the game on the font-passenger seat
                    with a Samsung GearVR attached. An x-IMU measures the vehicle’s
                    inertia. An OBD-II reader attached to the car’s diagnostic port is used
                    to measure the car’s velocity.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_3.jpg</url>
                <caption>Figure 3. Route generation based on predeﬁned route. (left) The trajec-
                    tory is extracted from the topology, (center) A depth map based on the
                    trajectory is generated. (right) The terrain according to the depth map
                    is generated.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_4.jpg</url>
                <caption>Figure 4. The warp effect can be used to visualize acceleration.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_5.jpg</url>
                <caption>Figure 5. Two approaches of visualizing roll rotation. Left: the cockpit
                    and the camera rotates along the roll axis. Right: only the cockpit ro-
                    tates along the roll axis, the camera’s roll rotation stays in line with the
                    horizon. This concept also applies for the pitch axis.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>6</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_6.jpg</url>
                <caption>Figure 6. Effects of vehicular rotation while braking, accelerating and
                    turning. The weight transfer due to inertia in a car forces the car to
                    rotate forward when braking, backward when accelerating on the pitch
                    axis, and towards the outside of a curve when turning on the roll axis.
                    The rotation in a helicopter behaves inversely.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>7</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_7.jpg</url>
                <caption>Figure 7. The scene the player is ﬂying through. The map is a valley with
                    different highlights (a train, sheep, houses and a castle). The ﬁve small
                    images show the view from the ego perspective but without the cockpit.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>8</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_8.jpg</url>
                <caption>Figure 8. The view from inside the cockpit. A balloon is shot through
                    aiming via gaze and shooting via button press on a game controller.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>9</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_9.jpg</url>
                <caption>Figure 9. The track driven during the study consisted of 6 curves (3 left-
                    hand and 3 right-hand 90-degree curves) and three 360-degree turn (2
                    left-hand and 1 right-hand). It started in the parking lot (a). After three
                    right-hand curves, the ﬁrst 360-degree turn (b) was reached. Followed
                    by another right-hand curve, the second 360-degree turn (c) was reached.
                    Then after a left curve, the track went back to the ﬁrst 360-degree turn
                    (b). After three further left-hand curves, the track ended in the parking
                    lot from the beginning s(a).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>10</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_10.jpg</url>
                <caption>Figure 10. E2I total score and subscale score for the two conditions park-
                    ing and driving. It can be seen that the rating for driving in all three
                    scores (total, presence and enjoyment) is higher than for the parking
                    condition. The effect is signiﬁcant. Error bars represent one standard
                    deviation of uncertainty
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>11</id>
                <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_11.jpg</url>
                <caption>Figure 11. Directly compared simulator sickness between the parking
                    condition and the driving condition.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Crossing-Based Selection with Virtual Reality Head-Mounted Displays</filename>
        <data>
            <paper_id>Crossing-Based Selection with Virtual Reality Head-Mounted Displays</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300848</doi>
            <title>Crossing-Based Selection with Virtual Reality Head-Mounted Displays</title>
            <abstract>
                This paper presents the first investigation into using the goal-crossing paradigm for object selection with virtual reality (VR) head-mounted displays. Two experiments were carried out to evaluate ray-casting crossing tasks with target discs in 3D space and goal lines on 2D plane respectively in comparison to ray-casting pointing tasks. Five factors, i.e. task difficulty, the direction of movement constraint (collinear vs. orthogonal), the nature of the task (discrete vs. continuous), field of view of VR devices and target depth, were considered in both experiments. Our findings are: (1) crossing generally had shorter or no longer time, and higher or similar accuracy than pointing, indicating crossing can complement or substitute pointing; (2) crossing tasks can be well modelled with Fitts' Law; (3) crossing performance depended on target depth; (4) crossing target discs in 3D space differed from crossing goal lines on 2D plane in many aspects such as time and error performance, the effects of target depth and the parameters of Fitts' models. Based on these findings, we formulate a number of design recommendations for crossing-based interaction in VR.
            </abstract>
            <sections>
                <section>
                    <word_count>966</word_count>
                    <figure_citations>Figure 1) through ray-casting pointing is a difficult task [20, 23, 24], due to the fact that the interaction takes place in free space without physical support for the hand.Figure 1), corresponding to pointing at spheres in 3D space and circular targets on 2D plane respectively.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1386</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>3634</word_count>
                    <figure_citations>Figure 2).Figure 2(a)) that was made up by the positions of the two targets and the virtual camera with the camera’s position as the vertex.Figure 2(a)).Figure 2(a)) were within the apparatus’ FOV (horizontal FOV: around 80◦ [27]) while targets with long distance were not (target angle: 90.Figure 2(a)), indicating participants had to first locate a target using head motion before making a selection.Figure 2(b)) to confirm the selection.Figure 2(b)) of the controller only when crossing a goal.Figure 2(b)).Figure 3).Figure 3(b-e)), so that participants could move the ray cursor to “pass” through a disc from its one side to the other.Figure 3(a)).Figure 3(d)).Figure 3(e)).Figure 4(a)).Figure 5(a), C/DC follows a similar regression line to A/DP, suggesting a possibility of substituting the pointing task with the crossing task.Figure 6(a)).Figure 6(b)).</figure_citations>
                    <section_index>2</section_index>
                    <title>3D SPACE</title>
                </section>
                <section>
                    <word_count>2040</word_count>
                    <figure_citations>Figure 2).Figure 5(b), C/DC was uniformly faster than A/DP.Figure 7(a)), we adopted a standard 2D pointing design with circular targets [16] given pointing tasks in most 2D interfaces are typically twodimensional ([32]).Figure 7(b-e)), task design was the same as their counterpart in Experiment One, except that we used goal lines which were displayed on constrained planes.Figure 8(a)).Figure 8(b).Figure 9(a)).Figure 9(b)).</figure_citations>
                    <section_index>3</section_index>
                    <title>2D PLANE</title>
                </section>
                <section>
                    <word_count>1192</word_count>
                    <figure_citations>Figure 5), which further verifies their similar performances in target selection.Figure 5(a&amp;b)) shows an upward curvature of time away for the regression line for low values of ID.</figure_citations>
                    <section_index>4</section_index>
                    <title>GENERAL DISCUSSION</title>
                </section>
                <section>
                    <word_count>488</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>IMPLICATIONS FOR DESIGN</title>
                </section>
                <section>
                    <word_count>155</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>FUTURE WORK</title>
                </section>
                <section>
                    <word_count>150</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>63</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1742</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Crossing</li>
                <li>Pointing</li>
                <li>Virtual reality head-mounted displays</li>
                <li>Ray-casting selection</li>
                <li>Fitts’ law</li>
            </keywords>
            <authors>
                <li>Huawei Tu</li>
                <li> Susu Huang</li>
                <li> Jiabin Yuan</li>
                <li> Xiangshi Ren</li>
                <li> Feng Tian</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_1.jpg</url>
                <caption>Figure 1: Crossing a (a) target disc in 3D space; (b) goal line
                    on 2D plane.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_2.jpg</url>
                <caption>Figure 2: (a) Target positions in the virtual environment
                    from an overhead perspective. Z-axis is the default orien-
                    tation of the virtual camera in VR. Dotted lines represent
                    target depths (768, 2304 and 3456 mm). Circles with iden-
                    tical numbers indicate target pairs in an experiment trial.
                    Each target pair corresponds to a target angle, either 20.4◦
                    or 90.2◦. The shaded area indicates the apparatus’s default
                    FOV (≈ 80◦) (b) Participant wearing the Oculus Rift using
                    a touch controller. Inset: close-up of the Oculus Touch con-
                    troller. “Button A” is highlighted in a dotted red circle.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_3.jpg</url>
                <caption>Figure 3: The five tested task types in 3D space.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_4.jpg</url>
                <caption>Figure 4: Mean selection time for the five task types in (a)
                    three target depths and (b) seven IDs. Error bars represent
                    0.95 confidence interval.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_5.jpg</url>
                <caption>Figure 5: Fitts’ law models for the pointing and four crossing
                    tasks in (a) Experiment One and (b) Two.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_6.jpg</url>
                <caption>Figure 6: Error rates for the five task types in (a) three target
                    depths and (b) seven IDs.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_7.jpg</url>
                <caption>Figure 7: The five tested task types on 2D planes.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_8.jpg</url>
                <caption>Figure 8: Mean selection time for the five task types in (a)
                    three target depths and (b) seven IDs.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>9</id>
                <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_9.jpg</url>
                <caption>Figure 9: Error rates for the five task types in (a) three target
                    depths and (b) seven IDs.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials</filename>
        <data>
            <paper_id>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300657</doi>
            <title>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials</title>
            <abstract>Designing immersion is the key challenge in virtual reality; this challenge has driven advancements in displays, rendering and recently, haptics. To increase our sense of physical immersion, for instance, vibrotactile gloves render the sense of touching, while electrical muscle stimulation (EMS) renders forces. Unfortunately, the established metric to assess the effectiveness of haptic devices relies on the user's subjective interpretation of unspecific, yet standardized, questions.Here, we explore a new approach to detect a conflict in visuo-haptic integration (e.g., inadequate haptic feedback based on poorly configured collision detection) using electroencephalography (EEG). We propose analyzing event-related potentials (ERPs) during interaction with virtual objects. In our study, participants touched virtual objects in three conditions and received either no haptic feedback, vibration, or vibration and EMS feedback. To provoke a brain response in unrealistic VR interaction, we also presented the feedback prematurely in 25% of the trials.We found that the early negativity component of the ERP (so called prediction error) was more pronounced in the mismatch trials, indicating we successfully detected haptic conflicts using our technique. Our results are a first step towards using ERPs to automatically detect visuo-haptic mismatches in VR, such as those that can cause a loss of the user's immersion.</abstract>
            <sections>
                <section>
                    <word_count>343</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRO</title>
                </section>
                <section>
                    <word_count>672</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>IMMERSION</title>
                </section>
                <section>
                    <word_count>782</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1852</word_count>
                    <figure_citations>Figure 2, comprised: (1) a VR headset and a wrist-mounted wearable VIVE tracker, (2) a 64-channel EEG system, (3) one vibrotactile actuator worn on the fingertip, and (4) a medically-compliant EMS device connected via two electrodes worn on the forearm.Figure 3, was as follows: (1) participants moved their hands from the resting position to the ready position, to indicate they were ready to start the next trial; (2) participants waited for a new target to appear (the time of a new target spawning was randomized between 1-2 s); (3) then, the target (a cube) would appear in one of three possible positions (center, left, right), all equidistant from the participant’s ready position; (4) then, participants acquired the target by moving and touching the target with their index finger.Figure 4), we filtered the EEG data with a 0.</figure_citations>
                    <section_index>3</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1232</word_count>
                    <figure_citations>Figure 4(A).Figure 4(B), we observed a negative deflection around 170ms after a participant had selected the object (i.Figure 6(B), we observed no significant differences for the peak latencies over the three conditions.</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>323</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>19</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1919</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Force feedback</li>
                <li>EEG</li>
                <li>Elecrical muscle stimulation</li>
                <li>Virtual reality</li>
                <li>ERP</li>
                <li>Prediction error</li>
            </keywords>
            <authors>
                <li>Lukas Gehrke</li>
                <li> Sezen Akman</li>
                <li> Pedro Lopes</li>
                <li> Albert Chen</li>
                <li> Avinash Kumar Singh</li>
                <li> Hsiang-Ting Chen</li>
                <li> Chin-Teng Lin</li>
                <li> Klaus Gramann</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_1.jpg</url>
                <caption>Figure 1: We propose using the prediction error negativity
                    of the brain’s event related potential (ERP) to detect visuo-
                    haptic conflicts arising from unrealistic VR feedback. In our
                    study, participants selected objects in VR. To provoke their
                    brains to process an unrealistic interaction, we sometimes
                    provided the haptic feedback prematurely. When subtract-
                    ing these ERPs to the ERPs from realistic interactions, we
                    found that the negative amplitude of the error prediction
                    increased, hinting at a loss in immersion.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_2.jpg</url>
                <caption>Figure 2: Our experimental setup (image with consent from
                    participant).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_3.jpg</url>
                <caption>Figure 3: Interaction flow depicting one trial in our 3D object selection task.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_4.jpg</url>
                <caption>Figure 4: ERP amplitude (in µV ) and standard error of the
                    mean at the forehead electrode FCz across [A] all match tri-
                    als and [B] mismatch trials in a -300ms to 700ms window
                    centered at the object selection time, for each of the three
                    feedback conditions (Visual, Vibro and EMS).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_5.jpg</url>
                <caption>Figure 5: Amplitude and standard error of the mean of the
                    resulting ERPs obtained by subtracting the mean amplitude
                    of all match trials from the mean amplitude of all mismatch
                    trials for each participant (in µV ) at the forehead electrode
                    FCz; for all three feedback conditions (Visual, Vibro and
                    EMS)
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_6.jpg</url>
                <caption>Figure 6: Negative peak amplitudes (in µV ) and latencies (in
                    ms) 100 to 300ms post object selection event in difference
                    ERPs, see figure 5. Dots represent individual participants.
                    Uncorrected p-values of pairwise comparisons were com-
                    puted with non-parametric rank-sum tests.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay</filename>
        <data>
            <paper_id>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3026028</doi>
            <title>Effects of Sharing Physiological States of Players in a Collaborative Virtual Reality Gameplay</title>
            <abstract>Interfaces for collaborative tasks, such as multiplayer games can enable more effective and enjoyable collaboration. However, in these systems, the emotional states of the users are often not communicated properly due to their remoteness from one another. In this paper, we investigate the effects of showing emotional states of one collaborator to the other during an immersive Virtual Reality (VR) gameplay experience. We created two collaborative immersive VR games that display the real-time heart-rate of one player to the other. The two different games elicited different emotions, one joyous and the other scary. We tested the effects of visualizing heart-rate feedback in comparison with conditions where such a feedback was absent. The games had significant main effects on the overall emotional experience.</abstract>
            <sections>
                <section>
                    <word_count>773</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1072</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1122</word_count>
                    <figure_citations>Figure 1), ensured that that the sensors remained in the correct position on the hand.Figure 1).Figure 2(c)).</figure_citations>
                    <section_index>2</section_index>
                    <title>SYSTEM</title>
                </section>
                <section>
                    <word_count>1475</word_count>
                    <figure_citations>Figure 2 shows our independent variables.Figure 3).</figure_citations>
                    <section_index>3</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1315</word_count>
                    <figure_citations>Figure 4).Figure 5).Figure 6).Figure 7, demonstrated an interesting pattern difference between the two games.</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>864</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>257</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>LIMITATIONS</title>
                </section>
                <section>
                    <word_count>858</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>68</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1064</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Empathic Computing</li>
                <li>Collaborative Gameplay</li>
                <li>Physiological Sensors</li>
                <li>Emotions</li>
                <li>User Study</li>
            </keywords>
            <authors>
                <li>Arindam Dey</li>
                <li> Thammathip Piumsomboon</li>
                <li> Youngho Lee</li>
                <li> Mark Billinghurst</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_1.jpg</url>
                <caption>Figure 1. Participants were asked to stand during the gameplay sessions.
                    They were wearing HTC Vive HMD and two different heart-rate sensors
                    - Empatica E4 and Biometric Glove with Arduino sensors. They were
                    also wearing a Logitech noise canceling headphone to hear the sounds of
                    the game.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_2.jpg</url>
                <caption>Figure 2. In this user study we used two different games and two different conditions. In our control condition, a heart-rate was not shown to the
                    participants (a) and (c). In the experimental condition, we showed a heart-rate feedback to the participant that showed the real-time heart-rate of the
                    player (b) and (d). (a - b) show the calm butterﬂy game and (c - d) show the scary zombie game.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_3.jpg</url>
                <caption>Figure 3. The user study setup. Both player and the observer were standing during the gameplay sessions. They were physically located in a 15’ × 15’
                    calibrated area and could speak to each other to collaborate.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_4.jpg</url>
                <caption>Figure 4. Subjective feedback on three question. 4(a) how much did participants understood the emotional state of the player? 4(b) how much were
                    they attentive to the gameplay? 4(c) how much did enjoy the collaboration? Lower values indicate better outcome and whiskers represent ± 1 SE.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_5.jpg</url>
                <caption>Figure 5. Analysis of PANAS data showed an evidence of non-
                    signiﬁcantly more positive affect in the presence of heart-rate feedback.
                    Zombie game caused higher positive and negative affect than the Butter-
                    ﬂy game.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_6.jpg</url>
                <caption>Figure 6. Analysis of viewing angle shows that the butterﬂy game had
                    signiﬁcantly less difference in viewing angle between the observer and
                    the player than the zombie game. Whiskers represent ± 1 standard er-
                    ror.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_7.jpg</url>
                <caption>Figure 7. Observers looked differently in the butterﬂy game than in the zombie game. Their gaze direction was more aligned to the player in the
                    butterﬂy game but in the zombie game the direction was less aligned. 0° shows the normalized direction of player’s gaze.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects </filename>
        <data>
            <paper_id>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects </paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025488</doi>
            <title>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects</title>
            <abstract>
                In this paper, we report on a study investigating a novel haptic illusion for altering the perception of 3D shapes using a non-planar screen and vibrotactile friction. In our study, we presented an image of a rectangular prism on a cylindrical and a flat display. Participants were asked to move their index finger horizontally along the surface of the displays towards the edge of the rectangular prism. Participants were asked whether they were experiencing a flat, cylindrical or rectangular shape. In one condition, a vibrotactile stimulus simulated increasing friction towards the visible edge of the rectangular prism, with a sudden drop-off when this edge was crossed by the finger. Results suggest that presenting an image of a rectangular prism, and applying vibrotactile friction, particularly on a cylindrical display, significantly increased participant ratings stating that they were experiencing a physical rectangular shape.
            </abstract>
            <sections>
                <section>
                    <word_count>583</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>890</word_count>
                    <figure_citations>Figure 3a).Figure 3b).</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>673</word_count>
                    <figure_citations>Figure 1).Figure 2).Figure 2), with one of its corners facing the participant.</figure_citations>
                    <section_index>2</section_index>
                    <title>EXPERIMENTAL STUDY</title>
                </section>
                <section>
                    <word_count>217</word_count>
                    <figure_citations>Figure 4a).Figure 4b).Figure 4b).Figure 4b).</figure_citations>
                    <section_index>3</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>301</word_count>
                    <figure_citations>Figure 4c).</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>165</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>319</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>REFERENCES</title>
                </section>
                <section>
                    <word_count>1046</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSIONS</title>
                </section>
            </sections>
            <keywords>
                <li>Organic User Interfaces</li>
                <li>DisplayObjects</li>
                <li>Shaped Displays</li>
                <li>Vibrotactile Feedback</li>
            </keywords>
            <authors>
                <li>Juan Pablo Carrascal</li>
                <li> Roel Vertegaal</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects _crop_1.jpg</url>
                <caption>Figure 1. Cylindrical DisplayObject with rectangular prism
                    image.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects _crop_2.jpg</url>
                <caption>Figure 2. Apparatus used in the study: Cylindrical display
                    (left), Flat display (right), and Leap Motion (bottom).
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects _crop_3.jpg</url>
                <caption>Figure 3. Exploratory procedure used during the experiment
                    for the cylindrical display: a. Vibrotactile feedback is applied
                    when the participant’s finger is dragged within the boundaries
                    of the rectangular prism. Feedback amplitude increases
                    linearly as the finger is dragged towards the corner from
                    either side. b. A strong pulse is produced when the finger
                    crosses the corner line, and feedback is stopped.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects _crop_4.jpg</url>
                <caption>Figure 4. Boxplots for scores of participants’ responses (7-point Likert scale, N = 19).
                </caption>
                <page>4</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures</filename>
        <data>
            <paper_id>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300479</doi>
            <title>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures</title>
            <abstract>
                Experiencing materials in virtual reality (VR) is enhanced by combining visual and haptic feedback. While VR easily allows changes to visual appearances, modifying haptic impressions remains challenging. Existing passive haptic techniques require access to a large set of tangible proxies. To reduce the number of physical representations, we look towards fabrication to create more versatile counterparts. In a user study, 3D-printed hairs with length varying in steps of 2.5 mm were used to influence the feeling of roughness and hardness. By overlaying fabricated hair with visual textures, the resolution of the user's haptic perception increased. As changing haptic sensations are able to elicit perceptual switches, our approach can extend a limited set of textures to a much broader set of material impressions. Our results give insights into the effectiveness of 3D-printed hair for enhancing texture perception in VR.
            </abstract>
            <sections>
                <section>
                    <word_count>419</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1201</word_count>
                    <figure_citations>Figure 1 illustrates superimposing virtual texture images on top of physical textures.</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>883</word_count>
                    <figure_citations>Figure 2), the customer is able to Page 3 CHI 2019 Paper mm 2.Figure 3) was printed using an Autodesk Ember1 with an X-Y resolution of 50 µm and a layer thickness of 25 µm.Figure 3, were purchased from various sources and imported into the Unity environment.</figure_citations>
                    <section_index>2</section_index>
                    <title>PERCEPTION</title>
                </section>
                <section>
                    <word_count>1319</word_count>
                    <figure_citations>Figure 4, allowed participants to precisely hit a required hair structure without touching a diferent surface.Figure 4, while answering 6 questions, i.</figure_citations>
                    <section_index>3</section_index>
                    <title>STUDY</title>
                </section>
                <section>
                    <word_count>2390</word_count>
                    <figure_citations>Figure 5a and 5c) and the visual ratings of virtual textures without haptic information (see Figure 5b and 5d).Figure 5a and 5c indicate signifcant diferences.</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>678</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>156</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>37</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1225</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Texture perception</li>
                <li>Passive haptic feedback</li>
                <li>3D printing</li>
            </keywords>
            <authors>
                <li>Donald Degraen</li>
                <li> André Zenner</li>
                <li> Antonio Krüger</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_1.jpg</url>
                <caption>Figure 1: Augmenting texture perception by overlaying a
                    physical structure with diferent virtual textures.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_2.jpg</url>
                <caption>Figure 2: A sample book to explore diferent fabrics.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_3.jpg</url>
                <caption>Figure 3: Overview of the fve physical surfaces and fve virtual textures used in our study. Note that the material used for
                    glass is highly transparent and would refect the environment.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_4.jpg</url>
                <caption>Figure 4: Experiment Setup. Lef: A user touching a physi-
                    cal sample on the proxy plate with a Vive controller in the
                    center and a Leap Motion positioned above the user’s hand.
                    Right Top: First person view of a user touching a virtual plas-
                    tic material. Right Botom: A patch of 3D-printed hair.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_5.jpg</url>
                <caption>Figure 5: Boxplots depicting the baseline assessments. Haptic ratings of the physical samples without visual information
                    for roughness (a) and hardness (c). Visual ratings of the virtual textures without haptic information for roughness (b) and
                    hardness (d). Brackets connect groups with statistically signifcant diferences (p &lt;.05). </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_6.jpg</url>
                <caption>Figure 6: Stacked perception graphs per virtual texture indicating for each physical sample the percentage of samples identifed
                    per material category. Unassigned space depicts the percentage of times no meaningful material or object was assigned.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation</filename>
        <data>
            <paper_id>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300477</doi>
            <title>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation</title>
            <abstract>
                Virtual reality (VR) can be immersive to such a degree that users sometimes report feeling tactile sensations based on visualization of the touch, without any actual physical contact. This effect is not only interesting for studies of human perception, but can also be leveraged to improve the quality of VR by evoking tactile sensations without usage of specialized equipment. The aim of this paper is to study brain processing of the illusory touch and its enhancement for purposes of exploitation in VR scene design. To amplify the illusory touch, transcranial direct current stimulation (tDCS) was used. Participants attended two sessions with blinded stimulation and interacted with a virtual ball using tracked hands in VR. The effects were studied using electroencephalography (EEG), that allowed us to examine stimulation-induced changes in processing of the illusory touch in the brain, as well as to identify its neural correlates. Results confirm enhanced processing of the illusory touch after the stimulation, and some of these changes were correlated to subjective rating of its magnitude.
            </abstract>
            <sections>
                <section>
                    <word_count>1915</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>603</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>1</section_index>
                    <title>STIMULATION</title>
                </section>
                <section>
                    <word_count>1570</word_count>
                    <figure_citations>Figure 1)1 .Figure 2) consisted of just resting the hands on the desk, paying attention to the collision of the balls with the hands.Figure 3).Figure 3), participants were instructed to place their hands on the physical desk (which coincided with the VR desk) on their little fingers to comfortably hit the balls towards the middle of the desk, into the hole.</figure_citations>
                    <section_index>2</section_index>
                    <title>METHODOLOGY</title>
                </section>
                <section>
                    <word_count>750</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>872</word_count>
                    <figure_citations>Figure 4), but this effect was not statistically significant.Figure 4), as the passive phase hand-ball collisions elicited distinct P300, contrary to the active phase.</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1096</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>499</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>1959</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Electroencephalography</li>
                <li>Embodiment</li>
                <li>Illusory Touch</li>
                <li>Transcranial Direct Current Stimulation</li>
                <li>Virtual Reality</li>
            </keywords>
            <authors>
                <li>Filip Škola</li>
                <li> Fotis Liarokapis</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation_crop_1.jpg</url>
                <caption>Figure 1: EEG and tDCS channel set-up. All the channels ex-
                    cept for AF3 and AF4 were used for the EEG recording (white
                    and semi-white background). For right hand stimulation, C3
                    was the anode and AF4 the return electrode (yellow). For left
                    hand stimulation, C4 was the anode and AF3 the return elec-
                    trode (green).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation_crop_2.jpg</url>
                <caption>Figure 2: Experimental setting and the experimental VR scene.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation_crop_3.jpg</url>
                <caption>Figure 3: Active phase of the experiment.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation_crop_4.jpg</url>
                <caption>Figure 4: Grand-averaged ERPs for the dominant hand at electrode location Pz, passive condition. Left plot – non-stimulated,
                    right plot – stimulated. To generate these plots, data were cleaned with higher high-pass filter setting (cut-off 2.5 Hz) than for
                    the analysis purposes. Enhanced N140 and P100 are visible at these plots, as well as stronger P300 in the stimulated condition.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality</filename>
        <data>
            <paper_id>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300426</doi>
            <title>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality</title>
            <abstract>
                We present an empirical comparison of eleven bare hand, mid-air mode-switching techniques suitable for virtual reality in two experiments. The first evaluates seven techniques spanning dominant and non-dominant hand actions. Techniques represent common classes of actions selected by a methodical examination of 56 examples of prior art. The standard "subtraction method" protocol is adapted for 3D interfaces, with two baseline selection methods, bare hand pinch and device controller button. A second experiment with four techniques explores more subtle dominant-hand techniques and the effect of using a dominant hand device for selection. Results provide guidance to practitioners when choosing bare hand, mid-air mode-switching techniques, and for researchers when designing new mode-switching methods in VR.
            </abstract>
            <sections>
                <section>
                    <word_count>661</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>4670</word_count>
                    <figure_citations>Figure 2).Figure 3).</figure_citations>
                    <section_index>1</section_index>
                    <title>BACKGROUND AND RELATED WORK</title>
                </section>
                <section>
                    <word_count>1809</word_count>
                    <figure_citations>Figure 5).Figure 8).Figure 9).</figure_citations>
                    <section_index>2</section_index>
                    <title>BASELINE</title>
                </section>
                <section>
                    <word_count>60</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>PINCH</title>
                </section>
                <section>
                    <word_count>17</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>CONTROLLER</title>
                </section>
                <section>
                    <word_count>724</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>146</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>31</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>3322</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Interaction techniques</li>
                <li>Controlled experiments</li>
            </keywords>
            <authors>
                <li>Hemant Bhaskar Surale</li>
                <li> Fabrice Matulic</li>
                <li> Daniel Vogel</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_1.jpg</url>
                <caption>Figure 1: Selected Mode-switching techniques: (a) non dominant fist (nd-fist); (b) non dominant palm (nd-palm); (c) hand in
                    field of view (nd-fov); (d) touch head (nd-head); (e) dominant fist (d-fist); (f) dominant palm (d-palm); (g) point (d-point) .
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_2.jpg</url>
                <caption>Figure 2: Line drawing task: (a) baseline; (b) compound.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_3.jpg</url>
                <caption>Figure 3: Mode-switch time by techniqe (error bars in all
                    graphs are 95% CI).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_4.jpg</url>
                <caption>Figure 4: Overall error rate by techniqe.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_5.jpg</url>
                <caption>Figure 5: Proportion of specific error rates by techniqe.
                    Note multiple error types can occur in a cycle.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_6.jpg</url>
                <caption>Figure 6: Mode switching techniques evaluated in experi-
                    ment 2. (a) orientated pinch (d-orient); (b) middle finger
                    pinch (d-middle)
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_7.jpg</url>
                <caption>Figure 7: Mode-switch times by techniqe.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_8.jpg</url>
                <caption>Figure 8: Overall error rate by techniqe. Baseline tech-
                    niques have start and end error rates.
                </caption>
                <page>10</page>
            </figure>
            <figure>
                <id>9</id>
                <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_9.jpg</url>
                <caption>Figure 9: Proportion of specific error rates by techniqe.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements</filename>
        <data>
            <paper_id>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300644</doi>
            <title>Exploring Interaction Fidelity in Virtual Reality: Object Manipulation and Whole-Body Movements</title>
            <abstract>
                High degrees of interaction fidelity (IF) in virtual reality (VR) are said to improve user experience and immersion, but there is also evidence of low IF providing comparable experiences. VR games are now increasingly prevalent, yet we still do not fully understand the trade-off between realism and abstraction in this context. We conducted a lab study comparing high and low IF for object manipulation tasks in a VR game. In a second study, we investigated players' experiences of IF for whole-body movements in a VR game that allowed players to crawl underneath virtual boulders and "dangle'' along monkey bars. Our findings show that high IF is preferred for object manipulation, but for whole-body movements, moderate IF can suffice, as there is a trade-off with usability and social factors. We provide guidelines for the development of VR games based on our results.
            </abstract>
            <sections>
                <section>
                    <word_count>496</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>784</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>2299</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>RESEARCH QUESTIONS</title>
                </section>
                <section>
                    <word_count>4673</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>INTUI</title>
                </section>
                <section>
                    <word_count>527</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>OVERALL IMPLICATIONS</title>
                </section>
                <section>
                    <word_count>226</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>1670</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Interaction fidelity</li>
                <li>Games</li>
                <li>Whole body interaction</li>
                <li>Virtual objects</li>
                <li>Player experience</li>
            </keywords>
            <authors>
                <li>Katja Rogers</li>
                <li> Jana Funke</li>
                <li> Julian Frommel</li>
                <li> Sven Stamm</li>
                <li> Michael Weber</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements_crop_1.jpg</url>
                <caption>Figure 1: In two user studies, we explored player experiences with varying degrees of interaction fidelity in VR. One study
                    explored object manipulation (a &amp; b), while the other focused on whole-body movements (c).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements_crop_2.jpg</url>
                <caption>Figure 2: Task T1: In the HF condition, users had to physi-
                    cally use each buckle a⃝ and then push open the chest’s lid
                    b⃝, while the low-fidelity version offered clicks and a widget
                    c⃝ to allow users to interact with the chest and its contents.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements_crop_3.jpg</url>
                <caption>Figure 3: Potion brewing (T2) in HF involved physically
                    pouring potion vial contents into the cauldron a⃝. In the
                    LF variant, players dragged potion vials from their inven-
                    tory to the activated cauldron widget b⃝.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements_crop_4.jpg</url>
                <caption>Figure 4: The higher fidelity condition resulted in signifi-
                    cantly higher values for all IEQ factors except challenge.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements_crop_5.jpg</url>
                <caption>Figure 5: The player (b) and their view (a) while crawling,
                    compared to the player (d) and their view (c) while dangling
                    along two bars.
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Exploring Virtual Agents for Augmented Reality</filename>
        <data>
            <paper_id>Exploring Virtual Agents for Augmented Reality</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300511</doi>
            <title>Exploring Virtual Agents for Augmented Reality</title>
            <abstract>
                Prior work has shown that embodiment can benefit virtual agents, such as increasing rapport and conveying non-verbal information. However, it is unclear if users prefer an embodied to a speech-only agent for augmented reality (AR) headsets that are designed to assist users in completing real-world tasks. We conducted a study to examine users' perceptions and behaviors when interacting with virtual agents in AR. We asked 24 adults to wear the Microsoft HoloLens and find objects in a hidden object game while interacting with an agent that would offer assistance. We presented participants with four different agents: voice-only, non-human, full-size embodied, and a miniature embodied agent. Overall, users preferred the miniature embodied agent due to the novelty of his size and reduced uncanniness as opposed to the larger agent. From our results, we draw conclusions about how agent representation matters and derive guidelines on designing agents for AR headsets.
            </abstract>
            <sections>
                <section>
                    <word_count>650</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>856</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1180</word_count>
                    <figure_citations>Figure 2).Figure 2), and that he has a slightly higher-pitched voice than Jake to account for his smaller size.</figure_citations>
                    <section_index>2</section_index>
                    <title>AUGMENTED REALITY AGENTS</title>
                </section>
                <section>
                    <word_count>672</word_count>
                    <figure_citations>Figure 4).</figure_citations>
                    <section_index>3</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1679</word_count>
                    <figure_citations>Figure 5).Figure 5b) as well as the length of each utterance (Figure 5c).Figure 5d), as well as the duration of each gaze (Figure 5e).Figure 7).</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1511</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>432</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>LIMITATIONS AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>139</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>67</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1649</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Augmented reality</li>
                <li>Embodied conversational agents</li>
            </keywords>
            <authors>
                <li>Isaac Wang</li>
                <li> Jesse Smith</li>
                <li> Jaime Ruiz</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Exploring Virtual Agents for Augmented Reality_crop_1.jpg</url>
                <caption>Figure 1. Example of a user wearing an augmented reality
                    headset and interacting with a virtual agent projected onto
                    the real world through the headset.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Exploring Virtual Agents for Augmented Reality_crop_2.jpg</url>
                <caption>Figure 2. The three agents with visual representations, as
                    viewed in context from the HoloLens. The fourth agent,
                    Ava (not pictured), was a voice-only agent.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Exploring Virtual Agents for Augmented Reality_crop_3.jpg</url>
                <caption>Figure 3. An excerpt from one of the hidden object puzzles.
                    In this image, participants would need to find the rabbit
                    (highlighted with a red circle).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Exploring Virtual Agents for Augmented Reality_crop_4.jpg</url>
                <caption>Figure 4. The experiment setup. Participants wore a
                    Microsoft HoloLens and interacted with a hidden object
                    game running on an HP Sprout PC.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Exploring Virtual Agents for Augmented Reality_crop_5.jpg</url>
                <caption>Figure 5. Boxplots comparing (a) number of utterances per trial, (b) words per utterance,
                    (c) number of gaze events per trial, and (d) gaze duration across agents.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Exploring Virtual Agents for Augmented Reality_crop_6.jpg</url>
                <caption>Figure 6. Graph of the average subjective ratings from the
                    questionnaire. Each agent was rated against the eight
                    scales. Error bars represent the standard deviation.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Exploring Virtual Agents for Augmented Reality_crop_7.jpg</url>
                <caption>Figure 7. Participant ratings for most-liked and most-
                    disliked agent. Net score (likes – dislikes) is also shown.
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Extending the Body for Interaction with Reality</filename><data>
            <paper_id>Extending the Body for Interaction with Reality</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025689</doi>
            <title>Extending the Body for Interaction with Reality</title>
            <abstract>In this paper, we explore how users can control remote devices with a virtual long arm, while preserving the perception that the artificial arm is actually part of their own body. Instead of using pointing, speech, or a remote control, the users' arm is extended in augmented reality, allowing access to devices that are out of reach. Thus, we allow users to directly manipulate real-world objects from a distance using their bare hands. A core difficulty we focus on is how to maintain ownership for the unnaturally long virtual arm, which is the strong feeling that one's limbs are actually part of the own body. Fortunately, what the human brain experiences as being part of the own body is very malleable and we find that during interaction the user's virtual arm can be stretched to more than twice its real length, without breaking the user's sense of ownership for the virtual limb.</abstract>
            <sections>
                <section>
                    <word_count>1019</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>565</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>944</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>THE PSYCHOLOGICAL CONCEPT OF OWNERSHIP</title>
                </section>
                <section>
                    <word_count>1400</word_count>
                    <figure_citations>Figure 2).Figure 3, A).Figure 3, B), which is then filled with background-colored pixels (Figure 3, C).Figure 5, where the hand is cut off at the edge of the table, and thus appears to be reaching underneath it, or seems to hover above the table projecting a shadow onto it.</figure_citations>
                    <section_index>3</section_index>
                    <title>SUPPORTING OWNERSHIP IN INTERACTION</title>
                </section>
                <section>
                    <word_count>759</word_count>
                    <figure_citations>Figure 6, top left).Figure 6, top right).Figure 6, bottom left).Figure 6, bottom right).</figure_citations>
                    <section_index>4</section_index>
                    <title>DESIGN PROCESS</title>
                </section>
                <section>
                    <word_count>747</word_count>
                    <figure_citations>Figure 6) in counterbalanced order.Figure 7).</figure_citations>
                    <section_index>5</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>1346</word_count>
                    <figure_citations>Figure 8, left and center).Figure 8, right).Figure 9, right).Figure 9).Figure 10, left).Figure 10, center).</figure_citations>
                    <section_index>6</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>962</word_count>
                    <figure_citations>Figure 11).</figure_citations>
                    <section_index>7</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>86</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>1769</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Ownership</li>
                <li>Augmented Reality</li>
                <li>Ubiquitous Computing</li>
                <li>Virtual Hand Illusion</li>
            </keywords>
            <authors>
                <li>Tiare Feuchtner</li>
                <li> Jörg Müller</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Extending the Body for Interaction with Reality_crop_1.jpg</url>
                <caption>Figure 1: We present an ownership-preserving direct manipulation technique in augmented reality, which allows interaction with
                    remote devices in a ubicomp environment with the help of a long virtual arm. While the user’s real hand is close to the body the
                    virtual arm is of normal length (A) and by simply reaching out the user can make it extend to access remote devices in the room.
                    For instance we allow adjusting the height of a table (B), opening and closing a curtain (C) and adjusting the angle of a tilting
                    surface (D).
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Extending the Body for Interaction with Reality_crop_2.jpg</url>
                <caption>Figure 2: Optical tracking of the user and the actuated objects
                    in the room is achieved with retro-reﬂective markers. This
                    picture shows the view of one of the OptiTrack cameras.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Extending the Body for Interaction with Reality_crop_3.jpg</url>
                <caption>Figure 3: (A) Hand in green sleeve, (B) binary mask after chroma keying, (C) Memory Inpainting.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Extending the Body for Interaction with Reality_crop_4.jpg</url>
                <caption>Figure 4: The arm-stretch function was implemented based
                    on the Go-Go Interaction Technique. The graph shows the
                    function adjusted to a participant’s arm length, with the expo-
                    nent p = 4 and a maximum virtual arm length of Rv = 5.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Extending the Body for Interaction with Reality_crop_5.jpg</url>
                <caption>Figure 5: To provide a better sense of interaction with a 3D
                    environment, we implemented depth cues such as shadows
                    (A) and occlusion (B) of the virtual hand.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Extending the Body for Interaction with Reality_crop_6.jpg</url>
                <caption>Figure 6: In the user study we compared 3 different types of
                    hand representations in 4 conditions: (C1) arm, (C2) hand,
                    (C3) abstract-hand, and (C4) arm w/o inpainting (the real
                    arm was simultaneously visible).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Extending the Body for Interaction with Reality_crop_7.jpg</url>
                <caption>Figure 7: To establish a baseline participants were asked to
                    repeatedly tap 3 virtual cubes with their virtual hand, while
                    their virtual arm was of equal length to their real arm.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Extending the Body for Interaction with Reality_crop_8.jpg</url>
                <caption>Figure 8: Left: When interacting in the baseline and with the extended arm in C1 participants felt like the virtual hand was part
                    of their body. Center: In the baseline and the arm condition participants felt like the virtual hand was their own hand. Right:
                    When interacting with the long arm, some participants felt that their real arm was becoming longer.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>9</id>
                <url>Extending the Body for Interaction with Reality_crop_9.jpg</url>
                <caption>Figure 9: Left: Across all conditions and in the baseline participants maintained strong expectations of the virtual hand following
                    the movement of their real hand. Center: Participants indicated that they felt strongly in control of the devices in the environment
                    across all conditions. Right: When interacting with the devices the participants adapted the movement of their real hand based
                    on that of the virtual hand. E.g., they stopped when they saw the virtual hand stop at the table’s edge, and adjusted their hand
                    movement to the speed of the table movement.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>Extending the Body for Interaction with Reality_crop_10.jpg</url>
                <caption>Figure 10: Left: Seeing both the real arm and the virtual arm elicited the feeling of owning more than one right hand. Center:
                    During interaction participants concentrated more on the virtual hand than on their real hand, even if it was simultaneously visible
                    (as in C4, arm w/o inpainting). Right: After all trials were concluded, we asked each participant to stretch both arms out in front
                    of him and rate the following two statements: P1) I feel like my right arm is longer than my left. P2) I feel like my right arm can
                    reach further than my left.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>11</id>
                <url>Extending the Body for Interaction with Reality_crop_11.jpg</url>
                <caption>Figure 11: As a step beyond command languages, direct manipulation interfaces allow users to directly manipulate a representa-
                    tion of data. In the ﬁrst generation, with Graphical User Interfaces (GUIs), the screen provided the border between the physical
                    and digital worlds. Tangible User Interfaces (TUIs) and Radical Atoms provide a physical representation of the digital world,
                    thereby moving this border into the physical world. Body extension goes beyond these paradigms by moving the border between
                    the physical and the digital into the user’s own body.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display</filename>
        <data>
            <paper_id>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300763</doi>
            <title>FTVR in VR: Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display</title>
            <abstract>
                Spherical fish tank virtual reality (FTVR) displays attempt to create a virtual "crystal ball" experience using head-tracked rendering. Almost all of these systems have omitted stereo cues, making them easy to build, but it is not clear how much this omission degrades the 3D experience. In this study, we evaluate performance and subjective effects of stereo on 3D perception and interaction tasks with a spherical FTVR display. To control for calibration error and tracking latency, we perform the evaluation on a simulated spherical display in VR. The results of our study provide a clear recommendation for the design and use of spherical FTVR displays: while omitting stereo may not be readily apparent for users, their performance will be significantly degraded (20% - 91% increase in median task time). Therefore, including stereo viewing in spherical displays is critical for use in FTVR.
            </abstract>
            <sections>
                <section>
                    <word_count>661</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1339</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>314</word_count>
                    <figure_citations>Figure 1) and set the output to a VR headset.</figure_citations>
                    <section_index>2</section_index>
                    <title>SIMULATED SPHERICAL FTVR SYSTEM</title>
                </section>
                <section>
                    <word_count>3044</word_count>
                    <figure_citations>Figure 2).Figure 5), therefore we accept H1-3 (NonStereo is aligned to mid-point of the eyes).Figure 6).</figure_citations>
                    <section_index>3</section_index>
                    <title>EXPERIMENTS</title>
                </section>
                <section>
                    <word_count>1454</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>411</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>LIMITATIONS AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>109</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>21</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1717</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Fish tank virtual reality</li>
                <li>Spherical display</li>
                <li>3D perception</li>
            </keywords>
            <authors>
                <li>Dylan Fafard</li>
                <li> Ian Stavness</li>
                <li> Martin Dechant</li>
                <li> Regan Mandryk</li>
                <li> Qian Zhou</li>
                <li> Sidney Fels</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_1.jpg</url>
                <caption>Figure 1: A spherical FTVR display (right) is simulated in VR
                    (left) to evaluate the importance of stereo cues.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_2.jpg</url>
                <caption>Figure 2: An example, with exaggerated stereo disparity, of
                    what the left and right eyes would see in the Stereo (top), Non-
                    Stereo (middle), and Monocular (bottom) viewing conditions.
                    Note that non-stereo rendering creates a perspective mis-
                    match between the background 3D world and the display.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_3.jpg</url>
                <caption>Figure 3: Pattern alignment task: the pattern starts distorted
                    (left) and then the participant moves their head left, right,
                    up, or down to align their viewpoint so that the pattern ap-
                    pears to have straight lines and circular rings (right).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_4.jpg</url>
                <caption>Figure 4: Mean Time and Error vs. Viewing Condition for the
                    Pattern Alignment task. Error bars represent the standard
                    error of the mean, highlighted bars indicate significant best
                    results, and dashed lines indicate a significant difference.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_5.jpg</url>
                <caption>Figure 5: The geometric mean (red) of measurements (blue/orange) is shown relative to the ground truth (green). Calibrations
                    were perturbed by 5 cm (black circles) at the start of each trial. Plots are scaled to 6.3 cm pupillary distance.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_6.jpg</url>
                <caption>Figure 6: Subjective preference task: the participant was
                    forced to move left and right to induce a minimum amount
                    of head motion before selecting their preference between a
                    pair of viewing conditions.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_7.jpg</url>
                <caption>Figure 7: Point cloud visualizations on our simulated display
                    in the Distance (left), Selection (middle), and Manipulation
                    (right) tasks in Experiment 3.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>8</id>
                <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_8.jpg</url>
                <caption>Figure 8: Mean Time, Error and Head Speed vs. Viewing Con-
                    dition grouped by Task. Error bars represent the standard
                    error of the mean, highlighted bars indicate significant best
                    results, and dashed lines indicate a significant difference.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Handsfree Omnidirectional VR Navigation using Head Tilt</filename>
        <data>
            <paper_id>Handsfree Omnidirectional VR Navigation using Head Tilt</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025521</doi>
            <title>Handsfree Omnidirectional VR Navigation using Head Tilt</title>
            <abstract>
                Navigating mobile virtual reality (VR) is a challenge due to limited input options and/or a requirement for handsfree interaction. Walking-in-place (WIP) is considered to offer a higher presence than controller input but only allows unidirectional navigation in the direction of the user's gaze--which impedes navigation efficiency. Leaning input enables omnidirectional navigation but currently relies on bulky controllers, which aren't feasible in mobile VR contexts. This note evaluates the use of head-tilt - implemented using inertial sensing - to allow for handsfree omnidirectional VR navigation on mobile VR platforms. A user study with 24 subjects compared three input methods using an obstacle avoidance navigation task: (1) head-tilt alone (TILT) (2) a hybrid method (WIP-TILT) that uses head tilting for direction and WIP to control speed; and (3) traditional controller input. TILT was significantly faster than WIP-TILT and joystick input, while WIP-TILT and TILT offered the highest presence. There was no difference in cybersickness between input methods.
            </abstract>
            <sections>
                <section>
                    <word_count>642</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1075</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>BACKGROUND</title>
                </section>
                <section>
                    <word_count>883</word_count>
                    <figure_citations>Figure 3).Figure 4 shows the corridor and a visualization of colliding into an obstacle.</figure_citations>
                    <section_index>2</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>553</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>TILT</title>
                </section>
                <section>
                    <word_count>611</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION AND LIMITATIONS</title>
                </section>
                <section>
                    <word_count>1137</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Locomotion</li>
                <li>Mobile VR</li>
                <li>Walking-in-place</li>
                <li>Head-tilt</li>
                <li>Simulator-sickness</li>
                <li>Games</li>
                <li>Inertial sensing</li>
            </keywords>
            <authors>
                <li>Sam Tregillus</li>
                <li> Majed Al Zayer</li>
                <li> Eelke Folmer</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Handsfree Omnidirectional VR Navigation using Head Tilt_crop_1.jpg</url>
                <caption>Figure 1. Head tilt is used to indicate the direction of travel
                    Walking-in-place (WIP) closely mimics walking, e.g., users
                    provide step-like motions while remaining stationary [29].
                    WIP closely approximates real walking input in terms of per-
                    formance [25] and presence [27]. Compared to a controller,
                    WIP is handsfree; offers higher presence [33]; improves spa-
                    tial orientation [19]; and is less likely to induce cybersickness
                    [16], because of the generation of proprioceptive feedback.
                    However, a controller allows for 360◦ omnidirectional nav-
                    igation, where WIP only navigates users in the direction of
                    their gaze. This impedes navigation efﬁciency, for example,
                    if a user wants to back up a little bit, it requires them to turn
                    around, move forward, turn around again and then move to
                    where they want to be. Because prior studies [25, 13, 27,
                    37] have only evaluated navigation tasks that include forward
                    motion, they ﬁnd a similar performance for WIP as controller
                    input. However, these results are misleading, as VR naviga-
                    tion also contains lateral movements [15] and controller input
                    outperforms WIP as it allows omnidirectional navigation.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Handsfree Omnidirectional VR Navigation using Head Tilt_crop_2.jpg</url>
                <caption>Figure 2. head tilt de-
                    ﬁned using vectors.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Handsfree Omnidirectional VR Navigation using Head Tilt_crop_3.jpg</url>
                <caption>Figure 3. Reticle shows: 1) no 2) right
                    3) right-back 4) back motion.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Handsfree Omnidirectional VR Navigation using Head Tilt_crop_4.jpg</url>
                <caption>Figure 4. Left: navigation task showing corridor with the obstacles the
                    participant users must navigate. Right: when colliding with an obstacle
                    it turns red and becomes sticky; requiring users to navigate laterally.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Handsfree Omnidirectional VR Navigation using Head Tilt_crop_5.jpg</url>
                <caption>Figure 5. Avg Likert scores and standard deviation for each method
                </caption>
                <page>4</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality </filename>
        <data>
            <paper_id>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality </paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025684</doi>
            <title>HapticHead: A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality</title>
            <abstract>
                Current virtual and augmented reality head-mounted displays usually include no or only a single vibration motor for haptic feedback and do not use it for guidance. We present HapticHead, a system utilizing multiple vibrotactile actuators distributed in three concentric ellipses around the head for intuitive haptic guidance through moving tactile cues. We conducted three experiments, which indicate that HapticHead vibrotactile feedback is both faster (2.6 s vs. 6.9 s) and more precise (96.4% vs. 54.2% success rate) than spatial audio (generic head-related transfer function) for finding visible virtual objects in 3D space around the user. The baseline of visual feedback is as expected more precise (99.7% success rate) and faster (1.3 s) in comparison, but there are many applications in which visual feedback is not desirable or available due to lighting conditions, visual overload, or visual impairments. Mean final precision with HapticHead feedback on invisible targets is 2.3° compared to 0.8° with visual feedback. We successfully navigated blindfolded users to real household items at different heights using HapticHead vibrotactile feedback independently of a head-mounted display.
            </abstract>
            <sections>
                <section>
                    <word_count>943</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>192</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>340</word_count>
                    <figure_citations>Figure 2) consists of a bathing cap with 17 vibration motors (Parallax, 12 mm coin type, 3.Figure 2, left).Figure 2).</figure_citations>
                    <section_index>2</section_index>
                    <title>INITIAL PROTOTYPE</title>
                </section>
                <section>
                    <word_count>3016</word_count>
                    <figure_citations>Figure 2, right).Figure 3).Figure 4 shows targets in front and back of the user for the auditory condition.Figure 5 show the measured dependent variables with merged data from all participants and all trials, not just successful ones.Figure 6 shows, compared to auditory feedback, targets off the horizontal plane worked much better with vibrotactile feedback.Figure 7 shows the selection time by angular distance between the starting orientation of the user and the orientation of each target.Figure 8 shows the selection time by yaw (horizontal heading) distance between the starting orientation and each target.Figure 9 shows the development of completion time over all trials.Figure 10).Figure 11 shows the success rates over time.Figure 12 and Figure 13).</figure_citations>
                    <section_index>3</section_index>
                    <title>EXPERIMENTS</title>
                </section>
                <section>
                    <word_count>2929</word_count>
                    <figure_citations>Figure 3) were used, but here with tiny white 1-pixel targets instead of the large ones (4 repetitions × 20 targets per user).Figure 15, participants agreed that the HapticHead vibrotactile feedback was helpful for finding virtual targets and most of the participants could intuitively map the feedback to the targets.Figure 15, participants found the vibrotactile feedback helpful for finding real targets around them and could intuitively map vibrotactile signals to targets, thus research question 2 can be answered positively.Figure 16).</figure_citations>
                    <section_index>4</section_index>
                    <title>REFINEMENT OF CONCEPT AND PROTOTYPE</title>
                </section>
                <section>
                    <word_count>184</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>63</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>LIMITATIONS</title>
                </section>
                <section>
                    <word_count>874</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Guidance</li>
                <li>Navigation</li>
                <li>Haptic feedback</li>
                <li>Vibrotactile</li>
                <li>Virtual reality</li>
                <li>Augmented reality</li>
                <li>Spatial interaction</li>
                <li>3D output</li>
            </keywords>
            <authors>
                <li>Oliver Beren Kaul</li>
                <li> Michael Rohs</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_1.jpg</url>
                <caption>Figure 1. Placement of actuators in HapticHead. Note the three
                    concentric ellipses around the user’s head and no actuators
                    close to the ear openings. The red ellipse contains 8 equidistant
                    actuators, the green and blue ellipses each contain 6 actuators.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_2.jpg</url>
                <caption>Figure 2. First HapticHead prototype (left) and Unity scene
                    view with visible targets from the outside (right). The user’s
                    camera is in the center.
                    We built a simple VR environment in Unity 5.3 that spawns
                    20 small (r = 1 m) equidistant spheres on the surface of a
                    larger (r = 5 m) invisible sphere with the viewer at its center
                    (see Figure 2). The spheres were distributed with pack.3.20
                    coordinates [27]. As the user rotates the head the location of
                    the spheres stays fixed with respect to the environment. The
                    target spheres do not coincide with the actuator positions.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_3.jpg</url>
                <caption>Figure 3. Attention funnels with a tiny red crosshair in the
                    view’s center. Visual feedback from the user’s perspective.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>5</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_5.jpg</url>
                <caption>Figure 5. Boxplots of completion times for all conditions
                    with merged data from all participants.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>7</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_7.jpg</url>
                <caption>Figure 7. Selection time by angular distance between the start
                    box and the target center. Visual (R2 = 0.89) and vibration (R2
                    = 0.59) conditions show a linear relationship.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>8</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_8.jpg</url>
                <caption>Figure 8. Selection time by yaw (horizontal heading) distance
                    between the start box and the target center. Visual (R2 = 0.86)
                    and vibrotactile (R2 = 0.72) conditions show a linear relation-
                    ship. The auditory condition is slow with targets in front of and
                    behind the user.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>9</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_9.jpg</url>
                <caption>Figure 9. Average learning effect – time. Merged data, all par-
                    ticipants. Curves: Gaussian weighted moving average
                    (width=3, blue=visual, green=vibrotactile, red=auditory).
                    and below the plane and roughly has a parabola shape. The
                    visual and vibrotactile conditions do not exhibit this effect.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>10</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_10.jpg</url>
                <caption>Figure 10. Qualitative results of Experiment 1. Diverging stacked bar chart: scales in percent, and absolute values on bars.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>11</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_11.jpg</url>
                <caption>Figure 11. Average learning effect - success rate. Merged data,
                    all participants. Curves: Gaussian weighted moving average
                    (width=3, blue=visual, green=vibrotactile, red=auditory).
                    Figure 11 shows the success rates over time. In the auditory
                    condition, participants had a steep learning curve, which flat-
                    tens after trial 40 but still shows a high variance compared to
                    the other conditions. Despite the fact that participants learned
                    to be a lot faster after trial 65 in the auditory condition, the
                    success rate did not drop but even increased a bit. In the vi-
                    brotactile condition, participants needed less than 15 trials to
                    accommodate themselves with this new form of feedback.
                    After the first few trials, the success rate curve for vibrotac-
                    tile feedback flattens and stays close to 97% without much
                    variance or measurable fatigue effects.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>12</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_12.jpg</url>
                <caption>Figure 12. Second, refined HapticHead prototype, side and
                    front view. Notice actuators located on the outside, the flexible
                    chinstrap, and five instead of three actuators on each, the fore-
                    head and chinstrap. Positions of the 10 forehead and chin ac-
                    tuators forming a “ring” around the face are marked in red.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>13</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_13.jpg</url>
                <caption>Figure 13. Side and front view of modeled actuator positions.
                    Does not fit perfectly due to arbitrary size and asymmetries of
                    the Styrofoam head. The refined guidance algorithm uses trian-
                    gles between actuators, including a virtual point zero (in green)
                    between the eyes.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>14</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_14.jpg</url>
                <caption>Figure 14. Intensity cal-
                    culation visualization.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>15</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_15.jpg</url>
                <caption>Figure 15. Qualitative results of Experiments 2 and 3. Diverging stacked bar chart: scales in percent, and absolute values on bars.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>16</id>
                <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_16.jpg</url>
                <caption>Figure 16. 10 items for Experiment 3. From left to right (height): Three boo
                    (1.7 m, 6 cm diameter), a screwdriver (0.8 m), a remote control (1.4 m) and
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR</filename>
        <data>
            <paper_id>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3026046</doi>
            <title>I Am The Passenger: How Visual Motion Cues Can Influence Sickness For In-Car VR</title>
            <abstract>
                This paper explores the use of VR Head Mounted Displays (HMDs) in-car and in-motion for the first time. Immersive HMDs are becoming everyday consumer items and, as they offer new possibilities for entertainment and productivity, people will want to use them during travel in, for example, autonomous cars. However, their use is confounded by motion sickness caused in-part by the restricted visual perception of motion conflicting with physically perceived vehicle motion (accelerations/rotations detected by the vestibular system). Whilst VR HMDs restrict visual perception of motion, they could also render it virtually, potentially alleviating sensory conflict. To study this problem, we conducted the first on-road and in motion study to systematically investigate the effects of various visual presentations of the real-world motion of a car on the sickness and immersion of VR HMD wearing passengers. We established new baselines for VR in-car motion sickness, and found that there is no one best presentation with respect to balancing sickness and immersion. Instead, user preferences suggest different solutions are required for differently susceptible users to provide usable VR in-car. This work provides formative insights for VR designers and an entry point for further research into enabling use of VR HMDs, and the rich experiences they offer, when travelling.
            </abstract>
            <sections>
                <section>
                    <word_count>794</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>4510</word_count>
                    <figure_citations>Figure 1) paired with a Samsung S7 smartphone (VR framework version 11, service 1 To see how conditions operated in-motion, view the attached video.</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED RESEARCH</title>
                </section>
                <section>
                    <word_count>2422</word_count>
                    <figure_citations>Figure 3, with sickness being in-part minimized by preference.Figure 4), we see how preferences were influenced by perceived sickness.</figure_citations>
                    <section_index>2</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>551</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>624</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>RECOMMENDATIONS FOR FURTHER RESEARCH</title>
                </section>
                <section>
                    <word_count>313</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>2298</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>In-motion</li>
                <li>In-car</li>
                <li>Automobile</li>
                <li>Autonomous Car</li>
                <li>Passenger</li>
                <li>Virtual Reality</li>
                <li>Mixed Reality</li>
                <li>Motion Sickness</li>
                <li>HMD</li>
            </keywords>
            <authors>
                <li>Mark McGill</li>
                <li> Alexander Ng</li>
                <li> Stephen Brewster</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR_crop_1.jpg</url>
                <caption>Figure 1. Left: Gear VR HMD used in study. Right: Peripheral blending
                    of Condition 6, combining motion landscape and 360° video.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>2</id>
                <url>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR_crop_2.jpg</url>
                <caption>Figure 2. 0.86km test route velocity proﬁle, as captured throughout the
                    study across participants using GPS and OBD2 velocity.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>3</id>
                <url>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR_crop_3.jpg</url>
                <caption>Figure 3. Stacked density plot (geom_density in R, using ..count.. and
                    “stack”) of motion sickness susceptibility against preferred condition
                    (higher is more susceptible), with labels indicating susceptibility per-
                    centiles for the general population from [41]. 50th%ile is considered
                    “slightly susceptible”, and 75th%ile “moderately susceptible”.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>4</id>
                <url>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR_crop_4.jpg</url>
                <caption>Figure 4. Plot of smoothed conditional mean real-time sickness ratings against condition and preference across participants (in black), with individual
                    participants plotted (in colour) and if they stopped the condition prematurely (coloured circles show time stopped).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>5</id>
                <url>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR_crop_5.jpg</url>
                <caption>Figure 5. Interaction plot of SSQ total sickness score against condition
                    and preference (higher is worse). Boxplots indicate the ﬁrst and third
                    quartiles (25th and 75th percentiles). Coloured lines indicate means.
                    Grey background line indicates the sickness score of the 2F64C ﬂight
                    simulator, suggested as indicating a problematic level of sickness [37].
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony</filename>
        <data>
            <paper_id>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300787</doi>
            <title>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony</title>
            <abstract>
                Previous research has shown that when White people embody a black avatar in virtual reality (VR) with full body visuomotor synchrony, this can reduce their implicit racial bias. In this paper, we put men in female and male avatars in VR with full visuomotor synchrony using wearable trackers and investigated implicit gender bias and embodiment. We found that participants embodied in female avatars displayed significantly higher levels of implicit gender bias than those embodied in male avatars. The implicit gender bias actually increased after exposure to female embodiment in contrast to male embodiment. Results also showed that participants felt embodied in their avatars regardless of gender matching, demonstrating that wearable trackers can be used for a realistic sense of avatar embodiment in VR. We discuss the future implications of these findings for both VR scenarios and embodiment technologies.
            </abstract>
            <sections>
                <section>
                    <word_count>492</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1701</word_count>
                    <figure_citations>Figure 1 in [2], it appears as though the trackers are worn only on the wrists of participants.Figure 1).</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1574</word_count>
                    <figure_citations>Figure 2).</figure_citations>
                    <section_index>2</section_index>
                    <title>SYNCHRONY AND AVATAR TRACKING</title>
                </section>
                <section>
                    <word_count>788</word_count>
                    <figure_citations>Figure 4) to get acquainted with their VR bodies.Figure 4).</figure_citations>
                    <section_index>3</section_index>
                    <title>METHODOLOGY</title>
                </section>
                <section>
                    <word_count>953</word_count>
                    <figure_citations>Figure 7 compares the ratings (from Table 2) of MyBody (Q1) compared to TwoBodies (Q2).Figure 8).</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1393</word_count>
                    <figure_citations>Figure 5) and that participants embodied in female avatars actually had a mean increase in postIAT scores compared to a decrease in those embodied in male avatars (Figure 6).Figure 6) demonstrates the strength of the findings.Figure 7 shows that the levels of body ownership are similar in both conditions.Figure 7 shows that even though participants embodied in female avatars did not feel that the avatar resembled them in appearance, they still felt strongly that the movements of the virtual avatar were caused by their own movements.</figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>193</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>FUTURE WORK</title>
                </section>
                <section>
                    <word_count>168</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>59</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1544</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality (VR)</li>
                <li>Embodied Avatars</li>
                <li>Implict Gender Bias</li>
                <li>Implicit Association Test (IAT)</li>
            </keywords>
            <authors>
                <li>Sarah Lopez</li>
                <li> Yi Yang</li>
                <li> Kevin Beltran</li>
                <li> Soo Jung Kim</li>
                <li> Jennifer Cruz Hernandez</li>
                <li> Chelsy Simran</li>
                <li> Bingkun Yang</li>
                <li> Beste F. Yuksel</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_1.jpg</url>
                <caption>Figure 1: Left: Participant wearing all nine HTC Vive track-
                    ers and headset (highlighted by red cirles). Right: HTC Vive
                    trackers, headset, and accompanying devices.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_2.jpg</url>
                <caption>Figure 2: Male and female avatars used for embodiment.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_3.jpg</url>
                <caption>Figure 3: VR and avatar setup: Participant’s physical body
                    (left) and tracked virtual body in female avatar (right). Par-
                    ticipant follows movements of the virtual Tai Chi Teacher.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_4.jpg</url>
                <caption>Figure 4: Top: Participant’s point of view when looking
                    down at virtual body in female (left) and male (right) avatar.
                    Bottom: VR setting with virtual mirrors to the front and left
                    of participant. Participant is following movements of vir-
                    tual Tai Chi teacher.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_5.jpg</url>
                <caption>Figure 5: Bar chart (means and standard errors) of ∆IAT by
                    embodiment (male or female). For those embodied in a male
                    avatar (red) the mean ± SE is -0.08 ± 0.010 and for those em-
                    bodied in a female avatar (green) 0.30 ± 0.115.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_6.jpg</url>
                <caption>Figure 6: For male-embodying participants (red), preIAT
                    mean ± SE is 0.255 ± 0.085 and postIAT mean ± SE is 0.179
                    ± 0.115. For female-embodying participants (green), preIAT
                    mean ± SE is 0.174 ± 0.138 and postIAT mean ± SE is 0.476 ±
                    0.117.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_7.jpg</url>
                <caption>Figure 7: Box plot of body ownership questions for top:
                    MyBody and TwoBodies and bottom: MyMovements and
                    MyFeatures. The thick black horizontal lines are the medi-
                    ans, the boxes are the interquartile ranges, and the whiskers
                    extend to ±1.5 x IQR, or the range. Individual points are out-
                    liers.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_8.jpg</url>
                <caption>Figure 8: Bar graph of participants’ subjective ratings on
                    physical attraction of male and female virtual avatars
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit </filename><data>
            <paper_id>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit </paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025479</doi>
            <title>Looking Inside the Wires: Understanding Museum Visitor Learning with an Augmented Circuit Exhibit</title>
            <abstract>Understanding electrical circuits can be difficult for novices of all ages. In this paper, we describe a science museum exhibit that enables visitors to make circuits on an interactive tabletop and observe a simulation of electrons flowing through the circuit. Our goal is to use multiple representations to help convey basic concepts of current and resistance. To study visitor interaction and learning, we tested the design at a popular science museum with 60 parent-child dyads in three conditions: a control condition with no electron simulation; a condition with the simulation displayed alongside the circuit on the same screen; and an augmented reality condition, with the simulation displayed on a tablet that acts as a lens to see into the circuit. Our findings show that children did significantly better on a post-test in both experimental conditions, with children performing best in the AR condition. However, analysis of session videos shows unexpected parent-child collaboration in the AR condition.</abstract>
            <sections>
                <section>
                    <word_count>832</word_count>
                    <figure_citations>Figure 1).Figure 2a), the simulation is displayed alongside the circuit on the same display.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>557</word_count>
                    <figure_citations>Figure 4).</figure_citations>
                    <section_index>1</section_index>
                    <title>BACKGROUND</title>
                </section>
                <section>
                    <word_count>245</word_count>
                    <figure_citations>Figure 3): a DC circuit simulator, an agent-based model of current flow, a virtual multimeter, and a brief textual description of basic concepts on electricity.Figure 4).</figure_citations>
                    <section_index>2</section_index>
                    <title>DESIGN OF SPARK</title>
                </section>
                <section>
                    <word_count>1710</word_count>
                    <figure_citations>Figure 3).Figure 5).Figure 5).Figure 6).Figure 7 illustrates two snapshots of the synchronized video recordings.</figure_citations>
                    <section_index>3</section_index>
                    <title>METHODS</title>
                </section>
                <section>
                    <word_count>2903</word_count>
                    <figure_citations>Figure 5), families are shown two circuits; the first circuit has one lightbulb, and the second circuit has one resistor and one bulb after the resistor (assuming a counter-clockwise direction of current flow).Figure 8 shows the differences in usage of incorrect and correct conceptions for current path in each condition.Figure 9).Figure 10 shows the number of incorrect and correct predictions in each condition.Figure 11 illustrates the distribution of parental roles in each group.</figure_citations>
                    <section_index>4</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>451</word_count>
                    <figure_citations>Figure 12), similar to the digital circuit simulator.</figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>145</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>66</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1413</word_count>
                    <figure_citations>Figure 12).</figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Electrical circuits</li>
                <li>Multiple representations</li>
                <li>Augmented reality</li>
                <li>Agent-based modeling</li>
                <li>Design</li>
                <li>Interactive surfaces</li>
                <li>Museum learning</li>
            </keywords>
            <authors>
                <li>Elham Beheshti</li>
                <li> David Kim</li>
                <li> Gabrielle Ecanow</li>
                <li> Michael S. Horn</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_1.jpg</url>
                <caption>Figure 1. Spark interactive tabletop exhibit.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_2.jpg</url>
                <caption>Figure 2. Linking two representations of a circuit (a) side-by-
                    side on one display and (b) with AR on a second display.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_3.jpg</url>
                <caption>Figure 3. Snapshot of Spark system in the single-display
                    condition.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_4.jpg</url>
                <caption>Figure 4. Electron model display in the AR condition. The
                    blue dots are moving electrons and the red dots represent ions
                    in conductive materials. Resistors have higher ion densities
                    than wires.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_5.jpg</url>
                <caption>Figure 5. Final task (task 3) in the series of tasks.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_6.jpg</url>
                <caption>Figure 6. Post-interview questions and two examples of
                    children’s drawings to show the flow of current.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_7.jpg</url>
                <caption>Figure 7. Snapshots of parent-child dyads using the exhibit in
                    the single-display condition (left) and the AR condition (right)
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_8.jpg</url>
                <caption>Figure 8. Children’s mental models of current flow path in
                    control condition (C1); single-display condition (C2); and AR
                    condition (C3).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_9.jpg</url>
                <caption>Figure 9. Children’s mental models of mechanism of current
                    flow with cyclic model and concurrent model grouped as
                    ”progressed” conceptions.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>10</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_10.jpg</url>
                <caption>Figure 10. Parent-child dyad predictions for task 3.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>11</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_11.jpg</url>
                <caption>Figure 11. Different types of parental roles in task 2 in the
                    single-display condition (C2) and the AR condition (C3).
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>12</id>
                <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_12.jpg</url>
                <caption>Figure 12. A demonstration of Tangible Spark.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>MagicFace- Stepping into Character through an Augmented Reality Mirror</filename>
        <data>
            <paper_id>MagicFace- Stepping into Character through an Augmented Reality Mirror</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025722</doi>
            <title>MagicFace: Stepping into Character through an Augmented Reality Mirror</title>
            <abstract>
                Augmented Reality (AR) is coming of age and appearing in various smartphone apps. One emerging AR type uses the front-facing camera and overlays a user's face with digital features that transform the physical appearance, making the user look like someone else, such as a popstar or a historical character. However, little is known about how people react to such stepping into character and how convincing they perceive it to be. We developed an app with two Egyptian looks, MagicFace, which was situated both in an opera house and a museum. In the first setting, people were invited to use the app, while in the second setting they came across it on their own when visiting the exhibition. Our findings show marked differences in how people approach and experience the MagicFace in these different contexts. We discuss how realistic and compelling this kind of AR technology is, as well as its implications for educational and cultural settings.
            </abstract>
            <sections>
                <section>
                    <word_count>817</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>917</word_count>
                    <figure_citations>Figure 3).</figure_citations>
                    <section_index>1</section_index>
                    <title>BACKGROUND</title>
                </section>
                <section>
                    <word_count>108</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>RESEARCH AIMS</title>
                </section>
                <section>
                    <word_count>4934</word_count>
                    <figure_citations>Figure 1).Figure 2).</figure_citations>
                    <section_index>3</section_index>
                    <title>MAGICFACE APP AND INSTALLATION</title>
                </section>
                <section>
                    <word_count>1229</word_count>
                    <figure_citations>Figure 5), turning their heads in different directions and pouting to watch themselves with the virtual looks from various angles.</figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>346</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>44</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>701</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Augmented reality</li>
                <li>Opera characters</li>
                <li>Interface design</li>
                <li>Inthe-wild study</li>
            </keywords>
            <authors>
                <li>Ana Javornik</li>
                <li> Yvonne Rogers</li>
                <li> Delia Gander</li>
                <li> Ana Moutinho</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>MagicFace- Stepping into Character through an Augmented Reality Mirror_crop_1.jpg</url>
                <caption>Figure 1. Four landing pages: (i) screen saver that is automatically replaced with virtual make-up once a user appears in front of
                    the camera (first from the left); (ii) screen saver with buttons that a user clicks on to see the virtual make-up (second from the left);
                    (iii) screen saver is automatically replaced with the virtual mirror with the buttons for switching between the two looks (third from
                    the left); (iv) screen saver with the choice of two names that the user taps on to see the virtual looks (fourth from the left)
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>2</id>
                <url>MagicFace- Stepping into Character through an Augmented Reality Mirror_crop_2.jpg</url>
                <caption>Figure 2: Info page that dropped down if an info button in the
                    upper right corner of the virtual mirror was tapped
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>MagicFace- Stepping into Character through an Augmented Reality Mirror_crop_3.jpg</url>
                <caption>Figure 3. Different groups of visitors trying on virtual make-up in the opera dressing room: opera singer (first from the left);
                    pupil (second from the left); make-up artist (third from the left), opera singer (fourth from the left).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>MagicFace- Stepping into Character through an Augmented Reality Mirror_crop_5.jpg</url>
                <caption>Figure 5: A museum visitor doing an Egyptian walk while
                    interacting with the MagicFace
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality</filename>
        <data>
            <paper_id>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300897</doi>
            <title>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality</title>
            <abstract>
                Millions of photos are shared online daily, but the richness of interaction compared with face-to-face (F2F) sharing is still missing. While this may change with social Virtual Reality (socialVR), we still lack tools to measure such immersive and interactive experiences. In this paper, we investigate photo sharing experiences in immersive environments, focusing on socialVR. Running context mapping (N=10), an expert creative session (N=6), and an online experience clustering questionnaire (N=20), we develop and statistically evaluate a questionnaire to measure photo sharing experiences. We then ran a controlled, within-subject study (N=26 pairs) to compare photo sharing under F2F, Skype, and Facebook Spaces. Using interviews, audio analysis, and our questionnaire, we found that socialVR can closely approximate F2F sharing. We contribute empirical findings on the immersiveness differences between digital communication media, and propose a socialVR questionnaire that can in the future generalize beyond photo sharing.
            </abstract>
            <sections>
                <section>
                    <word_count>863</word_count>
                    <figure_citations>Figure 1), use a 2D real-time user video as a virtual view of that user, or in the near future, use a highly detailed photo-realistic point cloud video [44].Figure 2) 1 https://www.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>2994</word_count>
                    <figure_citations>Figure 2) was to identify typical experiences of photo sharing activities, and to develop a questionnaire for measuring socialVR photo sharing experiences later used in Part 2.Figure 3 illustrates the five phases with an example of a timeline, where actions were coded with blue dots and interpreted with a phrase.Figure 3) identified in the context mapping sessions were introduced.</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1455</word_count>
                    <figure_citations>Figure 2), we present a within-subject controlled user study compare photo sharing experiences in three conditions: face-to-face (F2F), Facebook Spaces (FBS), Skype (SKP).Figure 5).</figure_citations>
                    <section_index>2</section_index>
                    <title>STUDY</title>
                </section>
                <section>
                    <word_count>2201</word_count>
                    <figure_citations>Figure 6a.Figure 6b.Figure 6c.Figure 6d), which shows FBS was perceived to be more exciting and cheerful than F2F and SKP.Figure 6d.</figure_citations>
                    <section_index>3</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1188</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>161</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>FUTURE WORK AND CONCLUSION</title>
                </section>
                <section>
                    <word_count>36</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>2086</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Social</li>
                <li>Virtual reality</li>
                <li>Photo sharing</li>
                <li>Questionnaire</li>
                <li>Immersion</li>
                <li>Presence</li>
            </keywords>
            <authors>
                <li>Jie Li</li>
                <li> Yiping Kong</li>
                <li> Thomas Röggla</li>
                <li> Francesca De Simone</li>
                <li> Swamy Ananthanarayan</li>
                <li> Huib de Ridder</li>
                <li> Abdallah El Ali</li>
                <li> Pablo Cesar</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_1.jpg</url>
                <caption>Figure 1: Photo sharing experience in Facebook Spaces© with
                    remote partners (a) and (b) looking at the same photo.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_2.jpg</url>
                <caption>Figure 2: Methodological approach for constructing our So-
                    cialVR questionnaire.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_3.jpg</url>
                <caption>Figure 3: An example of experience timeline and five phases
                    of actions during face-to-face photo sharing.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_4.jpg</url>
                <caption>Figure 4: Correspondence analysis, showing three distinct
                    experience components, along with the 20 (numerically dis-
                    played) interactions.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_5.jpg</url>
                <caption>Figure 5: Illustrations of the three experimental conditions:
                    (a) Face-to-face (b) Skype (c) FB Spaces.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_6.jpg</url>
                <caption>Figure 6: (a)-(c) Sum of scores boxplots for across photo sharing conditions for face-to-face (F2F), FB Spaces (FBS), Skype (SKP).
                    (d) Self-reported emotion ratings for each condition. ** = p&lt;.001 </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Personalising the TV Experience using Augmented</filename>
        <data>
            <paper_id>Personalising the TV Experience using Augmented</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300762</doi>
            <title>Personalising the TV Experience using Augmented Reality: An Exploratory Study on Delivering Synchronised Sign Language Interpretation</title>
            <abstract>
                Augmented Reality (AR) technology has the potential to extend the screen area beyond the rigid frames of televisions. The additional display area can be used to augment televisions (TVs) with extra information tailored to individuals, for instance, the provision of access services like sign language interpretations. We invited 23 (11 in the UK, 12 in Germany) users of signed content to evaluate three methods of watching a sign language interpreted programme - one traditional in-vision method with signed programme content on TV and two AR-enabled methods in which an AR sign language interpreter (a 'half-body' version and a 'full-body' version) is projected just outside the frame of the TV presenting the programme. In the UK, participants were split 3-ways in their preferences while in Germany, half the participants preferred the traditional method followed closely by the 'half-body' version. We discuss our participants reasoning behind their preferences and implications for future research.
            </abstract>
            <sections>
                <section>
                    <word_count>305</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>877</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1230</word_count>
                    <figure_citations>Figure 1).Figure 1).Figure 1): 1.</figure_citations>
                    <section_index>2</section_index>
                    <title>RESEARCH OBJECTIVES</title>
                </section>
                <section>
                    <word_count>1597</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>METHOD</title>
                </section>
                <section>
                    <word_count>947</word_count>
                    <figure_citations>Figure 2 show descriptive statistics on participant responses.Figure 3a.Figure 3b illustrates participant responses to questions on the acceptance of the TV+HoloLens system.Figure 3c) illustrates the distribution of responses of our participants.</figure_citations>
                    <section_index>4</section_index>
                    <title>QUANTITATIVE RESULTS</title>
                </section>
                <section>
                    <word_count>1748</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>QUALITATIVE RESULTS</title>
                </section>
                <section>
                    <word_count>903</word_count>
                    <figure_citations>Figure 3b and Figure 3c).</figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>31</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1479</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Accessibility</li>
                <li>Augmented Reality</li>
                <li>BSL</li>
                <li>SSE</li>
                <li>Companion Screen</li>
                <li>Connected Experiences</li>
                <li>DGS</li>
                <li>HbbTV 2.0</li>
                <li>HoloLens</li>
                <li>Interaction Techniques</li>
                <li>Personalisation</li>
                <li>Second Screen</li>
                <li>Sign Language</li>
                <li>Synchronisation</li>
                <li>Television</li>
            </keywords>
            <authors>
                <li>Vinoba Vinayagamoorthy</li>
                <li> Maxine Glancy</li>
                <li> Christoph Ziegler</li>
                <li> Richard Schäffer</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Personalising the TV Experience using Augmented_crop_1.jpg</url>
                <caption>Figure 1: Conditions in our study (Left column: The UK,
                    Right column: Germany; Top images: traditional control in-
                    vision condition (T), Middle images: ‘half-body’ AR condi-
                    tions (H), Bottom images: ‘full-body’ AR conditions (F)
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Personalising the TV Experience using Augmented_crop_2.jpg</url>
                <caption>Figure 2: Descriptive statistics on responses to post condition questions [39]: max: maximum, 3rd: third quartile, 2nd: median,
                    1st: first quartile, min: minimum. Solid boxes indicate where Friedman test indicates an influence of method (T: traditional,
                    H: half-body, F: full-body). Dashed boxes indicate where Mann-Whitney U test indicate a difference across countries.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Personalising the TV Experience using Augmented_crop_3.jpg</url>
                <caption>Figure 3: a) Participant preference of the methods; b) Responses on questions about the acceptance of the TV+HoloLens system;
                    c) Responses to the question on whether the HoloLens changed the television experience.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation </filename>
        <data>
            <paper_id>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation </paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025600</doi>
            <title>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation</title>
            <abstract>We explore how to add haptics to walls and other heavy objects in virtual reality. When a user tries to push such an object, our system actuates the user's shoulder, arm, and wrist muscles by means of electrical muscle stimulation, creating a counter force that pulls the user's arm backwards. Our device accomplishes this in a wearable form factor.In our first user study, participants wearing a head-mounted display interacted with objects provided with different types of EMS effects. The repulsion design (visualized as an electrical field) and the soft design (visualized as a magnetic field) received high scores on "prevented me from passing through" as well as "realistic".In a second study, we demonstrate the effectiveness of our approach by letting participants explore a virtual world in which all objects provide haptic EMS effects, including walls, gates, sliders, boxes, and projectiles.</abstract>
            <sections>
                <section>
                    <word_count>387</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>555</word_count>
                    <figure_citations>Figure 1a shows an example.Figure 1b illustrates how our system implements the physicality of the cube, i.Figure 2 illustrates the idea in more detail.Figure 3, our system stimulates up to four different muscle groups.Figure 4 illustrates the naïve approach to rendering objects using EMS: (a) From the moment the user’s fingertips reach the virtual wall, we actuate the user’s hand just strongly enough to prevent it from passing through.Figure 22 in the Implementation section), which we control via USB from within our VR simulators.</figure_citations>
                    <section_index>1</section_index>
                    <title>ELECTRICAL MUSCLE STIMULATION HAPTICS FOR VR</title>
                </section>
                <section>
                    <word_count>2065</word_count>
                    <figure_citations>Figure 1, which is designed to suggest an increasingly solid inside under a soft, permeable surface.Figure 5 shows the same concept wrapped in visuals suggesting a magnetic field, suggesting a magnetic force that carefully pushes the user’s hand backwards.Figure 6 shows what we call electro visuals.Figure 6).Figure 7 shows the five “walls” arranged in a pentagon with the participant inside.Figure 8, eight participants picked the repulsion wall as their favorite.Figure 9 shows how participants rated the five conditions with respect to the question “what I feel matches what I see.Figure 10 shows participants’ assessment of “this wall was able to prevent me from passing through”.Figure 11, measured using our optical tracking system Optitrack 17w).</figure_citations>
                    <section_index>2</section_index>
                    <title>DESIGN</title>
                </section>
                <section>
                    <word_count>215</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>BENEFITS AND CONTRIBUTION</title>
                </section>
                <section>
                    <word_count>1969</word_count>
                    <figure_citations>Figure 12, this room is designed as a jail cell with an “electrified” gate and walls.Figure 15, users operate a pair of traditional sliders in order to align the pipeline elements that establish a hydraulic link.Figure 15).Figure 16, their hand is pushed backwards by the water’s viscosity.Figure 18), they feel a resistance when their hands come together as to grasp the cube.Figure 18, we see how users pick up the cube and throw it over a glass wall, down a chute, which activates a second button.Figure 19 shows a third cube that rests on a slide, which leads up to the last of the three buttons.Figure 20, the EMS condition received substantially higher ratings.Figure 21).</figure_citations>
                    <section_index>4</section_index>
                    <title>EXAMPLE WIDGETS</title>
                </section>
                <section>
                    <word_count>363</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>696</word_count>
                    <figure_citations>Figure 23 depicts the exact electrode placement we used in our prototypes to actuate the user’s arm and hand.</figure_citations>
                    <section_index>6</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>161</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>1256</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Muscle interfaces</li>
                <li>Virtual reality</li>
                <li>EMS</li>
                <li>Force feedback</li>
            </keywords>
            <authors>
                <li>Pedro Lopes</li>
                <li> Sijing You</li>
                <li> Lung-Pan Cheng</li>
                <li> Sebastian Marwecki</li>
                <li> Patrick Baudisch</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_1.jpg</url>
                <caption>Figure 1: (a) As this user lifts a virtual cube, our system
                    lets the user feel the weight and resistance of the cube.
                    (b) Our system implements this by actuating the user’s
                    opposing muscles using electrical muscle stimulation.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_2.jpg</url>
                <caption>Figure 2: (a) When a user picks up a physical cube, its
                    weight causes tension in the user’s biceps. (b) Our sys-
                    tem creates this tension by instead actuating the oppos-
                    ing muscles, here the user’s triceps and shoulders.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_3.jpg</url>
                <caption>Figure 3: We use up to 8 electrode pairs, actuating
                    (a) wrist, (b) biceps, (c) triceps, and (d) shoulders.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_4.jpg</url>
                <caption>Figure 4: Implementing rigid walls requires stimulating
                    muscles with strong impulses over long periods. This
                    draws undesired attention to the electrical stimulation.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_5.jpg</url>
                <caption>Figure 5: The magnetic visuals allow the user’s hand to
                    penetrate the surfaces of objects.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_6.jpg</url>
                <caption>Figure 6: The electro visuals.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_7.jpg</url>
                <caption>Figure 7: Study participant in the virtual world of the
                    study, here facing the vibro only condition.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>11</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_11.jpg</url>
                <caption>Figure 11: The repulsion wall stopped participants’
                    hands on average 3.6 cm (error bars denote std. dev.).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>12</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_12.jpg</url>
                <caption>Figure 12: The jail cell features “electrified” walls and a
                    gate. Touching any of these repel the user’s hand.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>13</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_13.jpg</url>
                <caption>Figure 13: Pushing this soft button opens the gate.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>14</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_14.jpg</url>
                <caption>Figure 14: Projectiles based on the repulsion effect.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>15</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_15.jpg</url>
                <caption>Figure 15: The user is dragging the knob of a slider
                    mechanism. The two buttons on the left form a rocker.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>16</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_16.jpg</url>
                <caption>Figure 16: Playing with the water in these fish tanks
                    pushes the user’s hand backwards.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>17</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_17.jpg</url>
                <caption>Figure 17: Two cubes that users can push onto the
                    button on the right, or pick up and carry around.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>18</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_18.jpg</url>
                <caption>Figure 18: The user has picked up a cube and is about
                    to throw it over the glass barrier.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>20</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_20.jpg</url>
                <caption>Figure 20: Participants rated their experience in the
                    EMS condition higher than in baseline.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>21</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_21.jpg</url>
                <caption>Figure 21: Participants preferred the experience in the
                    EMS condition.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>22</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_22.jpg</url>
                <caption>Figure 22: The muscle stimulator we used.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>23</id>
                <url>Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_23.jpg</url>
                <caption>Figure 23: Electrode placement for arm and shoulder.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives</filename>
        <data>
            <paper_id>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300673</doi>
            <title>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives</title>
            <abstract>Virtual reality (VR) strives to replicate the sensation of the physical environment by mimicking people's perceptions and experience of being elsewhere. These experiences are of-ten mediated by the objects and tools we interact with in the virtual world (e.g., a controller). Evidence from psychology posits that when using the tool proficiently, it becomes em-bodied (i.e., an extension of one's body). There is little work,however, on how to measure this phenomenon in VR, andon how different types of tools and controllers can affect the experience of interaction. In this work, we leverage cognitive psychology and philosophy literature to construct the Locus-of-Attention Index (LAI), a measure of tool embodiment. We designed and conducted a study that measures readiness-to-hand and unreadiness-to-hand for three VR interaction techniques: hands, a physical tool, and a VR controller. The study shows that LAI can measure differences in embodiment with working and broken tools and that using the hand directly results in more embodiment than using controllers.</abstract>
            <sections>
                <section>
                    <word_count>543</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>4243</word_count>
                    <figure_citations>Figure 1).Figure 2) wearing the Oculus Rift and holding one of three controllers (see Conditions).Figure 3) to successively find and grab letter cubes from the bookshelf and place them (by releasing with the tool) in the placeholders marked with the corresponding letter.Figure 3).Figure 4c shows the physical apparatus for this condition.Figure 4b).Figure 5a).Figure 5a.Figure 6).Figure 7).</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1242</word_count>
                    <figure_citations>Figure 9 shows the distribution of the indices split by interaction technique, state and phase.Figure 10 shows the distributions of the GEQ scores by technique, state and phase.</figure_citations>
                    <section_index>2</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>738</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>GENERAL DISCUSSION</title>
                </section>
                <section>
                    <word_count>83</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>42</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1224</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Embodiment</li>
                <li>Tool Embodiment</li>
                <li>Embodied Interaction</li>
                <li>Virtual Reality</li>
                <li>Ready-to-hand</li>
                <li>Unready-to-hand</li>
                <li>Tools</li>
            </keywords>
            <authors>
                <li>Ayman Alzayat</li>
                <li> Mark Hancock</li>
                <li> Miguel A. Nacenta</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_1.jpg</url>
                <caption>Figure 1: Our operationalization of tool embodiment relies
                    on the idea that, when embodied with a tool, a person’s
                    attention will be on the task, rather than on the tool.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_2.jpg</url>
                <caption>Figure 2: A photograph (a) and diagram (b) of the physical study setup.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_3.jpg</url>
                <caption>Figure 3: shows the virtual tool used in our study
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_4.jpg</url>
                <caption>Figure 4: The three controllers used in our study.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_5.jpg</url>
                <caption>Figure 5: Virtual study Scene Setup. a. Virtual study scene setup with a virtual tool, bookshelf, task plates, and information
                    display. b. The virtual study scene when the pause button is pressed. Numbers on visual dots support the secondary task.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_6.jpg</url>
                <caption>Figure 6: From left to right, example of breakage (cube falls).
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_7.jpg</url>
                <caption>Figure 7: Our LAI measure corresponds to our theory above
                    and measures across the entirety of a trial whether the
                    attention is on the tool (negative) or on the task (positive).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_8.jpg</url>
                <caption>Figure 8: SLAI measures a shift in attention from either the
                    task to the tool (e.g., when a tool breaks) or vice versa.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_9.jpg</url>
                <caption>Figure 9: Results for the locus of attention index (LAI). There
                    were main effects of interaction technique, state of the
                    tool (working/broken) and phase. Note only [0,1] range of
                    possible [-1,1] range is represented.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>10</id>
                <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_10.jpg</url>
                <caption>Figure 10: Results for engagement (GEQ). There was a signif-
                    icant main effect of state (working/broken) and a marginally
                    significant effect of interaction technique.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality</filename>
        <data>
            <paper_id>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025688</doi>
            <title>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality</title>
            <abstract>A video tutorial effectively conveys complex motions, but may be hard to follow precisely because of its restriction to a predetermined viewpoint. Augmented reality (AR) tutorials have been demonstrated to be more effective. We bring the advantages of both together by interactively retargeting conventional, two-dimensional videos into three-dimensional AR tutorials. Unlike previous work, we do not simply overlay video, but synthesize 3D-registered motion from the video. Since the information in the resulting AR tutorial is registered to 3D objects, the user can freely change the viewpoint without degrading the experience. This approach applies to many styles of video tutorials. In this work, we concentrate on a class of tutorials which alter the surface of an object.</abstract>
            <sections>
                <section>
                    <word_count>923</word_count>
                    <figure_citations>Figure 1 demonstrates drawing thick dots.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>732</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>508</word_count>
                    <figure_citations>Figure 2), or the user interactively models the 3D object of interest.Figure 2 (left) the system recognizes the face and automatically registers a deformable face mesh.Figure 2 (middle), we retarget the trajectory of the make-up brush by registering the face mesh along with its previously generated 2D texture to the user’s face.Figure 2 (right), we abstract the motion by using one arrow for each segment of the trajectory.Figure 2 (right), resulting in a presentation which uses arrows along the outline of the trajectory (see Figure 7).</figure_citations>
                    <section_index>2</section_index>
                    <title>OVERVIEW</title>
                </section>
                <section>
                    <word_count>1223</word_count>
                    <figure_citations>Figure 2, a deformable model can be tracked by determining an update to the deformation parameters in each frame.Figure 3).Figure 3).Figure 3(b)).</figure_citations>
                    <section_index>3</section_index>
                    <title>EXTRACTION</title>
                </section>
                <section>
                    <word_count>608</word_count>
                    <figure_citations>Figure 2 were originally applied to a jewelry box, but later retargeted to a teapot.Figure 4(c).</figure_citations>
                    <section_index>4</section_index>
                    <title>EDITING</title>
                </section>
                <section>
                    <word_count>458</word_count>
                    <figure_citations>Figure 2, middle).Figure 6(b)), followed by an additional segmentation of the paths into smaller segments.Figure 6(c) uses a threshold of 90◦ ).Figure 6(c)).</figure_citations>
                    <section_index>5</section_index>
                    <title>VISUALIZATION</title>
                </section>
                <section>
                    <word_count>1539</word_count>
                    <figure_citations>Figure 4 illustrates the painting tutorial.Figure 5 illustrates the video tutorial.Figure 5(a)).Figure 5(b).Figure 6(d)).Figure 6(d)).</figure_citations>
                    <section_index>6</section_index>
                    <title>EVALUATING THE AUTHORING</title>
                </section>
                <section>
                    <word_count>1534</word_count>
                    <figure_citations>Figure 7(c)).Figure 7(a)).Figure 7(a)).Figure 7).Figure 7).Figure 7(b)).Figure 7(b)).Figure 8 shows the boxplots of the measurements.</figure_citations>
                    <section_index>7</section_index>
                    <title>EVALUATING EFFICIENCY OF AR KANJI TUTORIAL</title>
                </section>
                <section>
                    <word_count>75</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>SUS</title>
                </section>
                <section>
                    <word_count>796</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>DISCUSSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>36</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1419</word_count>
                    <figure_citations></figure_citations>
                    <section_index>11</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Augmented reality</li>
                <li>Virtual reality</li>
                <li>Retargeting</li>
                <li>Video tutorial</li>
            </keywords>
            <authors>
                <li>Peter Mohr</li>
                <li> David Mandl</li>
                <li> Markus Tatzgern</li>
                <li> Eduardo Veas</li>
                <li> Dieter Schmalstieg</li>
                <li> Denis Kalkofen</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_1.jpg</url>
                <caption>Figure 1. Retargeting a ’henna decoration’ video tutorial to a teapot decoration scenario in the user’s workspace. (left) The user extracts relevant
                    motion from the video, (middle) scales it and aligns the result to a 3D scan of a teapot in the current workspace. With our editing tools, the user can
                    quickly alter the original tutorial to meet their requirements. In this example, the original video tutorial shows a decoration consisting of dots, which
                    requires a special henna pen. The user chooses to connect the dots into lines which can be drawn with a ceramic pen on the teapot. The user also scales
                    down the entire ornament to better ﬁt the desired aesthetics. (right) Using augmented reality, the user validates the result directly on the real teapot.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_2.jpg</url>
                <caption>Figure 2. System overview. (left) We extract object and user motions by tracking known model features in the 2D video. Here, tracked features are
                    used to record the path of the brush and to align a face model in each frame. (middle) After validating and possibly editing the extracted motion, we
                    retarget the motion data to real world 3D objects. This requires registering the same 3D model as used in the extraction stage, in this case, a face model,
                    to the live camera image. By tracking the model in 3D, we are able to relate video data to the real world. In this example, we present the recorded path
                    of the brush directly on the user’s face. (right) Since we retarget the extracted motion data in 3D, we can choose an arbitrary point of view. To provide
                    effective visual instructions, we generate dynamic glyphs (here: timed arrows) and we indicate the position of the brush over time using a red circle.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_3.jpg</url>
                <caption>Figure 3. Extracting motion with surface contact. (a) We extract the 2D trajectory by tracking the tip of the tool in unwrapped texture space. (b) We
                    convert the 2D trajectory to 3D by back-projecting the video data to a corresponding 3D model, in this case, a face.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_4.jpg</url>
                <caption>Figure 4. Segmentation and Layering. (a) We interactively segment the input data by selecting starting and ending frames. (b) This results in a set of
                    actions (red), which we can use to derive image layers.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_5.jpg</url>
                <caption>Figure 5. Experiment setup for a retargeted make-up tutorial. (a) Input video tutorial. (b) We showed the resulting AR tutorial using an AR mirror,
                    which consisted of a camera and an USB display. (c) Participants could use the AR mirror and the video which we placed next to the mirror.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_6.jpg</url>
                <caption>Figure 6. Path generation and ﬁrst revision of AR visualization. (a) We generate path illustrations from motion capture data. (b) The extracted path
                    data is analyzed and simpliﬁed. In particular, we remove zig-zag overdraw along the trajectory by clustering and detect turning points (marked in
                    green). (c) We generate arrows in-between turning points, the start point and end point. (d) At runtime, we use the arrows to provide a preview of the
                    motions. To minimize occlusion, the arrow is replaced by the border of the tool’s trajectory. The red dot shows the extracted tool position over time. (e)
                    The combination of visualization techniques provide an overview ﬁrst, before the user can follow the exact motion.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_7.jpg</url>
                <caption>Figure 7. Retargeted Kanji tutorial and ﬁnal revision of AR visualization. (a) The AR visualization is presented using an Optical See-Through HMD
                    (Microsoft Hololens) and a handheld clicker that the user is holding in one hand. (b) The video tutorial is shown on a tablet mounted right above the
                    drawing area. This reduced the inﬂuence of head motion. (c) Our ﬁnal glyph design encodes the direction of the stroke on its border using arrow heads.
                    The system presents one glyph at a time next to a full preview of the ﬁnal drawing. This picture shows the six instructions presented to the user in AR.
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_8.jpg</url>
                <caption>Figure 8. Kanji study results. Stars indicate signiﬁcant differences.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users</filename>
        <data>
            <paper_id>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025683</doi>
            <title>ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users</title>
            <abstract>
                Virtual reality (VR) head-mounted displays (HMD) allow for a highly immersive experience and are currently becoming part of the living room entertainment. Current VR systems focus mainly on increasing the immersion and enjoyment for the user wearing the HMD (HMD user), resulting in all the bystanders (Non-HMD users) being excluded from the experience. We propose ShareVR, a proof-of-concept prototype using floor projection and mobile displays in combination with positional tracking to visualize the virtual world for the Non-HMD user, enabling them to interact with the HMD user and become part of the VR experience. We designed and implemented ShareVR based on the insights of an initial online survey (n=48) with early adopters of VR HMDs. We ran a user study (n=16) comparing ShareVRto a baseline condition showing how the interaction using ShareVR led to an increase of enjoyment, presence and social interaction. In a last step we implemented several experiences for ShareVR, exploring its design space and giving insights for designers of co-located asymmetric VR experiences.
            </abstract>
            <sections>
                <section>
                    <word_count>829</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>339</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1111</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>ENVISIONED SCENARIO</title>
                </section>
                <section>
                    <word_count>786</word_count>
                    <figure_citations>Figure 2).</figure_citations>
                    <section_index>3</section_index>
                    <title>ONLINE SURVEY</title>
                </section>
                <section>
                    <word_count>980</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>SHAREVR CONCEPT AND IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>1745</word_count>
                    <figure_citations>Figure 4).</figure_citations>
                    <section_index>5</section_index>
                    <title>IMPLEMENTED EXPERIENCES</title>
                </section>
                <section>
                    <word_count>1434</word_count>
                    <figure_citations>Figure 7 summarizes the collected data of the GEQ and SUS Experiences with Virtual Reality CHI 2017, May 6–11, 2017, Denver, CO, USA Figure 7.Figure 8 shows an overview of the final comparison (enjoyment, presence and social interaction).</figure_citations>
                    <section_index>6</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1338</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>DESIGN SPACE AND GUIDELINES</title>
                </section>
                <section>
                    <word_count>269</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>22</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>2065</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Co-located virtual reality</li>
                <li>ShareVR</li>
                <li>Asymmetric virtual reality</li>
                <li>Multi-user virtual reality</li>
                <li>Consumer virtual reality</li>
            </keywords>
            <authors>
                <li>Jan Gugenheimer</li>
                <li> Evgeny Stemasov</li>
                <li> Julian Frommel</li>
                <li> Enrico Rukzio</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_1.jpg</url>
                <caption>Figure 1. ShareVR enables co-located asymmetric interaction between users wearing an HMD and users without an HMD. ShareVR uses a tracked
                    display (a, e) as a window into the virtual world and a ﬂoor projection to visualize the virtual environment to all Non-HMD users. It enables collaborative
                    experiences such as exploring a dungeon together (b), drawing (h), sports (c) or solving puzzles (e, f) as well as competitive experiences such as “Statues” (d)
                    or a swordﬁght (g). ShareVR facilitates a shared physical and virtual space, increasing the presence and enjoyment for both HMD and Non-HMD users.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_2.jpg</url>
                <caption>Figure 2. An excerpt from our online survey on the questions: (a) “When demonstrating my VR headset to friends and family I tend to:”, (b) “Assuming
                    that you own and actively use only one headset, please rate the following statements:”, (c) “A technology which would allow me to actively inﬂuence the
                    virtual environment of the immersed user should...” (Note: the statements are shortened and rephrased to ﬁt into one ﬁgure.)
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_3.jpg</url>
                <caption>Figure 3. Left: Display mounted on the controller of the Non-HMD user.
                    Right: Physical setup of ShareVR, replicating a living-room layout
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_4.jpg</url>
                <caption>Figure 4. Two users (a: handheld view, b: HMD view) ﬁghting monsters
                    in the caves of BeMyLight. Note that the HMD user (b) only can see where
                    the Non-HMD user (a) shines light on.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_5.jpg</url>
                <caption>Figure 5. Two users playing SneakyBoxes and their individual views:
                    (a) handheld (b) inside the HMD. Note that the HMD user (b) can not
                    distinguish between a regular box and the Non-HMD box.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_6.jpg</url>
                <caption>Figure 6. An overview of the individual applications with overlaid visualizations from the Sandbox: (a) throwing a ball to the HMD user in the soccer
                    application, (b) instructing the HMD user in the puzzle application, (c) drawing a palm tree together and (d) having a lightsaber duel.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_7.jpg</url>
                <caption>Figure 7. Averages (with standard deviation) of the positive experiences
                    subscale (GEQ), behavioural involvement (GEQ) and presence (SUS).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>8</id>
                <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_8.jpg</url>
                <caption>Figure 8. Averages (+/- sd) of the ﬁnal questions on enjoyment (“I enjoyed
                    using {System}”), presence (“I felt being in the game using {System}”) and
                    social interaction (“I felt engagement with the other using {System}”).
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction</filename>
        <data>
            <paper_id>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300301</doi>
            <title>TORC: A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction</title>
            <abstract>
                Recent hand-held controllers have explored a variety of haptic feedback sensations for users in virtual reality by producing both kinesthetic and cutaneous feedback from virtual objects. These controllers are grounded to the user's hand and can only manipulate objects through arm and wrist motions, not using the dexterity of their fingers as they would in real life. In this paper, we present TORC, a rigid haptic controller that renders virtual object characteristics and behaviors such as texture and compliance. Users hold and squeeze TORC using their thumb and two fingers and interact with virtual objects by sliding their thumb on TORC's trackpad. During the interaction, vibrotactile motors produce sensations to each finger that represent the haptic feel of squeezing, shearing or turning an object. Our evaluation showed that using TORC, participants could manipulate virtual objects more precisely (e.g., position and rotate objects in 3D) than when using a conventional VR controller.
            </abstract>
            <sections>
                <section>
                    <word_count>706</word_count>
                    <figure_citations>Figure 1), a novel hand-held haptic controller for VR that has a rigid shape and no moving parts, making it a suitable candidate for reliable mass manufacturing.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1103</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1995</word_count>
                    <figure_citations>Figure 2, we created a controller form factor that allows the user to move the thumb freely in a plane on a pad, parallel to the rests for the two fingers.Figure 2a, we designed the device to be held with thumb and two fingers of the right hand with the remaining fingers gripping the handle.Figure 2b), we mounted force sensors (Honey-well FSS1500) and voice coil actuators (VCA, Dayton Audio DAEX9-4SM).Figure 3 depicts the schematic of the desk-fixed prototype.Figure 5).Figure 6 shows the distribution of scores for each combination of locations of VCAs.Figure 7), which has a considerably lighter form factor.Figure 8).</figure_citations>
                    <section_index>2</section_index>
                    <title>TORC CONTROLLER</title>
                </section>
                <section>
                    <word_count>1069</word_count>
                    <figure_citations>Figure 9 (left)).Figure 9 (middle &amp; right)).Figure 10).Figure 11).Figure 12).</figure_citations>
                    <section_index>3</section_index>
                    <title>TORC INTERACTION SCENARIOS</title>
                </section>
                <section>
                    <word_count>1228</word_count>
                    <figure_citations>Figure 13): Locatinд (Figure 13a–c) and Rotatinд (Figure 13d–f).Figure 13a).Figure 13b).Figure 13c), she presses the trigger in the left hand.Figure 13d).Figure 13e shows the angular difference of the key and the key hole.Figure 13f).Figure 13a–f).Figure 14), we found that people preferred TORC over VIVE (Q26).Figure 15 (middle) shows the distance error from VIVE and TORC in the Locatinд task.Figure 15 (right) shows the angular error (calculated as intrinsic geodesic distance between q0 and q1.Figure 15 (left)).</figure_citations>
                    <section_index>4</section_index>
                    <title>USER STUDY</title>
                </section>
                <section>
                    <word_count>1008</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>115</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>11</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>2047</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Haptics</li>
                <li>VR object manipulation</li>
                <li>Haptic texture</li>
                <li>Haptic compliance</li>
            </keywords>
            <authors>
                <li>Jaeyeon Lee</li>
                <li> Mike Sinclair</li>
                <li> Mar Gonzalez-Franco</li>
                <li> Eyal Ofek</li>
                <li> Christian Holz</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_1.jpg</url>
                <caption>Figure 1: TORC interaction – real vs VR animation rendering.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_2.jpg</url>
                <caption>Figure 2: (a) Desk-fixed prototype used in Experiments 1 and 2 (b) diagram of end view, left side view, and right side view.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_3.jpg</url>
                <caption>Figure 3: Schematic diagram of desk-fixed prototype.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_4.jpg</url>
                <caption>Figure 4: Measured acceleration on index finger rest with vi-
                    bration from the VCA under the index finger rest. (a) with-
                    out fingers touching (b) with fingers touching the finger
                    rests.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_5.jpg</url>
                <caption>Figure 5: Experiment setup. The participant compared two
                    rendered sensations A and B from our device (right hand)
                    to the sensation of an analog object, a
                    5.08 cm silicone
                    ball made of Eco-flex 00-30 (durometer 27.4 (Shore hardness,
                    scale: OO)) (left hand).
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>6</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_6.jpg</url>
                <caption>Figure 6: Boxplot representing distribution of the psy-
                    chophysical preference score between different combina-
                    tions of the location of VCAs.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>7</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_7.jpg</url>
                <caption>Figure 7: Final TORC controller. (a) photos (b) diagram of end view and right side view.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_8.jpg</url>
                <caption>Figure 8: Schematic diagram of TORC controller.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>9</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_9.jpg</url>
                <caption>Figure 9: Fingers are holding the object, with joint position
                    estimated by inverse kinematics (left). Applying a little pres-
                    sure outward by opening the index &amp; middle fingers releases
                    the grabbed object (middle &amp; right).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>10</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_10.jpg</url>
                <caption>Figure 10: Context-based release. When holding the hand
                    up, the thumb may be lifted without releasing the held ob-
                    ject (left). In contrast, doing the same while the hand points
                    down will release and drop the object (right).
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>11</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_11.jpg</url>
                <caption>Figure 11: Applying force on the controller allows users to
                    squeeze virtual objects. A simulated haptic sensation ren-
                    dered on TORC completes the visual simulation.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>12</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_12.jpg</url>
                <caption>Figure 12: TORC allows precise manipulation of a held ob-
                    jects rotation by sensing finger motion (left). Rotation is
                    around the center of rotation between the fingers and it is
                    controlled by the thumb motion. The index and middle fin-
                    gers rotate in the opposite direction (middle &amp; right).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>13</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_13.jpg</url>
                <caption>Figure 13: Two-level docking task. Participants had to match
                    the key they were holding with the controller to a target
                    key hole. The first docking task (a, b, c) only aimed at posi-
                    tional docking (Locatinд), therefore the key hole and the key
                    did not display any superficial pattern. In the second dock-
                    ing task (d, e, f), the key hole displayed a particular pattern
                    that participants were asked to match. In essence that meant
                    they needed to rotate their hand and key (Rotatinд).
                </caption>
                <page>9</page>
            </figure>
            <figure>
                <id>14</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_14.jpg</url>
                <caption>Figure 14: Questionnaire Responses. (top) Boxplots of the
                    scores, only showing questions with significant difference
                    between conditions. (bottom) Histogram of preference be-
                    tween two methods (response to Q26).
                </caption>
                <page>10</page>
            </figure>
            <figure>
                <id>15</id>
                <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_15.jpg</url>
                <caption>Figure 15: Task Performance. Boxplots of the task comple-
                    tion time, distance error and angular error during the differ-
                    ent parts of the experiment (Locatinд task and Rotatinд task),
                    for the two controllers: TORC and VIVE.
                </caption>
                <page>10</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality</filename><data>
            <paper_id>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300243</doi>
            <title>TabletInVR: Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality</title>
            <abstract>
                Complex virtual reality (VR) tasks, like 3D solid modelling, are challenging with standard input controllers. We propose exploiting the affordances and input capabilities when using a 3D-tracked multi-touch tablet in an immersive VR environment. Observations gained during semi-structured interviews with general users, and those experienced with 3D software, are used to define a set of design dimensions and guidelines. These are used to develop a vocabulary of interaction techniques to demonstrate how a tablet's precise touch input capability, physical shape, metaphorical associations, and natural compatibility with barehand mid-air input can be used in VR. For example, transforming objects with touch input, "cutting" objects by using the tablet as a physical "knife", navigating in 3D by using the tablet as a viewport, and triggering commands by interleaving bare-hand input around the tablet. Key aspects of the vocabulary are evaluated with users, with results validating the approach.
            </abstract>
            <sections>
                <section>
                    <word_count>501</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>723</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>BACKGROUND AND RELATED WORK</title>
                </section>
                <section>
                    <word_count>1046</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>FORMATIVE STUDY</title>
                </section>
                <section>
                    <word_count>1273</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>DESIGN SPACE</title>
                </section>
                <section>
                    <word_count>2384</word_count>
                    <figure_citations>Figure 1).Figure 2).Figure 3, described in TapSense [25] and in the modeswitching study by Surale et al.Figure 3 (b-c)).Figure 4 (a)).Figure 4(b-c)) follow selection, and can be performed simultaneously.Figure 5) (O1, O3, O5, O6, D9).Figure 5).Figure 6 (a)).Figure 6 (b)), and while navigating, the scene quickly fades to black, except for the tablet and viewport.Figure 6 (c)) and query into the mic (O4).</figure_citations>
                    <section_index>4</section_index>
                    <title>EXAMPLE INTERACTION VOCABULARY</title>
                </section>
                <section>
                    <word_count>1555</word_count>
                    <figure_citations>Figure 7 (a).Figure 7 (b-g)).Figure 7 (h-k)).</figure_citations>
                    <section_index>5</section_index>
                    <title>USER EVALUATION</title>
                </section>
                <section>
                    <word_count>154</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>31</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>2070</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Interaction techniques</li>
                <li>Virtual reality</li>
                <li>Touch interaction</li>
            </keywords>
            <authors>
                <li>Hemant Bhaskar Surale</li>
                <li> Aakar Gupta</li>
                <li> Mark Hancock</li>
                <li> Daniel Vogel</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_1.jpg</url>
                <caption>Figure 1: Creation. (a) Flip the tablet, select the object for
                    creation, (b) Tap on tablet viewport to create.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>2</id>
                <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_2.jpg</url>
                <caption>Figure 2: Object selection. (a) Tap on tablet viewport, (b)
                    Pierce tablet corner in the object and pinch, (c) Tap on the
                    face of the object.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>3</id>
                <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_3.jpg</url>
                <caption>Figure 3: (a) Knuckle for multiple object selection, (b) Pinch
                    to deselect, (c) Tap on viewport to deselect.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>4</id>
                <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_4.jpg</url>
                <caption>Figure 4: (a) Swipe-in to delete the object, (b) Two finger scale,
                    (c) Two finger drag to translate.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_5.jpg</url>
                <caption>Figure 5: Select axis of transformation using the orientation
                    of the tablet. (a) Facing up to select the x-axis, (c) Portrait
                    vertical to select the y-axis, and (c) Landscape left to select
                    the z-axis.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_6.jpg</url>
                <caption>Figure 6: (a) Slicing an object, (b) Five-finger touch to navi-
                    gate, (c) Speak to tablet and ask for help.
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_7.jpg</url>
                <caption>Figure 7: Sample results from ‘replication’ and ‘freeform exploration’ task. (a) Target Model, (b-g) Participant’s replication
                    (P1-P6), (h-k) Participant’s creations in ‘freeform exploration’ (P1-P4)
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Teaching Language and Culture with a Virtual Reality Game</filename>
        <data>
            <paper_id>Teaching Language and Culture with a Virtual Reality Game</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025857</doi>
            <title>Teaching Language and Culture with a Virtual Reality Game</title>
            <abstract>
                Many people want to learn a language but find it difficult to stay engaged. Ideally, we would have language learning tools that can make language learning more enjoyable by simulating immersion in a foreign language environment. Therefore, we adapted Crystallize, a 3D video game for learning Japanese, so that it can be played in virtual reality with the Oculus Rift. Specifically, we explored whether we could leverage virtual reality technology to teach embodied cultural interaction, such as bowing in Japanese greetings. To evaluate the impact of our virtual reality game designs, we conducted a formative user study with 68 participants. We present results showing that the virtual reality design trained players how and when to bow, and that it increased participants' sense of involvement in Japanese culture. Our results suggest that virtual reality technology provides an opportunity to leverage culturally-relevant physical interaction, which can enhance the design of language learning technology and virtual reality games.
            </abstract>
            <sections>
                <section>
                    <word_count>554</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>942</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>576</word_count>
                    <figure_citations>Figure 1).Figure 2).</figure_citations>
                    <section_index>2</section_index>
                    <title>CRYSTALLIZE IN VIRTUAL REALITY</title>
                </section>
                <section>
                    <word_count>1164</word_count>
                    <figure_citations>Figure 3), and (2) conversation dialogue, in which the player engages a single NPC in conversation.Figure 4) and, in some cases, asked to construct a sentence given several vocabulary words (Figure 5).</figure_citations>
                    <section_index>3</section_index>
                    <title>TEACHING PLAYERS TO BOW</title>
                </section>
                <section>
                    <word_count>571</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>FORMATIVE USER STUDY</title>
                </section>
                <section>
                    <word_count>1399</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>274</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>1018</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Language learning</li>
                <li>Video games</li>
                <li>Virtual reality</li>
            </keywords>
            <authors>
                <li>Alan Cheng</li>
                <li> Lei Yang</li>
                <li> Erik Andersen</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Teaching Language and Culture with a Virtual Reality Game_crop_1.jpg</url>
                <caption>Figure 1. Overhead view of the Crystallize virtual reality scenario that we set up to test the ability of virtual reality to communicate cultural and
                    language information. Players navigated through a Japanese teahouse and solved language learning questions by interacting with non-player-controlled
                    characters.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Teaching Language and Culture with a Virtual Reality Game_crop_2.jpg</url>
                <caption>Figure 2. Using Unity, we made Crystallize work with the Oculus Rift.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Teaching Language and Culture with a Virtual Reality Game_crop_3.jpg</url>
                <caption>Figure 3. An example of an eavesdropping dialogue. The player can
                    listen in on NPC dialogues and collect new words to add to the inventory.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Teaching Language and Culture with a Virtual Reality Game_crop_4.jpg</url>
                <caption>Figure 4. An example of a multiple-choice conversation dialogue in VR.
                    The player must select an appropriate response based on what the NPC
                    said.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Teaching Language and Culture with a Virtual Reality Game_crop_5.jpg</url>
                <caption>Figure 5. An example of a sentence-construction conversation dialogue
                    in VR. The player must arrange vocabulary words in the correct order
                    in order to respond to the NPC.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Teaching Language and Culture with a Virtual Reality Game_crop_7.jpg</url>
                <caption>Figure 7. Using the Oculus Rift head tracking, we trained players to bow when greeting other characters in Japanese. A few seconds after the NPC
                    bows, the player is given a prompt to bow. The third panel shows the player’s ﬁeld of view mid-bow.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>8</id>
                <url>Teaching Language and Culture with a Virtual Reality Game_crop_8.jpg</url>
                <caption>Figure 8. Analysis of survey questions. Response options used a Likert scale, spanning from 1 (None/not at all) to 5 (Very much/a great deal).
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality</filename>
        <data>
            <paper_id>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025581</doi>
            <title>The Geometry of Storytelling: Theatrical Use of Space for 360-degree Videos and Virtual Reality</title>
            <abstract>
                360-degree filming and head-mounted displays (HMDs) give recorded media a new sense of space. Theatre practitioners' expertise in manipulating spatial interactions has much to contribute to immersive recorded content. Four theatre directors led teams of three actors to stage the same scene for both immersive theatre and for 360-degree filming. Each team was recorded performing the scene at least six times, three in each condition, to extract actors' coordinates. This study establishes how to quantify theatre practitioners' use of spatial interactions and examines the spatial adaptations made when transferring these relationships to 360-degree filming.Staging for a 360-degree camera compared to staging for an audience member had shorter distances from the camera and between performers, along with fewer instances of the camera being in the middle of the action. Across all groups, interpersonal distance between characters and between the audience/camera dropped at the end of the scene when the characters come together as a team, suggesting that elements of Proxemics may be applicable to narrative performance.
            </abstract>
            <sections>
                <section>
                    <word_count>678</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1604</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>2080</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>METHODOLOGY</title>
                </section>
                <section>
                    <word_count>2806</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>655</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>LIMITATIONS</title>
                </section>
                <section>
                    <word_count>310</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>FUTURE DIRECTIONS</title>
                </section>
                <section>
                    <word_count>283</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>450</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>203</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>399</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Narrative</li>
                <li>Theatre</li>
                <li>Performance</li>
                <li>Cinematic VR</li>
                <li>Interpersonal Space</li>
                <li>Head-Mounted Display</li>
                <li>Workﬂow</li>
                <li>360-degree video</li>
            </keywords>
            <authors>
                <li>Vanessa C. Pope</li>
                <li> Robert Dawes</li>
                <li> Florian Schweiger</li>
                <li> Alia Sheikh</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_1.jpg</url>
                <caption>Figure 1. Diagrams of F-formations (left) and Proxemics (right), based
                    on [6, 11]
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>2</id>
                <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_2.jpg</url>
                <caption>Figure 2. Diagram showing the placement of recording equipment
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_3.jpg</url>
                <caption>Figure 3. Distance between Red and Blue characters compared to the
                    average distance between Green/Blue and Green/Red dyads
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_4.jpg</url>
                <caption>Figure 4. Mean interpersonal distance between dyads for each group
                    over time
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_5.jpg</url>
                <caption>Figure 5. Distance between Red character and centre compared to the
                    average distance from centre of Blue and Green characters
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_6.jpg</url>
                <caption>Figure 6. Mean ranks of distance from centre in theatre and 360-degree
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_7.jpg</url>
                <caption>Figure 7. Mean ranks of angles in staging for theatre and 360-degree
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>8</id>
                <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_8.jpg</url>
                <caption>Figure 8. Normalized instances of audience being inside or outside the
                    actors’ F-formations
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>9</id>
                <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_9.jpg</url>
                <caption>Figure 9. Group 4’s ‘huddle’ in theatre (left) and 360-degree (right)
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World </filename>
        <data>
            <paper_id>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World </paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025955</doi>
            <title>The World-as-Support: Embodied Exploration, Understanding and Meaning-Making of the Augmented World</title>
            <abstract>
                Current technical capabilities of mobile technologies are consolidating the interest in developing context-aware Augmented/Mixed Reality applications. Most of these applications are designed based on the Window-on-the-World (WoW) interaction paradigm. A significant decrease in cost of projection technology and advances in pico-sized projectors have spurred applications of Projective Augmented Reality. This research has focused mainly on technological development. However, there is still a need to fully understand its communicational and expressive potential. Hence, we define a conceptual paradigm that we call World-as-Support (WaS). We compare the WaS and WoW paradigms by contrasting their assumptions and cultural values, as well as through a study of an application aimed at supporting the collaborative improvisation of site-specific narratives by children. Our analysis of children's understanding of the physical and social environment and of their imaginative play allowed us to identify the affordances, strengths and weaknesses of these two paradigms.
            </abstract>
            <sections>
                <section>
                    <word_count>402</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>625</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>PHYSICAL WORLDS</title>
                </section>
                <section>
                    <word_count>2824</word_count>
                    <figure_citations>Figure 1).Figure 2).Figure 3), which allows visualizing both spatial and embodied resources as well as the time-based unfolding of the experience.</figure_citations>
                    <section_index>2</section_index>
                    <title>PERSPECTIVE</title>
                </section>
                <section>
                    <word_count>2021</word_count>
                    <figure_citations>Figure 6), the two paradigms may affect the unfolding social relationships and the instances for participation and coconstruction of meaning.</figure_citations>
                    <section_index>3</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1265</word_count>
                    <figure_citations>Figure 7), trying to catch him, etc.</figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>238</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>25</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1444</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Mixed Reality</li>
                <li>Augmented Reality</li>
                <li>Embodied interaction</li>
                <li>Embodied cognition</li>
                <li>Meaning making</li>
                <li>Window-on-theWorld</li>
                <li>World-as-Support</li>
            </keywords>
            <authors>
                <li>Laura Malinverni</li>
                <li> Julian Maya</li>
                <li> Marie-Monique Schaper</li>
                <li> Narcis Pares</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_1.jpg</url>
                <caption>Figure 1. Child playing with the WoW based system
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>2</id>
                <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_2.jpg</url>
                <caption>Figure 2. Children playing with the WaS based system
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_3.jpg</url>
                <caption>Figure 3. Example of a transcription storyboard
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_4.jpg</url>
                <caption>Figure 4. Children focusing on the screen while playing with
                    the WoW based system
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>5</id>
                <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_5.jpg</url>
                <caption>Figure 5. Distribution of ways of using space
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>6</id>
                <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_6.jpg</url>
                <caption>Figure 6. Distribution of ways of participating
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>7</id>
                <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_7.jpg</url>
                <caption>Figure 7. Child playing with the virtual character in WaSc
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays</filename>
        <data>
            <paper_id>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025824</doi>
            <title>ThermoVR: Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays</title>
            <abstract>
                Head Mounted Displays (HMDs) provide a promising opportunity for providing haptic feedback on the head for an enhanced immersive experience. In ThermoVR, we integrated five thermal feedback modules on the HMD to provide thermal feedback directly onto the user's face. We conducted evaluations with 15 participants using two approaches: Firstly, we provided simultaneously actuated thermal stimulations (hot and cold) as directional cues and evaluated the accuracy of recognition; secondly, we evaluated the overall immersive thermal experience that the users experience when provided with thermal feedback on the face. Results indicated that the recognition accuracy for cold stimuli were of approx. 89.5% accuracy while the accuracy for hot stimuli were 68.6%. Also, participants reported that they felt a higher level of immersion on the face when all modules were simultaneously stimulated (hot and cold). The presented applications demonstrate the ThermoVR's directional cueing and immersive experience.
            </abstract>
            <sections>
                <section>
                    <word_count>448</word_count>
                    <figure_citations>Figure 1(a)-(c) demonstrates ThermoVR being used to provide thermal cues for a virtual reality game and applications.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>350</word_count>
                    <figure_citations>Figure 1(c).Figure 2 (a).Figure 2 (b).Figure 2 (c)) are used for the closed loop control.Figure 2 (a)).</figure_citations>
                    <section_index>1</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>847</word_count>
                    <figure_citations>Figure 1 (b-4) as the visual that is related to the hot stimulation and, Figure 1 (b-2) for the cold.Figure 4 (a)) and the HMD displayed the user interface developed using Unity3 depicted in Figure 4 (b).</figure_citations>
                    <section_index>2</section_index>
                    <title>EVALUATIONS</title>
                </section>
                <section>
                    <word_count>354</word_count>
                    <figure_citations>Figure 5(a) shows the overall accuracy for the perception of thermal directional cues.Figure 5(a)).Figure 5(b)(c) indicates the perception accuracy for each cue in hot and cold conditions.Figure 5(d)(e) shows the confusion matrix that depicts which cues were misidentified.Figure 5(f), the study results of visual involvement, Q1, reported 4.</figure_citations>
                    <section_index>3</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>560</word_count>
                    <figure_citations>Figure 5(d)), all the cues on the forehead (Left Up, Up, Right Up) were heavily misidentified with each other and the Front cue.Figure 5(e)) indicated that the Right Up cue was the most difficult to understand as it was confused as Up, Front, Right, Left Down cues.Figure 5(f)), participants rated visual involvement and thermal aspects for no-stimulation, hot and cold stimulations conditions.</figure_citations>
                    <section_index>4</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>260</word_count>
                    <figure_citations>Figure 1(a)) is a first-person point of view game.Figure 1(b)) we designed for enhancing virtual reality experience with thermal haptic feedback.</figure_citations>
                    <section_index>5</section_index>
                    <title>APPLICATIONS</title>
                </section>
                <section>
                    <word_count>194</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>50</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>547</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Thermal display</li>
                <li>Thermal haptics</li>
                <li>Head mounted display</li>
            </keywords>
            <authors>
                <li>Roshan Lalintha Peiris</li>
                <li> Wei Peng</li>
                <li> Zikun Chen</li>
                <li> Liwei Chan</li>
                <li> Kouta Minamizawa</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays_crop_1.jpg</url>
                <caption>Figure 1. ThermoVR applications (insets depict example stimulations) (a) “Where is my camp?”: This game provides immersive or directional cues
                    in the game as players try to locate their camp before it gets too cold. (a-1) Receiving immersive cold feedback in the winter night (a-2,3) Receiving
                    directional cues as hot stimulations (a-4) Users immersively feel the warmth of the campﬁre; (b) “What is the weather?” Application provides thermally
                    immersive weather feedback to let users explore the weather of different locations (b-1) Users can select the location to explore weather (b-2,3,4)
                    Immersively explore the weather (c) User wearing the ThermoVR
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays_crop_2.jpg</url>
                <caption>Figure 2. (a) The Prototype System of ThermoVR that integrates ﬁve
                    thermal modules (b) Contact locations of the thermal modules on the
                    face (c) Close up of thermal module. T1, T2 and T3 are temperature
                    sensors where T1 contacts the skin during use
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays_crop_3.jpg</url>
                <caption>Figure 3. Stimulation patterns for directional cues. The modules in black
                    denote the thermally (hot or cold) stimulated modules
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>4</id>
                <url>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays_crop_4.jpg</url>
                <caption>Figure 4. The study setup (a)The evaluation setup for identifying ther-
                    mal cues (b) Interface displayed on the HMD for selecting perceived cues
                    during the evaluation
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>5</id>
                <url>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays_crop_5.jpg</url>
                <caption>Figure 5. Results: (L-Left, U-Up, D-Down, R-Right) (a) Perception accuracy and Average Task Completion Times for thermal cues: Hot and Cold cues;
                    Heat map visualization of the accuracy of perception for individual cues for (b) hot and (c) cold stimulation; Confusion matrix for the (d) hot and (e)
                    for cold stimulations (f) Qualitative study results
                </caption>
                <page>3</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Using Presence Questionnaires in Virtual Reality</filename>
        <data>
            <paper_id>Using Presence Questionnaires in Virtual Reality</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300590</doi>
            <title>Using Presence Questionnaires in Virtual Reality</title>
            <abstract>
                Virtual Reality (VR) is gaining increasing importance in science, education, and entertainment. A fundamental characteristic of VR is creating presence, the experience of 'being' or 'acting', when physically situated in another place. Measuring presence is vital for VR research and development. It is typically repeatedly assessed through questionnaires completed after leaving a VR scene. Requiring participants to leave and re-enter the VR costs time and can cause disorientation. In this paper, we investigate the effect of completing presence questionnaires directly in VR. Thirty-six participants experienced two immersion levels and filled three standardized presence questionnaires in the real world or VR. We found no effect on the questionnaires' mean scores; however, we found that the variance of those measures significantly depends on the realism of the virtual scene and if the subjects had left the VR. The results indicate that, besides reducing a study's duration and reducing disorientation, completing questionnaires in VR does not change the measured presence but can increase the consistency of the variance.
            </abstract>
            <sections>
                <section>
                    <word_count>601</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1329</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1024</word_count>
                    <figure_citations>Figure 1).Figure 1).Figure 1).Figure 2).</figure_citations>
                    <section_index>2</section_index>
                    <title>METHOD</title>
                </section>
                <section>
                    <word_count>1841</word_count>
                    <figure_citations>Figure 4 shows the Gaussian distribution curve fits of those differences.</figure_citations>
                    <section_index>3</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>29</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>REAL</title>
                </section>
                <section>
                    <word_count>6</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>HAPTC</title>
                </section>
                <section>
                    <word_count>163</word_count>
                    <figure_citations>Figure 6, F (2, 203) = 150.</figure_citations>
                    <section_index>6</section_index>
                    <title>IFQUAL</title>
                </section>
                <section>
                    <word_count>1278</word_count>
                    <figure_citations>Figure 6).</figure_citations>
                    <section_index>7</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>238</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>35</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>1446</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Presence</li>
                <li>Questionnaire</li>
                <li>Evaluation</li>
            </keywords>
            <authors>
                <li>Valentin Schwind</li>
                <li> Pascal Knierim</li>
                <li> Nico Haas</li>
                <li> Niels Henze</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Using Presence Questionnaires in Virtual Reality_crop_1.jpg</url>
                <caption>Figure 1: Virtual (l) and real (r) environment with question-
                    naire and input controller.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Using Presence Questionnaires in Virtual Reality_crop_2.jpg</url>
                <caption>Figure 2: Abstract and realistic scene of a first person shooter game developed to induce different levels of presence.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Using Presence Questionnaires in Virtual Reality_crop_3.jpg</url>
                <caption>Figure 3: Means and score variances of the presence measures. While presence scores of the three questionnaires did not differ
                    significantly between in- and outside the VR, variance measures show significant interaction effects between the questionnaire
                    environment and the realism during the VR experience. Error bars show 95% confidence intervals (CI95)
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Using Presence Questionnaires in Virtual Reality_crop_4.jpg</url>
                <caption>Figure 4: Histogram, means, and fitted Gaussian distribution using the average scores of all questionnaires. While the distribu-
                    tions remain nearly constant when the questionnaires are used in VR, there is a significant difference between the variances
                    in the real-world responses after BIP using abstract and after BIP using a realistic virtual scene.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Using Presence Questionnaires in Virtual Reality_crop_5.jpg</url>
                <caption>Figure 5: Subscales of the IPQ and WS presence questionnaires. IPQ: General Presence (GP), spatial presence (SP), involvement
                    (INV), and realism (REAL); WS: Involvement (INV), natural (NAT), auditory (AUD), haptics (HAPTC), resolution (RES), and
                    interface quality (IFQUAL).
                </caption>
                <page>8</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Using Presence Questionnaires in Virtual Reality_crop_6.jpg</url>
                <caption>Figure 6: Completion times of the SUS, IPQ, and WS ques-
                    tionnaire. Error bars show CI95.
                </caption>
                <page>9</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>VRRRRoom- Virtual Reality for Radiologists in the Reading Room</filename>
        <data>
            <paper_id>VRRRRoom- Virtual Reality for Radiologists in the Reading Room</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025566</doi>
            <title>VRRRRoom: Virtual Reality for Radiologists in the Reading Room</title>
            <abstract>
                Reading room conditions such as illumination, ambient light, human factors and display luminance, play an important role on how radiologists analyze and interpret images. Indeed, serious diagnostic errors can appear when observing images through everyday monitors. Typically, these occur whenever professionals are ill-positioned with respect to the display or visualize images under improper light and luminance conditions. In this work, we show that virtual reality can assist radiodiagnostics by considerably diminishing or cancel out the effects of unsuitable ambient conditions. Our approach combines immersive head-mounted displays with interactive surfaces to support professional radiologists in analyzing medical images and formulating diagnostics. We evaluated our prototype with two senior medical doctors and four seasoned radiology fellows. Results indicate that our approach constitutes a viable, flexible, portable and cost-efficient option to traditional radiology reading rooms.
            </abstract>
            <sections>
                <section>
                    <word_count>620</word_count>
                    <figure_citations>Figure 1A illustrates a typical radiology reading room.Figure 1B.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>433</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>601</word_count>
                    <figure_citations>Figure 2).Figure 2 on the left.Figure 2 on the right).</figure_citations>
                    <section_index>2</section_index>
                    <title>INTERACTION DESIGN</title>
                </section>
                <section>
                    <word_count>603</word_count>
                    <figure_citations>Figure 4 displays the graphical elements of our prototype that the user can interact with.</figure_citations>
                    <section_index>3</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>708</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>199</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>49</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>884</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual Reality</li>
                <li>Multitouch Surfaces</li>
                <li>Medical Visualization</li>
                <li>Interaction Design</li>
            </keywords>
            <authors>
                <li>Maurício Sousa</li>
                <li> Daniel Mendes</li>
                <li> Soraia Paulo</li>
                <li> Nuno Matela</li>
                <li> Joaquim Jorge</li>
                <li> Daniel Simões Lopes</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>VRRRRoom- Virtual Reality for Radiologists in the Reading Room_crop_1.jpg</url>
                <caption>Figure 1. A) A typical radiology reading room; B) Our approach combining virtual reality and desktop touch interactions.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>VRRRRoom- Virtual Reality for Radiologists in the Reading Room_crop_2.jpg</url>
                <caption>Figure 2. Desk surface gestures for left and right hands.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>VRRRRoom- Virtual Reality for Radiologists in the Reading Room_crop_3.jpg</url>
                <caption>Figure 3. The setup consists of an Oculus Rift HMD with a head track-
                    ing sensor and a multitouch frame to detect touches on the desk surface.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>VRRRRoom- Virtual Reality for Radiologists in the Reading Room_crop_4.jpg</url>
                <caption>Figure 4. Virtual desk and volume rendered from medical images.
                </caption>
                <page>3</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>VaiR- Simulating 3D Airflows in Virtual Reality</filename>
        <data>
            <paper_id>VaiR- Simulating 3D Airflows in Virtual Reality</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3026009</doi>
            <title>VaiR: Simulating 3D Airflows in Virtual Reality</title>
            <abstract>
                The integration of multi-sensory stimuli, e.g. haptic airflow, in virtual reality (VR) has become an important topic of VR research and proved to enhance the feeling of presence. VaiR focuses on an accurate and realistic airflow simulation that goes far beyond wind. While previous works on the topic of airflow in VR are restricted to wind, while focusing on the feeling of presence, there is to the best of our knowledge no work considering the conceptual background or on the various application areas. Our pneumatic prototype emits short and long term flows with a minimum delay and is able to animate wind sources in 3D space around the user's head. To get insights on how airflow can be used in VR and how such a device should be designed, we arranged focus groups and discussed the topic. Based on the gathered knowledge, we developed a prototype which proved to increase presence, as well as enjoyment and realism, while not disturbing the VR experience.
            </abstract>
            <sections>
                <section>
                    <word_count>925</word_count>
                    <figure_citations>Figure 1, can easily be combined with current head mounted displays.</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>1517</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1325</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>IMPLEMENTATION</title>
                </section>
                <section>
                    <word_count>2004</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>EVALUATION</title>
                </section>
                <section>
                    <word_count>590</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>691</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Airﬂow</li>
                <li>Presence</li>
                <li>Evaluation</li>
            </keywords>
            <authors>
                <li>Michael Rietzler</li>
                <li> Katrin Plaumann</li>
                <li> Taras Kränzle</li>
                <li> Marcel Erath</li>
                <li> Alexander Stahl</li>
                <li> Enrico Rukzio</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_1.jpg</url>
                <caption>Figure 1. The VaiR Prototype worn in combination with a HTC Vive
                    head mounted display. Fixed to frame mounted onto the Vive’s straps
                    are two bows. These bows are moved by two separate motors, allowing
                    both bows to each rotate 135◦. Fixes on each bow are ﬁve nozzles (each
                    36◦ apart), were the air streams come out. Several nozzles can be used at
                    the same time. Due to the modular design, nozzles can be moved along
                    the bows and changed as needed. That way, angle and dispersion of the
                    air streams can be customized. VaiR is so designed that a container of
                    pressurized air, valves, power source and an arduino controller can be
                    worn as a backpack.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_2.jpg</url>
                <caption>Figure 2. Overview of the focus group results.
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>3</id>
                <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_3.jpg</url>
                <caption>Figure 3. Examples of different nozzles to vary spray angle and intensity.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>4</id>
                <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_4.jpg</url>
                <caption>Figure 4. The VaiR setup: Two bows with nozzles having a angular
                    distance of 36◦ and two servo motors to control the angle of each bow
                    seperately (left). Each bow can be rotated 135◦, overall 270◦ around the
                    user’s head and neck can be displayed (right).
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>5</id>
                <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_5.jpg</url>
                <caption>Figure 5. The Backpack consists of a compressed air bottle and 14 valves
                    (ten for the nozzles, and four to vary the intensity) as well as the control
                    unit.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>6</id>
                <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_6.jpg</url>
                <caption>Figure 6. The ﬁrst and second scene used for the evaluation. a) The cliff scene looking towards the sea and b) view when turned around. c) The fair
                    scene regarded from outside and d) the ﬁrst person view. e) The golf cart scene from ﬁrst person view.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>7</id>
                <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_7.jpg</url>
                <caption>Figure 7. The third and fourth scene used for the evaluation. a) The cart scene in ﬁrst person view. b) The horror scene: bats ﬂying over the head, c) a
                    rotating saw runs through the viewer’s neck and d) a Zombie appears.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>8</id>
                <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_8.jpg</url>
                <caption>Figure 8. The results of the enjoyment scores per scene.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>9</id>
                <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_9.jpg</url>
                <caption>Figure 9. The results of the presence scores per scene.
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>10</id>
                <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_10.jpg</url>
                <caption>Figure 10. The results of the E2I presence and enjoyment scores.
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>Vremiere- In-Headset Virtual Reality Video Editing</filename>
        <data>
            <paper_id>Vremiere- In-Headset Virtual Reality Video Editing</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025675</doi>
            <title>Vremiere: In-Headset Virtual Reality Video Editing</title>
            <abstract>
                Creative professionals are creating Virtual Reality (VR) experiences today by capturing spherical videos, but video editing is still done primarily in traditional 2D desktop GUI applications such as Premiere. These interfaces provide limited capabilities for previewing content in a VR headset or for directly manipulating the spherical video in an intuitive way. As a result, editors must alternate between editing on the desktop and previewing in the headset, which is tedious and interrupts the creative process. We demonstrate an application that enables a user to directly edit spherical video while fully immersed in a VR headset. We first interviewed professional VR filmmakers to understand current practice and derived a suitable workflow for in-headset VR video editing. We then developed a prototype system implementing this new workflow. Our system is built upon a familiar timeline design, but is enhanced with custom widgets to enable intuitive editing of spherical video inside the headset. We conducted an expert review study and found that with our prototype, experts were able to edit videos entirely within the headset. Experts also found our interface and widgets useful, providing intuitive controls for their editing needs.
            </abstract>
            <sections>
                <section>
                    <word_count>618</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>234</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>1364</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>VR PROFESSIONAL INTERVIEWS</title>
                </section>
                <section>
                    <word_count>2961</word_count>
                    <figure_citations>Figure 2 presents the main timeline interface for our system, which is overlaid on a spherical video view.Figure 2D.Figure 2D.Figure 2D.Figure 2C "Graphics" track), and bookmarks can be placed on the video as well (Figure 2D.Figure 2B).Figure 2 D.Figure 2 D.Figure 3).Figure 3, the user quickly maps her current view in the video with the view in the visualization; she can also spot the jumping lady behind her that she might have not noticed before.Figure 4).Figure 5B).Figure 5C, the user has aligned the ski lift before the cut to the skier after the clip, rather than transitioning from the ski lift to an empty view.Figure 5B).Figure 5B, if the toggle is off, the user can change the rotation of each video individually; otherwise, rotating the blue (ski_1) video will also rotate the red video (ski_2) by the same amount.Figure 6 left).Figure 6, the editor can easily position the logo image on the parachute (Figure 6 right), which is difficult on the desktop due to distortion (Figure 6 left).Figure 7, Figure 2 D.</figure_citations>
                    <section_index>3</section_index>
                    <title>THE VREMIERE SYSTEM</title>
                </section>
                <section>
                    <word_count>1430</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>EXPERT REVIEW</title>
                </section>
                <section>
                    <word_count>716</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>185</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>66</word_count>
                    <figure_citations>Figure 1, 4 and 5 use images from YouTube users TOYO TIRES JAPAN and Ábaco Digital Zaragoza under a Creative Commons license.Figure 6 and 7 use images with permission from Youtube users P J Orravan and Jacob Phillips.</figure_citations>
                    <section_index>7</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>877</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Virtual reality</li>
                <li>Video editing</li>
            </keywords>
            <authors>
                <li>Cuong Nguyen</li>
                <li> Stephen DiVerdi</li>
                <li> Aaron Hertzmann</li>
                <li> Feng Liu</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_1.jpg</url>
                <caption>Figure 1: The same shot, viewed in a headset versus on the
                    screen. VR views look very different than on the desktop
                    screen, and provide a much stronger sense of immersion.
                    © TOYO TIRES JAPAN
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_2.jpg</url>
                <caption>Figure 2: The Vremiere interface contains a video view in the background (A) and ﬂoating UI components such as the timeline
                    (B), editing tracks (C) and editing widgets (D). By using familiar mouse and keyboard interaction, an editor can edit and view the
                    spherical video directly inside the VR headset.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_3.jpg</url>
                <caption>Figure 3: Left: the Little Planet is shown in a small window
                    above the timeline to aid in-headset video navigation. Right:
                    close-up view of the visualization; it can be used both as a
                    minimap and a compass of the 360° image. As noted by the
                    yellow arrow, the user can quickly spot an interesting event
                    outside her ﬁeld of view (indicated by the yellow fan at the
                    center) that would be otherwise difﬁcult to ﬁnd.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_4.jpg</url>
                <caption>Figure 4: Our system dynamically contracts a pair of vignettes
                    in front of the user’s eyes based on the perceived motion in
                    the video, to help reduce discomfort during video playback
                    or scrubbing. Note, on the left, the default diameter of the
                    vignette is 120°, which corresponds to no vignetting. © Ábaco
                    Digital Zaragoza
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>5</id>
                <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_5.jpg</url>
                <caption>Figure 5: A: an example of a jarring cut in VR video. The cut transitions from the ski lift to an empty view of the mountain in the
                    ski_2 video. B: Our Rotation Alignment tool is shown above the timeline. In the close-up view, the user can visualize how the cut
                    transitions from one clip to another given a speciﬁc viewpoint in the video (visualized by yellow bars); she can also directly drag
                    on the clips to align events before and after a cut, thereby helping the viewer to follow key elements in the video. Here, the Ripple
                    Rotation mode is shown currently as an Unlink toggle. C: after rotation, the skier in clip ski_2 is aligned to the previous shot,
                    resulting in a much better cut. © Ábaco Digital Zaragoza
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_6.jpg</url>
                <caption>Figure 6: Left: On desktop, adding 2D text or images to a
                    spherical video is often unintuitive. Right: in the headset, our
                    system provides a natural view of the image (Sample Logo)
                    and enables users to directly place it anywhere in the scene.
                    © P J Orravan
                </caption>
                <page>7</page>
            </figure>
            <figure>
                <id>7</id>
                <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_7.jpg</url>
                <caption>Figure 7: The user can directly click on the video to add
                    markers. These markers are visualized on the timeline and
                    can be reviewed quickly using keyboard shortcuts. © Jacob
                    Phillips
                </caption>
                <page>7</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality</filename><data>
            <paper_id>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality</paper_id>
            <venue>CHI 17</venue>
            <doi>10.1145/3025453.3025852</doi>
            <title>WatchThru: Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality</title>
            <abstract>We introduce WatchThru, an interactive method for extended wrist-worn display on commercially-available smartwatches. To address the limited visual and interaction space, WatchThru expands the device into 3D through a transparent display. This enables novel interactions that leverage and extend smartwatch glanceability. We describe three novel interaction techniques, Pop-up Visuals, Second Perspective and Peek-through, and discuss how they can complement interaction on current devices. We also describe two types of prototypes that helped us to explore standalone interactions, as well as, proof-of-concept AR interfaces using our platform.</abstract>
            <sections>
                <section>
                    <word_count>334</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>280</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>704</word_count>
                    <figure_citations>Figure 1 (top, right) shows a conceptual app using the WatchThru screen to display static and animated notifications.Figure 2 shows a prototypical app with a 2D map on the main screen and an oriented 3D arrow on the WatchThru screen.Figure 3 (c, d) show a smart home scenario where objects in a room, e.Figure 3 (e), a learning scenario, a virtual globe is presented in the middle of a room.Figure 3 (f) shows an assembly scenario; WatchThru shows the user how to connect electronic components.</figure_citations>
                    <section_index>2</section_index>
                    <title>WATCHTHRU INTERACTIONS</title>
                </section>
                <section>
                    <word_count>601</word_count>
                    <figure_citations>Figure 1, bottom).Figure 3 a,b).</figure_citations>
                    <section_index>3</section_index>
                    <title>IMPLEMENTATION OF PROTOTYPES</title>
                </section>
                <section>
                    <word_count>648</word_count>
                    <figure_citations>Figure 4) such that the WatchThru screen could be pulled out (manually or automatically) when needed.</figure_citations>
                    <section_index>4</section_index>
                    <title>LIMITATIONS AND FUTURE WORK</title>
                </section>
                <section>
                    <word_count>208</word_count>
                    <figure_citations></figure_citations>
                    <section_index>5</section_index>
                    <title>CONCLUSIONS</title>
                </section>
                <section>
                    <word_count>85</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>760</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Smartwatches</li>
                <li>Micro-Interaction</li>
                <li>Wearable Devices</li>
            </keywords>
            <authors>
                <li>Dirk Wenig</li>
                <li> Johannes Schöning</li>
                <li> Alex Olwal</li>
                <li> Mathias Oben</li>
                <li> Rainer Malaka</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality_crop_1.jpg</url>
                <caption>Figure 1: WatchThru is a lightweight and easy-to-build display
                    extension for existing smartwatches.
                </caption>
                <page>1</page>
            </figure>
            <figure>
                <id>2</id>
                <url>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality_crop_2.jpg</url>
                <caption>Figure 2: Second Perspective navigation with a 2D map (left)
                    and directional arrows (right).
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>3</id>
                <url>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality_crop_3.jpg</url>
                <caption>Figure 3: The Peek-through prototype uses external cameras
                    to track retroreﬂective markers on the (a) device and (b) user,
                    which enabled the several implemented AR scenarios (c–f).
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>4</id>
                <url>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality_crop_4.jpg</url>
                <caption>Figure 4: In the future for WatchThru, we envision a transpa-
                    rent screen that can be folded out when needed.
                </caption>
                <page>4</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics</filename>
        <data>
            <paper_id>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300774</doi>
            <title>What Can We Learn from Augmented Reality (AR)?: Benefits and Drawbacks of AR for Inquiry-based Learning of Physics</title>
            <abstract>
                Emerging technologies such as Augmented Reality (AR), have the potential to radically transform education by making challenging concepts visible and accessible to novices. In this project, we have designed a Hololens-based system in which collaborators are exposed to an unstructured learning activity in which they learned about the invisible physics involved in audio speakers. They learned topics ranging from spatial knowledge, such as shape of magnetic fields, to abstract conceptual knowledge, such as relationships between electricity and magnetism. We compared participants' learning, attitudes and collaboration with a tangible interface through multiple experimental conditions containing varying layers of AR information. We found that educational AR representations were beneficial for learning specific knowledge and increasing participants' self-efficacy (i.e., their ability to learn concepts in physics). However, we also found that participants in conditions that did not contain AR educational content, learned some concepts better than other groups and became more curious about physics. We discuss learning and collaboration differences, as well as benefits and detriments of implementing augmented reality for unstructured learning activities.
            </abstract>
            <sections>
                <section>
                    <word_count>995</word_count>
                    <figure_citations></figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>290</word_count>
                    <figure_citations></figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>668</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>SYSTEM DESIGN</title>
                </section>
                <section>
                    <word_count>397</word_count>
                    <figure_citations></figure_citations>
                    <section_index>3</section_index>
                    <title>RESEARCH QUESTIONS AND STUDY DESIGN</title>
                </section>
                <section>
                    <word_count>380</word_count>
                    <figure_citations></figure_citations>
                    <section_index>4</section_index>
                    <title>METHODS</title>
                </section>
                <section>
                    <word_count>665</word_count>
                    <figure_citations>Figure 3 question 5), electricity and movement (e.Figure 3 question 6), electricity and magnetic field (e.Figure 3 question 3) The concept of sequential reasoning indicates the style in which participants answered the open-ended question of “How is electrical energy turned into sound inside the speaker?” A large number of responses included a narrative which explained the connection between different components as a sequence (Figure 3 q1 top) rather than directly explaining the core physics phenomena driving the speaker.</figure_citations>
                    <section_index>5</section_index>
                    <title>AR</title>
                </section>
                <section>
                    <word_count>1323</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>RESULTS</title>
                </section>
                <section>
                    <word_count>1452</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>60</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>ACKNOWLEDGEMENTS</title>
                </section>
                <section>
                    <word_count>1185</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Augmented Reality</li>
                <li>Physics Education</li>
                <li>Collaborative Learning</li>
            </keywords>
            <authors>
                <li>Iulian Radu</li>
                <li> Bertrand Schneider</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics_crop_1.jpg</url>
                <caption>Figure 1. The Augmented Reality system developed for
                    this project (top image: two users interacting with the
                    speaker activity; bottom image: the magnetic fields around
                    the coil and the magnet that are generating the sound
                    waves).
                </caption>
                <page>3</page>
            </figure>
            <figure>
                <id>2</id>
                <url>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics_crop_2.jpg</url>
                <caption>Figure 2. Changing AR visualizations of electricity,
                    amplification, and magnetic fields overlaid on the physical
                    object. (Note, AR visualization misalignments are due to
                    the photo camera, and not visible to participants)
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics_crop_3.jpg</url>
                <caption>Figure 3. Examples of questions from the learning test.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>4</id>
                <url>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics_crop_4.jpg</url>
                <caption>Figure 4. Information shown on the physical poster in
                    front of the participants.
                </caption>
                <page>6</page>
            </figure>
            <figure>
                <id>6</id>
                <url>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics_crop_6.jpg</url>
                <caption>Figure 6. Group differences in relative learning gains
                    (Range 0-1; Red = EdAR group, Blue = NoEdAR group; Bars
                    = standard error)
                </caption>
                <page>8</page>
            </figure>
        </figures>
    </article>
    <article>
        <filename>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom</filename>
        <data>
            <paper_id>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom</paper_id>
            <venue>CHI 19</venue>
            <doi>10.1145/3290605.3300464</doi>
            <title>"What's Happening at that Hip?": Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom</title>
            <abstract>
                We present two studies to discuss the design, usability analysis, and educational outcome resulting from our system Augmented Body in physiotherapy classroom. We build on prior user-centric design work that investigates existing teaching methods and discuss opportunities for intervention. We present the design and implementation of a hybrid system for physiotherapy education combining an on-body projection based virtual anatomy supplemented by pen-based tablets to create real-time annotations. We conducted a usability evaluation of this system, comparing with projection only and traditional teaching conditions. Finally, we focus on a comparative study to evaluate learning outcome among students in actual classroom settings. Our studies showed increased usage of visual representation techniques in students'11 note taking behavior and statistically significant improvement in some learning aspects. We discuss challenges for designing augmented reality systems for education, including minimizing attention split, addressing text-entry issues, and digital annotations on a moving physical body.
            </abstract>
            <sections>
                <section>
                    <word_count>667</word_count>
                    <figure_citations>Figure 1).</figure_citations>
                    <section_index>0</section_index>
                    <title>INTRODUCTION</title>
                </section>
                <section>
                    <word_count>752</word_count>
                    <figure_citations>Figure 1(b)) as an interaction mechanism alongside the projection mapping system to create a hybrid user interface.Figure 1(c)).Figure 2(c)), which was then projected on the human model.</figure_citations>
                    <section_index>1</section_index>
                    <title>RELATED WORK</title>
                </section>
                <section>
                    <word_count>216</word_count>
                    <figure_citations></figure_citations>
                    <section_index>2</section_index>
                    <title>RESEARCH DESIGN</title>
                </section>
                <section>
                    <word_count>1143</word_count>
                    <figure_citations>Figure 1(a)).Figure 1(b)).Figure 2(a)).Figure 2(c)).Figure 2(c)).</figure_citations>
                    <section_index>3</section_index>
                    <title>INTERFACE</title>
                </section>
                <section>
                    <word_count>1699</word_count>
                    <figure_citations>Figure 2(c)).</figure_citations>
                    <section_index>4</section_index>
                    <title>AUGMENTED BODY SYSTEM</title>
                </section>
                <section>
                    <word_count>750</word_count>
                    <figure_citations>Figure 2(b)) and used retroreflective markers with Velcro stickers to put on the volunteer student’s clothing (Figure 1(a)).Figure 3(a)), thus enabling the teacher and students to draw directly on the volunteer’s body and to select muscle(s) or bone(s).Figure 3(b)) which the teacher used to select brush size, color, eraser, create groups of muscles/bones, and to selectively turn on/off the muscle and skeleton view as well as remove specific muscles to reveal the inner layers (Figure 4(a)) using the developed pen.Figure 4(b)).</figure_citations>
                    <section_index>5</section_index>
                    <title>CUSTOM PEN INTERFACE</title>
                </section>
                <section>
                    <word_count>2277</word_count>
                    <figure_citations></figure_citations>
                    <section_index>6</section_index>
                    <title>OUTCOMES OF AUGMENTED BODY SYSTEM</title>
                </section>
                <section>
                    <word_count>1020</word_count>
                    <figure_citations></figure_citations>
                    <section_index>7</section_index>
                    <title>DISCUSSION</title>
                </section>
                <section>
                    <word_count>144</word_count>
                    <figure_citations></figure_citations>
                    <section_index>8</section_index>
                    <title>CONCLUSION</title>
                </section>
                <section>
                    <word_count>66</word_count>
                    <figure_citations></figure_citations>
                    <section_index>9</section_index>
                    <title>ACKNOWLEDGMENTS</title>
                </section>
                <section>
                    <word_count>753</word_count>
                    <figure_citations></figure_citations>
                    <section_index>10</section_index>
                    <title>REFERENCES</title>
                </section>
            </sections>
            <keywords>
                <li>Augmented reality</li>
                <li>Pen-based interactions</li>
                <li>Annotation</li>
                <li>Projection mapping</li>
                <li>Educational system</li>
            </keywords>
            <authors>
                <li>Hasan Shahid Ferdous</li>
                <li> Thuong Hoang</li>
                <li> Zaher Joukhadar</li>
                <li> Martin N. Reinoso</li>
                <li> Frank Vetere</li>
                <li> David Kelly</li>
                <li> Louisa Remedios</li>
            </authors>
        </data>
        <figures>
            <figure>
                <id>1</id>
                <url>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom_crop_1.jpg</url>
                <caption>Figure 1: Augmented Body system for physiotherapy educa-
                    tion - (a) projection mapping on a student’s body, (b) pen-
                    based tablet interface for annotation, and (c) pen-based selec-
                    tion/drawing on the physical body.
                </caption>
                <page>2</page>
            </figure>
            <figure>
                <id>2</id>
                <url>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom_crop_2.jpg</url>
                <caption>Figure 2: Schematic diagram of the setup and tablet interface (a) Study one setup, (b) Study two setup, and (c) Hand drawing
                    annotations (green and red strokes on the upper thigh) and text annotation (green window) in the tablet device.
                </caption>
                <page>4</page>
            </figure>
            <figure>
                <id>3</id>
                <url>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom_crop_3.jpg</url>
                <caption>Figure 3: (a) 3D printed pen, (b) Projected palette, and (c) Sam-
                    ple of notes taken during projection-based classes.
                </caption>
                <page>5</page>
            </figure>
            <figure>
                <id>4</id>
                <url>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom_crop_4.jpg</url>
                <caption>Figure 4: (a) Teacher could remove upper layer to reveal inner
                    layer of muscles and bones, (b) Part of the classroom.
                </caption>
                <page>5</page>
            </figure>
        </figures>
    </article>
</items>
