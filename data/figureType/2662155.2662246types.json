{
    "1": {
        "type": "human",
        "caption_text": "Figure 1.  A virtual window moved at slow speed and during an eye blink to unobtrusively manipulate users\u2019 posture.  With two perception threshold \nstudies, We estimated unnoticeable motion speed and displacement per blink.  The formative study showed that unobtrusive motions triggered body \nmovements in a sequence (neck, torso, and chair) without affecting users\u2019 performance. \n",
        "image_labels": [
            [
                "Footwear",
                0.9838379621505737
            ],
            [
                "Trousers",
                0.9654543399810791
            ],
            [
                "Shoe",
                0.9618755578994751
            ],
            [
                "Leg",
                0.9128629565238953
            ],
            [
                "Shorts",
                0.8777278661727905
            ],
            [
                "Knee",
                0.8536252379417419
            ],
            [
                "Sneakers",
                0.7876554727554321
            ],
            [
                "Office chair",
                0.7824615836143494
            ],
            [
                "Elbow",
                0.7727838158607483
            ],
            [
                "Chair",
                0.7461521625518799
            ]
        ]
    },
    "2": {
        "type": "diagram",
        "caption_text": "Figure  2.  We  tested  six  blocks  of  motion:  three  translations  {BTX \n(left, right), BTY (up, down), BTZ (forward, backward)} and three ro-\ntations  {BRX  (up,  down),  BRY  (left,  right),  BRZ  (clockwise,  counter-\nclockwise)}. \n",
        "image_labels": [
            [
                "Slope",
                0.8918744921684265
            ],
            [
                "Organism",
                0.8677355647087097
            ],
            [
                "Gesture",
                0.852604866027832
            ],
            [
                "Font",
                0.837387204170227
            ],
            [
                "Line",
                0.8174643516540527
            ],
            [
                "Parallel",
                0.787397027015686
            ],
            [
                "Elbow",
                0.7676591277122498
            ],
            [
                "Circle",
                0.7446417212486267
            ],
            [
                "Symmetry",
                0.7403585910797119
            ],
            [
                "Pattern",
                0.7248106598854065
            ]
        ]
    },
    "3": {
        "type": "diagram",
        "caption_text": "Figure 3.  Based on ergonomics guidelines,  we positioned the virtual window within a comfortable viewing distance and angle.  The 0.73 by 0.73 m \nwindow was placed at 6 degrees below and 2 meters away from the eyes. \n",
        "image_labels": [
            [
                "Joint",
                0.9764668941497803
            ],
            [
                "Hand",
                0.9579055309295654
            ],
            [
                "Eye",
                0.9408246278762817
            ],
            [
                "Human body",
                0.8884807229042053
            ],
            [
                "Gesture",
                0.852604866027832
            ],
            [
                "Triangle",
                0.8344062566757202
            ],
            [
                "Line",
                0.8188470005989075
            ],
            [
                "Font",
                0.8181419372558594
            ],
            [
                "Slope",
                0.8165618777275085
            ],
            [
                "Elbow",
                0.7945366501808167
            ]
        ]
    },
    "4": {
        "type": "chart",
        "caption_text": "Figure 4.  The plotted result of users\u2019 detection threshold on constant slow translations (up) and rotations (bottom):  The x-axis shows motion speeds, \nand the y-axis shows the probability of the participants\u2019 responses for observed motion direction. The psychometric function shows upper threshold (p \n= 75%) for positive direction and lower threshold (p = 25%) for negative direction. Speeds within the blue area are considered unobtrusive. \n",
        "image_labels": [
            [
                "Product",
                0.9074094891548157
            ],
            [
                "Azure",
                0.8982409834861755
            ],
            [
                "Rectangle",
                0.8975188732147217
            ],
            [
                "Slope",
                0.8948636054992676
            ],
            [
                "Font",
                0.8429242372512817
            ],
            [
                "Line",
                0.819064199924469
            ],
            [
                "Parallel",
                0.7874690890312195
            ],
            [
                "Pattern",
                0.7704672813415527
            ],
            [
                "Number",
                0.724178671836853
            ],
            [
                "Diagram",
                0.7134228944778442
            ]
        ]
    },
    "5": {
        "type": "chart",
        "caption_text": "Figure 5.  The plotted result of users\u2019 detection threshold on the translations (up) and rotations (down) during a blink:  Displacements within the blue \narea are considered unobtrusive. \n",
        "image_labels": [
            [
                "Colorfulness",
                0.9619269967079163
            ],
            [
                "Rectangle",
                0.908554196357727
            ],
            [
                "Product",
                0.908089816570282
            ],
            [
                "Azure",
                0.9025905132293701
            ],
            [
                "Slope",
                0.8998447060585022
            ],
            [
                "Font",
                0.8578701019287109
            ],
            [
                "Line",
                0.8157672882080078
            ],
            [
                "Parallel",
                0.7878565192222595
            ],
            [
                "Pattern",
                0.7717899084091187
            ],
            [
                "Number",
                0.7016584277153015
            ]
        ]
    },
    "6": {
        "type": "graphic",
        "caption_text": "Figure 6. We measured the participants\u2019 IPD with a wearable ruler and \nadjusted the HMD (top). We logged the participants\u2019 motion with HMD \nand each tracker on their chest as well as the chair (bottom). \n",
        "image_labels": [
            [
                "Musical instrument",
                0.9105615615844727
            ],
            [
                "Chair",
                0.8432666063308716
            ],
            [
                "Musician",
                0.8357027173042297
            ],
            [
                "Line",
                0.8178580403327942
            ],
            [
                "Font",
                0.7859464883804321
            ],
            [
                "Music",
                0.7752169370651245
            ],
            [
                "Electronic instrument",
                0.7670919895172119
            ],
            [
                "Technology",
                0.7338107228279114
            ],
            [
                "Drum",
                0.7316197156906128
            ],
            [
                "Audio equipment",
                0.7278638482093811
            ]
        ]
    },
    "7": {
        "type": "table",
        "caption_text": "Figure  7.  We  manipulated  participants\u2019  posture  from  upright  sitting \nto  leaning  left  (A),  forward  (B),  and  backward  (C).  For  each  posture \nmanipulation, we compared three conditions: not-moving as the baseline \nbehavior, swift motion, and unobtrusive motion. \n",
        "image_labels": [
            [
                "Rectangle",
                0.8951802253723145
            ],
            [
                "Slope",
                0.8763337135314941
            ],
            [
                "Font",
                0.8401980400085449
            ],
            [
                "Parallel",
                0.7909203171730042
            ],
            [
                "Pattern",
                0.7475697994232178
            ],
            [
                "Number",
                0.6962868571281433
            ],
            [
                "Diagram",
                0.6324191093444824
            ],
            [
                "Circle",
                0.5947527885437012
            ],
            [
                "Electric blue",
                0.5488976836204529
            ]
        ]
    },
    "8": {
        "type": "chart",
        "caption_text": "Figure 8. We plotted the motion data of a representative participant for each motion: Left translation (top), receding (middle), and rotating up (bottom). \nDuring the left translation and up rotation, unobtrusive motions triggered continuous neck and back movements.  The swift motion triggered abrupt \nand mixed body movements. During the receding motions, the swift motion triggered a couple of chair movements in big steps, whereas the unobtrusive \nmotion triggered several chair movements in smaller steps. \n",
        "image_labels": [
            [
                "Rectangle",
                0.9042928218841553
            ],
            [
                "Slope",
                0.8710575103759766
            ],
            [
                "Font",
                0.8442609906196594
            ],
            [
                "Line",
                0.8253229856491089
            ],
            [
                "Plot",
                0.8109711408615112
            ],
            [
                "Parallel",
                0.7908174991607666
            ],
            [
                "Triangle",
                0.7707618474960327
            ],
            [
                "Technology",
                0.7557811737060547
            ],
            [
                "Pattern",
                0.7553244233131409
            ],
            [
                "Circle",
                0.7538408041000366
            ]
        ]
    },
    "9": {
        "type": "human",
        "caption_text": "Figure 9.  We designed three concept applications.  The \ufb01rst application \nmakes a user rollover from prone to supine and to lying position (top). \nThe second application coerces a user to stand up (left). The third appli-\ncation triggers active sitting (right). \n",
        "image_labels": [
            [
                "Clothing",
                0.9873540997505188
            ],
            [
                "Joint",
                0.9763984680175781
            ],
            [
                "Arm",
                0.945381224155426
            ],
            [
                "Photograph",
                0.9441344738006592
            ],
            [
                "White",
                0.9218668937683105
            ],
            [
                "Leg",
                0.9149612784385681
            ],
            [
                "Hat",
                0.8906753659248352
            ],
            [
                "Shorts",
                0.8861598968505859
            ],
            [
                "Comfort",
                0.8810833692550659
            ],
            [
                "Sleeve",
                0.8724489808082581
            ]
        ]
    }
}