<article>
    <filename>2662155.2662246</filename>
    <data>
        <paper_id>2662155.2662246</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376794</doi>
        <sections>
            <word_count>75</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>KAIST</title>
        </sections>
        <sections>
            <word_count>893</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>921</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>3666</word_count>
            <figure_citations>Figure 2).Figure 3, we placed the target object 2 m from the eyes and six degrees below the eye level based on an ergonomic recommendation [49].Figure 5 shows the positive responses means and standard errors, and the ﬁtted psychometric func- Paper 665 tion.Figure 7), we compared the three conditions (unnoticeable slow motion, single swift motion, and no motion) on three distinct manipulations left (1.</figure_citations>
            <section_index>3</section_index>
            <title>PERCEPTION THRESHOLD STUDY</title>
        </sections>
        <sections>
            <word_count>913</word_count>
            <figure_citations>Figure 8).</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>458</word_count>
            <figure_citations>Figure 9) to explore the design space of interfaces that induce posture changes.</figure_citations>
            <section_index>5</section_index>
            <title>APPLICATIONS</title>
        </sections>
        <sections>
            <word_count>636</word_count>
            <figure_citations>Figure 8, there is a range that the participants engage continuous neck rotation and beyond that range with abrupt body movements (e.</figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>330</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>34</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>273</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <sections>
            <word_count>2098</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>CANADIAN INFORMATION PROCESSING</title>
        </sections>
        <title>Body Follows Eye: Unobtrusive Posture Manipulation Through a Dynamic Content Position in Virtual Reality</title>
        <abstract>While virtual objects are likely to be a part of future interfaces, we lack knowledge of how the dynamic position of virtual objects influences users' posture. In this study, we investigated users' posture change following the unobtrusive and swift motions of a content window in virtual reality (VR). In two perception studies, we estimated the perception threshold on undetectable slow motions and displacement during an eye blink. In a formative study, we compared users' performance, posture change as well as subjective responses on unobtrusive, swift, and no motions. Based on the result, we designed concept applications and explored potential design space of moving virtual content for unobtrusive posture change. With our study, we discuss the interfaces that control users and the initial design guidelines of unobtrusive posture manipulation.</abstract>
        <keywords>
            <li>Posture change</li>
            <li>Unobtrusive interaction</li>
            <li>Virtual Reality</li>
        </keywords>
        <authors>
            <li>Joon Gi Shin</li>
            <li> Doheon Kim</li>
            <li> Chaehan So</li>
            <li> Daniel Saakes</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>2662155.2662246_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>2662155.2662246_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>2662155.2662246_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>2662155.2662246_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>2662155.2662246_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>2662155.2662246_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>2662155.2662246_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>2662155.2662246_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>2662155.2662246_crop_9.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173587</filename>
    <data>
        <paper_id>3173574.3173587</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173587</doi>
        <sections>
            <word_count>526</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1486</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>712</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>METHODS</title>
        </sections>
        <sections>
            <word_count>1356</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1411</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSIONS</title>
        </sections>
        <sections>
            <word_count>492</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>LIMITATIONS OF THE STUDY</title>
        </sections>
        <sections>
            <word_count>18</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>273</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <sections>
            <word_count>841</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <title>MABLE: Mediating Young Children's Smart Media Usage with Augmented Reality</title>
        <abstract>There has been a growing concern over the huge increase in use of smart media by young children. This study explores the possibility of using augmented-reality(AR) for regulat-ing preschoolers' media usage behavior. With MABLE (mobile application for behavioral learning and education), parents can provide AR-assisted feedback by changing facial expressions and sound effects. When overlaying a smart media, which has MABLE running, in front of a QR marker on a puppet, a facial expression is displayed on top of the puppet's face. A two-week long experiment with 36 parent-child pairs showed that compared to using just the puppet, using MABLE showed higher amount of engage-ment among preschoolers. For the effectiveness of parental mediation in terms of self-control, our data showed mixed results. MABLE had positive effects in that the amount of rule-compliance increased and problematic behaviors de-creased, whereas the level of behavioral dependency on smart media was not influenced.</abstract>
        <keywords>
            <li>Preschoolers</li>
            <li>Rule compliance</li>
            <li>Engagement</li>
            <li>Parental mediation</li>
            <li>Augmented reality (AR)</li>
            <li>Smart media usage</li>
        </keywords>
        <authors>
            <li>Gahgene Gweon</li>
            <li> Bugeun Kim</li>
            <li> Jinyoung Kim</li>
            <li> Kung Jin Lee</li>
            <li> Jungwook Rhim</li>
            <li> Jueun Choi</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173587_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173587_crop_2.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173616</filename>
    <data>
        <paper_id>3173574.3173616</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173616</doi>
        <sections>
            <word_count>748</word_count>
            <figure_citations>Figure 1 illustrates such an interaction.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>5118</word_count>
            <figure_citations>Figure 1, we sampled 60 positions on a circular sphere surface around the user’s body.Figure 2 shows the setting of the experiment and the task interface.Figure 3 summarizes the result on different horizontal and vertical angle levels.Figure 3) and participants could see the targets when acquiring them.Figure 4 shows, the minimum distance between targets and comfort levels distributed symmetrically on both sides.Figure 4, we found a negative correlation between the distance and the rating, supported by a Point-Biserial test (correlation = -0.Figure 5, the user rotated from the left to the right, and the relative horizontal angle of the target to the user changed from alpha to phi.Figure 6 visualized the main results of this experiment.Figure 7(c) visualizes the difference of the averaged offsets for twelve horizontal angles of the target positions.Figure 7(d) shows, the spatial offset of the acquisition increased with the number of rotations users performed before the acquisition.Figure 7(a), along the horizontal angle of the target position, the horizontal angular offset distributed symmetrically about the point of 0 degrees.Figure 7(c) shows, similar to the spatial offset, the movement amplitude increased as the target positions changed from 0 degrees in the front to both sides.Figure 8 shows the interpolation result of the standard deviations, which designers could refer to arrange target locations for eyes-free target acquisitions.</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>2009</word_count>
            <figure_citations>Figure 9 shows, the targets were rendered as two 3×3 grids of spheres out of view, with one on each side of the user’s body.Figure 9 shows, the spheres were arranged as two three by three grids on both sides of users, which had equal distance (65cm) to users’ chest.Figure 11 visualizes the subjective ratings in all dimensions.</figure_citations>
            <section_index>2</section_index>
            <title>TARGET ACQUISITION</title>
        </sections>
        <sections>
            <word_count>457</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>197</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>LIMITATION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>191</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>59</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGEMENT</title>
        </sections>
        <sections>
            <word_count>1780</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality</title>
        <abstract>Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Target acquisition</li>
            <li>Eyes-free</li>
            <li>Proprioception</li>
        </keywords>
        <authors>
            <li>Yukang Yan</li>
            <li> Chun Yu</li>
            <li> Xiaojuan Ma</li>
            <li> Shuai Huang</li>
            <li> Hasan Iqbal</li>
            <li> Yuanchun Shi</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173616_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173616_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173616_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173616_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173616_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3173616_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3173616_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3173616_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3173574.3173616_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3173574.3173616_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3173574.3173616_crop_11.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173620</filename>
    <data>
        <paper_id>3173574.3173620</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173620</doi>
        <sections>
            <word_count>502</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>3685</word_count>
            <figure_citations>Figure 2a and 2b).Figure 2c and 2d).Figure 2e and 2f).Figure 3 shows an example of this with an application named “Snow Dome,” a remote MR collaboration application that demonstrates how AR and VR collaboration can be enhanced with multi-scale interaction.Figure 3a.Figure 3b.</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>3526</word_count>
            <figure_citations>Figure 3d and 3e show the original AR space and the result of the reconstruction showed to the VR user.Figure 4).Figure 4).Figure 4).</figure_citations>
            <section_index>2</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>610</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>257</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>CONCLUSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>23</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1623</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Mini-Me: An Adaptive Avatar for Mixed Reality Remote Collaboration</title>
        <abstract>We present Mini-Me, an adaptive avatar for enhancing Mixed Reality (MR) remote collaboration between a local Augmented Reality (AR) user and a remote Virtual Reality (VR) user. The Mini-Me avatar represents the VR user's gaze direction and body gestures while it transforms in size and orientation to stay within the AR user's field of view. A user study was conducted to evaluate Mini-Me in two collaborative scenarios: an asymmetric remote expert in VR assisting a local worker in AR, and a symmetric collaboration in urban planning. We found that the presence of the Mini-Me significantly improved Social Presence and the overall experience of MR collaboration.</abstract>
        <keywords>
            <li>Mixed reality</li>
            <li>Remote collaboration</li>
            <li>Augmented reality</li>
            <li>Virtual reality</li>
            <li>Remote embodiment</li>
            <li>Avatar</li>
            <li>Redirected</li>
            <li>Gaze</li>
            <li>Gesture</li>
            <li>Awareness</li>
        </keywords>
        <authors>
            <li>Thammathip Piumsomboon</li>
            <li> Gun A. Lee</li>
            <li> Jonathon D. Hart</li>
            <li> Barrett Ens</li>
            <li> Robert W. Lindeman</li>
            <li> Bruce H. Thomas</li>
            <li> Mark Billinghurst</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173620_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173620_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173620_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173620_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173620_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173628</filename>
    <data>
        <paper_id>3173574.3173628</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173628</doi>
        <sections>
            <word_count>1075</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>783</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1714</word_count>
            <figure_citations>Figure 4 shows the interaction/engagement gradient starting from the most engaged (touch) to the least engaged (observing).Figure 4 a,b).Figure 4 c,d) enables additional applications without the need to battle Paper 54 exclusion and isolation (e.</figure_citations>
            <section_index>2</section_index>
            <title>FACEDISPLAY</title>
        </sections>
        <sections>
            <word_count>1312</word_count>
            <figure_citations>Figure 7 a).</figure_citations>
            <section_index>3</section_index>
            <title>APPLICATIONS</title>
        </sections>
        <sections>
            <word_count>829</word_count>
            <figure_citations>Figure 8 summarizes the scores of the GEQ, SUS and SAM and Figure 9 shows responses for our own questions.</figure_citations>
            <section_index>4</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>54</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>HMD</title>
        </sections>
        <sections>
            <word_count>1148</word_count>
            <figure_citations>Figure 10).</figure_citations>
            <section_index>6</section_index>
            <title>NHMD</title>
        </sections>
        <sections>
            <word_count>999</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>334</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>44</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1732</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>FaceDisplay: Towards Asymmetric Multi-User Interaction for Nomadic Virtual Reality</title>
        <abstract>Mobile VR HMDs enable scenarios where they are being used in public, excluding all the people in the surrounding (Non-HMD Users) and reducing them to be sole bystanders. We present FaceDisplay, a modified VR HMD consisting of three touch sensitive displays and a depth camera attached to its back. People in the surrounding can perceive the virtual world through the displays and interact with the HMD user via touch or gestures. To further explore the design space of FaceDisplay, we implemented three applications (FruitSlicer, SpaceFace and Conductor) each presenting different sets of aspects of the asymmetric co-located interaction (e.g. gestures vs touch). We conducted an exploratory user study (n=16), observing pairs of people experiencing two of the applications and showing a high level of enjoyment and social interaction with and without an HMD. Based on the findings we derive design considerations for asymmetric co-located VR applications and argue that VR HMDs are currently designed having only the HMD user in mind but should also include Non-HMD Users.</abstract>
        <keywords>
            <li>Nomadic virtual reality</li>
            <li>Asymmetric virtual reality</li>
            <li>Multi-user virtual reality</li>
            <li>Co-located multiplayer</li>
        </keywords>
        <authors>
            <li>Jan Gugenheimer</li>
            <li> Evgeny Stemasov</li>
            <li> Harpreet Sareen</li>
            <li> Enrico Rukzio</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173628_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173628_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173628_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173628_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173628_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3173628_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3173628_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3173628_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3173574.3173628_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3173574.3173628_crop_10.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173638</filename>
    <data>
        <paper_id>3173574.3173638</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173638</doi>
        <sections>
            <word_count>665</word_count>
            <figure_citations>Figure 1), there is a conflict between the stereopsis depth cue and the occlusion depth cue.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>447</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>382</word_count>
            <figure_citations>Figure 1 Left).</figure_citations>
            <section_index>2</section_index>
            <title>DEPTH CONFLICTS IN VR VIDEO INTERFACES</title>
        </sections>
        <sections>
            <word_count>118</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>TECHNIQUES CONSIDERED</title>
        </sections>
        <sections>
            <word_count>10</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>A</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>B</title>
        </sections>
        <sections>
            <word_count>1400</word_count>
            <figure_citations>Figure 1, the current scene is still not geometrically consistent.Figure 2: Overview of Dynamic Depth.Figure 2A).Figure 2C).Figure 3: Halo Blur blurs the video content around the UI.Figure 3).Figure 4: Illustration of the Subtitles task (Left) and the Search task (Right).</figure_citations>
            <section_index>6</section_index>
            <title>C</title>
        </sections>
        <sections>
            <word_count>2558</word_count>
            <figure_citations>Figure 4 Left), participants were asked to watch videos with subtitles.Figure 4 Right), participants were asked to search for a target scene as quickly and accurately as possible.Figure 5 summarizes the ratings in both tasks.Figure 5: (Left) Summary of participants’ ratings to the subjective questionnaire in both tasks.Figure 5).</figure_citations>
            <section_index>7</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>173</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>39</word_count>
            <figure_citations>Figure 1, 2, and 3 use images from YouTube users Kevin Kunze under a Creative Commons license.</figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGEMENT</title>
        </sections>
        <sections>
            <word_count>1462</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Depth Conflict Reduction for Stereo VR Video Interfaces</title>
        <abstract>Applications for viewing and editing 360° video often render user interface (UI) elements on top of the video. For stereoscopic video, in which the perceived depth varies over the image, the perceived depth of the video can conflict with that of the UI elements, creating discomfort and making it hard to shift focus. To address this problem, we explore two new techniques that adjust the UI rendering based on the video content. The first technique dynamically adjusts the perceived depth of the UI to avoid depth conflict, and the second blurs the video in a halo around the UI. We conduct a user study to assess the effectiveness of these techniques in two stereoscopic VR video tasks: video watching with subtitles, and video search.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Stereoscopic</li>
            <li>360</li>
            <li>Subtitles</li>
            <li>Video interface</li>
        </keywords>
        <authors>
            <li>Cuong Nguyen</li>
            <li> Stephen DiVerdi</li>
            <li> Aaron Hertzmann</li>
            <li> Feng Liu</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173638_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173638_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173638_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173638_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173638_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173673</filename>
    <data>
        <paper_id>3173574.3173673</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173673</doi>
        <sections>
            <word_count>827</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1556</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED RESEARCH</title>
        </sections>
        <sections>
            <word_count>1987</word_count>
            <figure_citations>Figure 1) consisted of a warehouse with floor, walls, ceiling lights and structural pillars.Figure 2).Figure 2).Figure 3).Figure 4).</figure_citations>
            <section_index>2</section_index>
            <title>TRANSLATIONAL GAIN</title>
        </sections>
        <sections>
            <word_count>2062</word_count>
            <figure_citations>Figure 6, where the fixed position of the target evident in Figure 6.Figure 8) and on all the TLX subscales, with differences predominantly arising between {1.</figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1396</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>135</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>15</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1852</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Object Manipulation in Virtual Reality Under Increasing Levels of Translational Gain</title>
        <abstract>Room-scale Virtual Reality (VR) has become an affordable consumer reality, with applications ranging from entertainment to productivity. However, the limited physical space available for room-scale VR in the typical home or office environment poses a significant problem. To solve this, physical spaces can be extended by amplifying the mapping of physical to virtual movement (translational gain). Although amplified movement has been used since the earliest days of VR, little is known about how it influences reach-based interactions with virtual objects, now a standard feature of consumer VR. Consequently, this paper explores the picking and placing of virtual objects in VR for the first time, with translational gains of between 1x (a one-to-one mapping of a 3.5m*3.5m virtual space to the same sized physical space) and 3x (10.5m*10.5m virtual mapped to 3.5m*3.5m physical). Results show that reaching accuracy is maintained for up to 2x gain, however going beyond this diminishes accuracy and increases simulator sickness and perceived workload. We suggest gain levels of 1.5x to 1.75x can be utilized without compromising the usability of a VR task, significantly expanding the bounds of interactive room-scale VR.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Amplified movement</li>
            <li>Object manipulation</li>
            <li>Translational gain</li>
            <li>Redirected walking</li>
        </keywords>
        <authors>
            <li>Graham Wilson</li>
            <li> Mark McGill</li>
            <li> Matthew Jamieson</li>
            <li> Julie R. Williamson</li>
            <li> Stephen A. Brewster</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173673_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173673_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173673_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173673_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173673_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3173673_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3173673_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3173673_crop_8.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173702</filename>
    <data>
        <paper_id>3173574.3173702</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173702</doi>
        <sections>
            <word_count>567</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1250</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1480</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>WEIGHT MODELING APPROACH</title>
        </sections>
        <sections>
            <word_count>798</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>FIRST STUDY</title>
        </sections>
        <sections>
            <word_count>2478</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>SECOND STUDY</title>
        </sections>
        <sections>
            <word_count>1061</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>LIMITATIONS OF THE APPROACH</title>
        </sections>
        <sections>
            <word_count>215</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>31</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1142</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Breaking the Tracking: Enabling Weight Perception using Perceivable Tracking Offsets</title>
        <abstract>Virtual reality (VR) technology strives to enable a highly immersive experience for the user by including a wide variety of modalities (e.g. visuals, haptics). Current VR hardware however lacks a sufficient way of communicating the perception of weight of an object, resulting in scenarios where users can not distinguish between lifting a bowling ball or a feather. We propose a solely software based approach of simulating weight in VR by deliberately using perceivable tracking offsets. These tracking offsets nudge users to lift their arm higher and result in a visual and haptic perception of weight. We conducted two user studies showing that participants intuitively associated them with the sensation of weight and accept them as part of the virtual world. We further show that compared to no weight simulation, our approach led to significantly higher levels of presence, immersion and enjoyment. Finally, we report perceptional thresholds and offset boundaries as design guidelines for practitioners.</abstract>
        <keywords>
            <li>Weight perception</li>
            <li>Virtual reality</li>
            <li>Pseudo haptics</li>
        </keywords>
        <authors>
            <li>Michael Rietzler</li>
            <li> Florian Geiselhart</li>
            <li> Jan Gugenheimer</li>
            <li> Enrico Rukzio</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173702_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173702_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173702_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173702_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173702_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3173702_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3173702_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3173702_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3173574.3173702_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3173574.3173702_crop_10.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173739</filename>
    <data>
        <paper_id>3173574.3173739</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173739</doi>
        <sections>
            <word_count>545</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1096</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1777</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>498</word_count>
            <figure_citations>Figure 5 shows how this was done for the validation study.</figure_citations>
            <section_index>3</section_index>
            <title>REPLICATION SETUP CONSIDERATIONS</title>
        </sections>
        <sections>
            <word_count>569</word_count>
            <figure_citations>Figure 4 for the participant view of the virtual driving simulation environment).Figure 5).</figure_citations>
            <section_index>4</section_index>
            <title>VALIDATION STUDY</title>
        </sections>
        <sections>
            <word_count>2417</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>102</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>147</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>85</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1382</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>VR-OOM: Virtual Reality On-rOad driving siMulation</title>
        <abstract>Researchers and designers of in-vehicle interactions and interfaces currently have to choose between performing evaluation and human factors experiments in laboratory driving simulators or on-road experiments. To enjoy the benefit of customizable course design in controlled experiments with the immediacy and rich sensations of on-road driving, we have developed a new method and tools to enable VR driving simulation in a vehicle as it travels on a road. In this paper, we describe how the cost-effective and flexible implementation of this platform allows for rapid prototyping. A preliminary pilot test (N = 6), centered on an autonomous driving scenario, yields promising results, illustrating proof of concept and indicating that a basic implementation of the system can invoke genuine responses from test participants.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Autonomous vehicles</li>
            <li>Prototyping</li>
            <li>Design evaluation</li>
        </keywords>
        <authors>
            <li>David Goedicke</li>
            <li> Jamy Li</li>
            <li> Vanessa Evers</li>
            <li> Wendy Ju</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173739_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173739_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173739_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173739_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173739_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173759</filename>
    <data>
        <paper_id>3173574.3173759</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173759</doi>
        <sections>
            <word_count>635</word_count>
            <figure_citations>Figure 2).Figure 4) of illustration styles, textures, and rendering techniques to depict geometric detail, material, and lighting [26, 42].Figure 6).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1276</word_count>
            <figure_citations>Figure 3).Figure 4).Figure 14a), mechanical wing augmentation (Figure 14c), mini car (participant creation), Flintstones’ house (author creation), and large fan (Figure 14d).</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>378</word_count>
            <figure_citations>Figure 6a).Figure 6b).Figure 8).</figure_citations>
            <section_index>2</section_index>
            <title>SYSTEM OVERVIEW AND SETUP</title>
        </sections>
        <sections>
            <word_count>2184</word_count>
            <figure_citations>Figure 1 demonstrates the overall workflow of our system.Figure 2).Figure 7c–d).Figure 9).Figure 10).Figure 10).Figure 11) shows an orthographic projection of the active drawing canvas, a color palette, and other functionality and configuration settings (also see Figure 8).Figure 11a).Figure 12).Figure 12).Figure 13).Figure 14a).Figure 14).Figure 15c), softness (drapery or clothes, see Figure 17), or size and reachability (buildings, ceilings, or cars, Figure 14d).</figure_citations>
            <section_index>3</section_index>
            <title>COMPONENTS</title>
        </sections>
        <sections>
            <word_count>834</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>IMPLEMENTATION DETAILS</title>
        </sections>
        <sections>
            <word_count>1393</word_count>
            <figure_citations>Figure 14 shows some of the resulting artifacts.Figure 15a).</figure_citations>
            <section_index>5</section_index>
            <title>USER EVALUATION</title>
        </sections>
        <sections>
            <word_count>310</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>113</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>38</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1769</word_count>
            <figure_citations>Figure 14c), war helicopter (Figure 14a), large wall-fan (Figure 14d), and some additional results: CHInosaur as the Toronto Raptor (participant creation) and skirt drawn over a human model (author creation).</figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>SymbiosisSketch: Combining 2D & 3D Sketching for Designing Detailed 3D Objects in Situ</title>
        <abstract>We present SymbiosisSketch, a hybrid sketching system that combines drawing in air (3D) and on a drawing surface (2D) to create detailed 3D designs of arbitrary scale in an augmented reality (AR) setting. SymbiosisSketch leverages the complementary affordances of 3D (immersive, unconstrained, life-sized) and 2D (precise, constrained, ergonomic) interactions for in situ 3D conceptual design. A defining aspect of our system is the ongoing creation of surfaces from unorganized collections of 3D curves. These surfaces serve a dual purpose: as 3D canvases to map strokes drawn on a 2D tablet, and as shape proxies to occlude the physical environment and hidden curves in a 3D sketch. SymbiosisSketch users draw interchangeably on a 2D tablet or in 3D within an ergonomically comfortable canonical volume, mapped to arbitrary scale in AR. Our evaluation study shows this hybrid technique to be easy to use in situ and effective in transcending the creative potential of either traditional sketching or drawing in air.</abstract>
        <keywords>
            <li>3D drawing</li>
            <li>Design sketching</li>
            <li>Augmented reality</li>
        </keywords>
        <authors>
            <li>Rahul Arora</li>
            <li> Rubaiat Habib Kazi</li>
            <li> Tovi Grossman</li>
            <li> George Fitzmaurice</li>
            <li> Karan Singh</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173759_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173759_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173759_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173759_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173759_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3173759_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3173759_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3173759_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3173574.3173759_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3173574.3173759_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3173574.3173759_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>3173574.3173759_crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>3173574.3173759_crop_13.png</url>
        </figure>
        <figure>
            <id>14</id>
            <url>3173574.3173759_crop_14.png</url>
        </figure>
        <figure>
            <id>15</id>
            <url>3173574.3173759_crop_15.png</url>
        </figure>
        <figure>
            <id>16</id>
            <url>3173574.3173759_crop_16.png</url>
        </figure>
        <figure>
            <id>17</id>
            <url>3173574.3173759_crop_17.png</url>
        </figure>
        <figure>
            <id>18</id>
            <url>3173574.3173759_crop_18.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173778</filename>
    <data>
        <paper_id>3173574.3173778</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173778</doi>
        <sections>
            <word_count>30</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>AUTHOR KEYWORDS</title>
        </sections>
        <sections>
            <word_count>660</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>796</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1458</word_count>
            <figure_citations>Figure 1-time spent talking bar).Figure 2B) did not result in changing one’s proximity when needed; and c.Figure 2A).Figure 2B), and yellow representing an intermediate warning between the two zones (see Figure 2D).Figure 2E).Figure 2D) or ‘step away’ respectively (see Figure 2 E).Figure 2C).</figure_citations>
            <section_index>3</section_index>
            <title>SYSTEM DESIGN</title>
        </sections>
        <sections>
            <word_count>830</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>2223</word_count>
            <figure_citations>Figure 3, left).Figure 3, middle), yet at the Paper 204 individual level, three participants improved in the intervention condition (P1, P9, P11), three performed well in baseline and intervention (P2, P6, P8), and three showed reduced performance in the intervention condition (P5, P7, P10).Figure 3, right).</figure_citations>
            <section_index>5</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1200</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>DESIGN CONSIDERATIONS FOR THERAPUETIC VR</title>
        </sections>
        <sections>
            <word_count>351</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>169</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>48</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1288</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>vrSocial: Toward Immersive Therapeutic VR Systems for Children with Autism</title>
        <abstract>Social communication frequently includes nuanced nonverbal communication cues, including eye contact, gestures, facial expressions, body language, and tone of voice. This type of communication is central to face-to-face interaction, but can be challenging for children and adults with autism. Innovative technologies can provide support by augmenting human-delivered cuing and automated prompting. Specifically, immersive virtual reality (VR) offers an option to generalize social skill interventions by concretizing nonverbal information in real-time social interactions. In this work, we explore the design and evaluation of three nonverbal communication applications in immersive VR. The results of this work indicate that delivering real-time visualizations of proximity, speaker volume, and duration of one's speech is feasible in immersive VR and effective for real-time support for proximity regulation for children with autism. We conclude with design considerations for therapeutic VR systems.</abstract>
        <keywords>
            <li>Visualization</li>
            <li>Assistive technology</li>
            <li>Autism</li>
            <li>Proximity</li>
            <li>Prosody</li>
            <li>Immersive VR</li>
            <li>Accessibility</li>
        </keywords>
        <authors>
            <li>LouAnne E. Boyd</li>
            <li> Saumya Gupta</li>
            <li> Sagar B. Vikmani</li>
            <li> Carlos M. Gutierrez</li>
            <li> Junxiang Yang</li>
            <li> Erik Linstead</li>
            <li> Gillian R. Hayes</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173778_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173778_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173778_crop_3.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173792</filename>
    <data>
        <paper_id>3173574.3173792</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173792</doi>
        <sections>
            <word_count>403</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>61</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1205</word_count>
            <figure_citations>Figure 2a).Figure 2b) the fingers, and completes the selection by making the tips of the fingers touch (Figure 2c).Figure 3a), i.Figure 3b) while maintaining the same apparent size, thus appearing the same to the user (Figure 3a, b insets) as in the image plane interaction proposed by Pierce et al.Figure 4a), or smaller by taking it away from the face (Figure 4b).Figure 4a, b), whereas the window is erected against a horizontal surface, perpendicular to the user’s gaze, to enforce the best viewing angle (Figure 4c).Figure 5a).</figure_citations>
            <section_index>2</section_index>
            <title>PROJECTIVE WINDOWS</title>
        </sections>
        <sections>
            <word_count>115</word_count>
            <figure_citations>Figure 6a) for tracking hand movement and posture within the user’s view frustum, the Leap Motion sensor’s front-facing camera for a video feed of the real world, the Unity 3D engine for integrating virtual and physical elements (Figure 6b), and an Intel quad-core i7 3.</figure_citations>
            <section_index>3</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>428</word_count>
            <figure_citations>Figure 5b), when the zoom is large, the slope is steep, and when the zoom is small, the slope is gentle, so the user has finer control when the zoom is smaller.Figure 7) to a widget-based ray-casting technique (RC) using the same controller.</figure_citations>
            <section_index>4</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>1487</word_count>
            <figure_citations>Figure 8h), and others were move widgets (Figure 8f, g).Figure 8, 9).Figure 9) to simultaneously compare the discrete levels of various factors in the later statistical analyses: For example, moving a window from A to a target at C requires a linear movement of 4 m and an angular movement of -60°, whereas A to D requires 2 m and 30° respectively.Figure 9).Figure 10a).Figure 10b).Figure 11a■, F4,1675 = 45, p < 0.Figure 11b■, F8,1671=11, p < 0.Figure 11c■, F8,1671=17, p < 0.Figure 11d■, F6,1673=54, p < 0.Figure 11a■, F4,1675=19, p < 0.Figure 11b■, F8,1671=13, p < 0.Figure 11d■, F6,1673=23, p < 0.Figure 12c■, F8,1671=59, p < 0.Figure 12d■, F6,1673=30, p < 0.Figure 12a■, F4,1675=8.Figure 12b■, F8,1671=5.Figure 12c■, F8,1761=15, p < 0.Figure 12d■, F6,1673=51, p < 0.Figure 12a■, F4,1675=15, p < 0.Figure 12b■, F8,1671=17, p < 0.Figure 13).</figure_citations> <section_index>5</section_index>
                    <title>EXPERIMENT SESSION</title>
        </sections>
        <sections>
            <word_count>247</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>743</word_count>
            <figure_citations>Figure 11d) and with an absolute size ratio of 1 for RC (Figure 11c), irrespective of the technique.Figure 11d, 12d).</figure_citations>
            <section_index>7</section_index>
            <title>EXPLORATION SESSION</title>
        </sections>
        <sections>
            <word_count>415</word_count>
            <figure_citations>Figure 14) using our implementation (Figure 6).Figure 14a), the user can pull picture windows out of a laptop screen and easily scale and place them anywhere on nearby walls for visual reference, just as he or she would sticky notes, but with the ability to freely change the size.Figure 14a), the user can pick up a window from a laptop screen and place it on a tablet device to quickly change the input from typing to drawing, without having to swap applications.Figure 14b), the user can perform the grab gesture to instantly scan a notebook page and then generate a projective window from it, to scale and place it anywhere for reference.Figure 14c), the user can pick up a small movie window from a nearby table, play the preview of the movie by bringing it closer to the face [1], and then start playing the movie by projecting it onto a vertical wall.Figure 14d), the user can use the entire unbounded scene as a workspace, even projecting windows across large distances.</figure_citations>
            <section_index>8</section_index>
            <title>USER SCENARIOS</title>
        </sections>
        <sections>
            <word_count>607</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Projective Windows: Bringing Windows in Space to the Fingertip</title>
        <abstract>In augmented and virtual reality (AR and VR), there may be many 3D planar windows with 2D texts, images, and videos on them. However, managing the position, orientation, and scale of such a window in an immersive 3D workspace can be difficult. Projective Windows strategically uses the absolute and apparent sizes of the window at various stages of the interaction to enable the grabbing, moving, scaling, and releasing of the window in one continuous hand gesture. With it, the user can quickly and intuitively manage and interact with windows in space without any controller hardware or dedicated widget. Through an evaluation, we demonstrate that our technique is performant and preferable, and that projective geometry plays an important role in the design of spatial user interfaces.</abstract>
        <keywords>
            <li>Augmented reality</li>
            <li>Virtual reality</li>
            <li>3D window management</li>
        </keywords>
        <authors>
            <li>Joon Hyub Lee</li>
            <li> Sang-Gyun An</li>
            <li> Yongkwan Kim</li>
            <li> Seok-Hyung Bae</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173792_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173792_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173792_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173792_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173792_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3173792_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3173792_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3173792_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3173574.3173792_crop_9.png</url>
        </figure>
        <figure>
            <id>14</id>
            <url>3173574.3173792_crop_14.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173793</filename><data>
        <paper_id>3173574.3173793</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173793</doi>
        <sections>
            <word_count>695</word_count>
            <figure_citations>Figure 1), an AR system which provides fast estimation of the 3D locations of smart things and exploits the spatial relationships for location aware interactions.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>696</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1484</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>SCENARIOT</title>
        </sections>
        <sections>
            <word_count>915</word_count>
            <figure_citations>Figure 3, IoT controllers are deployed to smart things as well as to the AR device.Figure 3, the overall size of the board is 100mm × 100mm × 20mm with the units installed in position.</figure_citations>
            <section_index>3</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>1425</word_count>
            <figure_citations>Figure 4, we conducted 9 surveyings to collect the surveying data with r ∈ {2, 3, 5}m and n ∈ {1, 2, 4}.Figure 5 (right), we observed a mean error of 0.Figure 6 (left), the mean error for {1.Figure 7, with the condition of r = 5m, n = 2 and n = 4 presented a larger error (> 0.</figure_citations>
            <section_index>4</section_index>
            <title>TECHNICAL EVALUATION</title>
        </sections>
        <sections>
            <word_count>1730</word_count>
            <figure_citations>Figure 1 and Figure 12, we not only visualize the digital interfaces when the corresponding physical object is located inside the view, but also the ones outside.Figure 9, the overall average error decreased to 0.Figure 9, the average of the localization error over all 8 devices yielded 0.Figure 9, we suspect that the accuracy degradation was not just caused by the localization accuracy.Figure 10, we observed an average of 0.</figure_citations>
            <section_index>5</section_index>
            <title>TASK EVALUATION</title>
        </sections>
        <sections>
            <word_count>440</word_count>
            <figure_citations>Figure 14 (a).Figure 14 (b)).Figure 14 (c, d)).</figure_citations>
            <section_index>6</section_index>
            <title>EXAMPLE USE CASES</title>
        </sections>
        <sections>
            <word_count>248</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>DISCUSSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>354</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>LIMITATION</title>
        </sections>
        <sections>
            <word_count>123</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>65</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>ACKNOWLEDGEMENT</title>
        </sections>
        <sections>
            <word_count>276</word_count>
            <figure_citations></figure_citations>
            <section_index>11</section_index>
            <title>REFERENCES</title>
        </sections>
        <sections>
            <word_count>1196</word_count>
            <figure_citations></figure_citations>
            <section_index>12</section_index>
            <title>ELECTRICAL ENGINEERING AND COMPUTER</title>
        </sections>
        <title>Scenariot: Spatially Mapping Smart Things Within Augmented Reality Scenes</title>
        <abstract>The emerging simultaneous localizing and mapping (SLAM) based tracking technique allows the mobile AR device spatial awareness of the physical world. Still, smart things are not fully supported with the spatial awareness in AR. Therefore, we present Scenariot, a method that enables instant discovery and localization of the surrounding smart things while also spatially registering them with a SLAM based mobile AR system. By exploiting the spatial relationships between mobile AR systems and smart things, Scenariot fosters in-situ interactions with connected devices. We embed Ultra-Wide Band (UWB) RF units into the AR device and the controllers of the smart things, which allows for measuring the distances between them. With a one-time initial calibration, users localize multiple IoT devices and map them within the AR scenes. Through a series of experiments and evaluations, we validate the localization accuracy as well as the performance of the enabled spatial aware interactions. Further, we demonstrate various use cases through Scenariot.</abstract>
        <keywords>
            <li>Augmented Reality</li>
            <li>IoT</li>
            <li>Smart Environment</li>
            <li>Spatial Interactions</li>
            <li>Context Awareness</li>
            <li>Localization</li>
            <li>UWB</li>
            <li>SLAM</li>
        </keywords>
        <authors>
            <li>Ke Huo</li>
            <li> Yuanzhi Cao</li>
            <li> Sang Ho Yoon</li>
            <li> Zhuangying Xu</li>
            <li> Guiming Chen</li>
            <li> Karthik Ramani</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173793_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173793_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173793_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173793_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173793_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3173793_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3173793_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3173793_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3173574.3173793_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3173574.3173793_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3173574.3173793_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>3173574.3173793_crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>3173574.3173793_crop_13.png</url>
        </figure>
        <figure>
            <id>14</id>
            <url>3173574.3173793_crop_14.png</url>
        </figure>
        <figure>
            <id>15</id>
            <url>3173574.3173793_crop_15.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173863</filename><data>
        <paper_id>3173574.3173863</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173863</doi>
        <sections>
            <word_count>689</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1196</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1417</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>METHODS</title>
        </sections>
        <sections>
            <word_count>2274</word_count>
            <figure_citations>Figure 2e.Figure 2f.Figure 2a).Figure 2g.Figure 2c).</figure_citations>
            <section_index>3</section_index>
            <title>ANNOTATED PARTICIPANT BEHAVIOR</title>
        </sections>
        <sections>
            <word_count>313</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>SEMANTIC DIFFERENCE MEASURE OF SOCIAL PRESENCE</title>
        </sections>
        <sections>
            <word_count>475</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>NETWORKED MINDS MEASURE OF SOCIAL PRESENCE</title>
        </sections>
        <sections>
            <word_count>1783</word_count>
            <figure_citations>Figure 2 the interaction.</figure_citations>
            <section_index>6</section_index>
            <title>PARTICIPANT PREFERENCES AND EXIT INTERVIEWS</title>
        </sections>
        <sections>
            <word_count>342</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>15</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <sections>
            <word_count>906</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>APPEARANCE TO CREATE ENERGY SAVINGS</title>
        </sections>
        <sections>
            <word_count>311</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>INDUSTRIAL NEGOTIATION AT THE PLANT</title>
        </sections>
        <title>Communication Behavior in Embodied Virtual Reality</title>
        <abstract>Embodied virtual reality faithfully renders users' movements onto an avatar in a virtual 3D environment, supporting nuanced nonverbal behavior alongside verbal communication. To investigate communication behavior within this medium, we had 30 dyads complete two tasks using a shared visual workspace: negotiating an apartment layout and placing model furniture on an apartment floor plan. Dyads completed both tasks under three different conditions: face-to-face, embodied VR with visible full-body avatars, and no embodiment VR, where the participants shared a virtual space, but had no visible avatars. Both subjective measures of users' experiences and detailed annotations of verbal and nonverbal behavior are used to understand how the media impact communication behavior. Embodied VR provides a high level of social presence with conversation patterns that are very similar to face-to-face interaction. In contrast, providing only the shared environment was generally found to be lonely and appears to lead to degraded communication.</abstract>
        <keywords>
            <li>Computer-mediated communication</li>
            <li>Virtual reality</li>
            <li>Embodiment</li>
            <li>Social presence</li>
        </keywords>
        <authors>
            <li>Harrison Jesse Smith</li>
            <li> Michael Neff</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173863_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173863_crop_2.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173902</filename><data>
        <paper_id>3173574.3173902</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173902</doi>
        <sections>
            <word_count>1014</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>6060</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>230</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>IEQ</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>VSA</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>VSM</title>
        </sections>
        <sections>
            <word_count>701</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>VSAM</title>
        </sections>
        <sections>
            <word_count>268</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>GENERAL DISCUSSION AND DESIGN GUIDELINES</title>
        </sections>
        <sections>
            <word_count>463</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>204</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>60</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1729</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Vanishing Importance: Studying Immersive Effects of Game Audio Perception on Player Experiences in Virtual Reality</title>
        <abstract>Sound and virtual reality (VR) are two important output modalities for creating an immersive player experience (PX). While prior research suggests that sounds might contribute to a more immersive experience in games played on screens and mobile displays, there is not yet evidence of these effects of sound on PX in VR. To address this, we conducted a within-subjects experiment using a commercial horror-adventure game to study the effects of a VR and monitor-display version of the same game on PX. Subsequently, we explored, in a between-subjects study, the effects of audio dimensionality on PX in VR. Results indicate that audio has a more implicit influence on PX in VR because of the impact of the overall sensory experience and that audio dimensionality in VR may not be a significant factor contributing to PX. Based on our findings and observations, we provide five design guidelines for VR games.</abstract>
        <keywords>
            <li>Games</li>
            <li>Audio</li>
            <li>Virtual reality</li>
            <li>Player experience</li>
            <li>Ambient noises</li>
            <li>Background music</li>
            <li>Sound effects</li>
        </keywords>
        <authors>
            <li>Katja Rogers</li>
            <li> Giovanni Ribeiro</li>
            <li> Rina R. Wehbe</li>
            <li> Michael Weber</li>
            <li> Lennart E. Nacke</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173902_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173902_crop_2.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173917</filename><data>
        <paper_id>3173574.3173917</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173917</doi>
        <sections>
            <word_count>591</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1031</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1053</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>STUDY METHODOLOGY</title>
        </sections>
        <sections>
            <word_count>4916</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1453</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>166</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>12</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1805</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Are You Dreaming?: A Phenomenological Study on Understanding Lucid Dreams as a Tool for Introspection in Virtual Reality</title>
        <abstract>Virtual reality (VR) is resurging in popularity with the advancement of low-cost hardware and more realistic graphics. How might this technology help others? That is, to increase mental well-being? The ultimate VR might look like lucid dreaming, the phenomenon of knowing one is dreaming while in the dream. Lucid dreaming can be used as an introspective tool and, ultimately, increase mental well-being. What these introspective experiences are like for lucid dreamers might be key in determining specific design guidelines for future creation of a technological tool used for helping people examine their own thoughts and emotions. This study describes nine active and proficient lucid dreamers' representations of their introspective experiences gained through phenomenological interviews. Four major themes emerged: sensations and feelings, actions and practices, influences on experience, and meaning making. This knowledge can help design a VR system that is grounded in genuine experience and preserving the human condition.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Design</li>
            <li>Lucid dreaming</li>
            <li>Introspection</li>
            <li>Wellbeing</li>
            <li>Positive technologies</li>
        </keywords>
        <authors>
            <li>Alexandra Kitson</li>
            <li> Thecla Schiphorst</li>
            <li> Bernhard E. Riecke</li>
        </authors>
    </data>
    <figures></figures>
</article>
<article>
    <filename>3173574.3173919</filename><data>
        <paper_id>3173574.3173919</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173919</doi>
        <sections>
            <word_count>698</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>477</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>280</word_count>
            <figure_citations>Figure 1: Side by side illustration of the real environment (left) and the virtual reality replica (right).</figure_citations>
            <section_index>2</section_index>
            <title>REALIZING TYPING IN VIRTUAL REALITY</title>
        </sections>
        <sections>
            <word_count>513</word_count>
            <figure_citations>Figure 2: Hand with 23 retroreflective markers (left) and the hardware setup for finger and keyboard tracking (right).</figure_citations>
            <section_index>3</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>708</word_count>
            <figure_citations>Figure 3: Pictures of the eight hand visualizations used in the study.</figure_citations>
            <section_index>4</section_index>
            <title>METHOD</title>
        </sections>
        <sections>
            <word_count>404</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>595</word_count>
            <figure_citations>Figure 4: Mean values of words per minute and corrected error rate for each condition.</figure_citations>
            <section_index>6</section_index>
            <title>WPM</title>
        </sections>
        <sections>
            <word_count>8</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>TRANSPARENCY</title>
        </sections>
        <sections>
            <word_count>5</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REAL WORLD</title>
        </sections>
        <sections>
            <word_count>66</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>M</title>
        </sections>
        <sections>
            <word_count>1204</word_count>
            <figure_citations>Figure 5: Subjective assessments of task load and presence.</figure_citations>
            <section_index>10</section_index>
            <title>SD</title>
        </sections>
        <sections>
            <word_count>801</word_count>
            <figure_citations></figure_citations>
            <section_index>11</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>36</word_count>
            <figure_citations></figure_citations>
            <section_index>12</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>132</word_count>
            <figure_citations></figure_citations>
            <section_index>13</section_index>
            <title>REFERENCES</title>
        </sections>
        <sections>
            <word_count>839</word_count>
            <figure_citations></figure_citations>
            <section_index>14</section_index>
            <title>CONCLUSION</title>
        </sections>
        <title>Physical Keyboards in Virtual Reality: Analysis of Typing Performance and Effects of Avatar Hands</title>
        <abstract>Entering text is one of the most common tasks when interacting with computing systems. Virtual Reality (VR) presents a challenge as neither the user's hands nor the physical input devices are directly visible. Hence, conventional desktop peripherals are very slow, imprecise, and cumbersome. We developed a apparatus that tracks the user's hands, and a physical keyboard, and visualize them in VR. In a text input study with 32 participants, we investigated the achievable text entry speed and the effect of hand representations and transparency on typing performance, workload, and presence. With our apparatus, experienced typists benefited from seeing their hands, and reach almost outside-VR performance. Inexperienced typists profited from semi-transparent hands, which enabled them to type just 5.6 WPM slower than with a regular desktop setup. We conclude that optimizing the visualization of hands in VR is important, especially for inexperienced typists, to enable a high typing performance.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Text entry</li>
            <li>Physical keyboard</li>
            <li>Hands</li>
        </keywords>
        <authors>
            <li>Pascal Knierim</li>
            <li> Valentin Schwind</li>
            <li> Anna Maria Feit</li>
            <li> Florian Nieuwenhuizen</li>
            <li> Niels Henze</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173919_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173919_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173919_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173919_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173919_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173927</filename><data>
        <paper_id>3173574.3173927</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173927</doi>
        <keywords>
            <li>H.5.2. Information interfaces and presentation: Input devices and strategies</li>
            <li>Interaction styles</li>
        </keywords>
        <authors>
            <li>undefined</li>
        </authors>
    </data>
</article>
<article>
    <filename>3173574.3173937</filename><data>
        <paper_id>3173574.3173937</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173937</doi>
        <sections>
            <word_count>493</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>739</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1377</word_count>
            <figure_citations>Figure 1).Figure 2).Figure 3-left), and to confirm the estimation by pressing controller’s trigger (using the soft "hair-trigger" trigger mode to mitigate unintended movement when clicking).Figure 3-right).Figure 7).</figure_citations>
            <section_index>2</section_index>
            <title>STUDY DESIGN</title>
        </sections>
        <sections>
            <word_count>23</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>CONDITION</title>
        </sections>
        <sections>
            <word_count>4050</word_count>
            <figure_citations>Figure 3).Figure 4).Figure 5 presents the error distribution per region and condition.Figure 6).Figure 6).Figure 7).Figure 7, 7-Likert scale) and the comments obtained during the interview were similar.Figure 8-left).Figure 8-left).Figure 8).Figure 9-top).Figure 9bottom).Figure 10).Figure 10 and Figure 6: both studies present similar distributions from the egocentric perspective, yet there is a shift towards zero for the Study 2.Figure 11).Figure 12).Figure 12).Figure 12) shows that both conditions present positive mean scores (MR: 1.</figure_citations>
            <section_index>4</section_index>
            <title>SAR</title>
        </sections>
        <sections>
            <word_count>898</word_count>
            <figure_citations>Figure 13 show three cases of distances between target and estimation.Figure 13).</figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>388</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>15</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1445</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Understanding Users' Capability to Transfer Information between Mixed and Virtual Reality: Position Estimation across Modalities and Perspectives</title>
        <abstract>Mixed Reality systems combine physical and digital worlds, with great potential for the future of HCI. It is possible to design systems that support flexible degrees of virtuality by combining complementary technologies. In order for such systems to succeed, users must be able to create unified mental models out of heterogeneous representations. In this paper, we present two studies focusing on the users' accuracy on heterogeneous systems using Spatial Augmented Reality (SAR) and immersive Virtual Reality (VR) displays, and combining viewpoints (egocentric and exocentric). The results show robust estimation capabilities across conditions and viewpoints.</abstract>
        <keywords>
            <li>Mixed Reality</li>
            <li>Spatial Augmented Reality</li>
            <li>Virtual Reality</li>
            <li>Evaluation</li>
            <li>Quantitative Methods</li>
        </keywords>
        <authors>
            <li>Joan Sol Roo</li>
            <li> Jean Basset</li>
            <li> Pierre-Antoine Cinquin</li>
            <li> Martin Hachet</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173937_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173937_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173937_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173937_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173937_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3173937_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3173937_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3173937_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3173574.3173937_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3173574.3173937_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3173574.3173937_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>3173574.3173937_crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>3173574.3173937_crop_13.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3173982</filename><data>
        <paper_id>3173574.3173982</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3173982</doi>
        <sections>
            <word_count>1368</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1119</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>693</word_count>
            <figure_citations>Figure 1a.Figure 1c).Figure 1d).Figure 1 b-d), similar to [74].Figure 1b).</figure_citations>
            <section_index>2</section_index>
            <title>EXERGAME DESIGN</title>
        </sections>
        <sections>
            <word_count>1712</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>EXPERIMENTAL DESIGN</title>
        </sections>
        <sections>
            <word_count>1064</word_count>
            <figure_citations>Figure 2 top-left).Figure 2 top-right).Figure 2 bottom-left).Figure 2 bottom-right) showed that SC led to a significantly higher performance, t(22) = 3.Figure 3 top-left).</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>26</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>VA</title>
        </sections>
        <sections>
            <word_count>13</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>NA</title>
        </sections>
        <sections>
            <word_count>15</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>FA</title>
        </sections>
        <sections>
            <word_count>8</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>NSC</title>
        </sections>
        <sections>
            <word_count>823</word_count>
            <figure_citations>Figure 3: IMI Interest/Enjoyment (left column) and IMI Pressure/Tension (right column) scores for the baseline (B), equal challenge (E) and harder challenge (H) conditions for 1) self competition (SC) framing in different levels of resistance awareness (top row) and 2) without resistance awareness (NA) in self competition (SC) vs.Figure 3 bottom-left).Figure 3 bottomright).Figure 4 top-left).Figure 4: FSQ Balance of Challenges and Skills (left column) and FSQ Absorption in the Task (right column) scores for the baseline (B), equal challenge (E) and harder challenge (H) conditions for 1) self competition (SC) framing in different levels of resistance awareness (top row) and 2) without resistance awareness (NA) in self competition (SC) vs.Figure 4 top-right).Figure 4 bottom-left).Figure 4 bottom-right).Figure 5: IEQ scores in different levels of resistance awareness (left) and competition framing (right).Figure 5 left).Figure 5 right) showed that there was a significant difference between SC and NSC, t(22) = 2.</figure_citations>
            <section_index>9</section_index>
            <title>H</title>
        </sections>
        <sections>
            <word_count>21</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>E</title>
        </sections>
        <sections>
            <word_count>375</word_count>
            <figure_citations>Figure 2: ∆Power (difference from baseline B) for the equal challenge (E) and harder challenge (H) conditions in different levels of resistance awareness (top-left) and in non-self (NSC) vs.Figure 3 top-right).</figure_citations>
            <section_index>11</section_index>
            <title>SC</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>12</section_index>
            <title>B</title>
        </sections>
        <sections>
            <word_count>8</word_count>
            <figure_citations></figure_citations>
            <section_index>13</section_index>
            <title>IEQ</title>
        </sections>
        <sections>
            <word_count>1049</word_count>
            <figure_citations></figure_citations>
            <section_index>14</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>142</word_count>
            <figure_citations></figure_citations>
            <section_index>15</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>77</word_count>
            <figure_citations></figure_citations>
            <section_index>16</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>2744</word_count>
            <figure_citations></figure_citations>
            <section_index>17</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Interactive Feedforward for Improving Performance and Maintaining Intrinsic Motivation in VR Exergaming</title>
        <abstract>Exergames commonly use low to moderate intensity exercise protocols. Their effectiveness in implementing high intensity protocols remains uncertain. We propose a method for improving performance while maintaining intrinsic motivation in high intensity VR exergaming. Our method is based on an interactive adaptation of the feedforward method: a psychophysical training technique achieving rapid improvement in performance by exposing participants to self models showing previously unachieved performance levels. We evaluated our method in a cycling-based exergame. Participants competed against (i) a self model which represented their previous speed; (ii) a self model representing their previous speed but increased resistance therefore requiring higher performance to keep up; or (iii) a virtual competitor at the same two levels of performance. We varied participants' awareness of these differences. Interactive feedforward led to improved performance while maintaining intrinsic motivation even when participants were aware of the interventions, and was superior to competing against a virtual competitor.</abstract>
        <keywords>
            <li>Feedforward</li>
            <li>Exergame</li>
            <li>Virtual reality (VR)</li>
            <li>Performance</li>
            <li>Intrinsic motivation</li>
        </keywords>
        <authors>
            <li>Soumya C. Barathi</li>
            <li> Daniel J. Finnegan</li>
            <li> Matthew Farrow</li>
            <li> Alexander Whaley</li>
            <li> Pippa Heath</li>
            <li> Jude Buckley</li>
            <li> Peter W. Dowrick</li>
            <li> Burkhard C. Wuensche</li>
            <li> James L. J. Bilzon</li>
            <li> Eamonn O'Neill</li>
            <li> Christof Lutteroth</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3173982_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3173982_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3173982_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3173982_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3173982_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3174020</filename><data>
        <paper_id>3173574.3174020</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3174020</doi>
        <sections>
            <word_count>595</word_count>
            <figure_citations>Figure 1: (a) In this Mixed Reality game that uses a physical tray as prop, our mobile system renders shifts in the tray’s center of gravity as the marble moves.Figure 1 illustrates this at the example of our Mixed Reality balance marble game using a physical tray as a game prop, which our approach augments via EMS-based force feedback.Figure 3 shows the user’s view through the HoloLens.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1298</word_count>
            <figure_citations>Figure 1a shows the system she is wearing, i.Figure 2 shows a user wearing our EMS for Mixed Reality system.Figure 2: Using a regular cup as an impromptu tangible brightness dial.Figure 3: The previous scene through the HoloLens.Figure 4: (a) This user physically drags the couch and feels the simulated friction against the floor.Figure 4 shows the user exploring different placements in the room by pushing a couch with her two hands.Figure 4a).Figure 4b).Figure 5, the user now explores a lamp from the catalog.Figure 5b).Figure 5: Turning on the virtual lamp.Figure 6: The user configures the intensity of the desired light bulb using a cup as a stand-in for a dial (passive prop).Figure 6, the user picks up a cup to serve as a tangible brightness dial.Figure 6b shows how our system also adds detents to the dial.Figure 7a).Figure 7: The user manipulates two cups to control the light temperature and intensity simultaneously.Figure 7b).Figure 7c, for example, the user chooses a “colder” light, causing the system to switch to a less intense bulb by actuating the user’s wrist as to reach that option (Figure 7d).Figure 8: Here, our system enhances a fully functional thermostat with detents.Figure 10 shows a simple MR game featuring a virtual catapult that appears in the user’s physical surroundings.</figure_citations>
            <section_index>1</section_index>
            <title>WALKTHROUGH OF A MIXED REALITY EXPERIENCE</title>
        </sections>
        <sections>
            <word_count>793</word_count>
            <figure_citations>Figure 1 depicted a classic MR marble maze, which was in fact inspired by that of Ohan and Feiner [48] and complemented with EMS-based force feedback for added realism.Figure 9: Walkthrough examples mapped to the realityvirtuality continuum by Milgram et al.Figure 10: While the user pulls the lever of this virtual catapult, our system provides force feedback simulating the catapult’s spring.Figure 11: (a) At the start of the game the marble falls from the sky.Figure 12 illustrates the user solving this room’s puzzle.Figure 12, the user finds that when moved the lamps have detents, rendered using our system, hence they can be only in one of three positions.</figure_citations>
            <section_index>2</section_index>
            <title>SUMMARY OF WALKTHROUGH</title>
        </sections>
        <sections>
            <word_count>1037</word_count>
            <figure_citations>Figure 12: (a) These gooseneck lamps are repurposed as levers, with force feedback, allowing the user to input the secret combination to (b) unlock the door.</figure_citations>
            <section_index>3</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1174</word_count>
            <figure_citations>Figure 3b.Figure 13: The hardware components and electrode placement (one arm only).Figure 13 details how 10 electrodes are placed on the user’s right arm and shoulder; the user’s left arm is equipped the same way.Figure 14: Stimulation parameters per haptic effect at the example of one study participant: amplitude (in mA), pulse-width (in µs) and duration (in ms).</figure_citations>
            <section_index>4</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>497</word_count>
            <figure_citations>Figure 13, which allowed for untethered use.Figure 15 shows participant’s average ratings in both conditions regarding perceived realism.</figure_citations>
            <section_index>5</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1014</word_count>
            <figure_citations>Figure 15: Participants rated their experience as more realistic in the EMS conditions.Figure 16, participants rated the enjoyment significantly higher when in the EMS condition for the furniture and the catapult tasks.Figure 16: Participants rated their experience as more enjoyable in most of the EMS conditions.Figure 17 summarizes participants’ preferences for each of the interface conditions.Figure 17: Most participants preferred the EMS to the no-EMS interface condition across tasks.Figure 18) polarized participants in that only 7 of them expressed a preference for experiencing it with EMS.Figure 18: Participant balancing the marble (image from the study, with consent of the participant).</figure_citations>
            <section_index>6</section_index>
            <title>EMS</title>
        </sections>
        <sections>
            <word_count>419</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSIONS AND OUTLOOK</title>
        </sections>
        <sections>
            <word_count>1976</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation</title>
        <abstract>We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered-not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMS-constraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMS-detents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline.</abstract>
        <keywords>
            <li>Games</li>
            <li>Mixed reality</li>
            <li>EMS</li>
        </keywords>
        <authors>
            <li>Pedro Lopes</li>
            <li> Sijing You</li>
            <li> Alexandra Ion</li>
            <li> Patrick Baudisch</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3174020_crop_1.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3174020_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3174020_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3174020_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3174020_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3174020_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3174020_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3173574.3174020_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3173574.3174020_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3173574.3174020_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>3173574.3174020_crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>3173574.3174020_crop_13.png</url>
        </figure>
        <figure>
            <id>14</id>
            <url>3173574.3174020_crop_14.png</url>
        </figure>
        <figure>
            <id>15</id>
            <url>3173574.3174020_crop_15.png</url>
        </figure>
        <figure>
            <id>16</id>
            <url>3173574.3174020_crop_16.png</url>
        </figure>
        <figure>
            <id>17</id>
            <url>3173574.3174020_crop_17.png</url>
        </figure>
        <figure>
            <id>18</id>
            <url>3173574.3174020_crop_18.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3174039</filename><data>
        <paper_id>3173574.3174039</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3174039</doi>
        <sections>
            <word_count>924</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>BACKGROUND AND RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1655</word_count>
            <figure_citations>Figure 1b, 2) which helps meet user requirements for using real surgical instruments instead of fake props.Figure 3a), which helped users judge the depth of the tip with greater ease.Figure 3c) and dot markers on 3 different planes were designed for CatAR calibration (Figure 3b) to solve the extrinsic and intrinsic camera parameters.Figure 4), which provided a realistic visual environment for microsurgery training.Figure 5b).Figure 5a, 5b).Figure 5c).Figure 5d).</figure_citations>
            <section_index>1</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>369</word_count>
            <figure_citations>Figure 6).Figure 7a).</figure_citations>
            <section_index>2</section_index>
            <title>SYSTEM EVALUATION</title>
        </sections>
        <sections>
            <word_count>294</word_count>
            <figure_citations>Figure 7b).Figure 7c).</figure_citations>
            <section_index>3</section_index>
            <title>RMSE</title>
        </sections>
        <sections>
            <word_count>445</word_count>
            <figure_citations>Figure 1c, 7e).Figure 7d).Figure 7f).</figure_citations>
            <section_index>4</section_index>
            <title>TRAINING MODULE DESIGN</title>
        </sections>
        <sections>
            <word_count>1096</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1419</word_count>
            <figure_citations>Figure 9).</figure_citations>
            <section_index>6</section_index>
            <title>RESULT</title>
        </sections>
        <sections>
            <word_count>633</word_count>
            <figure_citations>Figure 8) in our study.Figure 9).Figure 10).Figure 11a), the difference was not statistically significant.Figure 11b).</figure_citations>
            <section_index>7</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>134</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>187</word_count>
            <figure_citations>Figure 11c).</figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>599</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <sections>
            <word_count>694</word_count>
            <figure_citations></figure_citations>
            <section_index>11</section_index>
            <title>OF SURGICAL INSTRUMENTS IN THE EYE</title>
        </sections>
        <title>CatAR: A Novel Stereoscopic Augmented Reality Cataract Surgery Training System with Dexterous Instruments Tracking Technology</title>
        <abstract>We propose CatAR, a novel stereoscopic augmented reality (AR) cataract surgery training system. It provides dexterous instrument tracking ability using a specially designed infrared optical system with 2 cameras and 1 reflective marker. The tracking accuracy on the instrument tip is 20 µm, much higher than previous simulators. Moreover, our system allows trainees to use and to see real surgical instruments while practicing. Five training modules with 31 parameters were designed and 28 participants were enrolled to conduct efficacy and validity tests. The results revealed significant differences between novice and experienced surgeons. Improvements in surgical skills after practicing with CatAR were also significant.</abstract>
        <keywords>
            <li>Augmented reality</li>
            <li>Microsurgery</li>
            <li>Surgical simulator</li>
            <li>Surgical training</li>
            <li>Cataract</li>
            <li>Instrument tracking</li>
            <li>Dexterous input</li>
        </keywords>
        <authors>
            <li>Yu-Hsuan Huang</li>
            <li> Hao-Yu Chang</li>
            <li> Wan-ling Yang</li>
            <li> Yu-Kai Chiu</li>
            <li> Tzu-Chieh Yu</li>
            <li> Pei-Hsuan Tsai</li>
            <li> Ming Ouhyoung</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3174039_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3174039_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3174039_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3174039_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3174039_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3174039_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3174039_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3174039_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3173574.3174039_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3173574.3174039_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3173574.3174039_crop_11.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3174057</filename><data>
        <paper_id>3173574.3174057</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3174057</doi>
        <sections>
            <word_count>667</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>985</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>4083</word_count>
            <figure_citations>Figure 1, top left).Figure 1, bottom right).Figure 1, bottom left).Figure 2).</figure_citations>
            <section_index>2</section_index>
            <title>EXPLORING THE MOMENT OF EXIT</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>EXPERIENCE OF EXITING VR</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>LESSENING</title>
        </sections>
        <sections>
            <word_count>1144</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>THE</title>
        </sections>
        <sections>
            <word_count>1122</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>185</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>20</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1430</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>The Dream is Collapsing: The Experience of Exiting VR</title>
        <abstract>Research on virtual reality (VR) has studied users' experience of immersion, presence, simulator sickness, and learning effects. However, the momentary experience of exiting VR and transitioning back to the real-world is not well understood. Do users become self-conscious of their actions upon exit? Are users nervous of their surroundings? Using explicitation interviews, we explore the moment of exit from VR across four applications. Analysis of the interviews reveals five components of experience: space, control, sociality, time, and sensory adaptation. Participants described spatial disorientation, for example, regardless of the complexity of the VR scene. Participants also described a window across which they exit VR, for example mentally first and then physically. We present six designs for easing or heightening the exit experience, as described by the participants. Based on these findings, we further discuss the ?moment of exit' as an opportunity for designing engaging and enhanced VR experiences.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>User Experience</li>
        </keywords>
        <authors>
            <li>Jarrod Knibbe</li>
            <li> Jonas Schjerlund</li>
            <li> Mathias Petraeus</li>
            <li> Kasper Hornbæk</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3174057_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3174057_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3174057_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3174057_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3174057_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3174057_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3174057_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3174057_crop_8.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3174088</filename><data>
        <paper_id>3173574.3174088</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3174088.</doi>
        <keywords>
            <li>Dementia</li>
            <li>Care</li>
            <li>Virtual reality</li>
            <li>Augmented reality</li>
            <li>Creativity</li>
            <li>Experience</li>
            <li>Expression</li>
        </keywords>
        <authors>
            <li>undefined</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3174088_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3174088_crop_2.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3174151</filename><data>
        <paper_id>3173574.3174151</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3174151</doi>
        <sections>
            <word_count>906</word_count>
            <figure_citations>Figure 1: Season Traveller is a multisensory VR experience integrated with Samsung Gear VR HMD.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1029</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>BACKGROUND AND RELATED WORK</title>
        </sections>
        <sections>
            <word_count>2372</word_count>
            <figure_citations>Figure 1) that aims to distribute the weight of the system evenly between the front and the back of the user’s head.Figure 1).Figure 1).Figure 2, the chamber contained the smell emitting mechanisms, a control module, a fume extractor for clearing scented air between participant exposures, and a nose rest that ensured participants maintained a consistent distance from the stimuli.Figure 2: The experimental setup of the smell chamber used to compare olfactory delivery methods.Figure 3: Average sensation scores based on Labeled Magnitude Scale (LMS).Figure 4: Main modules of Season Traveller: (a) Control Module, (b) Olfactory Simulation Module, (c) Front Casing and Wind Simulation Module, (d) Thermal Simulation Module participants (10 males, average age = 26, SD = 2.</figure_citations>
            <section_index>2</section_index>
            <title>DESIGN</title>
        </sections>
        <sections>
            <word_count>615</word_count>
            <figure_citations>Figure 1, both the Control Module and all of the Simulation Modules were mounted on the Samsung Gear VR Paper 577 CHI 2018, April 21–26, 2018, Montréal, QC, Canada (a) Spring (b) Summer (c) Autumn (d) Winter Figure 5: The four seasons and their stimuli: a) Spring (Jasmine scent, medium wind strength and no thermal stimulation), b) Summer (Lemon scent, mild wind strength and heating stimulation), c) Autumn (Cinnamon scent, strong wind strength and no thermal stimulation) and d) Winter (Mint scent, medium wind strength and cooling stimulation).Figure 4).Figure 5).</figure_citations>
            <section_index>3</section_index>
            <title>SYSTEM IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>1854</word_count>
            <figure_citations>Figure 6 for study setup).Figure 6: The experimental setup of Season Traveller.Figure 7: Normalized average Sensory sub-factors scores for five system configurations (Error bars represent 95% CI, n = 20).Figure 8: Normalized average Sensory Factors scores for five system configurations (Error bars represent 95% CI, n = 20).Figure 9: Normalized average scores for item 30 (richness of experience) (Error bars represent 95% CI, n = 20).Figure 9, post-hoc tests further indicate a significant improvement (p = 0.Figure 10 presents the baseline adjusted HR of all the participants recorded through the different system configurations.Figure 11 presents the baseline adjusted electrical skin conductivity of all the participants.</figure_citations>
            <section_index>4</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>685</word_count>
            <figure_citations>Figure 10: Baseline adjusted HR of all the participants recorded through the five different system configurations.Figure 11: Baseline adjusted EDA of all the participants recorded through the five different system configurations.</figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION AND CONCLUSION</title>
        </sections>
        <sections>
            <word_count>29</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>42</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <sections>
            <word_count>1496</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ESSEX CORP WESTLAKE VILLAGE CA HUMAN</title>
        </sections>
        <title>Season Traveller: Multisensory Narration for Enhancing the Virtual Reality Experience</title>
        <abstract>In the same way that we experience the real-world through a range of senses, experiencing a virtual environment through multiple sensory modalities may augment both our presence within a scenario and our reaction to it. In this paper, we present Season Traveller, a multisensory virtual reality (VR) narration of a journey through four seasons within a mystical realm. By adding olfactory and haptic (thermal and wind) stimuli, we extend traditional audio-visual VR technologies to achieve enhanced sensory engagement within interactive experiences. Using both subjective measures of presence and elicited physiological responses, we evaluated the impact of different modalities on the virtual experience. Our results indicate that 1) the addition of any singular modality improves sense of presence with respect to traditional audio-visual experiences and 2) providing a combination of these modalities produces a further significant enhancement over the aforementioned improvements. Furthermore, insights into participants' psychophysiology were extrapolated from electrodermal activity (EDA) and heart rate (HR) measurements during each of the VR experiences.</abstract>
        <keywords>
            <li>Multisensory VR</li>
            <li>Virtual Reality</li>
            <li>Multimodal Interaction</li>
        </keywords>
        <authors>
            <li>Nimesha Ranasinghe</li>
            <li> Pravar Jain</li>
            <li> Nguyen Thi Ngoc Tram</li>
            <li> Koon Chuan Raymond Koh</li>
            <li> David Tolley</li>
            <li> Shienny Karwita</li>
            <li> Lin Lien-Ya</li>
            <li> Yan Liangkun</li>
            <li> Kala Shamaiah</li>
            <li> Chow Eason Wai Tung</li>
            <li> Ching Chiuan Yen</li>
            <li> Ellen Yi-Luen Do</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3174151_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3174151_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3174151_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3174151_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3174151_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3174151_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3174151_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3173574.3174151_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3173574.3174151_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3173574.3174151_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3173574.3174151_crop_11.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3173574.3174221</filename><data>
        <paper_id>3173574.3174221</paper_id>
        <venue>CHI 18</venue>
        <doi>10.1145/3173574.3174221</doi>
        <sections>
            <word_count>13</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>NG</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>HAND</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>T</title>
        </sections>
        <sections>
            <word_count>215</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>NUOUSCURSOR</title>
        </sections>
        <sections>
            <word_count>508</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>2892</word_count>
            <figure_citations>Figure 1): • Head Pointing (HP) — the participant selects a character by pointing to it with her head.</figure_citations>
            <section_index>5</section_index>
            <title>X</title>
        </sections>
        <sections>
            <word_count>1092</word_count>
            <figure_citations>Figure 1).Figure 1).Figure 1).Figure 1).</figure_citations>
            <section_index>6</section_index>
            <title>EVALUATED TEXT INPUT TECHNIQUES</title>
        </sections>
        <sections>
            <word_count>35</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>EMPIRICAL STUDY</title>
        </sections>
        <sections>
            <word_count>13</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>HP</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>CP</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>CT</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>11</section_index>
            <title>FH</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>12</section_index>
            <title>CC</title>
        </sections>
        <sections>
            <word_count>6</word_count>
            <figure_citations></figure_citations>
            <section_index>13</section_index>
            <title>DC</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>14</section_index>
            <title>WHO</title>
        </sections>
        <sections>
            <word_count>3</word_count>
            <figure_citations></figure_citations>
            <section_index>15</section_index>
            <title>WANTS TO</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>16</section_index>
            <title>VER</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>17</section_index>
            <title>TS TO LIVE</title>
        </sections>
        <sections>
            <word_count>746</word_count>
            <figure_citations>Figure 3).</figure_citations>
            <section_index>18</section_index>
            <title>FO</title>
        </sections>
        <sections>
            <word_count>508</word_count>
            <figure_citations></figure_citations>
            <section_index>19</section_index>
            <title>S</title>
        </sections>
        <sections>
            <word_count>174</word_count>
            <figure_citations>Figure 4).</figure_citations>
            <section_index>20</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>486</word_count>
            <figure_citations>Figure 5).</figure_citations>
            <section_index>21</section_index>
            <title>WPM</title>
        </sections>
        <sections>
            <word_count>961</word_count>
            <figure_citations></figure_citations>
            <section_index>22</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>23</section_index>
            <title>START</title>
        </sections>
        <sections>
            <word_count>11</word_count>
            <figure_citations></figure_citations>
            <section_index>24</section_index>
            <title>YES</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>25</section_index>
            <title>HEAD POINTING</title>
        </sections>
        <sections>
            <word_count>10</word_count>
            <figure_citations></figure_citations>
            <section_index>26</section_index>
            <title>NO</title>
        </sections>
        <sections>
            <word_count>138</word_count>
            <figure_citations></figure_citations>
            <section_index>27</section_index>
            <title>L</title>
        </sections>
        <sections>
            <word_count>14</word_count>
            <figure_citations></figure_citations>
            <section_index>28</section_index>
            <title>H</title>
        </sections>
        <sections>
            <word_count>97</word_count>
            <figure_citations></figure_citations>
            <section_index>29</section_index>
            <title>CURSOR CONTROL</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>30</section_index>
            <title>POINTING</title>
        </sections>
        <sections>
            <word_count>290</word_count>
            <figure_citations>Figure 7).</figure_citations>
            <section_index>31</section_index>
            <title>FREEHAND</title>
        </sections>
        <sections>
            <word_count>287</word_count>
            <figure_citations></figure_citations>
            <section_index>32</section_index>
            <title>CONCLUSION AND OUTLOOK</title>
        </sections>
        <sections>
            <word_count>1719</word_count>
            <figure_citations></figure_citations>
            <section_index>33</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Selection-based Text Entry in Virtual Reality</title>
        <abstract>In recent years, Virtual Reality (VR) and 3D User Interfaces (3DUI) have seen a drastic increase in popularity, especially in terms of consumer-ready hardware and software. While the technology for input as well as output devices is market ready, only a few solutions for text input exist, and empirical knowledge about performance and user preferences is lacking. In this paper, we study text entry in VR by selecting characters on a virtual keyboard. We discuss the design space for assessing selection-based text entry in VR. Then, we implement six methods that span different parts of the design space and evaluate their performance and user preferences. Our results show that pointing using tracked hand-held controllers outperforms all other methods. Other methods such as head pointing can be viable alternatives depending on available resources. We summarize our findings by formulating guidelines for choosing optimal virtual keyboard text entry methods in VR.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Text entry</li>
            <li>Pointing</li>
            <li>Mid-air</li>
            <li>User experience</li>
            <li>Task performance</li>
        </keywords>
        <authors>
            <li>Marco Speicher</li>
            <li> Anna Maria Feit</li>
            <li> Pascal Ziegler</li>
            <li> Antonio Krüger</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3173574.3174221_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3173574.3174221_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3173574.3174221_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3173574.3174221_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3173574.3174221_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3173574.3174221_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3173574.3174221_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376195</filename><data>
        <paper_id>3313831.3376195</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376195</doi>
        <sections>
            <word_count>760</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1376</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1117</word_count>
            <figure_citations>Figure 2a).Figure 2b).Figure 3c depicts a closeup perspective of the actuator tubes, as well as an example thermal camera view at 43 ◦C.Figure 3b shows a user wearing two actuators on the right arm and the abdomen.</figure_citations>
            <section_index>2</section_index>
            <title>THERMINATOR CONCEPTS AND SYSTEM</title>
        </sections>
        <sections>
            <word_count>2192</word_count>
            <figure_citations>Figure 1a) shows a participant while exposed to the snow visualization.Figure 3a.</figure_citations>
            <section_index>3</section_index>
            <title>METHODOLOGY</title>
        </sections>
        <sections>
            <word_count>1447</word_count>
            <figure_citations>Figure 5a.Figure 5b.Figure 5c, the median rating as well as the minimum and maximum ratings show differences with regards to the visuals.</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>895</word_count>
            <figure_citations>Figure 5a), the thermal stimuli have the highest impact on the perceived temperature.Figure 5b and Figure 5c), the involvement resulted in higher medians for both stimuli.Figure 5a), they only slightly affect the involvement if there is no visual stimulus displayed.Figure 6).</figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>433</word_count>
            <figure_citations>Figure 1d).Figure 1c), discovered on a treasure map.Figure 1b).</figure_citations>
            <section_index>6</section_index>
            <title>EXAMPLE APPLICATIONS</title>
        </sections>
        <sections>
            <word_count>421</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>LIMITATIONS AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>105</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>41</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>2593</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Therminator: Understanding the Interdependency of Visual and On-Body Thermal Feedback in Virtual Reality</title>
        <abstract>Recent advances have made Virtual Reality (VR) more realistic than ever before. This improved realism is attributed to today's ability to increasingly appeal to human sensations, such as visual, auditory or tactile. While research also examines temperature sensation as an important aspect, the interdependency of visual and thermal perception in VR is still underexplored. In this paper, we propose Therminator, a thermal display concept that provides warm and cold on-body feedback in VR through heat conduction of flowing liquids with different temperatures. Further, we systematically evaluate the interdependency of different visual and thermal stimuli on the temperature perception of arm and abdomen with 25 participants. As part of the results, we found varying temperature perception depending on the stimuli, as well as increasing involvement of users during conditions with matching stimuli.</abstract>
        <keywords>
            <li>Haptics</li>
            <li>Temperature</li>
            <li>Thermal Feedback</li>
            <li>Virtual Reality</li>
        </keywords>
        <authors>
            <li>Sebastian Günther</li>
            <li> Florian Müller</li>
            <li> Dominik Schön</li>
            <li> Omar Elmoghazy</li>
            <li> Max Mühlhäuser</li>
            <li> Martin Schmitz</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376195_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376195_crop_2.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376195_crop_4.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376195_crop_6.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376228</filename><data>
        <paper_id>3313831.3376228</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376228</doi>
        <sections>
            <word_count>646</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>525</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1922</word_count>
            <figure_citations>Figure 1, top left) was inspired by the Steam VR Home.Figure 1, top right) is a common VR activity for individuals wearing HMDs.Figure 1 bottom left) is a popular VR gaming format.Figure 1, bottom right) was inspired by a popular VR game called Beat Saber1.Figure 2).Figure 2a) was fixed to the upper left corner in the user’s field of view: specifically, in the near-peripheral region, about 25 degrees from the line of sight [61].Figure 2b) was inspired by NotifiVR [20].Figure 2).Figure 2c) was inspired by Facebook Space’s2 information pad.Figure 3, top).Figure 3, bottom), in each of which participant experienced one of the three VR activities (360 Video, Treasure Hunt, and Rhythm Game) preceded by a Loading activity to simulate system loading.</figure_citations>
            <section_index>2</section_index>
            <title>THE EXPERIMENT</title>
        </sections>
        <sections>
            <word_count>273</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>DATA ANALYSIS</title>
        </sections>
        <sections>
            <word_count>2191</word_count>
            <figure_citations>Figure 4 indicates, they were the most engaged in Rhythm Game (M=6.Figure 5, showing the percentages of notifications the participants actually looked at, indicates that they failed to see a significant portion of notifications in Rhythm Game, especially when alerts were sent via the controller (59.Figure 6, top left), followed by Treasure Hunter (M=3.Figure 6, bottom left), all the previously noted differences held true, but in Rhythm Game, alerts presented via controller display (M=5.Figure 6, top center): i.Figure 6, top right).Figure 6, bottom right), we observed a significant increase in the recall of notifications for all displays (HMD: 75%=> 77.</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>729</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>1096</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>DESIGN RECOMMENDATIONS</title>
        </sections>
        <sections>
            <word_count>390</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>LIMITATIONS</title>
        </sections>
        <sections>
            <word_count>266</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>56</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>2205</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Bridging the Virtual and Real Worlds: A Preliminary Study of Messaging Notifications in Virtual Reality</title>
        <abstract>Virtual reality (VR) platforms provide their users with immersive virtual environments, but disconnect them from real-world events. The increasing length of VR sessions can therefore be expected to boost users' needs to obtain information about external occurrences such as message arrival. Yet, how and when to present these real-world notifications to users engaged in VR activities remains underexplored. We conducted an experiment to investigate individuals' receptivity during four VR activities (Loading, 360 Video, Treasure Hunt, Rhythm Game) to message notifications delivered using three types of displays (head-mounted, controller, and movable panel). While higher engagement generally led to higher perceptions that notifications were ill-timed and/or disruptive, the suitability of notification displays to VR activities was influenced by the time-sensitiveness of VR content, overlapping use of modalities for delivering alerts, the display locations, and a requirement that the display be moved for notifications to be seen. Specific design suggestions are also provided.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Notification systems</li>
            <li>Interruptibility</li>
            <li>Receptivity</li>
            <li>Eye-tracking</li>
        </keywords>
        <authors>
            <li>Ching-Yu Hsieh</li>
            <li> Yi-Shyuan Chiang</li>
            <li> Hung-Yu Chiu</li>
            <li> Yung-Ju Chang</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>3</id>
            <url>3313831.3376228_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376228_crop_4.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376228_crop_6.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376243</filename><data>
        <paper_id>3313831.3376243</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376243</doi>
        <sections>
            <word_count>476</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1216</word_count>
            <figure_citations>Figure 1c) [3].</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>154</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>RESEARCH FOCUS</title>
        </sections>
        <sections>
            <word_count>717</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>1026</word_count>
            <figure_citations>Figure 1).Figure 2b).Figure 2a).Figure 2a), MoveInPlace was also enabled.Figure 2a.Figure 3 (all shown jumps were executed with a mean airtime of 250ms).</figure_citations>
            <section_index>4</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>770</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>C ONDITION PAIR</title>
        </sections>
        <sections>
            <word_count>116</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>SSQ</title>
        </sections>
        <sections>
            <word_count>25</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>IMI</title>
        </sections>
        <sections>
            <word_count>281</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>IPQ</title>
        </sections>
        <sections>
            <word_count>3</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>C ONDITION</title>
        </sections>
        <sections>
            <word_count>3</word_count>
            <figure_citations></figure_citations>
            <section_index>11</section_index>
            <title>J UMPS</title>
        </sections>
        <sections>
            <word_count>1981</word_count>
            <figure_citations></figure_citations>
            <section_index>12</section_index>
            <title>FALLS</title>
        </sections>
        <sections>
            <word_count>105</word_count>
            <figure_citations></figure_citations>
            <section_index>13</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>24</word_count>
            <figure_citations></figure_citations>
            <section_index>14</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1485</word_count>
            <figure_citations></figure_citations>
            <section_index>15</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>JumpVR: Jump-Based Locomotion Augmentation for Virtual Reality</title>
        <abstract>One of the great benefits of virtual reality (VR) is the implementation of features that go beyond realism. Common "unrealistic" locomotion techniques (like teleportation) can avoid spatial limitation of tracking, but minimize potential benefits of more realistic techniques (e.g. walking). As an alternative that combines realistic physical movement with hyper-realistic virtual outcome, we present JumpVR, a jump-based locomotion augmentation technique that virtually scales users' physical jumps. In a user study (N=28), we show that jumping in VR (regardless of scaling) can significantly increase presence, motivation and immersion compared to teleportation, while largely not increasing simulator sickness. Further, participants reported higher immersion and motivation for most scaled jumping variants than forward-jumping. Our work shows the feasibility and benefits of jumping in VR and explores suitable parameters for its hyper-realistic scaling. We discuss design implications for VR experiences and research.</abstract>
        <keywords>
            <li>VR</li>
            <li>Virtual reality</li>
            <li>Jumping</li>
            <li>Super human</li>
            <li>Immersion</li>
            <li>Hyper realism</li>
        </keywords>
        <authors>
            <li>Dennis Wolf</li>
            <li> Katja Rogers</li>
            <li> Christoph Kunder</li>
            <li> Enrico Rukzio</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376243_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376243_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376243_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376243_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376243_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376243_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376243_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376260</filename><data>
        <paper_id>3313831.3376260</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376260</doi>
        <sections>
            <word_count>867</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>818</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>STATE OF THE ART</title>
        </sections>
        <sections>
            <word_count>390</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>LITERATURE REVIEW</title>
        </sections>
        <sections>
            <word_count>868</word_count>
            <figure_citations>Figure 1 depicts 7 different realizations of IN VRQ S.</figure_citations>
            <section_index>3</section_index>
            <title>HUD</title>
        </sections>
        <sections>
            <word_count>1590</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>EXPERT SURVEY</title>
        </sections>
        <sections>
            <word_count>1018</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DESIGN STUDY</title>
        </sections>
        <sections>
            <word_count>538</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>989</word_count>
            <figure_citations>Figure 5b.</figure_citations>
            <section_index>7</section_index>
            <title>UMUX</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>INV</title>
        </sections>
        <sections>
            <word_count>1085</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REAL</title>
        </sections>
        <sections>
            <word_count>595</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>67</word_count>
            <figure_citations></figure_citations>
            <section_index>11</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>7378</word_count>
            <figure_citations></figure_citations>
            <section_index>12</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Examining Design Choices of Questionnaires in VR User Studies</title>
        <abstract>Questionnaires are among the most common research tools in virtual reality (VR) user studies. Transitioning from virtuality to reality for giving self-reports on VR experiences can lead to systematic biases. VR allows to embed questionnaires into the virtual environment which may ease participation and avoid biases. To provide a cohesive picture of methods and design choices for questionnaires in VR (inVRQ), we discuss 15 inVRQ studies from the literature and present a survey with 67 VR experts from academia and industry. Based on the outcomes, we conducted two user studies in which we tested different presentation and interaction methods of inVRQs and evaluated the usability and practicality of our design. We observed comparable completion times between inVRQs and questionnaires outside VR (nonVRQs) with higher enjoyment but lower usability for \inVRQs. These findings advocate the application of inVRQs and provide an overview of methods and considerations that lay the groundwork for inVRQ design.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>VR</li>
            <li>User studies</li>
            <li>In-VR questionnaires</li>
            <li>InVRQs</li>
            <li>Research methods</li>
        </keywords>
        <authors>
            <li>Dmitry Alexandrovsky</li>
            <li> Susanne Putze</li>
            <li> Michael Bonfert</li>
            <li> Sebastian Höffner</li>
            <li> Pitt Michelmann</li>
            <li> Dirk Wenig</li>
            <li> Rainer Malaka</li>
            <li> Jan David Smeddinck</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376260_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376260_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376260_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376260_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376260_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376265</filename><data>
        <paper_id>3313831.3376265</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376265</doi>
        <sections>
            <word_count>599</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>3556</word_count>
            <figure_citations>Figure 2).Figure 3).</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>147</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>PENS</title>
        </sections>
        <sections>
            <word_count>2588</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>TLX</title>
        </sections>
        <sections>
            <word_count>836</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>189</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>LIMITATIONS AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>144</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>41</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1056</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Virtual Reality Games for People Using Wheelchairs</title>
        <abstract>Virtual Reality (VR) holds the promise of providing engaging embodied experiences, but little is known about how people with disabilities engage with it. We explore challenges and opportunities of VR gaming for wheelchair users. First, we present findings from a survey that received 25 responses and gives insights into wheelchair users' motives to (non-) engage with VR and their experiences. Drawing from this survey, we derive design implications which we tested through implementation and qualitative evaluation of three full-body VR game prototypes with 18 participants. Our results show that VR gaming engages wheelchair users, though nuanced consideration is required for the design of embodied immersive experiences for minority bodies, and we illustrate how designers can create meaningful, positive experiences.</abstract>
        <keywords>
            <li>Accessibility</li>
            <li>Games</li>
            <li>Virtual reality</li>
        </keywords>
        <authors>
            <li>Kathrin Gerling</li>
            <li> Patrick Dickinson</li>
            <li> Kieran Hicks</li>
            <li> Liam Mason</li>
            <li> Adalberto L. Simeone</li>
            <li> Katta Spiel</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376265_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376265_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376265_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376265_crop_4.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376286</filename><data>
        <paper_id>3313831.3376286</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376286</doi>
        <sections>
            <word_count>518</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>610</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1206</word_count>
            <figure_citations>Figure 1 shows the setup of our prototype.Figure 3 B).Figure 3 A and 3 C).Figure 3 D shows, users can tie shared proxies (strings, ropes or sewing threads) to the robot to simulate force feedback such as tension.Figure 3 E indicates.Figure 3 F shows.Figure 3 H shows.Figure 3 I shows, users can attach the VR controller with tape onto a user-driving proxy simulating a stick or a ﬁshing rod.Figure 3 J shows.Figure 3 D).Figure 4A shows.</figure_citations>
            <section_index>2</section_index>
            <title>MOVEVR IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>3883</word_count>
            <figure_citations>Figure 4A shows, different levels of tensile strength can be simulated through changing the distance between the user and MoveVR: the further they are apart, the stronger the tension.Figure 5 A.Figure 5 C illustrates.Figure 5 D illustrates.Figure 5 D illustrates, the robot bumps into the user to simulate a dog rubbing up against his/her leg or a strike on his/her shoulder.Figure 5 B).Figure 5 B simulates two virtual objects built with different materials.Figure 5B.Figure 6 shows the accuracy and confusion matrix of each benchmark study.Figure 6 A), 96.Figure 6 B), 99.Figure 6 C), 97.Figure 6 D), and 96.Figure 6 E).Figure 7, the user has a joint experience with a dog leading her/him to the front door.Figure 8, the user notices a box blocking the entry.Figure 9 B where an enemy escapes from the user’s hit.Figure 10 shows that participants perceived the virtual world as more realistic and enjoyable in the MoveVR condition compared to the barehand (p = 0.Figure 11A indicates.Figure 11 indicates.</figure_citations>
            <section_index>3</section_index>
            <title>MOVEVR FORCE EXPRESSIONS</title>
        </sections>
        <sections>
            <word_count>888</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>143</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>249</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>REFERENCES</title>
        </sections>
        <sections>
            <word_count>1511</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGEMENT</title>
        </sections>
        <title>MoveVR: Enabling Multiform Force Feedback in Virtual Reality using Household Cleaning Robot</title>
        <abstract>Haptic feedback can significantly enhance the realism and immersiveness of virtual reality (VR) systems. In this paper, we propose MoveVR, a technique that enables realistic, multiform force feedback in VR leveraging commonplace cleaning robots. MoveVR can generate tension, resistance, impact and material rigidity force feedback with multiple levels of force intensity and directions. This is achieved by changing the robot's moving speed, rotation, position as well as the carried proxies. We demonstrated the feasibility and effectiveness of MoveVR through interactive VR gaming. In our quantitative and qualitative evaluation studies, participants found that MoveVR provides more realistic and enjoyable user experience when compared to commercially available haptic solutions such as vibrotactile haptic systems.</abstract>
        <keywords>
            <li>Force feedback</li>
            <li>Haptic feedback</li>
            <li>Virtual reality</li>
            <li>VR</li>
            <li>Robotics</li>
            <li>Cleaning robot</li>
            <li>Human-robot interaction</li>
        </keywords>
        <authors>
            <li>Yuntao Wang</li>
            <li> Zichao (Tyson) Chen</li>
            <li> Hanchuan Li</li>
            <li> Zhengyi Cao</li>
            <li> Huiyi Luo</li>
            <li> Tengxiang Zhang</li>
            <li> Ke Ou</li>
            <li> John Raiti</li>
            <li> Chun Yu</li>
            <li> Shwetak Patel</li>
            <li> Yuanchun Shi</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376286_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376286_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376286_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376286_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376286_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376286_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376286_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376286_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376286_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3313831.3376286_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3313831.3376286_crop_11.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376358</filename><data>
        <paper_id>3313831.3376358</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376358</doi>
        <sections>
            <word_count>708</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>784</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1766</word_count>
            <figure_citations>Figure 2 shows a system schematic of PoCoPo.Figure 3).Figure 4).Figure 6).Figure 7 demonstrates how the user can hold several objects on the table as seen in VR.Figure 8).</figure_citations>
            <section_index>2</section_index>
            <title>DESIGN AND IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>2791</word_count>
            <figure_citations>Figure 10) were chosen based on the previous studies [41, 46].Figure 11).Figure 12).</figure_citations>
            <section_index>3</section_index>
            <title>INFORMATION</title>
        </sections>
        <sections>
            <word_count>818</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>LIMITATION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>412</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>27</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>2317</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality</title>
        <abstract>We introduce PoCoPo, the first handheld pin-based shape display that can render various 2.5D shapes in hand in realtime. We designed the display small enough for a user to hold it in hand and carry it around, thereby enhancing the haptic experiences in a virtual environment. PoCoPo has 18 motor-driven pins on both sides of a cuboid, providing the sensation of skin contact on the user's palm and fingers. We conducted two user studies to understand the capability of PoCoPo. The first study showed that the participants were generally successful in distinguishing the shapes rendered by PoCoPo with an average success rate of 88.5%. In the second study, we investigated the acceptable visual size of a virtual object when PoCoPo rendered a physical object of a certain size. The result led to a better understanding of the acceptable differences between the perceptions of visual size and haptic size.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Shape Display</li>
            <li>Haptic Device</li>
            <li>Handheld Device</li>
        </keywords>
        <authors>
            <li>Shigeo Yoshida</li>
            <li> Yuqian Sun</li>
            <li> Hideaki Kuzuoka</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376358_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376358_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376358_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376358_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376358_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376358_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376358_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376358_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376358_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3313831.3376358_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3313831.3376358_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>3313831.3376358_crop_12.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376438</filename><data>
        <paper_id>3313831.3376438</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376438</doi>
        <sections>
            <word_count>700</word_count>
            <figure_citations>Figure 1, the Page 1 CHI 2020 Paper concept is to display the outlines of objects that lie in the direction in which the user points, and to generate a distinct motion around each of the outlines.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>722</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1019</word_count>
            <figure_citations>Figure 2 shows the cone casting technique.</figure_citations>
            <section_index>2</section_index>
            <title>OUTLINE PURSUITS</title>
        </sections>
        <sections>
            <word_count>1355</word_count>
            <figure_citations>Figure 5 illustrates Controller-based Outline Pursuits in a room planner setting.</figure_citations>
            <section_index>3</section_index>
            <title>TECHNIQUES</title>
        </sections>
        <sections>
            <word_count>307</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>USER STUDIES</title>
        </sections>
        <sections>
            <word_count>3736</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>ANOVA</title>
        </sections>
        <sections>
            <word_count>714</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>162</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>1993</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Outline Pursuits: Gaze-assisted Selection of Occluded Objects in Virtual Reality</title>
        <abstract>In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Occlusion</li>
            <li>Eye tracking</li>
            <li>Smooth pursuits</li>
        </keywords>
        <authors>
            <li>Ludwig Sidenmark</li>
            <li> Christopher Clarke</li>
            <li> Xuesong Zhang</li>
            <li> Jenny Phu</li>
            <li> Hans Gellersen</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376438_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376438_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376438_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376438_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376438_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376438_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376438_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376438_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376438_crop_9.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376470</filename><data>
        <paper_id>3313831.3376470</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376470</doi>
        <sections>
            <word_count>572</word_count>
            <figure_citations>Figure 1).Figure 1).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>560</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1002</word_count>
            <figure_citations>Figure 2).Figure 3).Figure 4).</figure_citations>
            <section_index>2</section_index>
            <title>WIREALITY IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>1302</word_count>
            <figure_citations>Figure 1).Figure 2), we included a 10-turn potentiometer that allowed us to precisely track a point’s distance from the module, but not the azimuth or altitude.Figure 6).</figure_citations>
            <section_index>3</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>427</word_count>
            <figure_citations>Figure 7 shows our main results.</figure_citations>
            <section_index>4</section_index>
            <title>QUALITATIVE STUDY</title>
        </sections>
        <sections>
            <word_count>968</word_count>
            <figure_citations>Figure 8, A-D).Figure 9, A-D).Figure 10 shows four example scenarios: an ATM screen, a button, a lever, and a piano.Figure 11, A-D).</figure_citations>
            <section_index>5</section_index>
            <title>EXAMPLE USES</title>
        </sections>
        <sections>
            <word_count>127</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>1975</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics</title>
        <abstract>Today's virtual reality (VR) systems allow users to explore immersive new worlds and experiences through sight. Unfortunately, most VR systems lack haptic feedback, and even high-end consumer systems use only basic vibration motors. This clearly precludes realistic physical interactions with virtual objects. Larger obstacles, such as walls, railings, and furniture are not simulated at all. In response, we developed Wireality, a self-contained worn system that allows for individual joints on the hands to be accurately arrested in 3D space through the use of retractable wires that can be programmatically locked. This allows for convincing tangible interactions with complex geometries, such as wrapping fingers around a railing. Our approach is lightweight, low-cost, and low-power, criteria important for future, worn consumer uses. In our studies, we further show that our system is fast-acting, spatially-accurate, high-strength, comfortable, and immersive.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Haptics</li>
            <li>Force Feedback</li>
            <li>String-Driven</li>
            <li>Touch</li>
            <li>Grasp</li>
        </keywords>
        <authors>
            <li>Cathy Fang</li>
            <li> Yang Zhang</li>
            <li> Matthew Dworman</li>
            <li> Chris Harrison</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376470_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376470_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376470_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376470_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376470_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376470_crop_6.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376470_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376470_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3313831.3376470_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3313831.3376470_crop_11.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376481</filename><data>
        <paper_id>3313831.3376481</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376481</doi>
        <sections>
            <word_count>1409</word_count>
            <figure_citations>Figure 1 and Figure 2).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION AND MOTIVATION</title>
        </sections>
        <sections>
            <word_count>759</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>792</word_count>
            <figure_citations>Figure 2) consists of a custom-made robot arm attached to a commercial HMD, currently an Oculus Rift CV1.</figure_citations>
            <section_index>2</section_index>
            <title>FACEHAPTICS SYSTEM</title>
        </sections>
        <sections>
            <word_count>5156</word_count>
            <figure_citations>Figure 3 and highest for the condition with oscillating head and static wind (M = 14.Figure 3 (middle)): The standard deviation of the signed point error differed signiﬁcantly between movement conditions (F(3, 45) = 3.Figure 3 (right).Figure 4) but from a static location.Figure 4) that contained 16 events along a 3 minute pre-deﬁned walkthrough.</figure_citations>
            <section_index>3</section_index>
            <title>USER STUDIES</title>
        </sections>
        <sections>
            <word_count>512</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>TECHNICAL CONSIDERATIONS AND LIMITATIONS</title>
        </sections>
        <sections>
            <word_count>734</word_count>
            <figure_citations>Figure 5), underlining the easy extensibility of the system, but also the need for a reloading mechanism or dispensing system (e.</figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>19</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>2386</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>FaceHaptics: Robot Arm based Versatile Facial Haptics for Immersive Environments</title>
        <abstract>This paper introduces FaceHaptics, a novel haptic display based on a robot arm attached to a head-mounted virtual reality display. It provides localized, multi-directional and movable haptic cues in the form of wind, warmth, moving and single-point touch events and water spray to dedicated parts of the face not covered by the head-mounted display.The easily extensible system, however, can principally mount any type of compact haptic actuator or object. User study 1 showed that users appreciate the directional resolution of cues, and can judge wind direction well, especially when they move their head and wind direction is adjusted dynamically to compensate for head rotations. Study 2 showed that adding FaceHaptics cues to a VR walkthrough can significantly improve user experience, presence, and emotional responses.</abstract>
        <keywords>
            <li>Haptics</li>
            <li>Robot arm</li>
            <li>Immersive environments</li>
            <li>Virtual reality</li>
            <li>User study</li>
            <li>Perception</li>
            <li>Presence</li>
            <li>Emotion</li>
        </keywords>
        <authors>
            <li>Alexander Wilberz</li>
            <li> Dominik Leschtschow</li>
            <li> Christina Trepkowski</li>
            <li> Jens Maiero</li>
            <li> Ernst Kruijff</li>
            <li> Bernhard Riecke</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376481_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376481_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376481_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376481_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376481_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376523</filename><data>
        <paper_id>3313831.3376523</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376523</doi>
        <sections>
            <word_count>809</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>2552</word_count>
            <figure_citations>Figure 3 illustrates the mechanical design of each RoomShift robot.Figure 5) ranges from 3.Figure 5 illustrates various static props that the RoomShift robot can actuate.Figure 6 depicts the space and mounted cameras on the ceiling (left) and tracking software (right).Figure 7).Figure 7).Figure 9 illustrates the schematic of RoomShift’s circuit.</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>9</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>MCU</title>
        </sections>
        <sections>
            <word_count>432</word_count>
            <figure_citations>Figure 8).</figure_citations>
            <section_index>3</section_index>
            <title>GND</title>
        </sections>
        <sections>
            <word_count>718</word_count>
            <figure_citations>Figure 10 illustrates the architecture of the RoomShift software.</figure_citations>
            <section_index>4</section_index>
            <title>UDP</title>
        </sections>
        <sections>
            <word_count>920</word_count>
            <figure_citations>Figure 1).Figure 12).Figure 13).Figure 14).</figure_citations>
            <section_index>5</section_index>
            <title>INTERACTION WITH ROOMSHIFT</title>
        </sections>
        <sections>
            <word_count>294</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>559</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>LIMITATIONS AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>110</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>43</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1939</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</title>
        <abstract>RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.</abstract>
        <keywords>
            <li>Haptic interfaces</li>
            <li>Room-scale haptics</li>
            <li>Virtual reality</li>
            <li>Swarm robots</li>
        </keywords>
        <authors>
            <li>Ryo Suzuki</li>
            <li> Hooman Hedayati</li>
            <li> Clement Zheng</li>
            <li> James L. Bohn</li>
            <li> Daniel Szafir</li>
            <li> Ellen Yi-Luen Do</li>
            <li> Mark D. Gross</li>
            <li> Daniel Leithinger</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376523_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376523_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376523_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376523_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376523_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376523_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376523_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376523_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376523_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3313831.3376523_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3313831.3376523_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>3313831.3376523_crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>3313831.3376523_crop_13.png</url>
        </figure>
        <figure>
            <id>14</id>
            <url>3313831.3376523_crop_14.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376550</filename><data>
        <paper_id>3313831.3376550</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376550</doi>
        <sections>
            <word_count>669</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1483</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1668</word_count>
            <figure_citations>Figure 1 shows an overview of our system.Figure 2a).Figure 2a.Figure 2b with the red frames.Figure 3) that could be shared from the remote VR mode to the local AR mode: • Eye Gaze A virtual raycast line of the remote user’s eye gaze overlaid onto the local user’s AR view from a third-person perspective.Figure 4).</figure_citations>
            <section_index>2</section_index>
            <title>SYSTEM OVERVIEW</title>
        </sections>
        <sections>
            <word_count>1347</word_count>
            <figure_citations>Figure 5b).Figure 5a.Figure 5c.Figure 5a.Figure 6a).Figure 6b).</figure_citations>
            <section_index>3</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1797</word_count>
            <figure_citations>Figure 7 shows the average CP rating.Figure 8 shows the average rating results of each condition for TLX.Figure 9).</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1918</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>254</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSIONS AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>2057</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing</title>
        <abstract>Supporting natural communication cues is critical for people to work together remotely and face-to-face. In this paper we present a Mixed Reality (MR) remote collaboration system that enables a local worker to share a live 3D panorama of his/her surroundings with a remote expert. The remote expert can also share task instructions back to the local worker using visual cues in addition to verbal communication. We conducted a user study to investigate how sharing augmented gaze and gesture cues from the remote expert to the local worker could affect the overall collaboration performance and user experience. We found that by combing gaze and gesture cues, our remote collaboration system could provide a significantly stronger sense of co-presence for both the local and remote users than using the gaze cue alone. The combined cues were also rated significantly higher than the gaze in terms of ease of conveying spatial actions.</abstract>
        <keywords>
            <li>Mixed Reality</li>
            <li>Augmented Reality</li>
            <li>Virtual Reality</li>
            <li>Remote collaboration</li>
            <li>3D panorama</li>
            <li>Scene reconstruction</li>
            <li>Eye gaze</li>
            <li>Hand gesture</li>
        </keywords>
        <authors>
            <li>Huidong Bai</li>
            <li> Prasanth Sasikumar</li>
            <li> Jing Yang</li>
            <li> Mark Billinghurst</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376550_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376550_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376550_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376550_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376550_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376550_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376550_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376550_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376550_crop_9.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376574</filename><data>
        <paper_id>3313831.3376574</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376574</doi>
        <sections>
            <word_count>623</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>576</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>2456</word_count>
            <figure_citations>Figure 1): 1.Figure 2 A.Figure 2 B).Figure 2 C).Figure 2 D), Page 3 CHI 2020 Paper CHI 2020, April 25–30, 2020, Honolulu, HI, USA Figure 2: A-F: Iterations of the IK layer of the walking animation.Figure 2 E).Figure 2 F).Figure 2 G as four discrete phases of a single step, where the ﬁlled and the blank pedals represent the right and left foot respectively.</figure_citations>
            <section_index>2</section_index>
            <title>VR STRIDER</title>
        </sections>
        <sections>
            <word_count>1649</word_count>
            <figure_citations>Figure 3).</figure_citations>
            <section_index>3</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1168</word_count>
            <figure_citations>Figure 4(left).Figure 4(center).Figure 4(right).Figure 5(left).Figure 5(center).Figure 5(right).Figure 6 illustrates the distribution of responses for items with signiﬁcant differences.</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1612</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>815</word_count>
            <figure_citations>Figure 7).</figure_citations>
            <section_index>6</section_index>
            <title>CONFIRMATORY STUDY</title>
        </sections>
        <sections>
            <word_count>343</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>66</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1477</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Walking by Cycling: A Novel In-Place Locomotion User Interface for Seated Virtual Reality Experiences</title>
        <abstract>We introduce VR Strider, a novel locomotion user interface (LUI) for seated virtual reality (VR) experiences, which maps cycling biomechanics of the user's legs to virtual walking movements. The core idea is to translate the motion of pedaling on a mini exercise bike to a corresponding walking animation of a virtual avatar while providing audio-based tactile feedback on virtual ground contacts. We conducted an experiment to evaluate the LUI and our novel anchor-turning rotation control method regarding task performance, spatial cognition, VR sickness, sense of presence, usability and comfort in a path-integration task. The results show that VR Strider has a significant positive effect on the participants' angular and distance estimation, sense of presence and feeling of comfort compared to other established locomotion techniques, such as teleportation and joystick-based navigation. A confirmatory study further indicates the necessity of synchronized avatar animations for virtual vehicles that rely on pedalling.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Navigation techniques</li>
            <li>Locomotion user interface</li>
            <li>Virtual walking</li>
        </keywords>
        <authors>
            <li>Jann Philipp Freiwald</li>
            <li> Oscar Ariza</li>
            <li> Omar Janeh</li>
            <li> Frank Steinicke</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376574_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376574_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376574_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376574_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376574_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376574_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376574_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376582</filename><data>
        <paper_id>3313831.3376582</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376582</doi>
        <sections>
            <word_count>934</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>3011</word_count>
            <figure_citations>Figure 2 displays the diorama.</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>488</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>METHOD</title>
        </sections>
        <sections>
            <word_count>1896</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>FINDINGS</title>
        </sections>
        <sections>
            <word_count>1318</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION AND IMPLICATIONS</title>
        </sections>
        <sections>
            <word_count>132</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>92</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1700</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Reflexive VR Storytelling Design Beyond Immersion: Facilitating Self-Reflection on Death and Loneliness</title>
        <abstract>This research examines the reflexive dimensions of cinematic virtual reality (CVR) storytelling. We created Anonymous, an interactive CVR piece that employs a reflexive storytelling method. This method is based on distancing effects and is used to elicit audience awareness and self-reflection about loneliness and death. To understand the audience's experiences, we conducted in-depth interviews to study which design factors and elements prompted reflexive thoughts and feelings. Our findings highlight how the audience experience was impacted by four reflexive dimensions: abstract and minimal aesthetics, everyday materials and textures, the restriction of control, and multiple, disembodied points of view. We use our findings to discuss how these dimensions can inform the design of VR storytelling experiences that provoke self and social reflection.</abstract>
        <keywords>
            <li>Alienation</li>
            <li>Cinematic VR</li>
            <li>Distancing Effect</li>
            <li>Estrangement</li>
            <li>Immersive Storytelling</li>
            <li>Reﬂexivity</li>
            <li>Virtual Reality</li>
        </keywords>
        <authors>
            <li>Sojung Bahng</li>
            <li> Ryan M. Kelly</li>
            <li> Jon McCormack</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376582_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376582_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376582_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376582_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376582_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376582_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376582_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376582_crop_8.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376614</filename><data>
        <paper_id>3313831.3376614</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376614</doi>
        <sections>
            <word_count>1062</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>648</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1730</word_count>
            <figure_citations>Figure 2).Figure 3a-e).Figure 4).Figure 5a).Figure 5b).Figure 5c), rotated by two-ﬁnger twirl (Figure 5d), and resized by two-ﬁnger pinch (Figure 5e) [2].Figure 5f).</figure_citations>
            <section_index>2</section_index>
            <title>DESIGN AND IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>1284</word_count>
            <figure_citations>Figure 3).Figure 6).Figure 7).</figure_citations>
            <section_index>3</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1804</word_count>
            <figure_citations>Figure 8 shows an overview for comparison between ARchitect and the baseline.</figure_citations>
            <section_index>4</section_index>
            <title>EXPERIMENTAL RESULT AND ANALYSIS</title>
        </sections>
        <sections>
            <word_count>1412</word_count>
            <figure_citations>Figure 9a).Figure 9b-d).Figure 10b).Figure 10a).Figure 10c).Figure 11a).Figure 11b).Figure 11c-d).</figure_citations>
            <section_index>5</section_index>
            <title>DESIGN GUIDELINES AND EXAMPLE EXPERIENCES</title>
        </sections>
        <sections>
            <word_count>395</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>LIMITATIONS AND FURTHER WORK</title>
        </sections>
        <sections>
            <word_count>105</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>69</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1744</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>ARchitect: Building Interactive Virtual Experiences from Physical Affordances by Bringing Human-in-the-Loop</title>
        <abstract>Automatic generation of Virtual Reality (VR) worlds which adapt to physical environments have been proposed to enable safe walking in VR. However, such techniques mainly focus on the avoidance of physical objects as obstacles and overlook their interaction affordances as passive haptics. Current VR experiences involving interaction with physical objects in surroundings still require verbal instruction from an assisting partner. We present ARchitect, a proof-of-concept prototype that allows flexible customization of a VR experience with human-in-the-loop. ARchitect brings in an assistant to map physical objects to virtual proxies of matching affordances using Augmented Reality (AR). In a within-subjects study (9 user pairs) comparing ARchitect to a baseline condition, assistants and players experienced decreased workload and players showed increased VR presence and trust in the assistant. Finally, we defined design guidelines of ARchitect for future designers and implemented three demonstrative experiences.</abstract>
        <keywords>
            <li>ARchitect</li>
            <li>Virtual reality</li>
            <li>Affordance</li>
            <li>Passive haptics</li>
            <li>Asymmetric</li>
        </keywords>
        <authors>
            <li>Chuan-en Lin</li>
            <li> Ta Ying Cheng</li>
            <li> Xiaojuan Ma</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376614_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376614_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376614_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376614_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376614_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376614_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376614_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376614_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376614_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3313831.3376614_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3313831.3376614_crop_11.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376626</filename><data>
        <paper_id>3313831.3376626</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376626</doi>
        <sections>
            <word_count>458</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>2047</word_count>
            <figure_citations>Figure 2 a).Figure 2 b).Figure 2 c).Figure 3 a) shows a schematic representation of this input modality.Figure 3 b)).Figure 3 c) further illustrates how this input method works.Figure 4 shows the internals of the prototype.Figure 5), with a light blue line ending in a circle with upwards fading walls and a downward pointing arrow in the center.</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1241</word_count>
            <figure_citations>Figure 5).Figure 8).</figure_citations>
            <section_index>2</section_index>
            <title>METHODOLOGY</title>
        </sections>
        <sections>
            <word_count>2266</word_count>
            <figure_citations>Figure 6 depicts the measured mean errors for all conditions.Figure 7a depicts the measured mean TCTs for all conditions.Figure 7b depicts the measured mean numbers of teleports for individual conditions.Figure 7c depicts the measured mean RTLX values the individual conditions.Figure 8 depicts all answers of the participants.Figure 8 depicts all answers of the participants.Figure 8 depicts all answers given.</figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1756</word_count>
            <figure_citations>Figure 7a, we found no signiﬁcant differences between directional input modalities.Figure 8).</figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>180</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>22</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGEMENT</title>
        </sections>
        <sections>
            <word_count>2410</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Podoportation: Foot-Based Locomotion in Virtual Reality</title>
        <abstract>Virtual Reality (VR) allows for infinitely large environments. However, the physical traversable space is always limited by real-world boundaries. This discrepancy between physical and virtual dimensions renders traditional locomotion methods used in real world unfeasible. To alleviate these limitations, research proposed various artificial locomotion concepts such as teleportation, treadmills, and redirected walking. However, these concepts occupy the user's hands, require complex hardware or large physical spaces. In this paper, we contribute nine VR locomotion concepts for foot-based locomotion, relying on the 3D position of the user's feet and the pressure applied to the sole as input modalities. We evaluate our concepts and compare them to state-of-the-art point &amp; teleport technique in a controlled experiment with 20 participants. The results confirm the viability of our approaches for foot-based and engaging locomotion. Further, based on the findings, we contribute a wireless hardware prototype implementation.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Locomotion</li>
            <li>Foot-based input</li>
        </keywords>
        <authors>
            <li>Julius von Willich</li>
            <li> Martin Schmitz</li>
            <li> Florian Müller</li>
            <li> Daniel Schmitt</li>
            <li> Max Mühlhäuser</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376626_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376626_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376626_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376626_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376626_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376626_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376626_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376626_crop_8.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376628</filename><data>
        <paper_id>3313831.3376628</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376628</doi>
        <sections>
            <word_count>639</word_count>
            <figure_citations>Figure 1b) and the combination of pen and tablet as 6DoF-tracked 3D input devices (e.Figure 1d/8a).Figure 7a) and sketch on them using pen on tablet (see Figure 1c/7b).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1309</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>901</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>PRELIMINARY EXPERT INTERVIEWS</title>
        </sections>
        <sections>
            <word_count>2439</word_count>
            <figure_citations>Figure 2) known as Zwicky box [64], which is a common design space tool (e.Figure 2/3).Figure 2).Figure 2).Figure 2).Figure 2).Figure 2).Figure 3), we will show how to place an interaction metaphor from SymbiosisSketch [3].Figure 3 for implementation and avoided functional gaps (2).Figure 3; see [23]).Figure 3 are not complete and were created and positioned during joint brainstorming sessions.Figure 5a).Figure 7a).Figure 8a).Figure 9a) that allows users to see distant and hidden objects inside the space.Figure 9b) in the direction of one axis [13].</figure_citations>
            <section_index>3</section_index>
            <title>DESIGN SPACE</title>
        </sections>
        <sections>
            <word_count>1880</word_count>
            <figure_citations>Figure 3).Figure 3), we implemented drawing surfaces as drawing aids that can be placed in space by the tablet position and orientation.Figure 3).Figure 3) that it is constrained to 2D surfaces.Figure 3).Figure 4a) on the pen to increase and the left to decrease.Figure 4a).Figure 4a).Figure 4b).Figure 4c).Figure 4a/5a).Figure 5a).Figure 5b).Figure 6a).Figure 6b) and scaling are supported as well.Figure 7a).Figure 7b).Figure 8a).Figure 8b).Figure 9a).Figure 9b).Figure 10).Figure 10).Figure 10).Figure 10).Figure 10).</figure_citations>
            <section_index>4</section_index>
            <title>THE VRSKETCHIN SYSTEM</title>
        </sections>
        <sections>
            <word_count>133</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1114</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>USABILITY WALKTHROUGH</title>
        </sections>
        <sections>
            <word_count>337</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>DISCUSSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>196</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>26</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>2129</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality</title>
        <abstract>Sketching in virtual reality (VR) enhances perception and understanding of 3D volumes, but is currently a challenging task, as spatial input devices (e.g., tracked controllers) do not provide any scaffolding or constraints for mid-air interaction. We present VRSketchIn, a VR sketching application using a 6DoF-tracked pen and a 6DoF-tracked tablet as input devices, combining unconstrained 3D mid-air with constrained 2D surface-based sketching. To explore what possibilities arise from this combination of 2D (pen on tablet) and 3D input (6DoF pen), we present a set of design dimensions and define the design space for 2D and 3D sketching interaction metaphors in VR. We categorize prior art inside our design space and implemented a subset of metaphors for pen and tablet sketching in our prototype. To gain a deeper understanding which specific sketching operations users perform with 2D and which with 3D metaphors, we present findings of usability walkthroughs with six participants.</abstract>
        <keywords>
            <li>Sketching</li>
            <li>Pen and tablet</li>
            <li>Mid-air painting</li>
            <li>Virtual reality</li>
            <li>Interaction metaphors</li>
            <li>Design space</li>
        </keywords>
        <authors>
            <li>Tobias Drey</li>
            <li> Jan Gugenheimer</li>
            <li> Julian Karlbauer</li>
            <li> Maximilian Milo</li>
            <li> Enrico Rukzio</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376628_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376628_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376628_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376628_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376628_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376628_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376628_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376628_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376628_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3313831.3376628_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3313831.3376628_crop_11.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376639</filename><data>
        <paper_id>3313831.3376639</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376639</doi>
        <sections>
            <word_count>411</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>563</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>435</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>PHYSICAL BACKGROUND</title>
        </sections>
        <sections>
            <word_count>380</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>IMPLEMENTATION OF THROWING IN VR</title>
        </sections>
        <sections>
            <word_count>711</word_count>
            <figure_citations>Figure 1).Figure 2 illustrates the accuracy of each hit point for the three participants over the course of the session.</figure_citations>
            <section_index>4</section_index>
            <title>LEARNING CURVE</title>
        </sections>
        <sections>
            <word_count>729</word_count>
            <figure_citations>Figure 1 shows the three stations in the real world and a throw in VR.Figure 1).</figure_citations>
            <section_index>5</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1228</word_count>
            <figure_citations>Figure 3).Figure 4, the precision is not only more than 2-3 times higher in the real world than in VR, it is also more stable between participants, namely they have more similar precision levels in real world, than in VR.Figure 5) highlight, that while the scattering of hit points is generally higher in all VR throwing styles, the hit points are also much more scattered along the vertical/longitudal axis, than the horizontal.Figure 6 shows the “darts-like” throw (left) and the underhand throw (right) for a representative participant.Figure 7 we can observe Page 5 CHI 2020 Paper CHI 2020, April 25–30, 2020, Honolulu, HI, USA Figure 7.</figure_citations>
            <section_index>6</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>341</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>81</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>1080</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Performance and Experience of Throwing in Virtual Reality</title>
        <abstract>Throwing is a fundamental movement in many sports and games. Given this, accurate throwing in VR applications today is surprisingly difficult. In this paper we explore the nature of the difficulties of throwing in VR in more detail. We present the results of a user study comparing throwing in VR and in the physical world. In a short pre-study with 3 participants we determine an optimal number of throwing repetitions for the main study by exploring the learning curve and subjective fatigue of throwing in VR. In the main study, with 12 participants, we find that throwing precision and accuracy in VR are lower particularly in the distance and height dimensions. It also requires more effort and exhibits different kinematic patterns.</abstract>
        <keywords>
            <li>Throwing</li>
            <li>Virtual Reality</li>
            <li>User Study</li>
        </keywords>
        <authors>
            <li>Tim Zindulka</li>
            <li> Myroslav Bachynskyi</li>
            <li> Jörg Müller</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376639_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376639_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376639_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376639_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376639_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376639_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376639_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376642</filename><data>
        <paper_id>3313831.3376642</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376642</doi>
        <sections>
            <word_count>615</word_count>
            <figure_citations>Figure 1, ReliveInVR allows users to relive the virtual experience together.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>453</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>900</word_count>
            <figure_citations>Figure 2).Figure 3, All events were different on each island.</figure_citations>
            <section_index>2</section_index>
            <title>PROTOTYPES AND VR ARCHERY GAME</title>
        </sections>
        <sections>
            <word_count>1420</word_count>
            <figure_citations>Figure 5 (similar to co-watching 360-degree video in Facebook Spaces).Figure 6, we implemented a VR networked environment and a state-based replay system with SteamVR plugins and Photon Networking framework in Unity.</figure_citations>
            <section_index>3</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>2132</word_count>
            <figure_citations>Figure 7 (a), the ANOVA for the linear mixed model of Shared Cognition yielded a signiﬁcant effect of the sharing condition, F(2, 102) = 13.Figure 7(c) shows a signiﬁcant effect of the sharing condition, F(2, 102) = 47.Figure 7 (e) shows that the the Paper 513 ANOVA for the linear mixed model of Conversation Engagement yielded a signiﬁcant effect of the sharing condition, F(2, 102) = 19.Figure 7 (f).Figure 7 (g) and (h), when asked about most prefered sharing tool for sharing VR experience remotely together, 96% of the participants chose ReliveInVR, 2% of the participants chose Co-watchVR and 2% chose Co-watchDT.Figure 8, the means and standard deviations for the sharing time ratios for each sharing condition are: Co-watchDT=1.Figure 9, a post-hoc test using Wilcoxon signed-rank with Bonferroni correction showed signiﬁcant differences between ReliveInVR and Co-watchDT Spaces (p < 0.</figure_citations> <section_index>4</section_index>
                    <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1048</word_count>
            <figure_citations>Figure 10 (a) demonstrates the percentage of where participants viewed their replayed avatars during Review session in 4 different areas around the replayed avatar.</figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>586</word_count>
            <figure_citations>Figure 10(b), participants only had average 18.</figure_citations>
            <section_index>6</section_index>
            <title>DESIGN IMPLICATIONS</title>
        </sections>
        <sections>
            <word_count>206</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>LIMITATIONS AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>136</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>1292</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Again, Together: Socially Reliving Virtual Reality Experiences When Separated</title>
        <abstract>To share a virtual reality (VR) experience remotely together, users usually record videos from an individual's point of view and then co-watch these videos. However, co-watching recorded videos limits users to reliving their memories from the perspective from which the video was captured. In this paper, we describe ReliveInVR, a new time-machine-like VR experience sharing method. ReliveInVR allows multiple users to immerse themselves in the relived experience together and independently view the experience from any perspective. We conducted a 1x3 within-subject study with 26 dyads to compare ReliveInVR with (1) co-watching 360-degree videos on desktop, and (2) co-watching 360-degree videos in VR. Our results suggest that participants reported higher levels of immersion and social presence in ReliveInVR. Participants in ReliveInVR also understood the shared experience better, discovered unnoticed things together and found the sharing experience more fulfilling. We discuss the design implications for sharing VR experiences over time and space.</abstract>
        <keywords>
            <li>Shared experience</li>
            <li>Virtual reality</li>
            <li>Social</li>
            <li>Replay</li>
            <li>Shared experience</li>
            <li>Presence</li>
            <li>Immersion</li>
        </keywords>
        <authors>
            <li>Cheng Yao Wang</li>
            <li> Mose Sakashita</li>
            <li> Upol Ehsan</li>
            <li> Jingjin Li</li>
            <li> Andrea Stevenson Won</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376642_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376642_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376642_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376642_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376642_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376642_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376642_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376642_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376642_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3313831.3376642_crop_10.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376652</filename><data>
        <paper_id>3313831.3376652</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376652</doi>
        <sections>
            <word_count>1106</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1499</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1761</word_count>
            <figure_citations>Figure 2).</figure_citations>
            <section_index>2</section_index>
            <title>DESIGN AND IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>1446</word_count>
            <figure_citations>Figure 4).Figure 5, A and B).Figure 5, A), we allow users to conﬁgure the VE with displays of different shapes, as well as real-world and abstract objects.Figure 5 B), only objects with collision status turned on remain graspable and can be rescaled or moved around in virtual space.</figure_citations>
            <section_index>3</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>2687</word_count>
            <figure_citations>Figure 6).Figure 7).Figure 7).</figure_citations>
            <section_index>4</section_index>
            <title>OBSERVED OPPORTUNITIES AND CHALLENGES</title>
        </sections>
        <sections>
            <word_count>461</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>LIMITATIONS AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>116</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>40</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>3693</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>"In VR, everything is possible!": Sketching and Simulating Spatially-Aware Interactive Spaces in Virtual Reality</title>
        <abstract>We propose using virtual reality (VR) as a design tool for sketching and simulating spatially-aware interactive spaces. Using VR, designers can quickly experience their envisioned spaces and interactions by simulating technologies such as motion tracking, multiple networked devices, or unusual form factors such as spherical touchscreens or bezel-less display tiles. Design ideas can be rapidly iterated without restrictions by the number, size, or shape and availability of devices or sensors in the lab. To understand the potentials and challenges of designing in VR, we conducted a user study with 12 interaction designers. As their tool, they used a custom-built virtual design environment with finger tracking and physics simulations for natural interactions with virtual devices and objects. Our study identified the designers' experience of space in relation to their own bodies and playful design explorations as key opportunities. Key challenges were the complexities of building a usable yet versatile VR-based "World Editor".</abstract>
        <keywords>
            <li>Interactive Spaces</li>
            <li>Spatial Awareness</li>
            <li>Interaction Design</li>
            <li>Virtual Reality</li>
            <li>Design tools</li>
        </keywords>
        <authors>
            <li>Hans-Christian Jetter</li>
            <li> Roman Rädle</li>
            <li> Tiare Feuchtner</li>
            <li> Christoph Anthes</li>
            <li> Judith Friedl</li>
            <li> Clemens Nylandsted Klokmose</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376652_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376652_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376652_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376652_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376652_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376652_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376652_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376687</filename><data>
        <paper_id>3313831.3376687</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376687</doi>
        <sections>
            <word_count>688</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>2670</word_count>
            <figure_citations>Figure 2a).Figure 2b.Figure 3 illustrates the curve confgurations used for Experiment 1.Figure 3).Figure 4).Figure 5a), and within each layout, the three amplifcation levels produced similar times.</figure_citations>
            <section_index>1</section_index>
            <title>BACKGROUND AND RELATED WORK</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>NONE</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>LOW</title>
        </sections>
        <sections>
            <word_count>948</word_count>
            <figure_citations>Figure 5b), with HIGH amplifcation reducing RULA more than LOW.Figure 5c).Figure 5d).</figure_citations>
            <section_index>4</section_index>
            <title>HIGH</title>
        </sections>
        <sections>
            <word_count>1000</word_count>
            <figure_citations>Figure 6).Figure 7).</figure_citations>
            <section_index>5</section_index>
            <title>AMPLIFICATION</title>
        </sections>
        <sections>
            <word_count>1777</word_count>
            <figure_citations>Figure 8a).Figure 8b).Figure 8c).Figure 8d).Figure 8e).Figure 9).Figure 10 shows responses for questions 1 to 4.</figure_citations>
            <section_index>6</section_index>
            <title>TANCES</title>
        </sections>
        <sections>
            <word_count>1066</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>GENERAL DISCUSSION</title>
        </sections>
        <sections>
            <word_count>168</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>36</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1198</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Improving Virtual Reality Ergonomics Through Reach-Bounded Non-Linear Input Amplification</title>
        <abstract>Input amplification enables easier movement in virtual reality (VR) for users with mobility issues or in confined spaces. However, current techniques either do not focus on maintaining feelings of body ownership, or are not applicable to general VR tasks. We investigate a general purpose non-linear transfer function that keeps the user's reach within reasonable bounds to maintain body ownership. The technique amplifies smaller movements from a user-definable neutral point into the expected larger movements using a configurable Hermite curve. Two experiments evaluate the approach. The first establishes that the technique has comparable performance to the state-of-the-art, increasing physical comfort while maintaining task performance and body ownership. The second explores the characteristics of the technique over a wide range of amplification levels. Using the combined results, design and implementation recommendations are provided with potential applications to related VR transfer functions.</abstract>
        <keywords>
            <li>Interaction techniques</li>
            <li>Ergonomics</li>
            <li>Input re-mapping</li>
        </keywords>
        <authors>
            <li>Johann Wentzel</li>
            <li> Greg d'Eon</li>
            <li> Daniel Vogel</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376687_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376687_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376687_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376687_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376687_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376687_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376687_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376687_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376687_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3313831.3376687_crop_10.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376698</filename><data>
        <paper_id>3313831.3376698</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376698</doi>
        <sections>
            <word_count>876</word_count>
            <figure_citations>Figure 1).Figure 7b).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1048</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>4817</word_count>
            <figure_citations>Figure 2 illustrates the ﬁve grip postures that were investigated in this study, their descriptions follow: Tripod at Front End (TFE): This is the most common grip posture for precise writing and drawing on a surface (Figure 2a).Figure 2c).Figure 2d).Figure 2e).Figure 3a.Figure 3b).Figure 5).Figure 6b and Figure 7b.Figure 6b and 7b) that ran the experimental (Unity) application.Figure 9 revealed a strong correlation between the trial time and ID for Tilt gestures, with both R2 values above 0.Figure 10).</figure_citations>
            <section_index>2</section_index>
            <title>AND FINGER MOTION</title>
        </sections>
        <sections>
            <word_count>272</word_count>
            <figure_citations>Figure 11).</figure_citations>
            <section_index>3</section_index>
            <title>INTERACTION TECHNIQUES AND APPLICATIONS</title>
        </sections>
        <sections>
            <word_count>888</word_count>
            <figure_citations>Figure 12), we show that the gestures can be utilized as interface widgets, which trigger scrolling or zooming operations on a web view with tilting and poking the pen respectively.Figure 13a, the user can select an object which is hidden by another object he can’t see with the Palm Grip, leading to misinterpretation.Figure 13b, the larger object is stacked below the target object.</figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>134</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>74</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>2413</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen</title>
        <abstract>The use of Virtual Reality (VR) in applications such as data analysis, artistic creation, and clinical settings requires high precision input. However, the current design of handheld controllers, where wrist rotation is the primary input approach, does not exploit the human fingers' capability for dexterous movements for high precision pointing and selection. To address this issue, we investigated the characteristics and potential of using a pen as a VR input device. We conducted two studies. The first examined which pen grip allowed the largest range of motion---we found a tripod grip at the rear end of the shaft met this criterion. The second study investigated target selection via 'poking' and ray-casting, where we found the pen grip outperformed the traditional wrist-based input in both cases. Finally, we demonstrate potential applications enabled by VR pen input and grip postures.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Pen input</li>
            <li>FInger and wrist dexterity</li>
            <li>Grip postures</li>
            <li>Handheld controller</li>
            <li>Spatial target selection</li>
        </keywords>
        <authors>
            <li>Nianlong Li</li>
            <li> Teng Han</li>
            <li> Feng Tian</li>
            <li> Jin Huang</li>
            <li> Minghui Sun</li>
            <li> Pourang Irani</li>
            <li> Jason Alexander</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376698_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376698_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376698_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376698_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376698_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376698_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376698_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>3313831.3376698_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>3313831.3376698_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>3313831.3376698_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>3313831.3376698_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>3313831.3376698_crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>3313831.3376698_crop_13.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376714</filename><data>
        <paper_id>3313831.3376714</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376714</doi>
        <sections>
            <word_count>562</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1811</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>2390</word_count>
            <figure_citations>Figure 1 shows a high-level view of Touché.</figure_citations>
            <section_index>2</section_index>
            <title>SYSTEM DESIGN</title>
        </sections>
        <sections>
            <word_count>176</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>INTERACTION DESIGN METHODOLOGY</title>
        </sections>
        <sections>
            <word_count>861</word_count>
            <figure_citations>Figure 3 plots the prediction accuracy for gesture class and progress.Figure 4 visualises this formula for a single pair of bones.Figure 5 shows the distribution of the pose difference, measured in square meters.</figure_citations>
            <section_index>4</section_index>
            <title>TECHNICAL EVALUATION</title>
        </sections>
        <sections>
            <word_count>1536</word_count>
            <figure_citations>Figure 6 shows the results of the study.Figure 7 and Table 1.</figure_citations>
            <section_index>5</section_index>
            <title>USER EVALUATION</title>
        </sections>
        <sections>
            <word_count>942</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>225</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>42</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>2441</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Touché: Data-Driven Interactive Sword Fighting in Virtual Reality</title>
        <abstract>VR games offer new freedom for players to interact naturally using motion. This makes it harder to design games that react to player motions convincingly. We present a framework for VR sword fighting experiences against a virtual character that simplifies the necessary technical work to achieve a convincing simulation. The framework facilitates VR design by abstracting from difficult details on the lower "physical" level of interaction, using data-driven models to automate both the identification of user actions and the synthesis of character animations. Designers are able to specify the character's behaviour on a higher "semantic" level using parameterised building blocks, which allow for control over the experience while minimising manual development work. We conducted a technical evaluation, a questionnaire study and an interactive user study. Our results suggest that the framework produces more realistic and engaging interactions than simple hand-crafted interaction logic, while supporting a controllable and understandable behaviour design.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Sword ﬁghting</li>
            <li>Machine learning</li>
            <li>Animation</li>
            <li>Gesture recognition</li>
        </keywords>
        <authors>
            <li>Javier Dehesa</li>
            <li> Andrew Vidler</li>
            <li> Christof Lutteroth</li>
            <li> Julian Padget</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376714_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376714_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376714_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376714_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376714_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376714_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376714_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376719</filename><data>
        <paper_id>3313831.3376719</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/10.1145/3313831.3376719</doi>
        <keywords>
            <li>Agency</li>
            <li>Social Touch</li>
            <li>Virtual Reality</li>
            <li>Human-likeness</li>
        </keywords>
        <authors>
            <li>undefined</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376719_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376719_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376719_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376719_crop_4.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376762</filename><data>
        <paper_id>3313831.3376762</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376762</doi>
        <sections>
            <word_count>649</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1373</word_count>
            <figure_citations>Figure 1 – (a) Path to Minerva (b) Calliope’s Reflection in Mirror (c ) The main area not.</figure_citations>
            <section_index>1</section_index>
            <title>LITERATURE REVIEW</title>
        </sections>
        <sections>
            <word_count>1408</word_count>
            <figure_citations>Figure 1a).Figure 1b).Figure 2 – (a) The Emotion Spells (b) Interacting with the spells (c) Activating a spell with the wand Paper 633 Page 4 CHI 2020 Paper CHI 2020, April 25–30, 2020, Honolulu, HI, USA Figure 3 - The interactional logic of TNFT's dialogue system, the emotion spells, and Minerva's responses Figure 3 illustrates the interactional logic of our dialogue system.Figure 3), following the same color mappings as the spells.Figure 3 we visualize this and show how a player might navigate the expressive space of TNFT.</figure_citations>
            <section_index>2</section_index>
            <title>DESIGNING A PARTICIPATORY THEATER EXPERIENCE</title>
        </sections>
        <sections>
            <word_count>386</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>METHODS</title>
        </sections>
        <sections>
            <word_count>2504</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>FINDINGS</title>
        </sections>
        <sections>
            <word_count>1830</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DESIGN RECOMMENDATIONS</title>
        </sections>
        <sections>
            <word_count>413</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>51</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1593</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Investigating Roleplaying and Identity Transformation in a Virtual Reality Narrative Experience</title>
        <abstract>In this paper we describe the design and evaluation of The Next Fairy Tale (TNFT) VR, a theatrical interactive storytelling system created in virtual reality and informed by performing arts theories. TNFT was designed to produce opportunities for interactors to experience role-taking and character identification using design principles drawn from actor training and theatrical performance. We report the results of a pilot qualitative study of interactors using TNFT to explore the elements of the design that supported or hindered roleplaying behavior. We identify four design patterns that supported roleplaying in the system: (1) using explicit roles to set player expectations, (2) embracing the "mask and the mirror" effect, (3) attending to visual and interactional details, and (4) easing the player gently into the roleplaying experience. These patterns speak to a broader need to support roleplay through explicit scaffolding of desired player behaviors in digital narrative experiences.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Interactive Digital Storytelling</li>
            <li>Interactive Performance</li>
            <li>Roleplaying</li>
            <li>Narrative</li>
            <li>Drama</li>
        </keywords>
        <authors>
            <li>Saumya Gupta</li>
            <li> Theresa Jean Tanenbaum</li>
            <li> Meena Devii Muralikumar</li>
            <li> Aparajita S. Marathe</li>
        </authors>
    </data>
    <figures></figures>
</article>
<article>
    <filename>3313831.3376788</filename><data>
        <paper_id>3313831.3376788</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376788</doi>
        <sections>
            <word_count>784</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1753</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1776</word_count>
            <figure_citations>Figure 1).Figure 1a).Figure 1c).</figure_citations>
            <section_index>2</section_index>
            <title>STUDY</title>
        </sections>
        <sections>
            <word_count>3828</word_count>
            <figure_citations>Figure 2).Figure 2).Figure 2).Figure 3).Figure 4).Figure 4).Figure 4).Figure 5).Figure 6).Figure 6).Figure 7).Figure 7).</figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1172</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>DESIGNING FOR ETHICAL AI TRAINING</title>
        </sections>
        <sections>
            <word_count>213</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>1232</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Would you do it?: Enacting Moral Dilemmas in Virtual Reality for Understanding Ethical Decision-Making</title>
        <abstract>A moral dilemma is a decision-making paradox without unambiguously acceptable or preferable options. This paper investigates if and how the virtual enactment of two renowned moral dilemmas---the Trolley and the Mad Bomber---influence decision-making when compared with mentally visualizing such situations. We conducted two user studies with two gender-balanced samples of 60 participants in total that compared between paper-based and virtual-reality (VR) conditions, while simulating 5 distinct scenarios for the Trolley dilemma, and 4 storyline scenarios for the Mad Bomber's dilemma. Our findings suggest that the VR enactment of moral dilemmas further fosters utilitarian decision-making, while it amplifies biases such as sparing juveniles and seeking retribution. Ultimately, we theorize that the VR enactment of renowned moral dilemmas can yield ecologically-valid data for training future Artificial Intelligence (AI) systems on ethical decision-making, and we elicit early design principles for the training of such systems.</abstract>
        <keywords>
            <li>Ethics</li>
            <li>Moral dilemmas</li>
            <li>VR</li>
            <li>Decision-making</li>
            <li>Ethical AI</li>
        </keywords>
        <authors>
            <li>Evangelos Niforatos</li>
            <li> Adam Palma</li>
            <li> Roman Gluszny</li>
            <li> Athanasios Vourvopoulos</li>
            <li> Fotis Liarokapis</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376788_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376788_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376788_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376788_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376788_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376788_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376788_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376803</filename><data>
        <paper_id>3313831.3376803</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376803</doi>
        <sections>
            <word_count>947</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1014</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>4150</word_count>
            <figure_citations>Figure 2).Figure 4a).Figure 4d).Figure 4b).Figure 4f).Figure 4c).</figure_citations>
            <section_index>2</section_index>
            <title>GENERAL APPROACH</title>
        </sections>
        <sections>
            <word_count>1435</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>SGD</title>
        </sections>
        <sections>
            <word_count>445</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>GENERAL DISCUSSION</title>
        </sections>
        <sections>
            <word_count>137</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>132</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>1586</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>HiveFive: Immersion Preserving Attention Guidance in Virtual Reality</title>
        <abstract>Recent advances in Virtual Reality (VR) technology, such as larger fields of view, have made VR increasingly immersive. However, a larger field of view often results in a user focusing on certain directions and missing relevant content presented elsewhere on the screen. With HiveFive, we propose a technique that uses swarm motion to guide user attention in VR. The goal is to seamlessly integrate directional cues into the scene without losing immersiveness. We evaluate HiveFive in two studies. First, we compare biological motion (from a prerecorded swarm) with non-biological motion (from an algorithm), finding further evidence that humans can distinguish between these motion types and that, contrary to our hypothesis, non-biological swarm motion results in significantly faster response times. Second, we compare HiveFive to four other techniques and show that it not only results in fast response times but also has the smallest negative effect on immersion.</abstract>
        <keywords>
            <li>Attention guidance</li>
            <li>Virtual reality</li>
            <li>Immersion</li>
            <li>Eye-tracking</li>
            <li>Particle swarms</li>
            <li>User studies</li>
        </keywords>
        <authors>
            <li>Daniel Lange</li>
            <li> Tim Claudius Stratmann</li>
            <li> Uwe Gruenefeld</li>
            <li> Susanne Boll</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376803_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376803_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376803_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376803_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376803_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376803_crop_6.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376821</filename><data>
        <paper_id>3313831.3376821</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376821</doi>
        <keywords>
            <li>Redirected Walking</li>
            <li>Virtual Reality</li>
            <li>Telewalk</li>
        </keywords>
        <authors>
            <li>undefined</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376821_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376821_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376821_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376821_crop_4.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>3313831.3376847</filename><data>
        <paper_id>3313831.3376847</paper_id>
        <venue>CHI 20</venue>
        <doi>10.1145/3313831.3376847</doi>
        <sections>
            <word_count>930</word_count>
            <figure_citations>Figure 1 and Figure 2, uses two small vibration motors controlled by an Arduino microcontroller and is entirely integrated into and powered by the VR headset.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>793</word_count>
            <figure_citations>Figure 3 (c)).Figure 3 (b)) vs.Figure 3 (a)).</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1945</word_count>
            <figure_citations>Figure 2).Figure 3a and Figure 3b, respectively) were chosen based on the settings of the commercial bone-conducted earphone and other previous systems [61, 62].Figure 3c).Figure 3a).Figure 3b).Figure 4c) that were scattered in three designed VR scenes: a city in Figure 4(a)(d), a forest in Figure 4(b)(e), and a science ﬁction-themed passage in Figure 4(c)(f).Figure 5, and are stated directly in the following: </figure_citations>
            <section_index>2</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>707</word_count>
            <figure_citations>Figure 5, 6 and 7, where error bars in each ﬁgure denoted standard error of the mean value), and report our ﬁndings in the following.Figure 5).Figure 6).Figure 6 and are described as follows: • 2-sided tapping (synchronized) vs.</figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1114</word_count>
            <figure_citations>Figure 7).</figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>106</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>59</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGEMENT</title>
        </sections>
        <sections>
            <word_count>2277</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>WalkingVibe: Reducing Virtual Reality Sickness and Improving Realism while Walking in VR using Unobtrusive Head-mounted Vibrotactile Feedback</title>
        <abstract>Virtual Reality (VR) sickness is common with symptoms such as headaches, nausea, and disorientation, and is a major barrier to using VR. We propose WalkingVibe, which applies unobtrusive vibrotactile feedback for VR walking experiences, and also reduces VR sickness and discomfort while improving realism. Feedback is delivered through two small vibration motors behind the ears at a frequency that strikes a balance in inducing vestibular response while minimizing annoyance. We conducted a 240-person study to explore how visual, audio, and various tactile feedback designs affect the locomotion experience of users walking passively in VR while seated statically in reality. Results showed timing and location for tactile feedback have significant effects on VR sickness and realism. With WalkingVibe, 2-sided step-synchronized design significantly reduces VR sickness and discomfort while significantly improving realism. Furthermore, its unobtrusiveness and ease of integration make WalkingVibe a practical approach for improving VR experiences with new and existing VR headsets.</abstract>
        <keywords>
            <li>Virtual reality sickness</li>
            <li>Discomfort</li>
            <li>Realism</li>
            <li>Vestibular system</li>
            <li>Vibrotactile feedback</li>
        </keywords>
        <authors>
            <li>Yi-Hao Peng</li>
            <li> Carolyn Yu</li>
            <li> Shi-Hong Liu</li>
            <li> Chung-Wei Wang</li>
            <li> Paul Taele</li>
            <li> Neng-Hao Yu</li>
            <li> Mike Y. Chen</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>3313831.3376847_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>3313831.3376847_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>3313831.3376847_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>3313831.3376847_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>3313831.3376847_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>3313831.3376847_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>3313831.3376847_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper</filename><data>
        <paper_id>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300826</doi>
        <sections>
            <word_count>365</word_count>
            <figure_citations>Figure 1 illustrates our method analogous to Rettig [32].</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>621</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>3689</word_count>
            <figure_citations>Figure 1 the running example in this section.Figure 2).Figure 3).Figure 5).Figure 6): (1) the View pane shows a simulated or live preview of the AR/VR prototype running on a smartphone (live if used in combination with the 360proto App); (2) the Collect pane shows thumbnails of the live video feed of the Camera tool, previous captures of paper mockups, and PNG images imported from the file system; the Layers pane shows thumbnails of the 2D, 360, and AR/VR specific layers described below; the Capture pane shows a larger preview of content selected in the Collect pane and provides tools for chroma keying to make pixels of a selected color with a specified tolerance level made transparent in layers.Figure 7 (left) shows how our designer is now starting to add content and compose the butterfly scene from layers of paper mockups.Figure 7 (right) shows a 360 preview of the scene using spheres to render the 360 layers.Figure 8 shows our designer using the AR marker mode in the Camera tool to superimpose an image of a butterfly over a kanji marker attached to a stick made from paper.</figure_citations>
            <section_index>2</section_index>
            <title>INITIAL DESIGN JAMS</title>
        </sections>
        <sections>
            <word_count>1185</word_count>
            <figure_citations>Figure 10).Figure 11).</figure_citations>
            <section_index>3</section_index>
            <title>AR USER</title>
        </sections>
        <sections>
            <word_count>2327</word_count>
            <figure_citations>Figure 13 shows 29 students’ ratings for three questions.Figure 14).Figure 15).</figure_citations>
            <section_index>4</section_index>
            <title>FINAL DESIGN JAMS</title>
        </sections>
        <sections>
            <word_count>304</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>141</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>866</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>360proto: Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper</title>
        <abstract>We explore 360 paper prototyping to rapidly create AR/VR prototypes from paper and bring them to life on AR/VR devices. Our approach is based on a set of emerging paper prototyping templates specifically for AR/VR. These templates resemble the key components of many AR/VR interfaces, including 2D representations of immersive environments, AR marker overlays and face masks, VR controller models and menus, and 2D screens and HUDs. To make prototyping with these templates effective, we developed 360proto, a suite of three novel physical--digital prototyping tools: (1) the 360proto Camera for capturing paper mockups of all components simply by taking a photo with a smartphone and seeing 360-degree panoramic previews on the phone or stereoscopic previews in Google Cardboard; (2) the 360proto Studio for organizing and editing captures, for composing AR/VR interfaces by layering the captures, and for making them interactive with Wizard of Oz via live video streaming; (3) the 360proto App for running and testing the interactive prototypes on AR/VR capable mobile devices and headsets. Through five student design jams with a total of 86 participants and our own design space explorations, we demonstrate that our approach with 360proto is useful to create relatively complex AR/VR applications.</abstract>
        <keywords>
            <li>AR/VR</li>
            <li>Physical–digital prototyping</li>
            <li>Wizard of Oz</li>
        </keywords>
        <authors>
            <li>Michael Nebeling</li>
            <li> Katy Madier</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_13.png</url>
        </figure>
        <figure>
            <id>14</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_14.png</url>
        </figure>
        <figure>
            <id>15</id>
            <url>360proto Making Interactive Virtual Reality & Augmented Reality Prototypes from Paper_crop_15.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation</filename><data>
        <paper_id>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300905</doi>
        <sections>
            <word_count>1250</word_count>
            <figure_citations>Figure 4,6) that can help reduce the initial time and effort and enable researchers to quickly focus on the design of novel applications and interaction techniques.Figure 5).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>958</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>380</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>CYBERSICKNESS VS SIMULATOR SICKNESS VS</title>
        </sections>
        <sections>
            <word_count>2087</word_count>
            <figure_citations>Figure 1 shows four example application scenarios for GVS induced feedback.Figure 2) and tested it with six participants (5 females, average age 22.Figure 2).Figure 3 shows the second GVS device we created after receiving feedback from the pilot study with the first prototype (Figure 2).Figure 4).Figure 4).Figure 5) as zero-mean current noise of an imperceptible magnitude.Figure 5 shows the mastoid, which is the back part of the temporal bones situated at the sides and base of the skull.Figure 6 shows the opened up neckband with the PCB and the connected GVS electrodes, EDA electrodes and the HR sensor.Figure 6 shows EDA and HR sensors connected to the PCB through the two bands of black and white wires.</figure_citations>
            <section_index>3</section_index>
            <title>GVS FOR VR</title>
        </sections>
        <sections>
            <word_count>1141</word_count>
            <figure_citations>Figure 7).</figure_citations>
            <section_index>4</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>1491</word_count>
            <figure_citations>Figure 8 shows the boxplot of 20 participants’ scores for both trials.Figure 9 shows the results of Flow in the two conditions.Figure 11).Figure 12).</figure_citations>
            <section_index>5</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>777</word_count>
            <figure_citations>Figure 8).</figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>100</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>LIMITATIONS AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>136</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>2023</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation</title>
        <abstract>We present a small and lightweight wearable device that enhances virtual reality experiences and reduces cybersickness by means of galvanic vestibular stimulation (GVS). GVS is a specific way to elicit vestibular reflexes that has been used for over a century to study the function of the vestibular system. In addition to GVS, we support physiological sensing by connecting heart rate, electrodermal activity and other sensors to our wearable device using a plug and play mechanism. An accompanying Android app communicates with the device over Bluetooth (BLE) for transmitting the GVS stimulus to the user through electrodes attached behind the ears. Our system supports multiple categories of virtual reality applications with different types of virtual motion such as driving, navigating by flying, teleporting, or riding. We present a user study in which participants (N = 20) experienced significantly lower cybersickness when using our device and rated experiences with GVS-induced haptic feedback as significantly more immersive than a no-GVS baseline.</abstract>
        <keywords>
            <li>Galvanic Vestibular Stimulation</li>
            <li>Wearables</li>
            <li>Interaction design</li>
            <li>Haptic feedback</li>
            <li>Virtual Reality</li>
            <li>Cybersickness</li>
        </keywords>
        <authors>
            <li>Misha Sra</li>
            <li> Abhinandan Jain</li>
            <li> Pattie Maes</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation_crop_12.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality</filename><data>
        <paper_id>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300628</doi>
        <sections>
            <word_count>674</word_count>
            <figure_citations>Figure 1a).Figure 1b), the designer can use the phone to look at distributions of sleeping schedules for each cluster (Figure 1c).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1029</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1070</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>A DESIGN PROBE FOR DAAD</title>
        </sections>
        <sections>
            <word_count>475</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>DESIGN GUIDELINES</title>
        </sections>
        <sections>
            <word_count>547</word_count>
            <figure_citations>Figure 2 captures the four main regions of the mobile interface: the largest, is dedicated to the camera and visualization augmentation (a), a contextual menu occupies the right edge of the display (b) and dynamically changes depending on what is present in the camera’s field of view, a data attribute Paper 398 CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK a b d cook eatout housing emp grade exercise c Figure 2: Affinity Lens User Interface.Figure 3) we follow a designer, Dave, as he uses DAAD to analyze the food choice dataset (this example is based on a combination of real use cases from our user studies).Figure 3a).Figure 3b).Figure 4 a).</figure_citations>
            <section_index>4</section_index>
            <title>USER EXPERIENCE</title>
        </sections>
        <sections>
            <word_count>890</word_count>
            <figure_citations>Figure 3 d) so he can continue working without pointing at the physical notes (D2, D3).Figure 3 e) and alerts him that all but one student in the on-campus sub-cluster are first years (D4).Figure 4 b).Figure 4 f).Figure 4) to support common tasks.</figure_citations>
            <section_index>5</section_index>
            <title>PRINT</title>
        </sections>
        <sections>
            <word_count>2</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>STILL IMAGE</title>
        </sections>
        <sections>
            <word_count>1140</word_count>
            <figure_citations>Figure 1a).Figure 4i).Figure 4c) displays those data values that are the same and those that are different (a weak representation of affinity).Figure 4d) will generate an overlay of common words (sized by frequency) on top of the notes.Figure 4g).Figure 4h).</figure_citations>
            <section_index>7</section_index>
            <title>LIVE</title>
        </sections>
        <sections>
            <word_count>979</word_count>
            <figure_citations>Figure 5, Affinity Lens is comprised of five main components: (1) Scene Analyzer, (2) Lens Controller, (3) Dynamic View Configurator, (4) lenses, and (5) the Data Access and Analytics Module.</figure_citations>
            <section_index>8</section_index>
            <title>SYSTEM ARCHITECTURE</title>
        </sections>
        <sections>
            <word_count>1834</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>462</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>DISCUSSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>130</word_count>
            <figure_citations></figure_citations>
            <section_index>11</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>475</word_count>
            <figure_citations></figure_citations>
            <section_index>12</section_index>
            <title>REFERENCES</title>
        </sections>
        <sections>
            <word_count>1068</word_count>
            <figure_citations></figure_citations>
            <section_index>13</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <title>Affinity Lens: Data-Assisted Affinity Diagramming with Augmented Reality</title>
        <abstract>Despite the availability of software to support Affinity Diagramming (AD), practitioners still largely favor physical sticky-notes. Physical notes are easy to set-up, can be moved around in space and offer flexibility when clustering un-structured data. However, when working with mixed data sources such as surveys, designers often trade off the physicality of notes for analytical power. We propose AffinityLens, a mobile-based augmented reality (AR) application for Data-Assisted Affinity Diagramming (DAAD). Our application provides just-in-time quantitative insights overlaid on physical notes. Affinity Lens uses several different types of AR overlays (called lenses) to help users find specific notes, cluster information, and summarize insights from clusters. Through a formative study of AD users, we developed design principles for data-assisted AD and an initial collection of lenses. Based on our prototype, we find that Affinity Lens supports easy switching between qualitative and quantitative 'views' of data, without surrendering the lightweight benefits of existing AD practice.</abstract>
        <keywords>
            <li>Affinity diagrams</li>
            <li>Visual analytics</li>
            <li>Augmented reality</li>
        </keywords>
        <authors>
            <li>Hariharan Subramonyam</li>
            <li> Steven M. Drucker</li>
            <li> Eytan Adar</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Affinity Lens Data-Assisted Affinity Diagramming with Augmented Reality_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions</filename><data>
        <paper_id>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025723</doi>
        <sections>
            <word_count>675</word_count>
            <figure_citations>Figure 1, the system is equipped with 1) an Ambient Temperature Simulation Module that utilizes Peltier elements to provide heating and cooling sensations (worn on the neck) and 2) a Wind Simulation Module that utilizes multidirectional fans focused towards the user’s face.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1326</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>BACKGROUND AND RELATED WORK</title>
        </sections>
        <sections>
            <word_count>2078</word_count>
            <figure_citations>Figure 2, were chosen for the study due to their proximity to the thermoregulatory centre of the central nervous system [15].Figure 3 (A)).Figure 3 (B)).Figure 4 (A): sensation scores).Figure 4 (B): sensation scores).Figure 4 (A): comfort scores).Figure 4 (B): comfort scores).</figure_citations>
            <section_index>2</section_index>
            <title>STIMULI DESIGN CONSIDERATIONS</title>
        </sections>
        <sections>
            <word_count>383</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>SYSTEM IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>2972</word_count>
            <figure_citations>Figure 7 (A)) and a snowy mountain environment (Figure 7 (B)) for 30 seconds each, presented in a randomized order.Figure 10).</figure_citations>
            <section_index>4</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>761</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>146</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>34</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1106</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Ambiotherm: Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions</title>
        <abstract>In this paper, we present and evaluate Ambiotherm, a wearable accessory for Head Mounted Displays (HMD) that provides thermal and wind stimuli to simulate real-world environmental conditions, such as ambient temperatures and wind conditions, to enhance the sense of presence in Virtual Reality (VR). Ambiotherm consists of a Ambient Temperature Module that is attached to the user's neck, a Wind Simulation Module focused towards the user's face, and a Control Module utilizing Bluetooth communication. We demonstrate Ambiotherm with two VR environments, a hot desert, and a snowy mountain, to showcase the different types of simulated environmental conditions. We conduct several studies to 1) address design factors of the system and 2) evaluate Ambiotherm's effect on factors related to a user's sense of presence. Our findings show that the addition of wind and thermal stimuli significantly improves sensory and realism factors, contributing towards an enhanced sense of presence when compared to traditional VR experiences.</abstract>
        <keywords>
            <li>Presence</li>
            <li>Ambient Temperature</li>
            <li>Virtual Wind</li>
            <li>Virtual Reality</li>
            <li>Multimodal Interaction</li>
        </keywords>
        <authors>
            <li>Nimesha Ranasinghe</li>
            <li> Pravar Jain</li>
            <li> Shienny Karwita</li>
            <li> David Tolley</li>
            <li> Ellen Yi-Luen Do</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>Ambiotherm- Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions_crop_12.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories</filename><data>
        <paper_id>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300377</doi>
        <sections>
            <word_count>410</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>2227</word_count>
            <figure_citations>Figure 2).Figure 3).Figure 4).Figure 5).Figure 6).</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>2822</word_count>
            <figure_citations>Figure 8).</figure_citations>
            <section_index>2</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>959</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>190</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>LIMITATIONS</title>
        </sections>
        <sections>
            <word_count>257</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>18</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1672</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories</title>
        <abstract>Room-scale Virtual Reality (VR) systems have arrived in users' homes where tracked environments are set up in limited physical spaces. As most Virtual Environments (VEs) are larger than the tracked physical space, locomotion techniques are used to navigate in VEs. Currently, in recent VR games, point &amp; teleport is the most popular locomotion technique. However, it only allows users to select the position of the teleportation and not the orientation that the user is facing after the teleport. This results in users having to manually correct their orientation after teleporting and possibly getting entangled by the cable of the headset. In this paper, we introduce and evaluate three different point &amp; teleport techniques that enable users to specify the target orientation while teleporting. The results show that, although the three teleportation techniques with orientation indication increase the average teleportation time, they lead to a decreased need for correcting the orientation after teleportation.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Locomotion</li>
            <li>Teleportation</li>
            <li>Orientation Indication</li>
            <li>Virtual Environments</li>
            <li>Point & Teleport</li>
        </keywords>
        <authors>
            <li>Markus Funk</li>
            <li> Florian Müller</li>
            <li> Marco Fendrich</li>
            <li> Megan Shene</li>
            <li> Moritz Kolvenbach</li>
            <li> Niclas Dobbertin</li>
            <li> Sebastian Günther</li>
            <li> Max Mühlhäuser</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>Assessing the Accuracy of Point & Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories_crop_12.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Augmented Reality Views for Occluded Interaction</filename><data>
        <paper_id>Augmented Reality Views for Occluded Interaction</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300676</doi>
        <sections>
            <word_count>429</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>983</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>426</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>TYPES OF OCCLUDED INTERACTION</title>
        </sections>
        <sections>
            <word_count>774</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>3</section_index>
            <title>VISUALIZING OCCLUDED OBJECTS</title>
        </sections>
        <sections>
            <word_count>1733</word_count>
            <figure_citations>Figure 2) were as follows: Pressing a Light Switch: Required either pressing (changing the state of) the left button, right button or both of the buttons on a two-button light switch.Figure 2).Figure 3a).Figure 3b).Figure 3c), to the left of the participants.Figure 3a).Figure 4).</figure_citations>
            <section_index>4</section_index>
            <title>EVALUATING OCCLUDED INTERACTION VIEWS</title>
        </sections>
        <sections>
            <word_count>2314</word_count>
            <figure_citations>Figure 5, the manipulation delay differed between the views.Figure 5 also shows how this delay differed depending on the task.Figure 6).Figure 6).Figure 7 shows how the view influenced the two kinds of manipulation Paper 446 Dynamic camera Cloned 3D See-through −0.Figure 7 also shows the interactions between task and error.Figure 8).Figure 8).</figure_citations>
            <section_index>5</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>869</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>107</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>25</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1343</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Augmented Reality Views for Occluded Interaction</title>
        <abstract>We rely on our sight when manipulating objects. When objects are occluded, manipulation becomes difficult. Such occluded objects can be shown via augmented reality to re-enable visual guidance. However, it is unclear how to do so to best support object manipulation. We compare four views of occluded objects and their effect on performance and satisfaction across a set of everyday manipulation tasks of varying complexity. The best performing views were a see-through view and a displaced 3D view. The former enabled participants to observe the manipulated object through the occluder, while the latter showed the 3D view of the manipulated object offset from the object's real location. The worst performing view showed remote imagery from a simulated hand-mounted camera. Our results suggest that alignment of virtual objects with their real-world location is less important than an appropriate point-of-view and view stability.</abstract>
        <keywords>
            <li>Augmented reality</li>
            <li>Manipulation task</li>
            <li>Finger-camera</li>
        </keywords>
        <authors>
            <li>Klemen Lilija</li>
            <li> Henning Pohl</li>
            <li> Sebastian Boring</li>
            <li> Kasper Hornbæk</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Augmented Reality Views for Occluded Interaction_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Augmented Reality Views for Occluded Interaction_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Augmented Reality Views for Occluded Interaction_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Augmented Reality Views for Occluded Interaction_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Augmented Reality Views for Occluded Interaction_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Augmented Reality Views for Occluded Interaction_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Augmented Reality Views for Occluded Interaction_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Augmented Reality Views for Occluded Interaction_crop_8.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences</filename><data>
        <paper_id>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300736</doi>
        <sections>
            <word_count>1651</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>464</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>741</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>THE CHARACTERISTICS OF VRNF</title>
        </sections>
        <sections>
            <word_count>849</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>METHOD</title>
        </sections>
        <sections>
            <word_count>2541</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1329</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>178</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>251</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>LIMITATIONS AND FURTHER WORK</title>
        </sections>
        <sections>
            <word_count>37</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1107</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Behind the Curtain of the "Ultimate Empathy Machine": On the Composition of Virtual Reality Nonfiction Experiences</title>
        <abstract>Virtual Reality nonfiction (VRNF) is an emerging form of immersive media experience created for consumption using panoramic "Virtual Reality" headsets. VRNF promises nonfiction content producers the potential to create new ways for audiences to experience "the real"; allowing viewers to transition from passive spectators to active participants. Our current project is exploring VRNF through a series of ethnographic and experimental studies. In order to document the content available, we embarked on an analysis of VR documentaries produced to date. In this paper, we present an analysis of a representative sample of 150 VRNF titles released between 2012-2018. We identify and quantify 64 characteristics of the medium over this period, discuss how producers are exploiting the affordances of VR, and shed light on new audience roles. Our findings provide insight into the current state of the art in VRNF and provide a digital resource for other researchers in this area.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Nonfiction</li>
            <li>Immersive media</li>
            <li>Interaction</li>
        </keywords>
        <authors>
            <li>Chris Bevan</li>
            <li> David Philip Green</li>
            <li> Harry Farmer</li>
            <li> Mandy Rose</li>
            <li> Kirsten Cater</li>
            <li> Danaë Stanton Fraser</li>
            <li> Helen Brown</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Behind the Curtain of the Ultimate Empathy Machine On the Composition of Virtual Reality Nonfiction Experiences_crop_4.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality</filename><data>
        <paper_id>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300589</doi>
        <sections>
            <word_count>730</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>3992</word_count>
            <figure_citations>Figure 2a) and 1.Figure 2b).Figure 2c).Figure 3a).Figure 3b).Figure 3c).Figure 3b.Figure 5b.</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>529</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>TECHNICAL EVALUATION</title>
        </sections>
        <sections>
            <word_count>2598</word_count>
            <figure_citations>Figure 10, consisted of a safe-to-touch quadcopter, Vicon motion capture cameras, two HTC Vive lighthouses, and the HTC Vive Head-Mounted Display (HMD).Figure 11).Figure 13, and placed it in their shopping basket.Figure 14a.Figure 14b.</figure_citations>
            <section_index>3</section_index>
            <title>USER EVALUATION</title>
        </sections>
        <sections>
            <word_count>89</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>39</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1689</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Beyond The Force: Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality</title>
        <abstract>Quadcopters have been used as hovering encountered-type haptic devices in virtual reality. We suggest that quadcopters can facilitate rich haptic interactions beyond force feedback by appropriating physical objects and the environment. We present HoverHaptics, an autonomous safe-to-touch quadcopter and its integration with a virtual shopping experience. HoverHaptics highlights three affordances of quadcopters that enable these rich haptic interactions: (1) dynamic positioning of passive haptics, (2) texture mapping, and (3) animating passive props. We identify inherent challenges of hovering encountered-type haptic devices, such as their limited speed, inadequate control accuracy, and safety concerns. We then detail our approach for tackling these challenges, including the use of display techniques, visuo-haptic illusions, and collision avoidance. We conclude by describing a preliminary study (n = 9) to better understand the subjective user experience when interacting with a quadcopter in virtual reality using these techniques.</abstract>
        <keywords>
            <li>Quadcopter</li>
            <li>Drone</li>
            <li>UAV</li>
            <li>Encountered-Type</li>
            <li>Human-Drone Interaction</li>
            <li>Robotic Graphics</li>
            <li>Haptics</li>
            <li>Virtual Reality</li>
        </keywords>
        <authors>
            <li>Parastoo Abtahi</li>
            <li> Benoit Landry</li>
            <li> Jackie (Junrui) Yang</li>
            <li> Marco Pavone</li>
            <li> Sean Follmer</li>
            <li> James A. Landay</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_13.png</url>
        </figure>
        <figure>
            <id>14</id>
            <url>Beyond The Force Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality_crop_14.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Can Mobile Augmented Reality Stimulate a Honeypot</filename><data>
        <paper_id>Can Mobile Augmented Reality Stimulate a Honeypot</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300515</doi>
        <sections>
            <word_count>3224</word_count>
            <figure_citations>Figure 1a).Figure 1b).Figure 1c).Figure 1d).Figure 1a), which is a bustling shopping area and tourist destination, and Federation Square (location F in Figure 1a), which is a major venue for arts, culture and public events.Figure 1b).Figure 1c).Figure 1c).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>793</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>METHODS</title>
        </sections>
        <sections>
            <word_count>3147</word_count>
            <figure_citations>Figure 1b).Figure 1d).Figure 1c), which required users to point their device at a marker positioned above a large red throne.Figure 2d).</figure_citations>
            <section_index>2</section_index>
            <title>FINDINGS</title>
        </sections>
        <sections>
            <word_count>1352</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>DISCUSSION AND IMPLICATIONS</title>
        </sections>
        <sections>
            <word_count>134</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>22</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>2432</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Can Mobile Augmented Reality Stimulate a Honeypot Effect?: Observations from Santa's Lil Helper</title>
        <abstract>In HCI, the honeypot effect describes a form of audience engagement in which a person's interaction with a technology stimulates passers-by to observe, approach and engage in an interaction themselves. In this paper we explore the potential for honeypot effects to arise in the use of mobile augmented reality (AR) applications in urban spaces. We present an observational study of Santa's Lil Helper, a mobile AR game that created a Christmas-themed treasure hunt in a metropolitan area. Our study supports a consideration of three factors that may impede the honeypot effect: the presence of people in relation to the game and its interactive components; the visibility of gameplay in urban space; and the extent to which the game permits a shared experience. We consider how these factors can inform the design of future AR experiences that are capable of stimulating honeypot effects in public space.</abstract>
        <keywords>
            <li>Audience</li>
            <li>Augmented reality</li>
            <li>Honeypot efect</li>
            <li>Public space</li>
        </keywords>
        <authors>
            <li>Ryan M. Kelly</li>
            <li> Hasan Shahid Ferdous</li>
            <li> Niels Wouters</li>
            <li> Frank Vetere</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Can Mobile Augmented Reality Stimulate a Honeypot_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Can Mobile Augmented Reality Stimulate a Honeypot_crop_2.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>CarVR- Enabling In-Car Virtual Reality Entertainment</filename><data>
        <paper_id>CarVR- Enabling In-Car Virtual Reality Entertainment</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025665</doi>
        <sections>
            <word_count>558</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1609</word_count>
            <figure_citations>Figure 2).</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1410</word_count>
            <figure_citations>Figure 3).Figure 4).Figure 5).Figure 6).</figure_citations>
            <section_index>2</section_index>
            <title>DESIGN SPACE</title>
        </sections>
        <sections>
            <word_count>851</word_count>
            <figure_citations>Figure 7).Figure 8).</figure_citations>
            <section_index>3</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>3349</word_count>
            <figure_citations>Figure 9).Figure 10).Figure 11 shows the results of the final comparison regarding general discomfort between both conditions.</figure_citations>
            <section_index>4</section_index>
            <title>STUDY</title>
        </sections>
        <sections>
            <word_count>193</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>39</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENT</title>
        </sections>
        <sections>
            <word_count>1102</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>CarVR: Enabling In-Car Virtual Reality Entertainment</title>
        <abstract>Mobile virtual reality (VR) head-mounted displays (HMDs) allow users to experience highly immersive entertainment whilst being in a mobile scenario. Long commute times make casual gaming in public transports and cars a common occupation. However, VR HMDs can currently not be used in moving vehicles since the car's rotation affects the HMD's sensors and simulator sickness occurs when the visual and vestibular system are stimulated with incongruent information. We present CarVR, a solution to enable VR in moving vehicles by subtracting the car's rotation and mapping vehicular movements with the visual information. This allows the user to actually feel correct kinesthetic forces during the VR experience. In a user study (n = 21), we compared CarVR inside a moving vehicle with the baseline of using VR without vehicle movements. We show that the perceived kinesthetic forces caused by CarVR increase enjoyment and immersion significantly while simulator sickness is reduced compared to a stationary VR experience. Finally, we explore the design space of in-car VR entertainment applications using real kinesthetic forces and derive design considerations for practitioners.</abstract>
        <keywords>
            <li>Force-feedback</li>
            <li>Motion platform</li>
            <li>Immersion</li>
            <li>Virtual reality</li>
            <li>Automotive</li>
            <li>Entertainment</li>
            <li>Gaming</li>
        </keywords>
        <authors>
            <li>Philipp Hock</li>
            <li> Sebastian Benedikter</li>
            <li> Jan Gugenheimer</li>
            <li> Enrico Rukzio</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>CarVR- Enabling In-Car Virtual Reality Entertainment_crop_11.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Crossing-Based Selection with Virtual Reality Head-Mounted Displays</filename><data>
        <paper_id>Crossing-Based Selection with Virtual Reality Head-Mounted Displays</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300848</doi>
        <sections>
            <word_count>966</word_count>
            <figure_citations>Figure 1) through ray-casting pointing is a difficult task [20, 23, 24], due to the fact that the interaction takes place in free space without physical support for the hand.Figure 1), corresponding to pointing at spheres in 3D space and circular targets on 2D plane respectively.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1386</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>3634</word_count>
            <figure_citations>Figure 2).Figure 2(a)) that was made up by the positions of the two targets and the virtual camera with the camera’s position as the vertex.Figure 2(a)).Figure 2(a)) were within the apparatus’ FOV (horizontal FOV: around 80◦ [27]) while targets with long distance were not (target angle: 90.Figure 2(a)), indicating participants had to first locate a target using head motion before making a selection.Figure 2(b)) to confirm the selection.Figure 2(b)) of the controller only when crossing a goal.Figure 2(b)).Figure 3).Figure 3(b-e)), so that participants could move the ray cursor to “pass” through a disc from its one side to the other.Figure 3(a)).Figure 3(d)).Figure 3(e)).Figure 4(a)).Figure 5(a), C/DC follows a similar regression line to A/DP, suggesting a possibility of substituting the pointing task with the crossing task.Figure 6(a)).Figure 6(b)).</figure_citations>
            <section_index>2</section_index>
            <title>3D SPACE</title>
        </sections>
        <sections>
            <word_count>2040</word_count>
            <figure_citations>Figure 2).Figure 5(b), C/DC was uniformly faster than A/DP.Figure 7(a)), we adopted a standard 2D pointing design with circular targets [16] given pointing tasks in most 2D interfaces are typically twodimensional ([32]).Figure 7(b-e)), task design was the same as their counterpart in Experiment One, except that we used goal lines which were displayed on constrained planes.Figure 8(a)).Figure 8(b).Figure 9(a)).Figure 9(b)).</figure_citations>
            <section_index>3</section_index>
            <title>2D PLANE</title>
        </sections>
        <sections>
            <word_count>1192</word_count>
            <figure_citations>Figure 5), which further verifies their similar performances in target selection.Figure 5(a&b)) shows an upward curvature of time away for the regression line for low values of ID.</figure_citations>
            <section_index>4</section_index>
            <title>GENERAL DISCUSSION</title>
        </sections>
        <sections>
            <word_count>488</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>IMPLICATIONS FOR DESIGN</title>
        </sections>
        <sections>
            <word_count>155</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>150</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>63</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1742</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Crossing-Based Selection with Virtual Reality Head-Mounted Displays</title>
        <abstract>This paper presents the first investigation into using the goal-crossing paradigm for object selection with virtual reality (VR) head-mounted displays. Two experiments were carried out to evaluate ray-casting crossing tasks with target discs in 3D space and goal lines on 2D plane respectively in comparison to ray-casting pointing tasks. Five factors, i.e. task difficulty, the direction of movement constraint (collinear vs. orthogonal), the nature of the task (discrete vs. continuous), field of view of VR devices and target depth, were considered in both experiments. Our findings are: (1) crossing generally had shorter or no longer time, and higher or similar accuracy than pointing, indicating crossing can complement or substitute pointing; (2) crossing tasks can be well modelled with Fitts' Law; (3) crossing performance depended on target depth; (4) crossing target discs in 3D space differed from crossing goal lines on 2D plane in many aspects such as time and error performance, the effects of target depth and the parameters of Fitts' models. Based on these findings, we formulate a number of design recommendations for crossing-based interaction in VR.</abstract>
        <keywords>
            <li>Crossing</li>
            <li>Pointing</li>
            <li>Virtual reality head-mounted displays</li>
            <li>Ray-casting selection</li>
            <li>Fitts’ law</li>
        </keywords>
        <authors>
            <li>Huawei Tu</li>
            <li> Susu Huang</li>
            <li> Jiabin Yuan</li>
            <li> Xiangshi Ren</li>
            <li> Feng Tian</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>Crossing-Based Selection with Virtual Reality Head-Mounted Displays_crop_9.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials</filename><data>
        <paper_id>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300657</doi>
        <sections>
            <word_count>343</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRO</title>
        </sections>
        <sections>
            <word_count>672</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>IMMERSION</title>
        </sections>
        <sections>
            <word_count>782</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1852</word_count>
            <figure_citations>Figure 2, comprised: (1) a VR headset and a wrist-mounted wearable VIVE tracker, (2) a 64-channel EEG system, (3) one vibrotactile actuator worn on the fingertip, and (4) a medically-compliant EMS device connected via two electrodes worn on the forearm.Figure 3, was as follows: (1) participants moved their hands from the resting position to the ready position, to indicate they were ready to start the next trial; (2) participants waited for a new target to appear (the time of a new target spawning was randomized between 1-2 s); (3) then, the target (a cube) would appear in one of three possible positions (center, left, right), all equidistant from the participant’s ready position; (4) then, participants acquired the target by moving and touching the target with their index finger.Figure 4), we filtered the EEG data with a 0.</figure_citations>
            <section_index>3</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1232</word_count>
            <figure_citations>Figure 4(A).Figure 4(B), we observed a negative deflection around 170ms after a participant had selected the object (i.Figure 6(B), we observed no significant differences for the peak latencies over the three conditions.</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>323</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>19</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1919</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials</title>
        <abstract>Designing immersion is the key challenge in virtual reality; this challenge has driven advancements in displays, rendering and recently, haptics. To increase our sense of physical immersion, for instance, vibrotactile gloves render the sense of touching, while electrical muscle stimulation (EMS) renders forces. Unfortunately, the established metric to assess the effectiveness of haptic devices relies on the user's subjective interpretation of unspecific, yet standardized, questions.Here, we explore a new approach to detect a conflict in visuo-haptic integration (e.g., inadequate haptic feedback based on poorly configured collision detection) using electroencephalography (EEG). We propose analyzing event-related potentials (ERPs) during interaction with virtual objects. In our study, participants touched virtual objects in three conditions and received either no haptic feedback, vibration, or vibration and EMS feedback. To provoke a brain response in unrealistic VR interaction, we also presented the feedback prematurely in 25% of the trials.We found that the early negativity component of the ERP (so called prediction error) was more pronounced in the mismatch trials, indicating we successfully detected haptic conflicts using our technique. Our results are a first step towards using ERPs to automatically detect visuo-haptic mismatches in VR, such as those that can cause a loss of the user's immersion.</abstract>
        <keywords>
            <li>Force feedback</li>
            <li>EEG</li>
            <li>Elecrical muscle stimulation</li>
            <li>Virtual reality</li>
            <li>ERP</li>
            <li>Prediction error</li>
        </keywords>
        <authors>
            <li>Lukas Gehrke</li>
            <li> Sezen Akman</li>
            <li> Pedro Lopes</li>
            <li> Albert Chen</li>
            <li> Avinash Kumar Singh</li>
            <li> Hsiang-Ting Chen</li>
            <li> Chin-Teng Lin</li>
            <li> Klaus Gramann</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials_crop_6.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay</filename><data>
        <paper_id>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3026028</doi>
        <sections>
            <word_count>773</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1072</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1122</word_count>
            <figure_citations>Figure 1), ensured that that the sensors remained in the correct position on the hand.Figure 1).Figure 2(c)).</figure_citations>
            <section_index>2</section_index>
            <title>SYSTEM</title>
        </sections>
        <sections>
            <word_count>1475</word_count>
            <figure_citations>Figure 2 shows our independent variables.Figure 3).</figure_citations>
            <section_index>3</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1315</word_count>
            <figure_citations>Figure 4).Figure 5).Figure 6).Figure 7, demonstrated an interesting pattern difference between the two games.</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>864</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>257</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>LIMITATIONS</title>
        </sections>
        <sections>
            <word_count>858</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>68</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1064</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Effects of Sharing Physiological States of Players in a Collaborative Virtual Reality Gameplay</title>
        <abstract>Interfaces for collaborative tasks, such as multiplayer games can enable more effective and enjoyable collaboration. However, in these systems, the emotional states of the users are often not communicated properly due to their remoteness from one another. In this paper, we investigate the effects of showing emotional states of one collaborator to the other during an immersive Virtual Reality (VR) gameplay experience. We created two collaborative immersive VR games that display the real-time heart-rate of one player to the other. The two different games elicited different emotions, one joyous and the other scary. We tested the effects of visualizing heart-rate feedback in comparison with conditions where such a feedback was absent. The games had significant main effects on the overall emotional experience.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Empathic Computing</li>
            <li>Collaborative Gameplay</li>
            <li>Physiological Sensors</li>
            <li>Emotions</li>
            <li>User Study</li>
        </keywords>
        <authors>
            <li>Arindam Dey</li>
            <li> Thammathip Piumsomboon</li>
            <li> Youngho Lee</li>
            <li> Mark Billinghurst</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Effects of Sharing Physiological States of Players in Collaborative Virtual Reality Gameplay_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects </filename><data>
        <paper_id>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects </paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025488</doi>
        <sections>
            <word_count>583</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>890</word_count>
            <figure_citations>Figure 3a).Figure 3b).</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>673</word_count>
            <figure_citations>Figure 1).Figure 2).Figure 2), with one of its corners facing the participant.</figure_citations>
            <section_index>2</section_index>
            <title>EXPERIMENTAL STUDY</title>
        </sections>
        <sections>
            <word_count>217</word_count>
            <figure_citations>Figure 4a).Figure 4b).Figure 4b).Figure 4b).</figure_citations>
            <section_index>3</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>301</word_count>
            <figure_citations>Figure 4c).</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>165</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>319</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>REFERENCES</title>
        </sections>
        <sections>
            <word_count>1046</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <title>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects</title>
        <abstract>In this paper, we report on a study investigating a novel haptic illusion for altering the perception of 3D shapes using a non-planar screen and vibrotactile friction. In our study, we presented an image of a rectangular prism on a cylindrical and a flat display. Participants were asked to move their index finger horizontally along the surface of the displays towards the edge of the rectangular prism. Participants were asked whether they were experiencing a flat, cylindrical or rectangular shape. In one condition, a vibrotactile stimulus simulated increasing friction towards the visible edge of the rectangular prism, with a sudden drop-off when this edge was crossed by the finger. Results suggest that presenting an image of a rectangular prism, and applying vibrotactile friction, particularly on a cylindrical display, significantly increased participant ratings stating that they were experiencing a physical rectangular shape.</abstract>
        <keywords>
            <li>Organic User Interfaces</li>
            <li>DisplayObjects</li>
            <li>Shaped Displays</li>
            <li>Vibrotactile Feedback</li>
        </keywords>
        <authors>
            <li>Juan Pablo Carrascal</li>
            <li> Roel Vertegaal</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects _crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects _crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects _crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Effects of Tactile Feedback on the Perception of Virtual Shapes on Non-Planar DisplayObjects _crop_4.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures</filename><data>
        <paper_id>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300479</doi>
        <sections>
            <word_count>419</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1201</word_count>
            <figure_citations>Figure 1 illustrates superimposing virtual texture images on top of physical textures.</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>883</word_count>
            <figure_citations>Figure 2), the customer is able to Page 3 CHI 2019 Paper mm 2.Figure 3) was printed using an Autodesk Ember1 with an X-Y resolution of 50 µm and a layer thickness of 25 µm.Figure 3, were purchased from various sources and imported into the Unity environment.</figure_citations>
            <section_index>2</section_index>
            <title>PERCEPTION</title>
        </sections>
        <sections>
            <word_count>1319</word_count>
            <figure_citations>Figure 4, allowed participants to precisely hit a required hair structure without touching a diferent surface.Figure 4, while answering 6 questions, i.</figure_citations>
            <section_index>3</section_index>
            <title>STUDY</title>
        </sections>
        <sections>
            <word_count>2390</word_count>
            <figure_citations>Figure 5a and 5c) and the visual ratings of virtual textures without haptic information (see Figure 5b and 5d).Figure 5a and 5c indicate signifcant diferences.</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>678</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>156</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>37</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1225</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures</title>
        <abstract>Experiencing materials in virtual reality (VR) is enhanced by combining visual and haptic feedback. While VR easily allows changes to visual appearances, modifying haptic impressions remains challenging. Existing passive haptic techniques require access to a large set of tangible proxies. To reduce the number of physical representations, we look towards fabrication to create more versatile counterparts. In a user study, 3D-printed hairs with length varying in steps of 2.5 mm were used to influence the feeling of roughness and hardness. By overlaying fabricated hair with visual textures, the resolution of the user's haptic perception increased. As changing haptic sensations are able to elicit perceptual switches, our approach can extend a limited set of textures to a much broader set of material impressions. Our results give insights into the effectiveness of 3D-printed hair for enhancing texture perception in VR.</abstract>
        <keywords>
            <li>Texture perception</li>
            <li>Passive haptic feedback</li>
            <li>3D printing</li>
        </keywords>
        <authors>
            <li>Donald Degraen</li>
            <li> André Zenner</li>
            <li> Antonio Krüger</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures_crop_6.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation</filename><data>
        <paper_id>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300477</doi>
        <sections>
            <word_count>1915</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>603</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>1</section_index>
            <title>STIMULATION</title>
        </sections>
        <sections>
            <word_count>1570</word_count>
            <figure_citations>Figure 1)1 .Figure 2) consisted of just resting the hands on the desk, paying attention to the collision of the balls with the hands.Figure 3).Figure 3), participants were instructed to place their hands on the physical desk (which coincided with the VR desk) on their little fingers to comfortably hit the balls towards the middle of the desk, into the hole.</figure_citations>
            <section_index>2</section_index>
            <title>METHODOLOGY</title>
        </sections>
        <sections>
            <word_count>750</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>872</word_count>
            <figure_citations>Figure 4), but this effect was not statistically significant.Figure 4), as the passive phase hand-ball collisions elicited distinct P300, contrary to the active phase.</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1096</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>499</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>1959</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation</title>
        <abstract>Virtual reality (VR) can be immersive to such a degree that users sometimes report feeling tactile sensations based on visualization of the touch, without any actual physical contact. This effect is not only interesting for studies of human perception, but can also be leveraged to improve the quality of VR by evoking tactile sensations without usage of specialized equipment. The aim of this paper is to study brain processing of the illusory touch and its enhancement for purposes of exploitation in VR scene design. To amplify the illusory touch, transcranial direct current stimulation (tDCS) was used. Participants attended two sessions with blinded stimulation and interacted with a virtual ball using tracked hands in VR. The effects were studied using electroencephalography (EEG), that allowed us to examine stimulation-induced changes in processing of the illusory touch in the brain, as well as to identify its neural correlates. Results confirm enhanced processing of the illusory touch after the stimulation, and some of these changes were correlated to subjective rating of its magnitude.</abstract>
        <keywords>
            <li>Electroencephalography</li>
            <li>Embodiment</li>
            <li>Illusory Touch</li>
            <li>Transcranial Direct Current Stimulation</li>
            <li>Virtual Reality</li>
        </keywords>
        <authors>
            <li>Filip Škola</li>
            <li> Fotis Liarokapis</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation_crop_4.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality</filename><data>
        <paper_id>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300426</doi>
        <sections>
            <word_count>661</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>4670</word_count>
            <figure_citations>Figure 2).Figure 3).</figure_citations>
            <section_index>1</section_index>
            <title>BACKGROUND AND RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1809</word_count>
            <figure_citations>Figure 5).Figure 8).Figure 9).</figure_citations>
            <section_index>2</section_index>
            <title>BASELINE</title>
        </sections>
        <sections>
            <word_count>60</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>PINCH</title>
        </sections>
        <sections>
            <word_count>17</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>CONTROLLER</title>
        </sections>
        <sections>
            <word_count>724</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>146</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>31</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>3322</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality</title>
        <abstract>We present an empirical comparison of eleven bare hand, mid-air mode-switching techniques suitable for virtual reality in two experiments. The first evaluates seven techniques spanning dominant and non-dominant hand actions. Techniques represent common classes of actions selected by a methodical examination of 56 examples of prior art. The standard "subtraction method" protocol is adapted for 3D interfaces, with two baseline selection methods, bare hand pinch and device controller button. A second experiment with four techniques explores more subtle dominant-hand techniques and the effect of using a dominant hand device for selection. Results provide guidance to practitioners when choosing bare hand, mid-air mode-switching techniques, and for researchers when designing new mode-switching methods in VR.</abstract>
        <keywords>
            <li>Interaction techniques</li>
            <li>Controlled experiments</li>
        </keywords>
        <authors>
            <li>Hemant Bhaskar Surale</li>
            <li> Fabrice Matulic</li>
            <li> Daniel Vogel</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality_crop_9.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements</filename><data>
        <paper_id>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300644</doi>
        <sections>
            <word_count>496</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>784</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>2299</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>RESEARCH QUESTIONS</title>
        </sections>
        <sections>
            <word_count>4673</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>INTUI</title>
        </sections>
        <sections>
            <word_count>527</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>OVERALL IMPLICATIONS</title>
        </sections>
        <sections>
            <word_count>226</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>1670</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Exploring Interaction Fidelity in Virtual Reality: Object Manipulation and Whole-Body Movements</title>
        <abstract>High degrees of interaction fidelity (IF) in virtual reality (VR) are said to improve user experience and immersion, but there is also evidence of low IF providing comparable experiences. VR games are now increasingly prevalent, yet we still do not fully understand the trade-off between realism and abstraction in this context. We conducted a lab study comparing high and low IF for object manipulation tasks in a VR game. In a second study, we investigated players' experiences of IF for whole-body movements in a VR game that allowed players to crawl underneath virtual boulders and "dangle'' along monkey bars. Our findings show that high IF is preferred for object manipulation, but for whole-body movements, moderate IF can suffice, as there is a trade-off with usability and social factors. We provide guidelines for the development of VR games based on our results.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Interaction fidelity</li>
            <li>Games</li>
            <li>Whole body interaction</li>
            <li>Virtual objects</li>
            <li>Player experience</li>
        </keywords>
        <authors>
            <li>Katja Rogers</li>
            <li> Jana Funke</li>
            <li> Julian Frommel</li>
            <li> Sven Stamm</li>
            <li> Michael Weber</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Exploring Interaction Fidelity in Virtual Reality Object Manipulation and Whole-Body Movements_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Exploring Virtual Agents for Augmented Reality</filename><data>
        <paper_id>Exploring Virtual Agents for Augmented Reality</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300511</doi>
        <sections>
            <word_count>650</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>856</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1180</word_count>
            <figure_citations>Figure 2).Figure 2), and that he has a slightly higher-pitched voice than Jake to account for his smaller size.</figure_citations>
            <section_index>2</section_index>
            <title>AUGMENTED REALITY AGENTS</title>
        </sections>
        <sections>
            <word_count>672</word_count>
            <figure_citations>Figure 4).</figure_citations>
            <section_index>3</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1679</word_count>
            <figure_citations>Figure 5).Figure 5b) as well as the length of each utterance (Figure 5c).Figure 5d), as well as the duration of each gaze (Figure 5e).Figure 7).</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1511</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>432</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>LIMITATIONS AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>139</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>67</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1649</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Exploring Virtual Agents for Augmented Reality</title>
        <abstract>Prior work has shown that embodiment can benefit virtual agents, such as increasing rapport and conveying non-verbal information. However, it is unclear if users prefer an embodied to a speech-only agent for augmented reality (AR) headsets that are designed to assist users in completing real-world tasks. We conducted a study to examine users' perceptions and behaviors when interacting with virtual agents in AR. We asked 24 adults to wear the Microsoft HoloLens and find objects in a hidden object game while interacting with an agent that would offer assistance. We presented participants with four different agents: voice-only, non-human, full-size embodied, and a miniature embodied agent. Overall, users preferred the miniature embodied agent due to the novelty of his size and reduced uncanniness as opposed to the larger agent. From our results, we draw conclusions about how agent representation matters and derive guidelines on designing agents for AR headsets.</abstract>
        <keywords>
            <li>Augmented reality</li>
            <li>Embodied conversational agents</li>
        </keywords>
        <authors>
            <li>Isaac Wang</li>
            <li> Jesse Smith</li>
            <li> Jaime Ruiz</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Exploring Virtual Agents for Augmented Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Exploring Virtual Agents for Augmented Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Exploring Virtual Agents for Augmented Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Exploring Virtual Agents for Augmented Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Exploring Virtual Agents for Augmented Reality_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Exploring Virtual Agents for Augmented Reality_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Exploring Virtual Agents for Augmented Reality_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Extending the Body for Interaction with Reality</filename><data>
        <paper_id>Extending the Body for Interaction with Reality</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025689</doi>
        <sections>
            <word_count>1019</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>565</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>944</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>THE PSYCHOLOGICAL CONCEPT OF OWNERSHIP</title>
        </sections>
        <sections>
            <word_count>1400</word_count>
            <figure_citations>Figure 2).Figure 3, A).Figure 3, B), which is then filled with background-colored pixels (Figure 3, C).Figure 5, where the hand is cut off at the edge of the table, and thus appears to be reaching underneath it, or seems to hover above the table projecting a shadow onto it.</figure_citations>
            <section_index>3</section_index>
            <title>SUPPORTING OWNERSHIP IN INTERACTION</title>
        </sections>
        <sections>
            <word_count>759</word_count>
            <figure_citations>Figure 6, top left).Figure 6, top right).Figure 6, bottom left).Figure 6, bottom right).</figure_citations>
            <section_index>4</section_index>
            <title>DESIGN PROCESS</title>
        </sections>
        <sections>
            <word_count>747</word_count>
            <figure_citations>Figure 6) in counterbalanced order.Figure 7).</figure_citations>
            <section_index>5</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>1346</word_count>
            <figure_citations>Figure 8, left and center).Figure 8, right).Figure 9, right).Figure 9).Figure 10, left).Figure 10, center).</figure_citations>
            <section_index>6</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>962</word_count>
            <figure_citations>Figure 11).</figure_citations>
            <section_index>7</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>86</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>1769</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Extending the Body for Interaction with Reality</title>
        <abstract>In this paper, we explore how users can control remote devices with a virtual long arm, while preserving the perception that the artificial arm is actually part of their own body. Instead of using pointing, speech, or a remote control, the users' arm is extended in augmented reality, allowing access to devices that are out of reach. Thus, we allow users to directly manipulate real-world objects from a distance using their bare hands. A core difficulty we focus on is how to maintain ownership for the unnaturally long virtual arm, which is the strong feeling that one's limbs are actually part of the own body. Fortunately, what the human brain experiences as being part of the own body is very malleable and we find that during interaction the user's virtual arm can be stretched to more than twice its real length, without breaking the user's sense of ownership for the virtual limb.</abstract>
        <keywords>
            <li>Ownership</li>
            <li>Augmented Reality</li>
            <li>Ubiquitous Computing</li>
            <li>Virtual Hand Illusion</li>
        </keywords>
        <authors>
            <li>Tiare Feuchtner</li>
            <li> Jörg Müller</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Extending the Body for Interaction with Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Extending the Body for Interaction with Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Extending the Body for Interaction with Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Extending the Body for Interaction with Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Extending the Body for Interaction with Reality_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Extending the Body for Interaction with Reality_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Extending the Body for Interaction with Reality_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Extending the Body for Interaction with Reality_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>Extending the Body for Interaction with Reality_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>Extending the Body for Interaction with Reality_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>Extending the Body for Interaction with Reality_crop_11.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display</filename><data>
        <paper_id>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300763</doi>
        <sections>
            <word_count>661</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1339</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>314</word_count>
            <figure_citations>Figure 1) and set the output to a VR headset.</figure_citations>
            <section_index>2</section_index>
            <title>SIMULATED SPHERICAL FTVR SYSTEM</title>
        </sections>
        <sections>
            <word_count>3044</word_count>
            <figure_citations>Figure 2).Figure 5), therefore we accept H1-3 (NonStereo is aligned to mid-point of the eyes).Figure 6).</figure_citations>
            <section_index>3</section_index>
            <title>EXPERIMENTS</title>
        </sections>
        <sections>
            <word_count>1454</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>411</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>LIMITATIONS AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>109</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>21</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1717</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>FTVR in VR: Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display</title>
        <abstract>Spherical fish tank virtual reality (FTVR) displays attempt to create a virtual "crystal ball" experience using head-tracked rendering. Almost all of these systems have omitted stereo cues, making them easy to build, but it is not clear how much this omission degrades the 3D experience. In this study, we evaluate performance and subjective effects of stereo on 3D perception and interaction tasks with a spherical FTVR display. To control for calibration error and tracking latency, we perform the evaluation on a simulated spherical display in VR. The results of our study provide a clear recommendation for the design and use of spherical FTVR displays: while omitting stereo may not be readily apparent for users, their performance will be significantly degraded (20% - 91% increase in median task time). Therefore, including stereo viewing in spherical displays is critical for use in FTVR.</abstract>
        <keywords>
            <li>Fish tank virtual reality</li>
            <li>Spherical display</li>
            <li>3D perception</li>
        </keywords>
        <authors>
            <li>Dylan Fafard</li>
            <li> Ian Stavness</li>
            <li> Martin Dechant</li>
            <li> Regan Mandryk</li>
            <li> Qian Zhou</li>
            <li> Sidney Fels</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>FTVR in VR Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display_crop_8.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Handsfree Omnidirectional VR Navigation using Head Tilt</filename><data>
        <paper_id>Handsfree Omnidirectional VR Navigation using Head Tilt</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025521</doi>
        <sections>
            <word_count>642</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1075</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>BACKGROUND</title>
        </sections>
        <sections>
            <word_count>883</word_count>
            <figure_citations>Figure 3).Figure 4 shows the corridor and a visualization of colliding into an obstacle.</figure_citations>
            <section_index>2</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>553</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>TILT</title>
        </sections>
        <sections>
            <word_count>611</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION AND LIMITATIONS</title>
        </sections>
        <sections>
            <word_count>1137</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Handsfree Omnidirectional VR Navigation using Head Tilt</title>
        <abstract>Navigating mobile virtual reality (VR) is a challenge due to limited input options and/or a requirement for handsfree interaction. Walking-in-place (WIP) is considered to offer a higher presence than controller input but only allows unidirectional navigation in the direction of the user's gaze--which impedes navigation efficiency. Leaning input enables omnidirectional navigation but currently relies on bulky controllers, which aren't feasible in mobile VR contexts. This note evaluates the use of head-tilt - implemented using inertial sensing - to allow for handsfree omnidirectional VR navigation on mobile VR platforms. A user study with 24 subjects compared three input methods using an obstacle avoidance navigation task: (1) head-tilt alone (TILT) (2) a hybrid method (WIP-TILT) that uses head tilting for direction and WIP to control speed; and (3) traditional controller input. TILT was significantly faster than WIP-TILT and joystick input, while WIP-TILT and TILT offered the highest presence. There was no difference in cybersickness between input methods.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Locomotion</li>
            <li>Mobile VR</li>
            <li>Walking-in-place</li>
            <li>Head-tilt</li>
            <li>Simulator-sickness</li>
            <li>Games</li>
            <li>Inertial sensing</li>
        </keywords>
        <authors>
            <li>Sam Tregillus</li>
            <li> Majed Al Zayer</li>
            <li> Eelke Folmer</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Handsfree Omnidirectional VR Navigation using Head Tilt_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Handsfree Omnidirectional VR Navigation using Head Tilt_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Handsfree Omnidirectional VR Navigation using Head Tilt_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Handsfree Omnidirectional VR Navigation using Head Tilt_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Handsfree Omnidirectional VR Navigation using Head Tilt_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality </filename><data>
        <paper_id>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality </paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025684</doi>
        <sections>
            <word_count>943</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>192</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>340</word_count>
            <figure_citations>Figure 2) consists of a bathing cap with 17 vibration motors (Parallax, 12 mm coin type, 3.Figure 2, left).Figure 2).</figure_citations>
            <section_index>2</section_index>
            <title>INITIAL PROTOTYPE</title>
        </sections>
        <sections>
            <word_count>3016</word_count>
            <figure_citations>Figure 2, right).Figure 3).Figure 4 shows targets in front and back of the user for the auditory condition.Figure 5 show the measured dependent variables with merged data from all participants and all trials, not just successful ones.Figure 6 shows, compared to auditory feedback, targets off the horizontal plane worked much better with vibrotactile feedback.Figure 7 shows the selection time by angular distance between the starting orientation of the user and the orientation of each target.Figure 8 shows the selection time by yaw (horizontal heading) distance between the starting orientation and each target.Figure 9 shows the development of completion time over all trials.Figure 10).Figure 11 shows the success rates over time.Figure 12 and Figure 13).</figure_citations>
            <section_index>3</section_index>
            <title>EXPERIMENTS</title>
        </sections>
        <sections>
            <word_count>2929</word_count>
            <figure_citations>Figure 3) were used, but here with tiny white 1-pixel targets instead of the large ones (4 repetitions × 20 targets per user).Figure 15, participants agreed that the HapticHead vibrotactile feedback was helpful for finding virtual targets and most of the participants could intuitively map the feedback to the targets.Figure 15, participants found the vibrotactile feedback helpful for finding real targets around them and could intuitively map vibrotactile signals to targets, thus research question 2 can be answered positively.Figure 16).</figure_citations>
            <section_index>4</section_index>
            <title>REFINEMENT OF CONCEPT AND PROTOTYPE</title>
        </sections>
        <sections>
            <word_count>184</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>63</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>LIMITATIONS</title>
        </sections>
        <sections>
            <word_count>874</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>HapticHead: A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality</title>
        <abstract>Current virtual and augmented reality head-mounted displays usually include no or only a single vibration motor for haptic feedback and do not use it for guidance. We present HapticHead, a system utilizing multiple vibrotactile actuators distributed in three concentric ellipses around the head for intuitive haptic guidance through moving tactile cues. We conducted three experiments, which indicate that HapticHead vibrotactile feedback is both faster (2.6 s vs. 6.9 s) and more precise (96.4% vs. 54.2% success rate) than spatial audio (generic head-related transfer function) for finding visible virtual objects in 3D space around the user. The baseline of visual feedback is as expected more precise (99.7% success rate) and faster (1.3 s) in comparison, but there are many applications in which visual feedback is not desirable or available due to lighting conditions, visual overload, or visual impairments. Mean final precision with HapticHead feedback on invisible targets is 2.3° compared to 0.8° with visual feedback. We successfully navigated blindfolded users to real household items at different heights using HapticHead vibrotactile feedback independently of a head-mounted display.</abstract>
        <keywords>
            <li>Guidance</li>
            <li>Navigation</li>
            <li>Haptic feedback</li>
            <li>Vibrotactile</li>
            <li>Virtual reality</li>
            <li>Augmented reality</li>
            <li>Spatial interaction</li>
            <li>3D output</li>
        </keywords>
        <authors>
            <li>Oliver Beren Kaul</li>
            <li> Michael Rohs</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_3.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_5.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_13.png</url>
        </figure>
        <figure>
            <id>14</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_14.png</url>
        </figure>
        <figure>
            <id>15</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_15.png</url>
        </figure>
        <figure>
            <id>16</id>
            <url>HapticHead- A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality _crop_16.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR</filename><data>
        <paper_id>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3026046</doi>
        <sections>
            <word_count>794</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>4510</word_count>
            <figure_citations>Figure 1) paired with a Samsung S7 smartphone (VR framework version 11, service 1 To see how conditions operated in-motion, view the attached video.</figure_citations>
            <section_index>1</section_index>
            <title>RELATED RESEARCH</title>
        </sections>
        <sections>
            <word_count>2422</word_count>
            <figure_citations>Figure 3, with sickness being in-part minimized by preference.Figure 4), we see how preferences were influenced by perceived sickness.</figure_citations>
            <section_index>2</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>551</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>624</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>RECOMMENDATIONS FOR FURTHER RESEARCH</title>
        </sections>
        <sections>
            <word_count>313</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>2298</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>I Am The Passenger: How Visual Motion Cues Can Influence Sickness For In-Car VR</title>
        <abstract>This paper explores the use of VR Head Mounted Displays (HMDs) in-car and in-motion for the first time. Immersive HMDs are becoming everyday consumer items and, as they offer new possibilities for entertainment and productivity, people will want to use them during travel in, for example, autonomous cars. However, their use is confounded by motion sickness caused in-part by the restricted visual perception of motion conflicting with physically perceived vehicle motion (accelerations/rotations detected by the vestibular system). Whilst VR HMDs restrict visual perception of motion, they could also render it virtually, potentially alleviating sensory conflict. To study this problem, we conducted the first on-road and in motion study to systematically investigate the effects of various visual presentations of the real-world motion of a car on the sickness and immersion of VR HMD wearing passengers. We established new baselines for VR in-car motion sickness, and found that there is no one best presentation with respect to balancing sickness and immersion. Instead, user preferences suggest different solutions are required for differently susceptible users to provide usable VR in-car. This work provides formative insights for VR designers and an entry point for further research into enabling use of VR HMDs, and the rich experiences they offer, when travelling.</abstract>
        <keywords>
            <li>In-motion</li>
            <li>In-car</li>
            <li>Automobile</li>
            <li>Autonomous Car</li>
            <li>Passenger</li>
            <li>Virtual Reality</li>
            <li>Mixed Reality</li>
            <li>Motion Sickness</li>
            <li>HMD</li>
        </keywords>
        <authors>
            <li>Mark McGill</li>
            <li> Alexander Ng</li>
            <li> Stephen Brewster</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>I Am The Passenger- How Visual Motion Cues Can Influence Sickness For In-Car VR_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony</filename><data>
        <paper_id>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300787</doi>
        <sections>
            <word_count>492</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1701</word_count>
            <figure_citations>Figure 1 in [2], it appears as though the trackers are worn only on the wrists of participants.Figure 1).</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1574</word_count>
            <figure_citations>Figure 2).</figure_citations>
            <section_index>2</section_index>
            <title>SYNCHRONY AND AVATAR TRACKING</title>
        </sections>
        <sections>
            <word_count>788</word_count>
            <figure_citations>Figure 4) to get acquainted with their VR bodies.Figure 4).</figure_citations>
            <section_index>3</section_index>
            <title>METHODOLOGY</title>
        </sections>
        <sections>
            <word_count>953</word_count>
            <figure_citations>Figure 7 compares the ratings (from Table 2) of MyBody (Q1) compared to TwoBodies (Q2).Figure 8).</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1393</word_count>
            <figure_citations>Figure 5) and that participants embodied in female avatars actually had a mean increase in postIAT scores compared to a decrease in those embodied in male avatars (Figure 6).Figure 6) demonstrates the strength of the findings.Figure 7 shows that the levels of body ownership are similar in both conditions.Figure 7 shows that even though participants embodied in female avatars did not feel that the avatar resembled them in appearance, they still felt strongly that the movements of the virtual avatar were caused by their own movements.</figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>193</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>168</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>59</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1544</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony</title>
        <abstract>Previous research has shown that when White people embody a black avatar in virtual reality (VR) with full body visuomotor synchrony, this can reduce their implicit racial bias. In this paper, we put men in female and male avatars in VR with full visuomotor synchrony using wearable trackers and investigated implicit gender bias and embodiment. We found that participants embodied in female avatars displayed significantly higher levels of implicit gender bias than those embodied in male avatars. The implicit gender bias actually increased after exposure to female embodiment in contrast to male embodiment. Results also showed that participants felt embodied in their avatars regardless of gender matching, demonstrating that wearable trackers can be used for a realistic sense of avatar embodiment in VR. We discuss the future implications of these findings for both VR scenarios and embodiment technologies.</abstract>
        <keywords>
            <li>Virtual Reality (VR)</li>
            <li>Embodied Avatars</li>
            <li>Implict Gender Bias</li>
            <li>Implicit Association Test (IAT)</li>
        </keywords>
        <authors>
            <li>Sarah Lopez</li>
            <li> Yi Yang</li>
            <li> Kevin Beltran</li>
            <li> Soo Jung Kim</li>
            <li> Jennifer Cruz Hernandez</li>
            <li> Chelsy Simran</li>
            <li> Bingkun Yang</li>
            <li> Beste F. Yuksel</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony_crop_8.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit </filename><data>
        <paper_id>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit </paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025479</doi>
        <sections>
            <word_count>832</word_count>
            <figure_citations>Figure 1).Figure 2a), the simulation is displayed alongside the circuit on the same display.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>557</word_count>
            <figure_citations>Figure 4).</figure_citations>
            <section_index>1</section_index>
            <title>BACKGROUND</title>
        </sections>
        <sections>
            <word_count>245</word_count>
            <figure_citations>Figure 3): a DC circuit simulator, an agent-based model of current flow, a virtual multimeter, and a brief textual description of basic concepts on electricity.Figure 4).</figure_citations>
            <section_index>2</section_index>
            <title>DESIGN OF SPARK</title>
        </sections>
        <sections>
            <word_count>1710</word_count>
            <figure_citations>Figure 3).Figure 5).Figure 5).Figure 6).Figure 7 illustrates two snapshots of the synchronized video recordings.</figure_citations>
            <section_index>3</section_index>
            <title>METHODS</title>
        </sections>
        <sections>
            <word_count>2903</word_count>
            <figure_citations>Figure 5), families are shown two circuits; the first circuit has one lightbulb, and the second circuit has one resistor and one bulb after the resistor (assuming a counter-clockwise direction of current flow).Figure 8 shows the differences in usage of incorrect and correct conceptions for current path in each condition.Figure 9).Figure 10 shows the number of incorrect and correct predictions in each condition.Figure 11 illustrates the distribution of parental roles in each group.</figure_citations>
            <section_index>4</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>451</word_count>
            <figure_citations>Figure 12), similar to the digital circuit simulator.</figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>145</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>66</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1413</word_count>
            <figure_citations>Figure 12).</figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Looking Inside the Wires: Understanding Museum Visitor Learning with an Augmented Circuit Exhibit</title>
        <abstract>Understanding electrical circuits can be difficult for novices of all ages. In this paper, we describe a science museum exhibit that enables visitors to make circuits on an interactive tabletop and observe a simulation of electrons flowing through the circuit. Our goal is to use multiple representations to help convey basic concepts of current and resistance. To study visitor interaction and learning, we tested the design at a popular science museum with 60 parent-child dyads in three conditions: a control condition with no electron simulation; a condition with the simulation displayed alongside the circuit on the same screen; and an augmented reality condition, with the simulation displayed on a tablet that acts as a lens to see into the circuit. Our findings show that children did significantly better on a post-test in both experimental conditions, with children performing best in the AR condition. However, analysis of session videos shows unexpected parent-child collaboration in the AR condition.</abstract>
        <keywords>
            <li>Electrical circuits</li>
            <li>Multiple representations</li>
            <li>Augmented reality</li>
            <li>Agent-based modeling</li>
            <li>Design</li>
            <li>Interactive surfaces</li>
            <li>Museum learning</li>
        </keywords>
        <authors>
            <li>Elham Beheshti</li>
            <li> David Kim</li>
            <li> Gabrielle Ecanow</li>
            <li> Michael S. Horn</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>Looking Inside the Wires- Understanding Museum Visitor Learning with an Augmented Circuit Exhibit _crop_12.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>MagicFace- Stepping into Character through an Augmented Reality Mirror</filename><data>
        <paper_id>MagicFace- Stepping into Character through an Augmented Reality Mirror</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025722</doi>
        <sections>
            <word_count>817</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>917</word_count>
            <figure_citations>Figure 3).</figure_citations>
            <section_index>1</section_index>
            <title>BACKGROUND</title>
        </sections>
        <sections>
            <word_count>108</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>RESEARCH AIMS</title>
        </sections>
        <sections>
            <word_count>4934</word_count>
            <figure_citations>Figure 1).Figure 2).</figure_citations>
            <section_index>3</section_index>
            <title>MAGICFACE APP AND INSTALLATION</title>
        </sections>
        <sections>
            <word_count>1229</word_count>
            <figure_citations>Figure 5), turning their heads in different directions and pouting to watch themselves with the virtual looks from various angles.</figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>346</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>44</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>701</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>MagicFace: Stepping into Character through an Augmented Reality Mirror</title>
        <abstract>Augmented Reality (AR) is coming of age and appearing in various smartphone apps. One emerging AR type uses the front-facing camera and overlays a user's face with digital features that transform the physical appearance, making the user look like someone else, such as a popstar or a historical character. However, little is known about how people react to such stepping into character and how convincing they perceive it to be. We developed an app with two Egyptian looks, MagicFace, which was situated both in an opera house and a museum. In the first setting, people were invited to use the app, while in the second setting they came across it on their own when visiting the exhibition. Our findings show marked differences in how people approach and experience the MagicFace in these different contexts. We discuss how realistic and compelling this kind of AR technology is, as well as its implications for educational and cultural settings.</abstract>
        <keywords>
            <li>Augmented reality</li>
            <li>Opera characters</li>
            <li>Interface design</li>
            <li>Inthe-wild study</li>
        </keywords>
        <authors>
            <li>Ana Javornik</li>
            <li> Yvonne Rogers</li>
            <li> Delia Gander</li>
            <li> Ana Moutinho</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>MagicFace- Stepping into Character through an Augmented Reality Mirror_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>MagicFace- Stepping into Character through an Augmented Reality Mirror_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>MagicFace- Stepping into Character through an Augmented Reality Mirror_crop_3.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>MagicFace- Stepping into Character through an Augmented Reality Mirror_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality</filename><data>
        <paper_id>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300897</doi>
        <sections>
            <word_count>863</word_count>
            <figure_citations>Figure 1), use a 2D real-time user video as a virtual view of that user, or in the near future, use a highly detailed photo-realistic point cloud video [44].Figure 2) 1 https://www.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>2994</word_count>
            <figure_citations>Figure 2) was to identify typical experiences of photo sharing activities, and to develop a questionnaire for measuring socialVR photo sharing experiences later used in Part 2.Figure 3 illustrates the five phases with an example of a timeline, where actions were coded with blue dots and interpreted with a phrase.Figure 3) identified in the context mapping sessions were introduced.</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1455</word_count>
            <figure_citations>Figure 2), we present a within-subject controlled user study compare photo sharing experiences in three conditions: face-to-face (F2F), Facebook Spaces (FBS), Skype (SKP).Figure 5).</figure_citations>
            <section_index>2</section_index>
            <title>STUDY</title>
        </sections>
        <sections>
            <word_count>2201</word_count>
            <figure_citations>Figure 6a.Figure 6b.Figure 6c.Figure 6d), which shows FBS was perceived to be more exciting and cheerful than F2F and SKP.Figure 6d.</figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1188</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>161</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>FUTURE WORK AND CONCLUSION</title>
        </sections>
        <sections>
            <word_count>36</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>2086</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality</title>
        <abstract>Millions of photos are shared online daily, but the richness of interaction compared with face-to-face (F2F) sharing is still missing. While this may change with social Virtual Reality (socialVR), we still lack tools to measure such immersive and interactive experiences. In this paper, we investigate photo sharing experiences in immersive environments, focusing on socialVR. Running context mapping (N=10), an expert creative session (N=6), and an online experience clustering questionnaire (N=20), we develop and statistically evaluate a questionnaire to measure photo sharing experiences. We then ran a controlled, within-subject study (N=26 pairs) to compare photo sharing under F2F, Skype, and Facebook Spaces. Using interviews, audio analysis, and our questionnaire, we found that socialVR can closely approximate F2F sharing. We contribute empirical findings on the immersiveness differences between digital communication media, and propose a socialVR questionnaire that can in the future generalize beyond photo sharing.</abstract>
        <keywords>
            <li>Social</li>
            <li>Virtual reality</li>
            <li>Photo sharing</li>
            <li>Questionnaire</li>
            <li>Immersion</li>
            <li>Presence</li>
        </keywords>
        <authors>
            <li>Jie Li</li>
            <li> Yiping Kong</li>
            <li> Thomas Röggla</li>
            <li> Francesca De Simone</li>
            <li> Swamy Ananthanarayan</li>
            <li> Huib de Ridder</li>
            <li> Abdallah El Ali</li>
            <li> Pablo Cesar</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality_crop_6.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Personalising the TV Experience using Augmented</filename><data>
        <paper_id>Personalising the TV Experience using Augmented</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300762</doi>
        <sections>
            <word_count>305</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>877</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1230</word_count>
            <figure_citations>Figure 1).Figure 1).Figure 1): 1.</figure_citations>
            <section_index>2</section_index>
            <title>RESEARCH OBJECTIVES</title>
        </sections>
        <sections>
            <word_count>1597</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>METHOD</title>
        </sections>
        <sections>
            <word_count>947</word_count>
            <figure_citations>Figure 2 show descriptive statistics on participant responses.Figure 3a.Figure 3b illustrates participant responses to questions on the acceptance of the TV+HoloLens system.Figure 3c) illustrates the distribution of responses of our participants.</figure_citations>
            <section_index>4</section_index>
            <title>QUANTITATIVE RESULTS</title>
        </sections>
        <sections>
            <word_count>1748</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>QUALITATIVE RESULTS</title>
        </sections>
        <sections>
            <word_count>903</word_count>
            <figure_citations>Figure 3b and Figure 3c).</figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>31</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1479</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Personalising the TV Experience using Augmented Reality: An Exploratory Study on Delivering Synchronised Sign Language Interpretation</title>
        <abstract>Augmented Reality (AR) technology has the potential to extend the screen area beyond the rigid frames of televisions. The additional display area can be used to augment televisions (TVs) with extra information tailored to individuals, for instance, the provision of access services like sign language interpretations. We invited 23 (11 in the UK, 12 in Germany) users of signed content to evaluate three methods of watching a sign language interpreted programme - one traditional in-vision method with signed programme content on TV and two AR-enabled methods in which an AR sign language interpreter (a 'half-body' version and a 'full-body' version) is projected just outside the frame of the TV presenting the programme. In the UK, participants were split 3-ways in their preferences while in Germany, half the participants preferred the traditional method followed closely by the 'half-body' version. We discuss our participants reasoning behind their preferences and implications for future research.</abstract>
        <keywords>
            <li>Accessibility</li>
            <li>Augmented Reality</li>
            <li>BSL</li>
            <li>SSE</li>
            <li>Companion Screen</li>
            <li>Connected Experiences</li>
            <li>DGS</li>
            <li>HbbTV 2.0</li>
            <li>HoloLens</li>
            <li>Interaction Techniques</li>
            <li>Personalisation</li>
            <li>Second Screen</li>
            <li>Sign Language</li>
            <li>Synchronisation</li>
            <li>Television</li>
        </keywords>
        <authors>
            <li>Vinoba Vinayagamoorthy</li>
            <li> Maxine Glancy</li>
            <li> Christoph Ziegler</li>
            <li> Richard Schäffer</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Personalising the TV Experience using Augmented_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Personalising the TV Experience using Augmented_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Personalising the TV Experience using Augmented_crop_3.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation </filename><data>
        <paper_id>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation </paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025600</doi>
        <sections>
            <word_count>387</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>555</word_count>
            <figure_citations>Figure 1a shows an example.Figure 1b illustrates how our system implements the physicality of the cube, i.Figure 2 illustrates the idea in more detail.Figure 3, our system stimulates up to four different muscle groups.Figure 4 illustrates the naïve approach to rendering objects using EMS: (a) From the moment the user’s fingertips reach the virtual wall, we actuate the user’s hand just strongly enough to prevent it from passing through.Figure 22 in the Implementation section), which we control via USB from within our VR simulators.</figure_citations>
            <section_index>1</section_index>
            <title>ELECTRICAL MUSCLE STIMULATION HAPTICS FOR VR</title>
        </sections>
        <sections>
            <word_count>2065</word_count>
            <figure_citations>Figure 1, which is designed to suggest an increasingly solid inside under a soft, permeable surface.Figure 5 shows the same concept wrapped in visuals suggesting a magnetic field, suggesting a magnetic force that carefully pushes the user’s hand backwards.Figure 6 shows what we call electro visuals.Figure 6).Figure 7 shows the five “walls” arranged in a pentagon with the participant inside.Figure 8, eight participants picked the repulsion wall as their favorite.Figure 9 shows how participants rated the five conditions with respect to the question “what I feel matches what I see.Figure 10 shows participants’ assessment of “this wall was able to prevent me from passing through”.Figure 11, measured using our optical tracking system Optitrack 17w).</figure_citations>
            <section_index>2</section_index>
            <title>DESIGN</title>
        </sections>
        <sections>
            <word_count>215</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>BENEFITS AND CONTRIBUTION</title>
        </sections>
        <sections>
            <word_count>1969</word_count>
            <figure_citations>Figure 12, this room is designed as a jail cell with an “electrified” gate and walls.Figure 15, users operate a pair of traditional sliders in order to align the pipeline elements that establish a hydraulic link.Figure 15).Figure 16, their hand is pushed backwards by the water’s viscosity.Figure 18), they feel a resistance when their hands come together as to grasp the cube.Figure 18, we see how users pick up the cube and throw it over a glass wall, down a chute, which activates a second button.Figure 19 shows a third cube that rests on a slide, which leads up to the last of the three buttons.Figure 20, the EMS condition received substantially higher ratings.Figure 21).</figure_citations>
            <section_index>4</section_index>
            <title>EXAMPLE WIDGETS</title>
        </sections>
        <sections>
            <word_count>363</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>696</word_count>
            <figure_citations>Figure 23 depicts the exact electrode placement we used in our prototypes to actuate the user’s arm and hand.</figure_citations>
            <section_index>6</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>161</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>1256</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation</title>
        <abstract>We explore how to add haptics to walls and other heavy objects in virtual reality. When a user tries to push such an object, our system actuates the user's shoulder, arm, and wrist muscles by means of electrical muscle stimulation, creating a counter force that pulls the user's arm backwards. Our device accomplishes this in a wearable form factor.In our first user study, participants wearing a head-mounted display interacted with objects provided with different types of EMS effects. The repulsion design (visualized as an electrical field) and the soft design (visualized as a magnetic field) received high scores on "prevented me from passing through" as well as "realistic".In a second study, we demonstrate the effectiveness of our approach by letting participants explore a virtual world in which all objects provide haptic EMS effects, including walls, gates, sliders, boxes, and projectiles.</abstract>
        <keywords>
            <li>Muscle interfaces</li>
            <li>Virtual reality</li>
            <li>EMS</li>
            <li>Force feedback</li>
        </keywords>
        <authors>
            <li>Pedro Lopes</li>
            <li> Sijing You</li>
            <li> Lung-Pan Cheng</li>
            <li> Sebastian Marwecki</li>
            <li> Patrick Baudisch</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_7.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_13.png</url>
        </figure>
        <figure>
            <id>14</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_14.png</url>
        </figure>
        <figure>
            <id>15</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_15.png</url>
        </figure>
        <figure>
            <id>16</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_16.png</url>
        </figure>
        <figure>
            <id>17</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_17.png</url>
        </figure>
        <figure>
            <id>18</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_18.png</url>
        </figure>
        <figure>
            <id>20</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_20.png</url>
        </figure>
        <figure>
            <id>21</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_21.png</url>
        </figure>
        <figure>
            <id>22</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_22.png</url>
        </figure>
        <figure>
            <id>23</id>
            <url>Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation _crop_23.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives</filename><data>
        <paper_id>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300673</doi>
        <sections>
            <word_count>543</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>4243</word_count>
            <figure_citations>Figure 1).Figure 2) wearing the Oculus Rift and holding one of three controllers (see Conditions).Figure 3) to successively find and grab letter cubes from the bookshelf and place them (by releasing with the tool) in the placeholders marked with the corresponding letter.Figure 3).Figure 4c shows the physical apparatus for this condition.Figure 4b).Figure 5a).Figure 5a.Figure 6).Figure 7).</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1242</word_count>
            <figure_citations>Figure 9 shows the distribution of the indices split by interaction technique, state and phase.Figure 10 shows the distributions of the GEQ scores by technique, state and phase.</figure_citations>
            <section_index>2</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>738</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>GENERAL DISCUSSION</title>
        </sections>
        <sections>
            <word_count>83</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>42</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1224</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives</title>
        <abstract>Virtual reality (VR) strives to replicate the sensation of the physical environment by mimicking people's perceptions and experience of being elsewhere. These experiences are of-ten mediated by the objects and tools we interact with in the virtual world (e.g., a controller). Evidence from psychology posits that when using the tool proficiently, it becomes em-bodied (i.e., an extension of one's body). There is little work,however, on how to measure this phenomenon in VR, andon how different types of tools and controllers can affect the experience of interaction. In this work, we leverage cognitive psychology and philosophy literature to construct the Locus-of-Attention Index (LAI), a measure of tool embodiment. We designed and conducted a study that measures readiness-to-hand and unreadiness-to-hand for three VR interaction techniques: hands, a physical tool, and a VR controller. The study shows that LAI can measure differences in embodiment with working and broken tools and that using the hand directly results in more embodiment than using controllers.</abstract>
        <keywords>
            <li>Embodiment</li>
            <li>Tool Embodiment</li>
            <li>Embodied Interaction</li>
            <li>Virtual Reality</li>
            <li>Ready-to-hand</li>
            <li>Unready-to-hand</li>
            <li>Tools</li>
        </keywords>
        <authors>
            <li>Ayman Alzayat</li>
            <li> Mark Hancock</li>
            <li> Miguel A. Nacenta</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives_crop_10.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality</filename><data>
        <paper_id>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025688</doi>
        <sections>
            <word_count>923</word_count>
            <figure_citations>Figure 1 demonstrates drawing thick dots.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>732</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>508</word_count>
            <figure_citations>Figure 2), or the user interactively models the 3D object of interest.Figure 2 (left) the system recognizes the face and automatically registers a deformable face mesh.Figure 2 (middle), we retarget the trajectory of the make-up brush by registering the face mesh along with its previously generated 2D texture to the user’s face.Figure 2 (right), we abstract the motion by using one arrow for each segment of the trajectory.Figure 2 (right), resulting in a presentation which uses arrows along the outline of the trajectory (see Figure 7).</figure_citations>
            <section_index>2</section_index>
            <title>OVERVIEW</title>
        </sections>
        <sections>
            <word_count>1223</word_count>
            <figure_citations>Figure 2, a deformable model can be tracked by determining an update to the deformation parameters in each frame.Figure 3).Figure 3).Figure 3(b)).</figure_citations>
            <section_index>3</section_index>
            <title>EXTRACTION</title>
        </sections>
        <sections>
            <word_count>608</word_count>
            <figure_citations>Figure 2 were originally applied to a jewelry box, but later retargeted to a teapot.Figure 4(c).</figure_citations>
            <section_index>4</section_index>
            <title>EDITING</title>
        </sections>
        <sections>
            <word_count>458</word_count>
            <figure_citations>Figure 2, middle).Figure 6(b)), followed by an additional segmentation of the paths into smaller segments.Figure 6(c) uses a threshold of 90◦ ).Figure 6(c)).</figure_citations>
            <section_index>5</section_index>
            <title>VISUALIZATION</title>
        </sections>
        <sections>
            <word_count>1539</word_count>
            <figure_citations>Figure 4 illustrates the painting tutorial.Figure 5 illustrates the video tutorial.Figure 5(a)).Figure 5(b).Figure 6(d)).Figure 6(d)).</figure_citations>
            <section_index>6</section_index>
            <title>EVALUATING THE AUTHORING</title>
        </sections>
        <sections>
            <word_count>1534</word_count>
            <figure_citations>Figure 7(c)).Figure 7(a)).Figure 7(a)).Figure 7).Figure 7).Figure 7(b)).Figure 7(b)).Figure 8 shows the boxplots of the measurements.</figure_citations>
            <section_index>7</section_index>
            <title>EVALUATING EFFICIENCY OF AR KANJI TUTORIAL</title>
        </sections>
        <sections>
            <word_count>75</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>SUS</title>
        </sections>
        <sections>
            <word_count>796</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>DISCUSSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>36</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1419</word_count>
            <figure_citations></figure_citations>
            <section_index>11</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality</title>
        <abstract>A video tutorial effectively conveys complex motions, but may be hard to follow precisely because of its restriction to a predetermined viewpoint. Augmented reality (AR) tutorials have been demonstrated to be more effective. We bring the advantages of both together by interactively retargeting conventional, two-dimensional videos into three-dimensional AR tutorials. Unlike previous work, we do not simply overlay video, but synthesize 3D-registered motion from the video. Since the information in the resulting AR tutorial is registered to 3D objects, the user can freely change the viewpoint without degrading the experience. This approach applies to many styles of video tutorials. In this work, we concentrate on a class of tutorials which alter the surface of an object.</abstract>
        <keywords>
            <li>Augmented reality</li>
            <li>Virtual reality</li>
            <li>Retargeting</li>
            <li>Video tutorial</li>
        </keywords>
        <authors>
            <li>Peter Mohr</li>
            <li> David Mandl</li>
            <li> Markus Tatzgern</li>
            <li> Eduardo Veas</li>
            <li> Dieter Schmalstieg</li>
            <li> Denis Kalkofen</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality_crop_8.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users</filename><data>
        <paper_id>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025683</doi>
        <sections>
            <word_count>829</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>339</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1111</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>ENVISIONED SCENARIO</title>
        </sections>
        <sections>
            <word_count>786</word_count>
            <figure_citations>Figure 2).</figure_citations>
            <section_index>3</section_index>
            <title>ONLINE SURVEY</title>
        </sections>
        <sections>
            <word_count>980</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>SHAREVR CONCEPT AND IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>1745</word_count>
            <figure_citations>Figure 4).</figure_citations>
            <section_index>5</section_index>
            <title>IMPLEMENTED EXPERIENCES</title>
        </sections>
        <sections>
            <word_count>1434</word_count>
            <figure_citations>Figure 7 summarizes the collected data of the GEQ and SUS Experiences with Virtual Reality CHI 2017, May 6–11, 2017, Denver, CO, USA Figure 7.Figure 8 shows an overview of the final comparison (enjoyment, presence and social interaction).</figure_citations>
            <section_index>6</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1338</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>DESIGN SPACE AND GUIDELINES</title>
        </sections>
        <sections>
            <word_count>269</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>22</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>2065</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users</title>
        <abstract>Virtual reality (VR) head-mounted displays (HMD) allow for a highly immersive experience and are currently becoming part of the living room entertainment. Current VR systems focus mainly on increasing the immersion and enjoyment for the user wearing the HMD (HMD user), resulting in all the bystanders (Non-HMD users) being excluded from the experience. We propose ShareVR, a proof-of-concept prototype using floor projection and mobile displays in combination with positional tracking to visualize the virtual world for the Non-HMD user, enabling them to interact with the HMD user and become part of the VR experience. We designed and implemented ShareVR based on the insights of an initial online survey (n=48) with early adopters of VR HMDs. We ran a user study (n=16) comparing ShareVRto a baseline condition showing how the interaction using ShareVR led to an increase of enjoyment, presence and social interaction. In a last step we implemented several experiences for ShareVR, exploring its design space and giving insights for designers of co-located asymmetric VR experiences.</abstract>
        <keywords>
            <li>Co-located virtual reality</li>
            <li>ShareVR</li>
            <li>Asymmetric virtual reality</li>
            <li>Multi-user virtual reality</li>
            <li>Consumer virtual reality</li>
        </keywords>
        <authors>
            <li>Jan Gugenheimer</li>
            <li> Evgeny Stemasov</li>
            <li> Julian Frommel</li>
            <li> Enrico Rukzio</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>ShareVR- Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users_crop_8.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction</filename><data>
        <paper_id>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300301</doi>
        <sections>
            <word_count>706</word_count>
            <figure_citations>Figure 1), a novel hand-held haptic controller for VR that has a rigid shape and no moving parts, making it a suitable candidate for reliable mass manufacturing.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1103</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1995</word_count>
            <figure_citations>Figure 2, we created a controller form factor that allows the user to move the thumb freely in a plane on a pad, parallel to the rests for the two fingers.Figure 2a, we designed the device to be held with thumb and two fingers of the right hand with the remaining fingers gripping the handle.Figure 2b), we mounted force sensors (Honey-well FSS1500) and voice coil actuators (VCA, Dayton Audio DAEX9-4SM).Figure 3 depicts the schematic of the desk-fixed prototype.Figure 5).Figure 6 shows the distribution of scores for each combination of locations of VCAs.Figure 7), which has a considerably lighter form factor.Figure 8).</figure_citations>
            <section_index>2</section_index>
            <title>TORC CONTROLLER</title>
        </sections>
        <sections>
            <word_count>1069</word_count>
            <figure_citations>Figure 9 (left)).Figure 9 (middle & right)).Figure 10).Figure 11).Figure 12).</figure_citations>
            <section_index>3</section_index>
            <title>TORC INTERACTION SCENARIOS</title>
        </sections>
        <sections>
            <word_count>1228</word_count>
            <figure_citations>Figure 13): Locatinд (Figure 13a–c) and Rotatinд (Figure 13d–f).Figure 13a).Figure 13b).Figure 13c), she presses the trigger in the left hand.Figure 13d).Figure 13e shows the angular difference of the key and the key hole.Figure 13f).Figure 13a–f).Figure 14), we found that people preferred TORC over VIVE (Q26).Figure 15 (middle) shows the distance error from VIVE and TORC in the Locatinд task.Figure 15 (right) shows the angular error (calculated as intrinsic geodesic distance between q0 and q1.Figure 15 (left)).</figure_citations>
            <section_index>4</section_index>
            <title>USER STUDY</title>
        </sections>
        <sections>
            <word_count>1008</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>115</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>11</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>2047</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>TORC: A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction</title>
        <abstract>Recent hand-held controllers have explored a variety of haptic feedback sensations for users in virtual reality by producing both kinesthetic and cutaneous feedback from virtual objects. These controllers are grounded to the user's hand and can only manipulate objects through arm and wrist motions, not using the dexterity of their fingers as they would in real life. In this paper, we present TORC, a rigid haptic controller that renders virtual object characteristics and behaviors such as texture and compliance. Users hold and squeeze TORC using their thumb and two fingers and interact with virtual objects by sliding their thumb on TORC's trackpad. During the interaction, vibrotactile motors produce sensations to each finger that represent the haptic feel of squeezing, shearing or turning an object. Our evaluation showed that using TORC, participants could manipulate virtual objects more precisely (e.g., position and rotate objects in 3D) than when using a conventional VR controller.</abstract>
        <keywords>
            <li>Haptics</li>
            <li>VR object manipulation</li>
            <li>Haptic texture</li>
            <li>Haptic compliance</li>
        </keywords>
        <authors>
            <li>Jaeyeon Lee</li>
            <li> Mike Sinclair</li>
            <li> Mar Gonzalez-Franco</li>
            <li> Eyal Ofek</li>
            <li> Christian Holz</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_10.png</url>
        </figure>
        <figure>
            <id>11</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_11.png</url>
        </figure>
        <figure>
            <id>12</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_12.png</url>
        </figure>
        <figure>
            <id>13</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_13.png</url>
        </figure>
        <figure>
            <id>14</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_14.png</url>
        </figure>
        <figure>
            <id>15</id>
            <url>TORC A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction_crop_15.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality</filename><data>
        <paper_id>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300243</doi>
        <sections>
            <word_count>501</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>723</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>BACKGROUND AND RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1046</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>FORMATIVE STUDY</title>
        </sections>
        <sections>
            <word_count>1273</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>DESIGN SPACE</title>
        </sections>
        <sections>
            <word_count>2384</word_count>
            <figure_citations>Figure 1).Figure 2).Figure 3, described in TapSense [25] and in the modeswitching study by Surale et al.Figure 3 (b-c)).Figure 4 (a)).Figure 4(b-c)) follow selection, and can be performed simultaneously.Figure 5) (O1, O3, O5, O6, D9).Figure 5).Figure 6 (a)).Figure 6 (b)), and while navigating, the scene quickly fades to black, except for the tablet and viewport.Figure 6 (c)) and query into the mic (O4).</figure_citations>
            <section_index>4</section_index>
            <title>EXAMPLE INTERACTION VOCABULARY</title>
        </sections>
        <sections>
            <word_count>1555</word_count>
            <figure_citations>Figure 7 (a).Figure 7 (b-g)).Figure 7 (h-k)).</figure_citations>
            <section_index>5</section_index>
            <title>USER EVALUATION</title>
        </sections>
        <sections>
            <word_count>154</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>31</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>2070</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>TabletInVR: Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality</title>
        <abstract>Complex virtual reality (VR) tasks, like 3D solid modelling, are challenging with standard input controllers. We propose exploiting the affordances and input capabilities when using a 3D-tracked multi-touch tablet in an immersive VR environment. Observations gained during semi-structured interviews with general users, and those experienced with 3D software, are used to define a set of design dimensions and guidelines. These are used to develop a vocabulary of interaction techniques to demonstrate how a tablet's precise touch input capability, physical shape, metaphorical associations, and natural compatibility with barehand mid-air input can be used in VR. For example, transforming objects with touch input, "cutting" objects by using the tablet as a physical "knife", navigating in 3D by using the tablet as a viewport, and triggering commands by interleaving bare-hand input around the tablet. Key aspects of the vocabulary are evaluated with users, with results validating the approach.</abstract>
        <keywords>
            <li>Interaction techniques</li>
            <li>Virtual reality</li>
            <li>Touch interaction</li>
        </keywords>
        <authors>
            <li>Hemant Bhaskar Surale</li>
            <li> Aakar Gupta</li>
            <li> Mark Hancock</li>
            <li> Daniel Vogel</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>TabletInVR Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Teaching Language and Culture with a Virtual Reality Game</filename><data>
        <paper_id>Teaching Language and Culture with a Virtual Reality Game</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025857</doi>
        <sections>
            <word_count>554</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>942</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>576</word_count>
            <figure_citations>Figure 1).Figure 2).</figure_citations>
            <section_index>2</section_index>
            <title>CRYSTALLIZE IN VIRTUAL REALITY</title>
        </sections>
        <sections>
            <word_count>1164</word_count>
            <figure_citations>Figure 3), and (2) conversation dialogue, in which the player engages a single NPC in conversation.Figure 4) and, in some cases, asked to construct a sentence given several vocabulary words (Figure 5).</figure_citations>
            <section_index>3</section_index>
            <title>TEACHING PLAYERS TO BOW</title>
        </sections>
        <sections>
            <word_count>571</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>FORMATIVE USER STUDY</title>
        </sections>
        <sections>
            <word_count>1399</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>274</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>1018</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Teaching Language and Culture with a Virtual Reality Game</title>
        <abstract>Many people want to learn a language but find it difficult to stay engaged. Ideally, we would have language learning tools that can make language learning more enjoyable by simulating immersion in a foreign language environment. Therefore, we adapted Crystallize, a 3D video game for learning Japanese, so that it can be played in virtual reality with the Oculus Rift. Specifically, we explored whether we could leverage virtual reality technology to teach embodied cultural interaction, such as bowing in Japanese greetings. To evaluate the impact of our virtual reality game designs, we conducted a formative user study with 68 participants. We present results showing that the virtual reality design trained players how and when to bow, and that it increased participants' sense of involvement in Japanese culture. Our results suggest that virtual reality technology provides an opportunity to leverage culturally-relevant physical interaction, which can enhance the design of language learning technology and virtual reality games.</abstract>
        <keywords>
            <li>Language learning</li>
            <li>Video games</li>
            <li>Virtual reality</li>
        </keywords>
        <authors>
            <li>Alan Cheng</li>
            <li> Lei Yang</li>
            <li> Erik Andersen</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Teaching Language and Culture with a Virtual Reality Game_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Teaching Language and Culture with a Virtual Reality Game_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Teaching Language and Culture with a Virtual Reality Game_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Teaching Language and Culture with a Virtual Reality Game_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Teaching Language and Culture with a Virtual Reality Game_crop_5.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Teaching Language and Culture with a Virtual Reality Game_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>Teaching Language and Culture with a Virtual Reality Game_crop_8.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality</filename><data>
        <paper_id>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025581</doi>
        <sections>
            <word_count>678</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1604</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>2080</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>METHODOLOGY</title>
        </sections>
        <sections>
            <word_count>2806</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>655</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>LIMITATIONS</title>
        </sections>
        <sections>
            <word_count>310</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>FUTURE DIRECTIONS</title>
        </sections>
        <sections>
            <word_count>283</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>450</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>203</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>399</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>The Geometry of Storytelling: Theatrical Use of Space for 360-degree Videos and Virtual Reality</title>
        <abstract>360-degree filming and head-mounted displays (HMDs) give recorded media a new sense of space. Theatre practitioners' expertise in manipulating spatial interactions has much to contribute to immersive recorded content. Four theatre directors led teams of three actors to stage the same scene for both immersive theatre and for 360-degree filming. Each team was recorded performing the scene at least six times, three in each condition, to extract actors' coordinates. This study establishes how to quantify theatre practitioners' use of spatial interactions and examines the spatial adaptations made when transferring these relationships to 360-degree filming.Staging for a 360-degree camera compared to staging for an audience member had shorter distances from the camera and between performers, along with fewer instances of the camera being in the middle of the action. Across all groups, interpersonal distance between characters and between the audience/camera dropped at the end of the scene when the characters come together as a team, suggesting that elements of Proxemics may be applicable to narrative performance.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Narrative</li>
            <li>Theatre</li>
            <li>Performance</li>
            <li>Cinematic VR</li>
            <li>Interpersonal Space</li>
            <li>Head-Mounted Display</li>
            <li>Workﬂow</li>
            <li>360-degree video</li>
        </keywords>
        <authors>
            <li>Vanessa C. Pope</li>
            <li> Robert Dawes</li>
            <li> Florian Schweiger</li>
            <li> Alia Sheikh</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>The Geometry of Storytelling- Theatrical Use of Space for 360-degree Videos and Virtual Reality_crop_9.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World </filename><data>
        <paper_id>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World </paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025955</doi>
        <sections>
            <word_count>402</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>625</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>PHYSICAL WORLDS</title>
        </sections>
        <sections>
            <word_count>2824</word_count>
            <figure_citations>Figure 1).Figure 2).Figure 3), which allows visualizing both spatial and embodied resources as well as the time-based unfolding of the experience.</figure_citations>
            <section_index>2</section_index>
            <title>PERSPECTIVE</title>
        </sections>
        <sections>
            <word_count>2021</word_count>
            <figure_citations>Figure 6), the two paradigms may affect the unfolding social relationships and the instances for participation and coconstruction of meaning.</figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1265</word_count>
            <figure_citations>Figure 7), trying to catch him, etc.</figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>238</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>25</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1444</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>The World-as-Support: Embodied Exploration, Understanding and Meaning-Making of the Augmented World</title>
        <abstract>Current technical capabilities of mobile technologies are consolidating the interest in developing context-aware Augmented/Mixed Reality applications. Most of these applications are designed based on the Window-on-the-World (WoW) interaction paradigm. A significant decrease in cost of projection technology and advances in pico-sized projectors have spurred applications of Projective Augmented Reality. This research has focused mainly on technological development. However, there is still a need to fully understand its communicational and expressive potential. Hence, we define a conceptual paradigm that we call World-as-Support (WaS). We compare the WaS and WoW paradigms by contrasting their assumptions and cultural values, as well as through a study of an application aimed at supporting the collaborative improvisation of site-specific narratives by children. Our analysis of children's understanding of the physical and social environment and of their imaginative play allowed us to identify the affordances, strengths and weaknesses of these two paradigms.</abstract>
        <keywords>
            <li>Mixed Reality</li>
            <li>Augmented Reality</li>
            <li>Embodied interaction</li>
            <li>Embodied cognition</li>
            <li>Meaning making</li>
            <li>Window-on-theWorld</li>
            <li>World-as-Support</li>
        </keywords>
        <authors>
            <li>Laura Malinverni</li>
            <li> Julian Maya</li>
            <li> Marie-Monique Schaper</li>
            <li> Narcis Pares</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>The World-as-Support- Embodied Exploration, Understanding and Meaning-Making of the Augmented World _crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays</filename><data>
        <paper_id>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025824</doi>
        <sections>
            <word_count>448</word_count>
            <figure_citations>Figure 1(a)-(c) demonstrates ThermoVR being used to provide thermal cues for a virtual reality game and applications.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>350</word_count>
            <figure_citations>Figure 1(c).Figure 2 (a).Figure 2 (b).Figure 2 (c)) are used for the closed loop control.Figure 2 (a)).</figure_citations>
            <section_index>1</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>847</word_count>
            <figure_citations>Figure 1 (b-4) as the visual that is related to the hot stimulation and, Figure 1 (b-2) for the cold.Figure 4 (a)) and the HMD displayed the user interface developed using Unity3 depicted in Figure 4 (b).</figure_citations>
            <section_index>2</section_index>
            <title>EVALUATIONS</title>
        </sections>
        <sections>
            <word_count>354</word_count>
            <figure_citations>Figure 5(a) shows the overall accuracy for the perception of thermal directional cues.Figure 5(a)).Figure 5(b)(c) indicates the perception accuracy for each cue in hot and cold conditions.Figure 5(d)(e) shows the confusion matrix that depicts which cues were misidentified.Figure 5(f), the study results of visual involvement, Q1, reported 4.</figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>560</word_count>
            <figure_citations>Figure 5(d)), all the cues on the forehead (Left Up, Up, Right Up) were heavily misidentified with each other and the Front cue.Figure 5(e)) indicated that the Right Up cue was the most difficult to understand as it was confused as Up, Front, Right, Left Down cues.Figure 5(f)), participants rated visual involvement and thermal aspects for no-stimulation, hot and cold stimulations conditions.</figure_citations>
            <section_index>4</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>260</word_count>
            <figure_citations>Figure 1(a)) is a first-person point of view game.Figure 1(b)) we designed for enhancing virtual reality experience with thermal haptic feedback.</figure_citations>
            <section_index>5</section_index>
            <title>APPLICATIONS</title>
        </sections>
        <sections>
            <word_count>194</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>50</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>547</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>ThermoVR: Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays</title>
        <abstract>Head Mounted Displays (HMDs) provide a promising opportunity for providing haptic feedback on the head for an enhanced immersive experience. In ThermoVR, we integrated five thermal feedback modules on the HMD to provide thermal feedback directly onto the user's face. We conducted evaluations with 15 participants using two approaches: Firstly, we provided simultaneously actuated thermal stimulations (hot and cold) as directional cues and evaluated the accuracy of recognition; secondly, we evaluated the overall immersive thermal experience that the users experience when provided with thermal feedback on the face. Results indicated that the recognition accuracy for cold stimuli were of approx. 89.5% accuracy while the accuracy for hot stimuli were 68.6%. Also, participants reported that they felt a higher level of immersion on the face when all modules were simultaneously stimulated (hot and cold). The presented applications demonstrate the ThermoVR's directional cueing and immersive experience.</abstract>
        <keywords>
            <li>Thermal display</li>
            <li>Thermal haptics</li>
            <li>Head mounted display</li>
        </keywords>
        <authors>
            <li>Roshan Lalintha Peiris</li>
            <li> Wei Peng</li>
            <li> Zikun Chen</li>
            <li> Liwei Chan</li>
            <li> Kouta Minamizawa</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>ThermoVR- Exploring Integrated Thermal Haptic Feedback with Head Mounted Displays_crop_5.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Using Presence Questionnaires in Virtual Reality</filename><data>
        <paper_id>Using Presence Questionnaires in Virtual Reality</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300590</doi>
        <sections>
            <word_count>601</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1329</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1024</word_count>
            <figure_citations>Figure 1).Figure 1).Figure 1).Figure 2).</figure_citations>
            <section_index>2</section_index>
            <title>METHOD</title>
        </sections>
        <sections>
            <word_count>1841</word_count>
            <figure_citations>Figure 4 shows the Gaussian distribution curve fits of those differences.</figure_citations>
            <section_index>3</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>29</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>REAL</title>
        </sections>
        <sections>
            <word_count>6</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>HAPTC</title>
        </sections>
        <sections>
            <word_count>163</word_count>
            <figure_citations>Figure 6, F (2, 203) = 150.</figure_citations>
            <section_index>6</section_index>
            <title>IFQUAL</title>
        </sections>
        <sections>
            <word_count>1278</word_count>
            <figure_citations>Figure 6).</figure_citations>
            <section_index>7</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>238</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>35</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>1446</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Using Presence Questionnaires in Virtual Reality</title>
        <abstract>Virtual Reality (VR) is gaining increasing importance in science, education, and entertainment. A fundamental characteristic of VR is creating presence, the experience of 'being' or 'acting', when physically situated in another place. Measuring presence is vital for VR research and development. It is typically repeatedly assessed through questionnaires completed after leaving a VR scene. Requiring participants to leave and re-enter the VR costs time and can cause disorientation. In this paper, we investigate the effect of completing presence questionnaires directly in VR. Thirty-six participants experienced two immersion levels and filled three standardized presence questionnaires in the real world or VR. We found no effect on the questionnaires' mean scores; however, we found that the variance of those measures significantly depends on the realism of the virtual scene and if the subjects had left the VR. The results indicate that, besides reducing a study's duration and reducing disorientation, completing questionnaires in VR does not change the measured presence but can increase the consistency of the variance.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Presence</li>
            <li>Questionnaire</li>
            <li>Evaluation</li>
        </keywords>
        <authors>
            <li>Valentin Schwind</li>
            <li> Pascal Knierim</li>
            <li> Nico Haas</li>
            <li> Niels Henze</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Using Presence Questionnaires in Virtual Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Using Presence Questionnaires in Virtual Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Using Presence Questionnaires in Virtual Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Using Presence Questionnaires in Virtual Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Using Presence Questionnaires in Virtual Reality_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Using Presence Questionnaires in Virtual Reality_crop_6.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>VRRRRoom- Virtual Reality for Radiologists in the Reading Room</filename><data>
        <paper_id>VRRRRoom- Virtual Reality for Radiologists in the Reading Room</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025566</doi>
        <sections>
            <word_count>620</word_count>
            <figure_citations>Figure 1A illustrates a typical radiology reading room.Figure 1B.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>433</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>601</word_count>
            <figure_citations>Figure 2).Figure 2 on the left.Figure 2 on the right).</figure_citations>
            <section_index>2</section_index>
            <title>INTERACTION DESIGN</title>
        </sections>
        <sections>
            <word_count>603</word_count>
            <figure_citations>Figure 4 displays the graphical elements of our prototype that the user can interact with.</figure_citations>
            <section_index>3</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>708</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>199</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>49</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>884</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>VRRRRoom: Virtual Reality for Radiologists in the Reading Room</title>
        <abstract>Reading room conditions such as illumination, ambient light, human factors and display luminance, play an important role on how radiologists analyze and interpret images. Indeed, serious diagnostic errors can appear when observing images through everyday monitors. Typically, these occur whenever professionals are ill-positioned with respect to the display or visualize images under improper light and luminance conditions. In this work, we show that virtual reality can assist radiodiagnostics by considerably diminishing or cancel out the effects of unsuitable ambient conditions. Our approach combines immersive head-mounted displays with interactive surfaces to support professional radiologists in analyzing medical images and formulating diagnostics. We evaluated our prototype with two senior medical doctors and four seasoned radiology fellows. Results indicate that our approach constitutes a viable, flexible, portable and cost-efficient option to traditional radiology reading rooms.</abstract>
        <keywords>
            <li>Virtual Reality</li>
            <li>Multitouch Surfaces</li>
            <li>Medical Visualization</li>
            <li>Interaction Design</li>
        </keywords>
        <authors>
            <li>Maurício Sousa</li>
            <li> Daniel Mendes</li>
            <li> Soraia Paulo</li>
            <li> Nuno Matela</li>
            <li> Joaquim Jorge</li>
            <li> Daniel Simões Lopes</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>VRRRRoom- Virtual Reality for Radiologists in the Reading Room_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>VRRRRoom- Virtual Reality for Radiologists in the Reading Room_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>VRRRRoom- Virtual Reality for Radiologists in the Reading Room_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>VRRRRoom- Virtual Reality for Radiologists in the Reading Room_crop_4.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>VaiR- Simulating 3D Airflows in Virtual Reality</filename><data>
        <paper_id>VaiR- Simulating 3D Airflows in Virtual Reality</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3026009</doi>
        <sections>
            <word_count>925</word_count>
            <figure_citations>Figure 1, can easily be combined with current head mounted displays.</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>1517</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1325</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>IMPLEMENTATION</title>
        </sections>
        <sections>
            <word_count>2004</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>EVALUATION</title>
        </sections>
        <sections>
            <word_count>590</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>691</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>VaiR: Simulating 3D Airflows in Virtual Reality</title>
        <abstract>The integration of multi-sensory stimuli, e.g. haptic airflow, in virtual reality (VR) has become an important topic of VR research and proved to enhance the feeling of presence. VaiR focuses on an accurate and realistic airflow simulation that goes far beyond wind. While previous works on the topic of airflow in VR are restricted to wind, while focusing on the feeling of presence, there is to the best of our knowledge no work considering the conceptual background or on the various application areas. Our pneumatic prototype emits short and long term flows with a minimum delay and is able to animate wind sources in 3D space around the user's head. To get insights on how airflow can be used in VR and how such a device should be designed, we arranged focus groups and discussed the topic. Based on the gathered knowledge, we developed a prototype which proved to increase presence, as well as enjoyment and realism, while not disturbing the VR experience.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Airﬂow</li>
            <li>Presence</li>
            <li>Evaluation</li>
        </keywords>
        <authors>
            <li>Michael Rietzler</li>
            <li> Katrin Plaumann</li>
            <li> Taras Kränzle</li>
            <li> Marcel Erath</li>
            <li> Alexander Stahl</li>
            <li> Enrico Rukzio</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_7.png</url>
        </figure>
        <figure>
            <id>8</id>
            <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_8.png</url>
        </figure>
        <figure>
            <id>9</id>
            <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_9.png</url>
        </figure>
        <figure>
            <id>10</id>
            <url>VaiR- Simulating 3D Airflows in Virtual Reality_crop_10.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>Vremiere- In-Headset Virtual Reality Video Editing</filename><data>
        <paper_id>Vremiere- In-Headset Virtual Reality Video Editing</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025675</doi>
        <sections>
            <word_count>618</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>234</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>1364</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>VR PROFESSIONAL INTERVIEWS</title>
        </sections>
        <sections>
            <word_count>2961</word_count>
            <figure_citations>Figure 2 presents the main timeline interface for our system, which is overlaid on a spherical video view.Figure 2D.Figure 2D.Figure 2D.Figure 2C "Graphics" track), and bookmarks can be placed on the video as well (Figure 2D.Figure 2B).Figure 2 D.Figure 2 D.Figure 3).Figure 3, the user quickly maps her current view in the video with the view in the visualization; she can also spot the jumping lady behind her that she might have not noticed before.Figure 4).Figure 5B).Figure 5C, the user has aligned the ski lift before the cut to the skier after the clip, rather than transitioning from the ski lift to an empty view.Figure 5B).Figure 5B, if the toggle is off, the user can change the rotation of each video individually; otherwise, rotating the blue (ski_1) video will also rotate the red video (ski_2) by the same amount.Figure 6 left).Figure 6, the editor can easily position the logo image on the parachute (Figure 6 right), which is difficult on the desktop due to distortion (Figure 6 left).Figure 7, Figure 2 D.</figure_citations>
            <section_index>3</section_index>
            <title>THE VREMIERE SYSTEM</title>
        </sections>
        <sections>
            <word_count>1430</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>EXPERT REVIEW</title>
        </sections>
        <sections>
            <word_count>716</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>185</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>66</word_count>
            <figure_citations>Figure 1, 4 and 5 use images from YouTube users TOYO TIRES JAPAN and Ábaco Digital Zaragoza under a Creative Commons license.Figure 6 and 7 use images with permission from Youtube users P J Orravan and Jacob Phillips.</figure_citations>
            <section_index>7</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>877</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>Vremiere: In-Headset Virtual Reality Video Editing</title>
        <abstract>Creative professionals are creating Virtual Reality (VR) experiences today by capturing spherical videos, but video editing is still done primarily in traditional 2D desktop GUI applications such as Premiere. These interfaces provide limited capabilities for previewing content in a VR headset or for directly manipulating the spherical video in an intuitive way. As a result, editors must alternate between editing on the desktop and previewing in the headset, which is tedious and interrupts the creative process. We demonstrate an application that enables a user to directly edit spherical video while fully immersed in a VR headset. We first interviewed professional VR filmmakers to understand current practice and derived a suitable workflow for in-headset VR video editing. We then developed a prototype system implementing this new workflow. Our system is built upon a familiar timeline design, but is enhanced with custom widgets to enable intuitive editing of spherical video inside the headset. We conducted an expert review study and found that with our prototype, experts were able to edit videos entirely within the headset. Experts also found our interface and widgets useful, providing intuitive controls for their editing needs.</abstract>
        <keywords>
            <li>Virtual reality</li>
            <li>Video editing</li>
        </keywords>
        <authors>
            <li>Cuong Nguyen</li>
            <li> Stephen DiVerdi</li>
            <li> Aaron Hertzmann</li>
            <li> Feng Liu</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_4.png</url>
        </figure>
        <figure>
            <id>5</id>
            <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_5.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_6.png</url>
        </figure>
        <figure>
            <id>7</id>
            <url>Vremiere- In-Headset Virtual Reality Video Editing_crop_7.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality</filename><data>
        <paper_id>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality</paper_id>
        <venue>CHI 17</venue>
        <doi>10.1145/3025453.3025852</doi>
        <sections>
            <word_count>334</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>280</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>704</word_count>
            <figure_citations>Figure 1 (top, right) shows a conceptual app using the WatchThru screen to display static and animated notifications.Figure 2 shows a prototypical app with a 2D map on the main screen and an oriented 3D arrow on the WatchThru screen.Figure 3 (c, d) show a smart home scenario where objects in a room, e.Figure 3 (e), a learning scenario, a virtual globe is presented in the middle of a room.Figure 3 (f) shows an assembly scenario; WatchThru shows the user how to connect electronic components.</figure_citations>
            <section_index>2</section_index>
            <title>WATCHTHRU INTERACTIONS</title>
        </sections>
        <sections>
            <word_count>601</word_count>
            <figure_citations>Figure 1, bottom).Figure 3 a,b).</figure_citations>
            <section_index>3</section_index>
            <title>IMPLEMENTATION OF PROTOTYPES</title>
        </sections>
        <sections>
            <word_count>648</word_count>
            <figure_citations>Figure 4) such that the WatchThru screen could be pulled out (manually or automatically) when needed.</figure_citations>
            <section_index>4</section_index>
            <title>LIMITATIONS AND FUTURE WORK</title>
        </sections>
        <sections>
            <word_count>208</word_count>
            <figure_citations></figure_citations>
            <section_index>5</section_index>
            <title>CONCLUSIONS</title>
        </sections>
        <sections>
            <word_count>85</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>760</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>WatchThru: Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality</title>
        <abstract>We introduce WatchThru, an interactive method for extended wrist-worn display on commercially-available smartwatches. To address the limited visual and interaction space, WatchThru expands the device into 3D through a transparent display. This enables novel interactions that leverage and extend smartwatch glanceability. We describe three novel interaction techniques, Pop-up Visuals, Second Perspective and Peek-through, and discuss how they can complement interaction on current devices. We also describe two types of prototypes that helped us to explore standalone interactions, as well as, proof-of-concept AR interfaces using our platform.</abstract>
        <keywords>
            <li>Smartwatches</li>
            <li>Micro-Interaction</li>
            <li>Wearable Devices</li>
        </keywords>
        <authors>
            <li>Dirk Wenig</li>
            <li> Johannes Schöning</li>
            <li> Alex Olwal</li>
            <li> Mathias Oben</li>
            <li> Rainer Malaka</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>WatchThru- Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality_crop_4.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics</filename><data>
        <paper_id>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300774</doi>
        <sections>
            <word_count>995</word_count>
            <figure_citations></figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>290</word_count>
            <figure_citations></figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>668</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>SYSTEM DESIGN</title>
        </sections>
        <sections>
            <word_count>397</word_count>
            <figure_citations></figure_citations>
            <section_index>3</section_index>
            <title>RESEARCH QUESTIONS AND STUDY DESIGN</title>
        </sections>
        <sections>
            <word_count>380</word_count>
            <figure_citations></figure_citations>
            <section_index>4</section_index>
            <title>METHODS</title>
        </sections>
        <sections>
            <word_count>665</word_count>
            <figure_citations>Figure 3 question 5), electricity and movement (e.Figure 3 question 6), electricity and magnetic field (e.Figure 3 question 3) The concept of sequential reasoning indicates the style in which participants answered the open-ended question of “How is electrical energy turned into sound inside the speaker?” A large number of responses included a narrative which explained the connection between different components as a sequence (Figure 3 q1 top) rather than directly explaining the core physics phenomena driving the speaker.</figure_citations>
            <section_index>5</section_index>
            <title>AR</title>
        </sections>
        <sections>
            <word_count>1323</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>RESULTS</title>
        </sections>
        <sections>
            <word_count>1452</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>60</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>ACKNOWLEDGEMENTS</title>
        </sections>
        <sections>
            <word_count>1185</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>What Can We Learn from Augmented Reality (AR)?: Benefits and Drawbacks of AR for Inquiry-based Learning of Physics</title>
        <abstract>Emerging technologies such as Augmented Reality (AR), have the potential to radically transform education by making challenging concepts visible and accessible to novices. In this project, we have designed a Hololens-based system in which collaborators are exposed to an unstructured learning activity in which they learned about the invisible physics involved in audio speakers. They learned topics ranging from spatial knowledge, such as shape of magnetic fields, to abstract conceptual knowledge, such as relationships between electricity and magnetism. We compared participants' learning, attitudes and collaboration with a tangible interface through multiple experimental conditions containing varying layers of AR information. We found that educational AR representations were beneficial for learning specific knowledge and increasing participants' self-efficacy (i.e., their ability to learn concepts in physics). However, we also found that participants in conditions that did not contain AR educational content, learned some concepts better than other groups and became more curious about physics. We discuss learning and collaboration differences, as well as benefits and detriments of implementing augmented reality for unstructured learning activities.</abstract>
        <keywords>
            <li>Augmented Reality</li>
            <li>Physics Education</li>
            <li>Collaborative Learning</li>
        </keywords>
        <authors>
            <li>Iulian Radu</li>
            <li> Bertrand Schneider</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics_crop_4.png</url>
        </figure>
        <figure>
            <id>6</id>
            <url>What Can We Learn from Augmented Reality (AR) Benefits and Drawbacks of AR for Inquiry-based Learning of Physics_crop_6.png</url>
        </figure>
    </figures>
</article>
<article>
    <filename>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom</filename><data>
        <paper_id>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom</paper_id>
        <venue>CHI 19</venue>
        <doi>10.1145/3290605.3300464</doi>
        <sections>
            <word_count>667</word_count>
            <figure_citations>Figure 1).</figure_citations>
            <section_index>0</section_index>
            <title>INTRODUCTION</title>
        </sections>
        <sections>
            <word_count>752</word_count>
            <figure_citations>Figure 1(b)) as an interaction mechanism alongside the projection mapping system to create a hybrid user interface.Figure 1(c)).Figure 2(c)), which was then projected on the human model.</figure_citations>
            <section_index>1</section_index>
            <title>RELATED WORK</title>
        </sections>
        <sections>
            <word_count>216</word_count>
            <figure_citations></figure_citations>
            <section_index>2</section_index>
            <title>RESEARCH DESIGN</title>
        </sections>
        <sections>
            <word_count>1143</word_count>
            <figure_citations>Figure 1(a)).Figure 1(b)).Figure 2(a)).Figure 2(c)).Figure 2(c)).</figure_citations>
            <section_index>3</section_index>
            <title>INTERFACE</title>
        </sections>
        <sections>
            <word_count>1699</word_count>
            <figure_citations>Figure 2(c)).</figure_citations>
            <section_index>4</section_index>
            <title>AUGMENTED BODY SYSTEM</title>
        </sections>
        <sections>
            <word_count>750</word_count>
            <figure_citations>Figure 2(b)) and used retroreflective markers with Velcro stickers to put on the volunteer student’s clothing (Figure 1(a)).Figure 3(a)), thus enabling the teacher and students to draw directly on the volunteer’s body and to select muscle(s) or bone(s).Figure 3(b)) which the teacher used to select brush size, color, eraser, create groups of muscles/bones, and to selectively turn on/off the muscle and skeleton view as well as remove specific muscles to reveal the inner layers (Figure 4(a)) using the developed pen.Figure 4(b)).</figure_citations>
            <section_index>5</section_index>
            <title>CUSTOM PEN INTERFACE</title>
        </sections>
        <sections>
            <word_count>2277</word_count>
            <figure_citations></figure_citations>
            <section_index>6</section_index>
            <title>OUTCOMES OF AUGMENTED BODY SYSTEM</title>
        </sections>
        <sections>
            <word_count>1020</word_count>
            <figure_citations></figure_citations>
            <section_index>7</section_index>
            <title>DISCUSSION</title>
        </sections>
        <sections>
            <word_count>144</word_count>
            <figure_citations></figure_citations>
            <section_index>8</section_index>
            <title>CONCLUSION</title>
        </sections>
        <sections>
            <word_count>66</word_count>
            <figure_citations></figure_citations>
            <section_index>9</section_index>
            <title>ACKNOWLEDGMENTS</title>
        </sections>
        <sections>
            <word_count>753</word_count>
            <figure_citations></figure_citations>
            <section_index>10</section_index>
            <title>REFERENCES</title>
        </sections>
        <title>"What's Happening at that Hip?": Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom</title>
        <abstract>We present two studies to discuss the design, usability analysis, and educational outcome resulting from our system Augmented Body in physiotherapy classroom. We build on prior user-centric design work that investigates existing teaching methods and discuss opportunities for intervention. We present the design and implementation of a hybrid system for physiotherapy education combining an on-body projection based virtual anatomy supplemented by pen-based tablets to create real-time annotations. We conducted a usability evaluation of this system, comparing with projection only and traditional teaching conditions. Finally, we focus on a comparative study to evaluate learning outcome among students in actual classroom settings. Our studies showed increased usage of visual representation techniques in students'11 note taking behavior and statistically significant improvement in some learning aspects. We discuss challenges for designing augmented reality systems for education, including minimizing attention split, addressing text-entry issues, and digital annotations on a moving physical body.</abstract>
        <keywords>
            <li>Augmented reality</li>
            <li>Pen-based interactions</li>
            <li>Annotation</li>
            <li>Projection mapping</li>
            <li>Educational system</li>
        </keywords>
        <authors>
            <li>Hasan Shahid Ferdous</li>
            <li>Thuong Hoang</li>
            <li> Zaher Joukhadar</li>
            <li> Martin N. Reinoso</li>
            <li> Frank Vetere</li>
            <li> David Kelly</li>
            <li> Louisa Remedios</li>
        </authors>
    </data>
    <figures>
        <figure>
            <id>1</id>
            <url>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom_crop_1.png</url>
        </figure>
        <figure>
            <id>2</id>
            <url>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom_crop_2.png</url>
        </figure>
        <figure>
            <id>3</id>
            <url>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom_crop_3.png</url>
        </figure>
        <figure>
            <id>4</id>
            <url>What's Happening at that Hip Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom_crop_4.png</url>
        </figure>
    </figures>
</article>
