{
    "fullpath": "/Users/takyeonlee/Documents/work/_\u1109\u116e\u110b\u1165\u11b8_2021/ID503_Design_Project_1/_PaperVis/papers/chi2020/3313831.3376481.pdf",
    "paper_id": "3313831.3376481",
    "venue": "chi2020",
    "keywords": [
        "Haptics",
        "Robot arm",
        "Immersive environments",
        "Virtual reality",
        "User study",
        "Perception",
        "Presence",
        "Emotion"
    ],
    "doi": "10.1145/3313831.3376481",
    "paragraph_containing_keyword": "Author Keywords \nHaptics; robot arm; immersive environments; virtual reality; \nuser study; perception; presence; emotion",
    "paragraph_after_keyword": "CCS Concepts \n\u2022Human-centered  computing  \u2192  Human  computer  inter-\naction (HCI); Haptic devices; User studies; \nINTRODUCTION AND MOTIVATION \nOver the last decade, Virtual Reality (VR) systems have been \nmassively improved, in particular driven by the gaming indus-\ntry but increasingly also other industry sectors. Predominantly, \nadvances have been made in providing affordable yet high \nquality visual displays.  However, non-visual cues can be a \nkey factor in immersive systems, for example to improve over-\nall simulation and perceptual \ufb01delity [36] or to invoke emo-\ntional reactions [14, 27]. While rendering audio cues is well \nsupported, haptic feedback is still challenging, and foremost \ntargeted towards the hands [31]. In this paper, we look at how \nhaptic feedback can be provided towards the face rather then \nthe hands.  We explore how this feedback could be of value \nin an immersive environment while wearing a head-mounted \ndisplay (HMD). The reason why we choose the face is that it is \nhighly sensitive to haptic cues and can perceive different kinds \nof haptic feedback well, as other areas of the body are often",
    "sections": [
        {
            "word_count": 1409,
            "figure_citations": {
                "1": [
                    "Figure 1 and Figure 2)."
                ]
            },
            "section_index": 0,
            "title": "INTRODUCTION AND MOTIVATION"
        },
        {
            "word_count": 759,
            "figure_citations": {},
            "section_index": 1,
            "title": "RELATED WORK"
        },
        {
            "word_count": 792,
            "figure_citations": {
                "2": [
                    "Figure 2) consists of a custom-made robot arm attached to a commercial HMD, currently an Oculus Rift CV1."
                ]
            },
            "section_index": 2,
            "title": "FACEHAPTICS SYSTEM"
        },
        {
            "word_count": 5156,
            "figure_citations": {
                "4": [
                    "Figure 4) but from a static location.",
                    "Figure 4) that contained 16 events along a 3 minute pre-de\ufb01ned walkthrough."
                ],
                "3": [
                    "Figure 3 and highest for the condition with oscillating head and static wind (M = 14.",
                    "Figure 3 (middle)): The standard deviation of the signed point error differed signi\ufb01cantly between movement conditions (F(3, 45) = 3.",
                    "Figure 3 (right)."
                ]
            },
            "section_index": 3,
            "title": "USER STUDIES"
        },
        {
            "word_count": 512,
            "figure_citations": {},
            "section_index": 4,
            "title": "TECHNICAL CONSIDERATIONS AND LIMITATIONS"
        },
        {
            "word_count": 734,
            "figure_citations": {
                "5": [
                    "Figure 5), underlining the easy extensibility of the system, but also the need for a reloading mechanism or dispensing system (e."
                ]
            },
            "section_index": 5,
            "title": "CONCLUSION AND FUTURE WORK"
        },
        {
            "word_count": 19,
            "figure_citations": {},
            "section_index": 6,
            "title": "ACKNOWLEDGMENTS"
        },
        {
            "word_count": 2386,
            "figure_citations": {},
            "section_index": 7,
            "title": "REFERENCES"
        }
    ],
    "title": "FaceHaptics: Robot Arm based Versatile Facial Haptics for Immersive Environments",
    "authors": "Alexander Wilberz, Dominik Leschtschow, Christina Trepkowski, Jens Maiero, Ernst Kruijff, Bernhard Riecke",
    "abstract": "This paper introduces FaceHaptics, a novel haptic display based on a robot arm attached to a head-mounted virtual reality display. It provides localized, multi-directional and movable haptic cues in the form of wind, warmth, moving and single-point touch events and water spray to dedicated parts of the face not covered by the head-mounted display.The easily extensible system, however, can principally mount any type of compact haptic actuator or object. User study 1 showed that users appreciate the directional resolution of cues, and can judge wind direction well, especially when they move their head and wind direction is adjusted dynamically to compensate for head rotations. Study 2 showed that adding FaceHaptics cues to a VR walkthrough can significantly improve user experience, presence, and emotional responses.",
    "publication": {
        "venue": "CHI '20",
        "venue_full": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
        "year": "2020",
        "date": "2020/4/21"
    },
    "version": 4
}